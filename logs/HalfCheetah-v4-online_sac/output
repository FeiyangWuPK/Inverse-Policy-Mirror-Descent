Using cpu device
Eval num_timesteps=1000, episode_reward=-2.25 +/- 1.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.25    |
| time/              |          |
|    total_timesteps | 1000     |
---------------------------------
New best mean reward!
Eval num_timesteps=2000, episode_reward=-3.20 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.2     |
| time/              |          |
|    total_timesteps | 2000     |
---------------------------------
Eval num_timesteps=3000, episode_reward=-0.60 +/- 0.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.598   |
| time/              |          |
|    total_timesteps | 3000     |
| train/             |          |
|    actor_loss      | -7.28    |
|    critic_loss     | 3.26     |
|    ent_coef        | 0.978    |
|    ent_coef_loss   | -0.225   |
|    learning_rate   | 0.005    |
|    n_updates       | 10       |
---------------------------------
New best mean reward!
Eval num_timesteps=4000, episode_reward=-0.70 +/- 0.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.698   |
| time/              |          |
|    total_timesteps | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -273     |
| time/              |          |
|    episodes        | 4        |
|    fps             | 560      |
|    time_elapsed    | 7        |
|    total_timesteps | 4000     |
---------------------------------
Eval num_timesteps=5000, episode_reward=-0.74 +/- 0.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.744   |
| time/              |          |
|    total_timesteps | 5000     |
| train/             |          |
|    actor_loss      | -7.66    |
|    critic_loss     | 0.806    |
|    ent_coef        | 0.93     |
|    ent_coef_loss   | -0.732   |
|    learning_rate   | 0.005    |
|    n_updates       | 20       |
---------------------------------
Eval num_timesteps=6000, episode_reward=-1.05 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.05    |
| time/              |          |
|    total_timesteps | 6000     |
---------------------------------
Eval num_timesteps=7000, episode_reward=-0.98 +/- 0.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.979   |
| time/              |          |
|    total_timesteps | 7000     |
| train/             |          |
|    actor_loss      | -7.39    |
|    critic_loss     | 0.495    |
|    ent_coef        | 0.885    |
|    ent_coef_loss   | -1.24    |
|    learning_rate   | 0.00499  |
|    n_updates       | 30       |
---------------------------------
Eval num_timesteps=8000, episode_reward=-0.91 +/- 0.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.908   |
| time/              |          |
|    total_timesteps | 8000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -270     |
| time/              |          |
|    episodes        | 8        |
|    fps             | 595      |
|    time_elapsed    | 13       |
|    total_timesteps | 8000     |
---------------------------------
Eval num_timesteps=9000, episode_reward=-0.87 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.867   |
| time/              |          |
|    total_timesteps | 9000     |
| train/             |          |
|    actor_loss      | -7.22    |
|    critic_loss     | 0.303    |
|    ent_coef        | 0.841    |
|    ent_coef_loss   | -1.74    |
|    learning_rate   | 0.00499  |
|    n_updates       | 40       |
---------------------------------
Eval num_timesteps=10000, episode_reward=-0.52 +/- 0.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.519   |
| time/              |          |
|    total_timesteps | 10000    |
---------------------------------
New best mean reward!
Eval num_timesteps=11000, episode_reward=-0.60 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.596   |
| time/              |          |
|    total_timesteps | 11000    |
| train/             |          |
|    actor_loss      | -7.18    |
|    critic_loss     | 0.241    |
|    ent_coef        | 0.8      |
|    ent_coef_loss   | -2.25    |
|    learning_rate   | 0.00499  |
|    n_updates       | 50       |
---------------------------------
Eval num_timesteps=12000, episode_reward=-0.42 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.416   |
| time/              |          |
|    total_timesteps | 12000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -261     |
| time/              |          |
|    episodes        | 12       |
|    fps             | 588      |
|    time_elapsed    | 20       |
|    total_timesteps | 12000    |
---------------------------------
Eval num_timesteps=13000, episode_reward=-0.11 +/- 0.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.114   |
| time/              |          |
|    total_timesteps | 13000    |
| train/             |          |
|    actor_loss      | -7.02    |
|    critic_loss     | 0.181    |
|    ent_coef        | 0.761    |
|    ent_coef_loss   | -2.75    |
|    learning_rate   | 0.00499  |
|    n_updates       | 60       |
---------------------------------
New best mean reward!
Eval num_timesteps=14000, episode_reward=-0.41 +/- 0.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.411   |
| time/              |          |
|    total_timesteps | 14000    |
---------------------------------
Eval num_timesteps=15000, episode_reward=-0.23 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.228   |
| time/              |          |
|    total_timesteps | 15000    |
| train/             |          |
|    actor_loss      | -6.89    |
|    critic_loss     | 0.154    |
|    ent_coef        | 0.724    |
|    ent_coef_loss   | -3.26    |
|    learning_rate   | 0.00499  |
|    n_updates       | 70       |
---------------------------------
Eval num_timesteps=16000, episode_reward=-0.32 +/- 0.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.316   |
| time/              |          |
|    total_timesteps | 16000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -261     |
| time/              |          |
|    episodes        | 16       |
|    fps             | 593      |
|    time_elapsed    | 26       |
|    total_timesteps | 16000    |
---------------------------------
Eval num_timesteps=17000, episode_reward=-0.21 +/- 0.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.211   |
| time/              |          |
|    total_timesteps | 17000    |
| train/             |          |
|    actor_loss      | -6.72    |
|    critic_loss     | 0.145    |
|    ent_coef        | 0.689    |
|    ent_coef_loss   | -3.77    |
|    learning_rate   | 0.00498  |
|    n_updates       | 80       |
---------------------------------
Eval num_timesteps=18000, episode_reward=-0.43 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.433   |
| time/              |          |
|    total_timesteps | 18000    |
---------------------------------
Eval num_timesteps=19000, episode_reward=-0.49 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.489   |
| time/              |          |
|    total_timesteps | 19000    |
| train/             |          |
|    actor_loss      | -6.54    |
|    critic_loss     | 0.11     |
|    ent_coef        | 0.655    |
|    ent_coef_loss   | -4.26    |
|    learning_rate   | 0.00498  |
|    n_updates       | 90       |
---------------------------------
Eval num_timesteps=20000, episode_reward=-1.04 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.04    |
| time/              |          |
|    total_timesteps | 20000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -278     |
| time/              |          |
|    episodes        | 20       |
|    fps             | 602      |
|    time_elapsed    | 33       |
|    total_timesteps | 20000    |
---------------------------------
Eval num_timesteps=21000, episode_reward=-0.45 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.452   |
| time/              |          |
|    total_timesteps | 21000    |
| train/             |          |
|    actor_loss      | -6.45    |
|    critic_loss     | 0.0952   |
|    ent_coef        | 0.623    |
|    ent_coef_loss   | -4.77    |
|    learning_rate   | 0.00498  |
|    n_updates       | 100      |
---------------------------------
Eval num_timesteps=22000, episode_reward=-0.51 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.508   |
| time/              |          |
|    total_timesteps | 22000    |
---------------------------------
Eval num_timesteps=23000, episode_reward=-0.47 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.47    |
| time/              |          |
|    total_timesteps | 23000    |
| train/             |          |
|    actor_loss      | -6.29    |
|    critic_loss     | 0.0892   |
|    ent_coef        | 0.593    |
|    ent_coef_loss   | -5.26    |
|    learning_rate   | 0.00498  |
|    n_updates       | 110      |
---------------------------------
Eval num_timesteps=24000, episode_reward=-0.05 +/- 0.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.0512  |
| time/              |          |
|    total_timesteps | 24000    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -279     |
| time/              |          |
|    episodes        | 24       |
|    fps             | 608      |
|    time_elapsed    | 39       |
|    total_timesteps | 24000    |
---------------------------------
Eval num_timesteps=25000, episode_reward=-0.36 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.36    |
| time/              |          |
|    total_timesteps | 25000    |
| train/             |          |
|    actor_loss      | -6.19    |
|    critic_loss     | 0.0707   |
|    ent_coef        | 0.564    |
|    ent_coef_loss   | -5.77    |
|    learning_rate   | 0.00498  |
|    n_updates       | 120      |
---------------------------------
Eval num_timesteps=26000, episode_reward=-0.70 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.703   |
| time/              |          |
|    total_timesteps | 26000    |
---------------------------------
Eval num_timesteps=27000, episode_reward=-0.24 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.24    |
| time/              |          |
|    total_timesteps | 27000    |
| train/             |          |
|    actor_loss      | -6.07    |
|    critic_loss     | 0.069    |
|    ent_coef        | 0.537    |
|    ent_coef_loss   | -6.28    |
|    learning_rate   | 0.00497  |
|    n_updates       | 130      |
---------------------------------
Eval num_timesteps=28000, episode_reward=-0.23 +/- 0.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.23    |
| time/              |          |
|    total_timesteps | 28000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -274     |
| time/              |          |
|    episodes        | 28       |
|    fps             | 611      |
|    time_elapsed    | 45       |
|    total_timesteps | 28000    |
---------------------------------
Eval num_timesteps=29000, episode_reward=-0.53 +/- 0.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.527   |
| time/              |          |
|    total_timesteps | 29000    |
| train/             |          |
|    actor_loss      | -5.94    |
|    critic_loss     | 0.0583   |
|    ent_coef        | 0.511    |
|    ent_coef_loss   | -6.77    |
|    learning_rate   | 0.00497  |
|    n_updates       | 140      |
---------------------------------
Eval num_timesteps=30000, episode_reward=-0.25 +/- 0.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.246   |
| time/              |          |
|    total_timesteps | 30000    |
---------------------------------
Eval num_timesteps=31000, episode_reward=-0.46 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.458   |
| time/              |          |
|    total_timesteps | 31000    |
| train/             |          |
|    actor_loss      | -5.85    |
|    critic_loss     | 0.0523   |
|    ent_coef        | 0.486    |
|    ent_coef_loss   | -7.28    |
|    learning_rate   | 0.00497  |
|    n_updates       | 150      |
---------------------------------
Eval num_timesteps=32000, episode_reward=-0.36 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.362   |
| time/              |          |
|    total_timesteps | 32000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -272     |
| time/              |          |
|    episodes        | 32       |
|    fps             | 615      |
|    time_elapsed    | 52       |
|    total_timesteps | 32000    |
---------------------------------
Eval num_timesteps=33000, episode_reward=0.20 +/- 0.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.2      |
| time/              |          |
|    total_timesteps | 33000    |
| train/             |          |
|    actor_loss      | -5.7     |
|    critic_loss     | 0.0561   |
|    ent_coef        | 0.462    |
|    ent_coef_loss   | -7.78    |
|    learning_rate   | 0.00497  |
|    n_updates       | 160      |
---------------------------------
New best mean reward!
Eval num_timesteps=34000, episode_reward=-0.38 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.378   |
| time/              |          |
|    total_timesteps | 34000    |
---------------------------------
Eval num_timesteps=35000, episode_reward=-0.38 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.383   |
| time/              |          |
|    total_timesteps | 35000    |
| train/             |          |
|    actor_loss      | -5.59    |
|    critic_loss     | 0.0462   |
|    ent_coef        | 0.44     |
|    ent_coef_loss   | -8.27    |
|    learning_rate   | 0.00497  |
|    n_updates       | 170      |
---------------------------------
Eval num_timesteps=36000, episode_reward=-0.03 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.0298  |
| time/              |          |
|    total_timesteps | 36000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -278     |
| time/              |          |
|    episodes        | 36       |
|    fps             | 616      |
|    time_elapsed    | 58       |
|    total_timesteps | 36000    |
---------------------------------
Eval num_timesteps=37000, episode_reward=-0.88 +/- 0.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.884   |
| time/              |          |
|    total_timesteps | 37000    |
| train/             |          |
|    actor_loss      | -5.48    |
|    critic_loss     | 0.0397   |
|    ent_coef        | 0.419    |
|    ent_coef_loss   | -8.77    |
|    learning_rate   | 0.00496  |
|    n_updates       | 180      |
---------------------------------
Eval num_timesteps=38000, episode_reward=-0.80 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.803   |
| time/              |          |
|    total_timesteps | 38000    |
---------------------------------
Eval num_timesteps=39000, episode_reward=-0.43 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.429   |
| time/              |          |
|    total_timesteps | 39000    |
| train/             |          |
|    actor_loss      | -5.4     |
|    critic_loss     | 0.0362   |
|    ent_coef        | 0.398    |
|    ent_coef_loss   | -9.27    |
|    learning_rate   | 0.00496  |
|    n_updates       | 190      |
---------------------------------
Eval num_timesteps=40000, episode_reward=-0.25 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.25    |
| time/              |          |
|    total_timesteps | 40000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -285     |
| time/              |          |
|    episodes        | 40       |
|    fps             | 618      |
|    time_elapsed    | 64       |
|    total_timesteps | 40000    |
---------------------------------
Eval num_timesteps=41000, episode_reward=-1.06 +/- 0.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.06    |
| time/              |          |
|    total_timesteps | 41000    |
| train/             |          |
|    actor_loss      | -5.3     |
|    critic_loss     | 0.0367   |
|    ent_coef        | 0.379    |
|    ent_coef_loss   | -9.78    |
|    learning_rate   | 0.00496  |
|    n_updates       | 200      |
---------------------------------
Eval num_timesteps=42000, episode_reward=-1.17 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.17    |
| time/              |          |
|    total_timesteps | 42000    |
---------------------------------
Eval num_timesteps=43000, episode_reward=-0.71 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.709   |
| time/              |          |
|    total_timesteps | 43000    |
---------------------------------
Eval num_timesteps=44000, episode_reward=-1.28 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.28    |
| time/              |          |
|    total_timesteps | 44000    |
| train/             |          |
|    actor_loss      | -5.28    |
|    critic_loss     | 0.0338   |
|    ent_coef        | 0.361    |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.00496  |
|    n_updates       | 210      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -288     |
| time/              |          |
|    episodes        | 44       |
|    fps             | 620      |
|    time_elapsed    | 70       |
|    total_timesteps | 44000    |
---------------------------------
Eval num_timesteps=45000, episode_reward=-1.41 +/- 0.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.41    |
| time/              |          |
|    total_timesteps | 45000    |
---------------------------------
Eval num_timesteps=46000, episode_reward=-0.54 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.537   |
| time/              |          |
|    total_timesteps | 46000    |
| train/             |          |
|    actor_loss      | -5.26    |
|    critic_loss     | 0.0422   |
|    ent_coef        | 0.343    |
|    ent_coef_loss   | -10.7    |
|    learning_rate   | 0.00495  |
|    n_updates       | 220      |
---------------------------------
Eval num_timesteps=47000, episode_reward=-0.74 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.739   |
| time/              |          |
|    total_timesteps | 47000    |
---------------------------------
Eval num_timesteps=48000, episode_reward=0.10 +/- 0.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.0972   |
| time/              |          |
|    total_timesteps | 48000    |
| train/             |          |
|    actor_loss      | -5.02    |
|    critic_loss     | 0.0293   |
|    ent_coef        | 0.327    |
|    ent_coef_loss   | -11.2    |
|    learning_rate   | 0.00495  |
|    n_updates       | 230      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -289     |
| time/              |          |
|    episodes        | 48       |
|    fps             | 621      |
|    time_elapsed    | 77       |
|    total_timesteps | 48000    |
---------------------------------
Eval num_timesteps=49000, episode_reward=-0.21 +/- 0.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.206   |
| time/              |          |
|    total_timesteps | 49000    |
---------------------------------
Eval num_timesteps=50000, episode_reward=-0.65 +/- 0.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.65    |
| time/              |          |
|    total_timesteps | 50000    |
| train/             |          |
|    actor_loss      | -5.02    |
|    critic_loss     | 0.0315   |
|    ent_coef        | 0.311    |
|    ent_coef_loss   | -11.7    |
|    learning_rate   | 0.00495  |
|    n_updates       | 240      |
---------------------------------
Eval num_timesteps=51000, episode_reward=-0.69 +/- 0.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.687   |
| time/              |          |
|    total_timesteps | 51000    |
---------------------------------
Eval num_timesteps=52000, episode_reward=-1.01 +/- 0.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.01    |
| time/              |          |
|    total_timesteps | 52000    |
| train/             |          |
|    actor_loss      | -4.94    |
|    critic_loss     | 0.0262   |
|    ent_coef        | 0.296    |
|    ent_coef_loss   | -12.2    |
|    learning_rate   | 0.00495  |
|    n_updates       | 250      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -287     |
| time/              |          |
|    episodes        | 52       |
|    fps             | 622      |
|    time_elapsed    | 83       |
|    total_timesteps | 52000    |
---------------------------------
Eval num_timesteps=53000, episode_reward=-0.63 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.635   |
| time/              |          |
|    total_timesteps | 53000    |
---------------------------------
Eval num_timesteps=54000, episode_reward=-0.65 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.652   |
| time/              |          |
|    total_timesteps | 54000    |
| train/             |          |
|    actor_loss      | -4.92    |
|    critic_loss     | 0.0257   |
|    ent_coef        | 0.282    |
|    ent_coef_loss   | -12.7    |
|    learning_rate   | 0.00495  |
|    n_updates       | 260      |
---------------------------------
Eval num_timesteps=55000, episode_reward=-1.34 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.34    |
| time/              |          |
|    total_timesteps | 55000    |
---------------------------------
Eval num_timesteps=56000, episode_reward=-0.76 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.761   |
| time/              |          |
|    total_timesteps | 56000    |
| train/             |          |
|    actor_loss      | -4.85    |
|    critic_loss     | 0.0245   |
|    ent_coef        | 0.268    |
|    ent_coef_loss   | -13.2    |
|    learning_rate   | 0.00494  |
|    n_updates       | 270      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -284     |
| time/              |          |
|    episodes        | 56       |
|    fps             | 623      |
|    time_elapsed    | 89       |
|    total_timesteps | 56000    |
---------------------------------
Eval num_timesteps=57000, episode_reward=-0.35 +/- 0.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.35    |
| time/              |          |
|    total_timesteps | 57000    |
---------------------------------
Eval num_timesteps=58000, episode_reward=-0.41 +/- 0.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.411   |
| time/              |          |
|    total_timesteps | 58000    |
| train/             |          |
|    actor_loss      | -4.75    |
|    critic_loss     | 0.0204   |
|    ent_coef        | 0.255    |
|    ent_coef_loss   | -13.7    |
|    learning_rate   | 0.00494  |
|    n_updates       | 280      |
---------------------------------
Eval num_timesteps=59000, episode_reward=-0.99 +/- 0.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.993   |
| time/              |          |
|    total_timesteps | 59000    |
---------------------------------
Eval num_timesteps=60000, episode_reward=-1.62 +/- 0.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.62    |
| time/              |          |
|    total_timesteps | 60000    |
| train/             |          |
|    actor_loss      | -4.79    |
|    critic_loss     | 0.0253   |
|    ent_coef        | 0.243    |
|    ent_coef_loss   | -14.2    |
|    learning_rate   | 0.00494  |
|    n_updates       | 290      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -285     |
| time/              |          |
|    episodes        | 60       |
|    fps             | 624      |
|    time_elapsed    | 96       |
|    total_timesteps | 60000    |
---------------------------------
Eval num_timesteps=61000, episode_reward=-1.24 +/- 1.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.24    |
| time/              |          |
|    total_timesteps | 61000    |
---------------------------------
Eval num_timesteps=62000, episode_reward=-0.71 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.712   |
| time/              |          |
|    total_timesteps | 62000    |
| train/             |          |
|    actor_loss      | -4.63    |
|    critic_loss     | 0.0239   |
|    ent_coef        | 0.231    |
|    ent_coef_loss   | -14.7    |
|    learning_rate   | 0.00494  |
|    n_updates       | 300      |
---------------------------------
Eval num_timesteps=63000, episode_reward=-0.53 +/- 0.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.534   |
| time/              |          |
|    total_timesteps | 63000    |
---------------------------------
Eval num_timesteps=64000, episode_reward=-0.78 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.783   |
| time/              |          |
|    total_timesteps | 64000    |
| train/             |          |
|    actor_loss      | -4.61    |
|    critic_loss     | 0.0217   |
|    ent_coef        | 0.22     |
|    ent_coef_loss   | -15.2    |
|    learning_rate   | 0.00494  |
|    n_updates       | 310      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -283     |
| time/              |          |
|    episodes        | 64       |
|    fps             | 625      |
|    time_elapsed    | 102      |
|    total_timesteps | 64000    |
---------------------------------
Eval num_timesteps=65000, episode_reward=-0.57 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.566   |
| time/              |          |
|    total_timesteps | 65000    |
---------------------------------
Eval num_timesteps=66000, episode_reward=-0.63 +/- 0.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.625   |
| time/              |          |
|    total_timesteps | 66000    |
| train/             |          |
|    actor_loss      | -4.52    |
|    critic_loss     | 0.0179   |
|    ent_coef        | 0.21     |
|    ent_coef_loss   | -15.7    |
|    learning_rate   | 0.00493  |
|    n_updates       | 320      |
---------------------------------
Eval num_timesteps=67000, episode_reward=-0.63 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.628   |
| time/              |          |
|    total_timesteps | 67000    |
---------------------------------
Eval num_timesteps=68000, episode_reward=-1.13 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.13    |
| time/              |          |
|    total_timesteps | 68000    |
| train/             |          |
|    actor_loss      | -4.5     |
|    critic_loss     | 0.019    |
|    ent_coef        | 0.2      |
|    ent_coef_loss   | -16.1    |
|    learning_rate   | 0.00493  |
|    n_updates       | 330      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -283     |
| time/              |          |
|    episodes        | 68       |
|    fps             | 626      |
|    time_elapsed    | 108      |
|    total_timesteps | 68000    |
---------------------------------
Eval num_timesteps=69000, episode_reward=-1.13 +/- 0.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.13    |
| time/              |          |
|    total_timesteps | 69000    |
---------------------------------
Eval num_timesteps=70000, episode_reward=-1.84 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.84    |
| time/              |          |
|    total_timesteps | 70000    |
| train/             |          |
|    actor_loss      | -4.43    |
|    critic_loss     | 0.0194   |
|    ent_coef        | 0.19     |
|    ent_coef_loss   | -16.6    |
|    learning_rate   | 0.00493  |
|    n_updates       | 340      |
---------------------------------
Eval num_timesteps=71000, episode_reward=-1.63 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.63    |
| time/              |          |
|    total_timesteps | 71000    |
---------------------------------
Eval num_timesteps=72000, episode_reward=-1.23 +/- 1.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.23    |
| time/              |          |
|    total_timesteps | 72000    |
| train/             |          |
|    actor_loss      | -4.38    |
|    critic_loss     | 0.0188   |
|    ent_coef        | 0.181    |
|    ent_coef_loss   | -17      |
|    learning_rate   | 0.00493  |
|    n_updates       | 350      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -284     |
| time/              |          |
|    episodes        | 72       |
|    fps             | 627      |
|    time_elapsed    | 114      |
|    total_timesteps | 72000    |
---------------------------------
Eval num_timesteps=73000, episode_reward=-1.82 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.82    |
| time/              |          |
|    total_timesteps | 73000    |
---------------------------------
Eval num_timesteps=74000, episode_reward=-1.53 +/- 0.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.53    |
| time/              |          |
|    total_timesteps | 74000    |
| train/             |          |
|    actor_loss      | -4.39    |
|    critic_loss     | 0.0176   |
|    ent_coef        | 0.172    |
|    ent_coef_loss   | -17.6    |
|    learning_rate   | 0.00493  |
|    n_updates       | 360      |
---------------------------------
Eval num_timesteps=75000, episode_reward=-2.06 +/- 0.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.06    |
| time/              |          |
|    total_timesteps | 75000    |
---------------------------------
Eval num_timesteps=76000, episode_reward=-1.44 +/- 0.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.44    |
| time/              |          |
|    total_timesteps | 76000    |
| train/             |          |
|    actor_loss      | -4.34    |
|    critic_loss     | 0.0178   |
|    ent_coef        | 0.164    |
|    ent_coef_loss   | -18      |
|    learning_rate   | 0.00492  |
|    n_updates       | 370      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -281     |
| time/              |          |
|    episodes        | 76       |
|    fps             | 626      |
|    time_elapsed    | 121      |
|    total_timesteps | 76000    |
---------------------------------
Eval num_timesteps=77000, episode_reward=-1.60 +/- 0.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.6     |
| time/              |          |
|    total_timesteps | 77000    |
---------------------------------
Eval num_timesteps=78000, episode_reward=-2.21 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.21    |
| time/              |          |
|    total_timesteps | 78000    |
| train/             |          |
|    actor_loss      | -4.27    |
|    critic_loss     | 0.0159   |
|    ent_coef        | 0.156    |
|    ent_coef_loss   | -18.5    |
|    learning_rate   | 0.00492  |
|    n_updates       | 380      |
---------------------------------
Eval num_timesteps=79000, episode_reward=-1.98 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.98    |
| time/              |          |
|    total_timesteps | 79000    |
---------------------------------
Eval num_timesteps=80000, episode_reward=-2.59 +/- 0.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.59    |
| time/              |          |
|    total_timesteps | 80000    |
| train/             |          |
|    actor_loss      | -4.27    |
|    critic_loss     | 0.0163   |
|    ent_coef        | 0.149    |
|    ent_coef_loss   | -18.9    |
|    learning_rate   | 0.00492  |
|    n_updates       | 390      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -279     |
| time/              |          |
|    episodes        | 80       |
|    fps             | 627      |
|    time_elapsed    | 127      |
|    total_timesteps | 80000    |
---------------------------------
Eval num_timesteps=81000, episode_reward=-3.00 +/- 0.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3       |
| time/              |          |
|    total_timesteps | 81000    |
---------------------------------
Eval num_timesteps=82000, episode_reward=-2.56 +/- 0.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.56    |
| time/              |          |
|    total_timesteps | 82000    |
| train/             |          |
|    actor_loss      | -4.23    |
|    critic_loss     | 0.0175   |
|    ent_coef        | 0.142    |
|    ent_coef_loss   | -19.4    |
|    learning_rate   | 0.00492  |
|    n_updates       | 400      |
---------------------------------
Eval num_timesteps=83000, episode_reward=-2.16 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.16    |
| time/              |          |
|    total_timesteps | 83000    |
---------------------------------
Eval num_timesteps=84000, episode_reward=-2.96 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.96    |
| time/              |          |
|    total_timesteps | 84000    |
| train/             |          |
|    actor_loss      | -4.16    |
|    critic_loss     | 0.0169   |
|    ent_coef        | 0.135    |
|    ent_coef_loss   | -19.8    |
|    learning_rate   | 0.00492  |
|    n_updates       | 410      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -276     |
| time/              |          |
|    episodes        | 84       |
|    fps             | 627      |
|    time_elapsed    | 133      |
|    total_timesteps | 84000    |
---------------------------------
Eval num_timesteps=85000, episode_reward=-2.30 +/- 0.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.3     |
| time/              |          |
|    total_timesteps | 85000    |
---------------------------------
Eval num_timesteps=86000, episode_reward=-2.25 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.25    |
| time/              |          |
|    total_timesteps | 86000    |
---------------------------------
Eval num_timesteps=87000, episode_reward=-1.39 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.39    |
| time/              |          |
|    total_timesteps | 87000    |
| train/             |          |
|    actor_loss      | -4.15    |
|    critic_loss     | 0.0146   |
|    ent_coef        | 0.129    |
|    ent_coef_loss   | -20.4    |
|    learning_rate   | 0.00491  |
|    n_updates       | 420      |
---------------------------------
Eval num_timesteps=88000, episode_reward=-2.36 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.36    |
| time/              |          |
|    total_timesteps | 88000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -276     |
| time/              |          |
|    episodes        | 88       |
|    fps             | 628      |
|    time_elapsed    | 140      |
|    total_timesteps | 88000    |
---------------------------------
Eval num_timesteps=89000, episode_reward=-2.16 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.16    |
| time/              |          |
|    total_timesteps | 89000    |
| train/             |          |
|    actor_loss      | -4.11    |
|    critic_loss     | 0.0165   |
|    ent_coef        | 0.122    |
|    ent_coef_loss   | -20.8    |
|    learning_rate   | 0.00491  |
|    n_updates       | 430      |
---------------------------------
Eval num_timesteps=90000, episode_reward=-3.16 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.16    |
| time/              |          |
|    total_timesteps | 90000    |
---------------------------------
Eval num_timesteps=91000, episode_reward=-2.08 +/- 0.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.08    |
| time/              |          |
|    total_timesteps | 91000    |
| train/             |          |
|    actor_loss      | -4.11    |
|    critic_loss     | 0.018    |
|    ent_coef        | 0.117    |
|    ent_coef_loss   | -21.2    |
|    learning_rate   | 0.00491  |
|    n_updates       | 440      |
---------------------------------
Eval num_timesteps=92000, episode_reward=-2.54 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.54    |
| time/              |          |
|    total_timesteps | 92000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -274     |
| time/              |          |
|    episodes        | 92       |
|    fps             | 628      |
|    time_elapsed    | 146      |
|    total_timesteps | 92000    |
---------------------------------
Eval num_timesteps=93000, episode_reward=-2.96 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.96    |
| time/              |          |
|    total_timesteps | 93000    |
| train/             |          |
|    actor_loss      | -4.07    |
|    critic_loss     | 0.0158   |
|    ent_coef        | 0.111    |
|    ent_coef_loss   | -21.8    |
|    learning_rate   | 0.00491  |
|    n_updates       | 450      |
---------------------------------
Eval num_timesteps=94000, episode_reward=-2.76 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.76    |
| time/              |          |
|    total_timesteps | 94000    |
---------------------------------
Eval num_timesteps=95000, episode_reward=-2.90 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.9     |
| time/              |          |
|    total_timesteps | 95000    |
| train/             |          |
|    actor_loss      | -4.05    |
|    critic_loss     | 0.0161   |
|    ent_coef        | 0.106    |
|    ent_coef_loss   | -22.3    |
|    learning_rate   | 0.00491  |
|    n_updates       | 460      |
---------------------------------
Eval num_timesteps=96000, episode_reward=-2.68 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.68    |
| time/              |          |
|    total_timesteps | 96000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -272     |
| time/              |          |
|    episodes        | 96       |
|    fps             | 629      |
|    time_elapsed    | 152      |
|    total_timesteps | 96000    |
---------------------------------
Eval num_timesteps=97000, episode_reward=-1.22 +/- 0.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.22    |
| time/              |          |
|    total_timesteps | 97000    |
| train/             |          |
|    actor_loss      | -4.03    |
|    critic_loss     | 0.0163   |
|    ent_coef        | 0.101    |
|    ent_coef_loss   | -22.7    |
|    learning_rate   | 0.0049   |
|    n_updates       | 470      |
---------------------------------
Eval num_timesteps=98000, episode_reward=-1.72 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.72    |
| time/              |          |
|    total_timesteps | 98000    |
---------------------------------
Eval num_timesteps=99000, episode_reward=-1.47 +/- 0.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.47    |
| time/              |          |
|    total_timesteps | 99000    |
| train/             |          |
|    actor_loss      | -4.02    |
|    critic_loss     | 0.0167   |
|    ent_coef        | 0.0961   |
|    ent_coef_loss   | -23.1    |
|    learning_rate   | 0.0049   |
|    n_updates       | 480      |
---------------------------------
Eval num_timesteps=100000, episode_reward=-1.59 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.59    |
| time/              |          |
|    total_timesteps | 100000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -272     |
| time/              |          |
|    episodes        | 100      |
|    fps             | 629      |
|    time_elapsed    | 158      |
|    total_timesteps | 100000   |
---------------------------------
Eval num_timesteps=101000, episode_reward=-1.79 +/- 1.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.79    |
| time/              |          |
|    total_timesteps | 101000   |
| train/             |          |
|    actor_loss      | -3.98    |
|    critic_loss     | 0.0168   |
|    ent_coef        | 0.0916   |
|    ent_coef_loss   | -23.4    |
|    learning_rate   | 0.0049   |
|    n_updates       | 490      |
---------------------------------
Eval num_timesteps=102000, episode_reward=-2.26 +/- 0.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.26    |
| time/              |          |
|    total_timesteps | 102000   |
---------------------------------
Eval num_timesteps=103000, episode_reward=-2.22 +/- 0.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.22    |
| time/              |          |
|    total_timesteps | 103000   |
| train/             |          |
|    actor_loss      | -3.95    |
|    critic_loss     | 0.0161   |
|    ent_coef        | 0.0873   |
|    ent_coef_loss   | -23.9    |
|    learning_rate   | 0.0049   |
|    n_updates       | 500      |
---------------------------------
Eval num_timesteps=104000, episode_reward=-3.15 +/- 0.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.15    |
| time/              |          |
|    total_timesteps | 104000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -272     |
| time/              |          |
|    episodes        | 104      |
|    fps             | 630      |
|    time_elapsed    | 165      |
|    total_timesteps | 104000   |
---------------------------------
Eval num_timesteps=105000, episode_reward=-3.18 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.18    |
| time/              |          |
|    total_timesteps | 105000   |
| train/             |          |
|    actor_loss      | -3.92    |
|    critic_loss     | 0.017    |
|    ent_coef        | 0.0832   |
|    ent_coef_loss   | -24.3    |
|    learning_rate   | 0.0049   |
|    n_updates       | 510      |
---------------------------------
Eval num_timesteps=106000, episode_reward=-2.80 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -2.8     |
| time/              |          |
|    total_timesteps | 106000   |
---------------------------------
Eval num_timesteps=107000, episode_reward=-3.95 +/- 0.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.95    |
| time/              |          |
|    total_timesteps | 107000   |
| train/             |          |
|    actor_loss      | -3.91    |
|    critic_loss     | 0.0165   |
|    ent_coef        | 0.0793   |
|    ent_coef_loss   | -24.8    |
|    learning_rate   | 0.00489  |
|    n_updates       | 520      |
---------------------------------
Eval num_timesteps=108000, episode_reward=-4.20 +/- 0.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.2     |
| time/              |          |
|    total_timesteps | 108000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -273     |
| time/              |          |
|    episodes        | 108      |
|    fps             | 630      |
|    time_elapsed    | 171      |
|    total_timesteps | 108000   |
---------------------------------
Eval num_timesteps=109000, episode_reward=-5.74 +/- 0.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.74    |
| time/              |          |
|    total_timesteps | 109000   |
| train/             |          |
|    actor_loss      | -3.88    |
|    critic_loss     | 0.0146   |
|    ent_coef        | 0.0756   |
|    ent_coef_loss   | -25.3    |
|    learning_rate   | 0.00489  |
|    n_updates       | 530      |
---------------------------------
Eval num_timesteps=110000, episode_reward=-5.57 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.57    |
| time/              |          |
|    total_timesteps | 110000   |
---------------------------------
Eval num_timesteps=111000, episode_reward=-5.54 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.54    |
| time/              |          |
|    total_timesteps | 111000   |
| train/             |          |
|    actor_loss      | -3.87    |
|    critic_loss     | 0.0165   |
|    ent_coef        | 0.0721   |
|    ent_coef_loss   | -25.5    |
|    learning_rate   | 0.00489  |
|    n_updates       | 540      |
---------------------------------
Eval num_timesteps=112000, episode_reward=-5.80 +/- 0.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.8     |
| time/              |          |
|    total_timesteps | 112000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -274     |
| time/              |          |
|    episodes        | 112      |
|    fps             | 630      |
|    time_elapsed    | 177      |
|    total_timesteps | 112000   |
---------------------------------
Eval num_timesteps=113000, episode_reward=-7.19 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.19    |
| time/              |          |
|    total_timesteps | 113000   |
| train/             |          |
|    actor_loss      | -3.86    |
|    critic_loss     | 0.0186   |
|    ent_coef        | 0.0687   |
|    ent_coef_loss   | -25.9    |
|    learning_rate   | 0.00489  |
|    n_updates       | 550      |
---------------------------------
Eval num_timesteps=114000, episode_reward=-7.33 +/- 0.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.33    |
| time/              |          |
|    total_timesteps | 114000   |
---------------------------------
Eval num_timesteps=115000, episode_reward=-5.30 +/- 0.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.3     |
| time/              |          |
|    total_timesteps | 115000   |
| train/             |          |
|    actor_loss      | -3.83    |
|    critic_loss     | 0.0162   |
|    ent_coef        | 0.0655   |
|    ent_coef_loss   | -26.4    |
|    learning_rate   | 0.00489  |
|    n_updates       | 560      |
---------------------------------
Eval num_timesteps=116000, episode_reward=-5.21 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.21    |
| time/              |          |
|    total_timesteps | 116000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -275     |
| time/              |          |
|    episodes        | 116      |
|    fps             | 631      |
|    time_elapsed    | 183      |
|    total_timesteps | 116000   |
---------------------------------
Eval num_timesteps=117000, episode_reward=-5.13 +/- 0.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.13    |
| time/              |          |
|    total_timesteps | 117000   |
| train/             |          |
|    actor_loss      | -3.83    |
|    critic_loss     | 0.0196   |
|    ent_coef        | 0.0625   |
|    ent_coef_loss   | -26.6    |
|    learning_rate   | 0.00488  |
|    n_updates       | 570      |
---------------------------------
Eval num_timesteps=118000, episode_reward=-4.92 +/- 0.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.92    |
| time/              |          |
|    total_timesteps | 118000   |
---------------------------------
Eval num_timesteps=119000, episode_reward=-3.90 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.9     |
| time/              |          |
|    total_timesteps | 119000   |
| train/             |          |
|    actor_loss      | -3.81    |
|    critic_loss     | 0.018    |
|    ent_coef        | 0.0596   |
|    ent_coef_loss   | -27.2    |
|    learning_rate   | 0.00488  |
|    n_updates       | 580      |
---------------------------------
Eval num_timesteps=120000, episode_reward=-3.19 +/- 0.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.19    |
| time/              |          |
|    total_timesteps | 120000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -271     |
| time/              |          |
|    episodes        | 120      |
|    fps             | 631      |
|    time_elapsed    | 190      |
|    total_timesteps | 120000   |
---------------------------------
Eval num_timesteps=121000, episode_reward=-5.09 +/- 0.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.09    |
| time/              |          |
|    total_timesteps | 121000   |
| train/             |          |
|    actor_loss      | -3.79    |
|    critic_loss     | 0.0154   |
|    ent_coef        | 0.0569   |
|    ent_coef_loss   | -27.6    |
|    learning_rate   | 0.00488  |
|    n_updates       | 590      |
---------------------------------
Eval num_timesteps=122000, episode_reward=-4.89 +/- 0.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.89    |
| time/              |          |
|    total_timesteps | 122000   |
---------------------------------
Eval num_timesteps=123000, episode_reward=-9.41 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -9.41    |
| time/              |          |
|    total_timesteps | 123000   |
| train/             |          |
|    actor_loss      | -3.78    |
|    critic_loss     | 0.0162   |
|    ent_coef        | 0.0542   |
|    ent_coef_loss   | -27.9    |
|    learning_rate   | 0.00488  |
|    n_updates       | 600      |
---------------------------------
Eval num_timesteps=124000, episode_reward=-8.44 +/- 0.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.44    |
| time/              |          |
|    total_timesteps | 124000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -268     |
| time/              |          |
|    episodes        | 124      |
|    fps             | 631      |
|    time_elapsed    | 196      |
|    total_timesteps | 124000   |
---------------------------------
Eval num_timesteps=125000, episode_reward=-4.53 +/- 0.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.53    |
| time/              |          |
|    total_timesteps | 125000   |
| train/             |          |
|    actor_loss      | -3.75    |
|    critic_loss     | 0.0163   |
|    ent_coef        | 0.0517   |
|    ent_coef_loss   | -28.2    |
|    learning_rate   | 0.00488  |
|    n_updates       | 610      |
---------------------------------
Eval num_timesteps=126000, episode_reward=-4.97 +/- 0.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.97    |
| time/              |          |
|    total_timesteps | 126000   |
---------------------------------
Eval num_timesteps=127000, episode_reward=-8.21 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.21    |
| time/              |          |
|    total_timesteps | 127000   |
| train/             |          |
|    actor_loss      | -3.9     |
|    critic_loss     | 0.0375   |
|    ent_coef        | 0.0494   |
|    ent_coef_loss   | -27.7    |
|    learning_rate   | 0.00487  |
|    n_updates       | 620      |
---------------------------------
Eval num_timesteps=128000, episode_reward=-8.99 +/- 0.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.99    |
| time/              |          |
|    total_timesteps | 128000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -268     |
| time/              |          |
|    episodes        | 128      |
|    fps             | 631      |
|    time_elapsed    | 202      |
|    total_timesteps | 128000   |
---------------------------------
Eval num_timesteps=129000, episode_reward=-8.66 +/- 0.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.66    |
| time/              |          |
|    total_timesteps | 129000   |
---------------------------------
Eval num_timesteps=130000, episode_reward=-8.75 +/- 1.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.75    |
| time/              |          |
|    total_timesteps | 130000   |
| train/             |          |
|    actor_loss      | -3.71    |
|    critic_loss     | 0.022    |
|    ent_coef        | 0.0472   |
|    ent_coef_loss   | -28.7    |
|    learning_rate   | 0.00487  |
|    n_updates       | 630      |
---------------------------------
Eval num_timesteps=131000, episode_reward=-9.52 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -9.52    |
| time/              |          |
|    total_timesteps | 131000   |
---------------------------------
Eval num_timesteps=132000, episode_reward=-3.76 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.76    |
| time/              |          |
|    total_timesteps | 132000   |
| train/             |          |
|    actor_loss      | -3.7     |
|    critic_loss     | 0.0198   |
|    ent_coef        | 0.045    |
|    ent_coef_loss   | -28.6    |
|    learning_rate   | 0.00487  |
|    n_updates       | 640      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -266     |
| time/              |          |
|    episodes        | 132      |
|    fps             | 631      |
|    time_elapsed    | 208      |
|    total_timesteps | 132000   |
---------------------------------
Eval num_timesteps=133000, episode_reward=-4.42 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.42    |
| time/              |          |
|    total_timesteps | 133000   |
---------------------------------
Eval num_timesteps=134000, episode_reward=-4.21 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.21    |
| time/              |          |
|    total_timesteps | 134000   |
| train/             |          |
|    actor_loss      | -3.71    |
|    critic_loss     | 0.0214   |
|    ent_coef        | 0.043    |
|    ent_coef_loss   | -29.1    |
|    learning_rate   | 0.00487  |
|    n_updates       | 650      |
---------------------------------
Eval num_timesteps=135000, episode_reward=-4.93 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.93    |
| time/              |          |
|    total_timesteps | 135000   |
---------------------------------
Eval num_timesteps=136000, episode_reward=-5.03 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.03    |
| time/              |          |
|    total_timesteps | 136000   |
| train/             |          |
|    actor_loss      | -3.69    |
|    critic_loss     | 0.0171   |
|    ent_coef        | 0.0411   |
|    ent_coef_loss   | -30      |
|    learning_rate   | 0.00486  |
|    n_updates       | 660      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -262     |
| time/              |          |
|    episodes        | 136      |
|    fps             | 631      |
|    time_elapsed    | 215      |
|    total_timesteps | 136000   |
---------------------------------
Eval num_timesteps=137000, episode_reward=-4.92 +/- 0.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.92    |
| time/              |          |
|    total_timesteps | 137000   |
---------------------------------
Eval num_timesteps=138000, episode_reward=-6.21 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -6.21    |
| time/              |          |
|    total_timesteps | 138000   |
| train/             |          |
|    actor_loss      | -3.73    |
|    critic_loss     | 0.0264   |
|    ent_coef        | 0.0393   |
|    ent_coef_loss   | -28.6    |
|    learning_rate   | 0.00486  |
|    n_updates       | 670      |
---------------------------------
Eval num_timesteps=139000, episode_reward=-6.06 +/- 0.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -6.06    |
| time/              |          |
|    total_timesteps | 139000   |
---------------------------------
Eval num_timesteps=140000, episode_reward=-7.61 +/- 0.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.61    |
| time/              |          |
|    total_timesteps | 140000   |
| train/             |          |
|    actor_loss      | -3.68    |
|    critic_loss     | 0.0218   |
|    ent_coef        | 0.0376   |
|    ent_coef_loss   | -30      |
|    learning_rate   | 0.00486  |
|    n_updates       | 680      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -255     |
| time/              |          |
|    episodes        | 140      |
|    fps             | 632      |
|    time_elapsed    | 221      |
|    total_timesteps | 140000   |
---------------------------------
Eval num_timesteps=141000, episode_reward=-7.00 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7       |
| time/              |          |
|    total_timesteps | 141000   |
---------------------------------
Eval num_timesteps=142000, episode_reward=-7.44 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.44    |
| time/              |          |
|    total_timesteps | 142000   |
| train/             |          |
|    actor_loss      | -3.69    |
|    critic_loss     | 0.0248   |
|    ent_coef        | 0.0359   |
|    ent_coef_loss   | -28.9    |
|    learning_rate   | 0.00486  |
|    n_updates       | 690      |
---------------------------------
Eval num_timesteps=143000, episode_reward=-7.07 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.07    |
| time/              |          |
|    total_timesteps | 143000   |
---------------------------------
Eval num_timesteps=144000, episode_reward=-4.52 +/- 1.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.52    |
| time/              |          |
|    total_timesteps | 144000   |
| train/             |          |
|    actor_loss      | -3.66    |
|    critic_loss     | 0.0206   |
|    ent_coef        | 0.0344   |
|    ent_coef_loss   | -29.8    |
|    learning_rate   | 0.00486  |
|    n_updates       | 700      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -250     |
| time/              |          |
|    episodes        | 144      |
|    fps             | 632      |
|    time_elapsed    | 227      |
|    total_timesteps | 144000   |
---------------------------------
Eval num_timesteps=145000, episode_reward=-4.22 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.22    |
| time/              |          |
|    total_timesteps | 145000   |
---------------------------------
Eval num_timesteps=146000, episode_reward=-4.13 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.13    |
| time/              |          |
|    total_timesteps | 146000   |
| train/             |          |
|    actor_loss      | -3.64    |
|    critic_loss     | 0.0189   |
|    ent_coef        | 0.0329   |
|    ent_coef_loss   | -31.5    |
|    learning_rate   | 0.00485  |
|    n_updates       | 710      |
---------------------------------
Eval num_timesteps=147000, episode_reward=-3.64 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.64    |
| time/              |          |
|    total_timesteps | 147000   |
---------------------------------
Eval num_timesteps=148000, episode_reward=-4.47 +/- 1.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.47    |
| time/              |          |
|    total_timesteps | 148000   |
| train/             |          |
|    actor_loss      | -3.62    |
|    critic_loss     | 0.0169   |
|    ent_coef        | 0.0314   |
|    ent_coef_loss   | -31.2    |
|    learning_rate   | 0.00485  |
|    n_updates       | 720      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -247     |
| time/              |          |
|    episodes        | 148      |
|    fps             | 632      |
|    time_elapsed    | 234      |
|    total_timesteps | 148000   |
---------------------------------
Eval num_timesteps=149000, episode_reward=-3.44 +/- 0.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -3.44    |
| time/              |          |
|    total_timesteps | 149000   |
---------------------------------
Eval num_timesteps=150000, episode_reward=-75.82 +/- 5.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -75.8    |
| time/              |          |
|    total_timesteps | 150000   |
| train/             |          |
|    actor_loss      | -3.62    |
|    critic_loss     | 0.0215   |
|    ent_coef        | 0.0301   |
|    ent_coef_loss   | -30.6    |
|    learning_rate   | 0.00485  |
|    n_updates       | 730      |
---------------------------------
Eval num_timesteps=151000, episode_reward=-63.72 +/- 25.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -63.7    |
| time/              |          |
|    total_timesteps | 151000   |
---------------------------------
Eval num_timesteps=152000, episode_reward=-27.73 +/- 0.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -27.7    |
| time/              |          |
|    total_timesteps | 152000   |
| train/             |          |
|    actor_loss      | -3.6     |
|    critic_loss     | 0.0209   |
|    ent_coef        | 0.0288   |
|    ent_coef_loss   | -31.2    |
|    learning_rate   | 0.00485  |
|    n_updates       | 740      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -243     |
| time/              |          |
|    episodes        | 152      |
|    fps             | 632      |
|    time_elapsed    | 240      |
|    total_timesteps | 152000   |
---------------------------------
Eval num_timesteps=153000, episode_reward=-28.07 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -28.1    |
| time/              |          |
|    total_timesteps | 153000   |
---------------------------------
Eval num_timesteps=154000, episode_reward=-14.60 +/- 1.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -14.6    |
| time/              |          |
|    total_timesteps | 154000   |
| train/             |          |
|    actor_loss      | -3.67    |
|    critic_loss     | 0.0217   |
|    ent_coef        | 0.0275   |
|    ent_coef_loss   | -32.8    |
|    learning_rate   | 0.00485  |
|    n_updates       | 750      |
---------------------------------
Eval num_timesteps=155000, episode_reward=-15.20 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -15.2    |
| time/              |          |
|    total_timesteps | 155000   |
---------------------------------
Eval num_timesteps=156000, episode_reward=-18.66 +/- 0.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 156000   |
| train/             |          |
|    actor_loss      | -3.64    |
|    critic_loss     | 0.0197   |
|    ent_coef        | 0.0263   |
|    ent_coef_loss   | -31.1    |
|    learning_rate   | 0.00484  |
|    n_updates       | 760      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -243     |
| time/              |          |
|    episodes        | 156      |
|    fps             | 632      |
|    time_elapsed    | 246      |
|    total_timesteps | 156000   |
---------------------------------
Eval num_timesteps=157000, episode_reward=-18.98 +/- 1.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 157000   |
---------------------------------
Eval num_timesteps=158000, episode_reward=-22.32 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.3    |
| time/              |          |
|    total_timesteps | 158000   |
| train/             |          |
|    actor_loss      | -3.62    |
|    critic_loss     | 0.0186   |
|    ent_coef        | 0.0252   |
|    ent_coef_loss   | -32      |
|    learning_rate   | 0.00484  |
|    n_updates       | 770      |
---------------------------------
Eval num_timesteps=159000, episode_reward=-22.24 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.2    |
| time/              |          |
|    total_timesteps | 159000   |
---------------------------------
Eval num_timesteps=160000, episode_reward=-12.95 +/- 0.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -12.9    |
| time/              |          |
|    total_timesteps | 160000   |
| train/             |          |
|    actor_loss      | -3.6     |
|    critic_loss     | 0.0209   |
|    ent_coef        | 0.0241   |
|    ent_coef_loss   | -31      |
|    learning_rate   | 0.00484  |
|    n_updates       | 780      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -240     |
| time/              |          |
|    episodes        | 160      |
|    fps             | 632      |
|    time_elapsed    | 252      |
|    total_timesteps | 160000   |
---------------------------------
Eval num_timesteps=161000, episode_reward=-12.31 +/- 0.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -12.3    |
| time/              |          |
|    total_timesteps | 161000   |
---------------------------------
Eval num_timesteps=162000, episode_reward=-8.14 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.14    |
| time/              |          |
|    total_timesteps | 162000   |
| train/             |          |
|    actor_loss      | -3.54    |
|    critic_loss     | 0.0191   |
|    ent_coef        | 0.0231   |
|    ent_coef_loss   | -32.5    |
|    learning_rate   | 0.00484  |
|    n_updates       | 790      |
---------------------------------
Eval num_timesteps=163000, episode_reward=-8.05 +/- 0.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.05    |
| time/              |          |
|    total_timesteps | 163000   |
---------------------------------
Eval num_timesteps=164000, episode_reward=-6.56 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -6.56    |
| time/              |          |
|    total_timesteps | 164000   |
| train/             |          |
|    actor_loss      | -3.51    |
|    critic_loss     | 0.0189   |
|    ent_coef        | 0.0222   |
|    ent_coef_loss   | -31.5    |
|    learning_rate   | 0.00484  |
|    n_updates       | 800      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -239     |
| time/              |          |
|    episodes        | 164      |
|    fps             | 632      |
|    time_elapsed    | 259      |
|    total_timesteps | 164000   |
---------------------------------
Eval num_timesteps=165000, episode_reward=-7.43 +/- 0.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.43    |
| time/              |          |
|    total_timesteps | 165000   |
---------------------------------
Eval num_timesteps=166000, episode_reward=-23.74 +/- 5.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -23.7    |
| time/              |          |
|    total_timesteps | 166000   |
| train/             |          |
|    actor_loss      | -3.52    |
|    critic_loss     | 0.0179   |
|    ent_coef        | 0.0212   |
|    ent_coef_loss   | -32.1    |
|    learning_rate   | 0.00483  |
|    n_updates       | 810      |
---------------------------------
Eval num_timesteps=167000, episode_reward=-27.66 +/- 6.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -27.7    |
| time/              |          |
|    total_timesteps | 167000   |
---------------------------------
Eval num_timesteps=168000, episode_reward=-212.20 +/- 1.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 168000   |
| train/             |          |
|    actor_loss      | -3.47    |
|    critic_loss     | 0.0155   |
|    ent_coef        | 0.0204   |
|    ent_coef_loss   | -33.7    |
|    learning_rate   | 0.00483  |
|    n_updates       | 820      |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -236     |
| time/              |          |
|    episodes        | 168      |
|    fps             | 631      |
|    time_elapsed    | 265      |
|    total_timesteps | 168000   |
---------------------------------
Eval num_timesteps=169000, episode_reward=-204.99 +/- 5.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 169000   |
---------------------------------
Eval num_timesteps=170000, episode_reward=-1.09 +/- 11.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.09    |
| time/              |          |
|    total_timesteps | 170000   |
| train/             |          |
|    actor_loss      | -3.5     |
|    critic_loss     | 0.0167   |
|    ent_coef        | 0.0195   |
|    ent_coef_loss   | -33.1    |
|    learning_rate   | 0.00483  |
|    n_updates       | 830      |
---------------------------------
Eval num_timesteps=171000, episode_reward=-16.18 +/- 14.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -16.2    |
| time/              |          |
|    total_timesteps | 171000   |
---------------------------------
Eval num_timesteps=172000, episode_reward=-10.16 +/- 14.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -10.2    |
| time/              |          |
|    total_timesteps | 172000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -230     |
| time/              |          |
|    episodes        | 172      |
|    fps             | 630      |
|    time_elapsed    | 272      |
|    total_timesteps | 172000   |
---------------------------------
Eval num_timesteps=173000, episode_reward=13.83 +/- 64.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 173000   |
| train/             |          |
|    actor_loss      | -3.48    |
|    critic_loss     | 0.0162   |
|    ent_coef        | 0.0187   |
|    ent_coef_loss   | -33.1    |
|    learning_rate   | 0.00483  |
|    n_updates       | 840      |
---------------------------------
New best mean reward!
Eval num_timesteps=174000, episode_reward=58.43 +/- 46.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 58.4     |
| time/              |          |
|    total_timesteps | 174000   |
---------------------------------
New best mean reward!
Eval num_timesteps=175000, episode_reward=-168.33 +/- 118.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 175000   |
| train/             |          |
|    actor_loss      | -3.52    |
|    critic_loss     | 0.0157   |
|    ent_coef        | 0.0179   |
|    ent_coef_loss   | -32.8    |
|    learning_rate   | 0.00483  |
|    n_updates       | 850      |
---------------------------------
Eval num_timesteps=176000, episode_reward=-96.35 +/- 72.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -96.4    |
| time/              |          |
|    total_timesteps | 176000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -228     |
| time/              |          |
|    episodes        | 176      |
|    fps             | 628      |
|    time_elapsed    | 279      |
|    total_timesteps | 176000   |
---------------------------------
Eval num_timesteps=177000, episode_reward=-162.14 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 177000   |
| train/             |          |
|    actor_loss      | -3.64    |
|    critic_loss     | 0.0174   |
|    ent_coef        | 0.0172   |
|    ent_coef_loss   | -34.1    |
|    learning_rate   | 0.00482  |
|    n_updates       | 860      |
---------------------------------
Eval num_timesteps=178000, episode_reward=-163.43 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 178000   |
---------------------------------
Eval num_timesteps=179000, episode_reward=186.49 +/- 65.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 179000   |
| train/             |          |
|    actor_loss      | -3.43    |
|    critic_loss     | 0.016    |
|    ent_coef        | 0.0165   |
|    ent_coef_loss   | -32.7    |
|    learning_rate   | 0.00482  |
|    n_updates       | 870      |
---------------------------------
New best mean reward!
Eval num_timesteps=180000, episode_reward=141.54 +/- 14.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 142      |
| time/              |          |
|    total_timesteps | 180000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -227     |
| time/              |          |
|    episodes        | 180      |
|    fps             | 626      |
|    time_elapsed    | 287      |
|    total_timesteps | 180000   |
---------------------------------
Eval num_timesteps=181000, episode_reward=-102.15 +/- 10.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -102     |
| time/              |          |
|    total_timesteps | 181000   |
| train/             |          |
|    actor_loss      | -3.44    |
|    critic_loss     | 0.0157   |
|    ent_coef        | 0.0158   |
|    ent_coef_loss   | -32.1    |
|    learning_rate   | 0.00482  |
|    n_updates       | 880      |
---------------------------------
Eval num_timesteps=182000, episode_reward=-105.75 +/- 7.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 182000   |
---------------------------------
Eval num_timesteps=183000, episode_reward=-400.91 +/- 39.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 183000   |
| train/             |          |
|    actor_loss      | -3.56    |
|    critic_loss     | 0.0122   |
|    ent_coef        | 0.0152   |
|    ent_coef_loss   | -36.5    |
|    learning_rate   | 0.00482  |
|    n_updates       | 890      |
---------------------------------
Eval num_timesteps=184000, episode_reward=-109.61 +/- 213.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 184000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -227     |
| time/              |          |
|    episodes        | 184      |
|    fps             | 625      |
|    time_elapsed    | 294      |
|    total_timesteps | 184000   |
---------------------------------
Eval num_timesteps=185000, episode_reward=186.00 +/- 47.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 185000   |
| train/             |          |
|    actor_loss      | -3.39    |
|    critic_loss     | 0.0122   |
|    ent_coef        | 0.0146   |
|    ent_coef_loss   | -36      |
|    learning_rate   | 0.00482  |
|    n_updates       | 900      |
---------------------------------
Eval num_timesteps=186000, episode_reward=193.52 +/- 38.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 194      |
| time/              |          |
|    total_timesteps | 186000   |
---------------------------------
New best mean reward!
Eval num_timesteps=187000, episode_reward=-131.88 +/- 225.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 187000   |
| train/             |          |
|    actor_loss      | -3.42    |
|    critic_loss     | 0.0129   |
|    ent_coef        | 0.0139   |
|    ent_coef_loss   | -33.2    |
|    learning_rate   | 0.00481  |
|    n_updates       | 910      |
---------------------------------
Eval num_timesteps=188000, episode_reward=-114.97 +/- 236.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 188000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -223     |
| time/              |          |
|    episodes        | 188      |
|    fps             | 624      |
|    time_elapsed    | 300      |
|    total_timesteps | 188000   |
---------------------------------
Eval num_timesteps=189000, episode_reward=476.19 +/- 41.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 189000   |
| train/             |          |
|    actor_loss      | -3.44    |
|    critic_loss     | 0.0162   |
|    ent_coef        | 0.0134   |
|    ent_coef_loss   | -30.4    |
|    learning_rate   | 0.00481  |
|    n_updates       | 920      |
---------------------------------
New best mean reward!
Eval num_timesteps=190000, episode_reward=465.16 +/- 28.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 190000   |
---------------------------------
Eval num_timesteps=191000, episode_reward=-857.30 +/- 6.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -857     |
| time/              |          |
|    total_timesteps | 191000   |
| train/             |          |
|    actor_loss      | -3.41    |
|    critic_loss     | 0.0141   |
|    ent_coef        | 0.0129   |
|    ent_coef_loss   | -33.8    |
|    learning_rate   | 0.00481  |
|    n_updates       | 930      |
---------------------------------
Eval num_timesteps=192000, episode_reward=-645.68 +/- 406.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -646     |
| time/              |          |
|    total_timesteps | 192000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -219     |
| time/              |          |
|    episodes        | 192      |
|    fps             | 624      |
|    time_elapsed    | 307      |
|    total_timesteps | 192000   |
---------------------------------
Eval num_timesteps=193000, episode_reward=94.30 +/- 30.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 94.3     |
| time/              |          |
|    total_timesteps | 193000   |
| train/             |          |
|    actor_loss      | -3.41    |
|    critic_loss     | 0.0174   |
|    ent_coef        | 0.0124   |
|    ent_coef_loss   | -30.3    |
|    learning_rate   | 0.00481  |
|    n_updates       | 940      |
---------------------------------
Eval num_timesteps=194000, episode_reward=80.65 +/- 53.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 80.7     |
| time/              |          |
|    total_timesteps | 194000   |
---------------------------------
Eval num_timesteps=195000, episode_reward=-195.33 +/- 21.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 195000   |
| train/             |          |
|    actor_loss      | -3.42    |
|    critic_loss     | 0.0171   |
|    ent_coef        | 0.0119   |
|    ent_coef_loss   | -31.7    |
|    learning_rate   | 0.00481  |
|    n_updates       | 950      |
---------------------------------
Eval num_timesteps=196000, episode_reward=-168.38 +/- 56.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 196000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -216     |
| time/              |          |
|    episodes        | 196      |
|    fps             | 623      |
|    time_elapsed    | 314      |
|    total_timesteps | 196000   |
---------------------------------
Eval num_timesteps=197000, episode_reward=-848.75 +/- 148.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -849     |
| time/              |          |
|    total_timesteps | 197000   |
| train/             |          |
|    actor_loss      | -3.37    |
|    critic_loss     | 0.0137   |
|    ent_coef        | 0.0115   |
|    ent_coef_loss   | -35.2    |
|    learning_rate   | 0.0048   |
|    n_updates       | 960      |
---------------------------------
Eval num_timesteps=198000, episode_reward=-502.36 +/- 282.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 198000   |
---------------------------------
Eval num_timesteps=199000, episode_reward=-223.67 +/- 83.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -224     |
| time/              |          |
|    total_timesteps | 199000   |
| train/             |          |
|    actor_loss      | -3.53    |
|    critic_loss     | 0.0146   |
|    ent_coef        | 0.0111   |
|    ent_coef_loss   | -35.7    |
|    learning_rate   | 0.0048   |
|    n_updates       | 970      |
---------------------------------
Eval num_timesteps=200000, episode_reward=-241.41 +/- 67.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 200000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -216     |
| time/              |          |
|    episodes        | 200      |
|    fps             | 622      |
|    time_elapsed    | 321      |
|    total_timesteps | 200000   |
---------------------------------
Eval num_timesteps=201000, episode_reward=-121.98 +/- 24.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 201000   |
| train/             |          |
|    actor_loss      | -3.53    |
|    critic_loss     | 0.014    |
|    ent_coef        | 0.0106   |
|    ent_coef_loss   | -33.1    |
|    learning_rate   | 0.0048   |
|    n_updates       | 980      |
---------------------------------
Eval num_timesteps=202000, episode_reward=-110.89 +/- 29.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 202000   |
---------------------------------
Eval num_timesteps=203000, episode_reward=-46.73 +/- 44.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -46.7    |
| time/              |          |
|    total_timesteps | 203000   |
| train/             |          |
|    actor_loss      | -3.53    |
|    critic_loss     | 0.0162   |
|    ent_coef        | 0.0102   |
|    ent_coef_loss   | -29      |
|    learning_rate   | 0.0048   |
|    n_updates       | 990      |
---------------------------------
Eval num_timesteps=204000, episode_reward=-91.06 +/- 46.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -91.1    |
| time/              |          |
|    total_timesteps | 204000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -214     |
| time/              |          |
|    episodes        | 204      |
|    fps             | 622      |
|    time_elapsed    | 327      |
|    total_timesteps | 204000   |
---------------------------------
Eval num_timesteps=205000, episode_reward=85.48 +/- 224.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 85.5     |
| time/              |          |
|    total_timesteps | 205000   |
| train/             |          |
|    actor_loss      | -3.37    |
|    critic_loss     | 0.0153   |
|    ent_coef        | 0.00986  |
|    ent_coef_loss   | -32.7    |
|    learning_rate   | 0.0048   |
|    n_updates       | 1000     |
---------------------------------
Eval num_timesteps=206000, episode_reward=175.28 +/- 51.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 175      |
| time/              |          |
|    total_timesteps | 206000   |
---------------------------------
Eval num_timesteps=207000, episode_reward=37.83 +/- 74.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 37.8     |
| time/              |          |
|    total_timesteps | 207000   |
| train/             |          |
|    actor_loss      | -3.34    |
|    critic_loss     | 0.0114   |
|    ent_coef        | 0.0095   |
|    ent_coef_loss   | -34.3    |
|    learning_rate   | 0.00479  |
|    n_updates       | 1010     |
---------------------------------
Eval num_timesteps=208000, episode_reward=18.70 +/- 114.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 18.7     |
| time/              |          |
|    total_timesteps | 208000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -210     |
| time/              |          |
|    episodes        | 208      |
|    fps             | 623      |
|    time_elapsed    | 333      |
|    total_timesteps | 208000   |
---------------------------------
Eval num_timesteps=209000, episode_reward=19.05 +/- 94.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 209000   |
| train/             |          |
|    actor_loss      | -3.32    |
|    critic_loss     | 0.0105   |
|    ent_coef        | 0.00914  |
|    ent_coef_loss   | -37.1    |
|    learning_rate   | 0.00479  |
|    n_updates       | 1020     |
---------------------------------
Eval num_timesteps=210000, episode_reward=19.03 +/- 130.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 19       |
| time/              |          |
|    total_timesteps | 210000   |
---------------------------------
Eval num_timesteps=211000, episode_reward=-73.30 +/- 300.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -73.3    |
| time/              |          |
|    total_timesteps | 211000   |
| train/             |          |
|    actor_loss      | -3.29    |
|    critic_loss     | 0.00955  |
|    ent_coef        | 0.00878  |
|    ent_coef_loss   | -38.6    |
|    learning_rate   | 0.00479  |
|    n_updates       | 1030     |
---------------------------------
Eval num_timesteps=212000, episode_reward=150.17 +/- 340.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 150      |
| time/              |          |
|    total_timesteps | 212000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -210     |
| time/              |          |
|    episodes        | 212      |
|    fps             | 623      |
|    time_elapsed    | 339      |
|    total_timesteps | 212000   |
---------------------------------
Eval num_timesteps=213000, episode_reward=101.48 +/- 67.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 101      |
| time/              |          |
|    total_timesteps | 213000   |
| train/             |          |
|    actor_loss      | -3.33    |
|    critic_loss     | 0.00913  |
|    ent_coef        | 0.00842  |
|    ent_coef_loss   | -39.1    |
|    learning_rate   | 0.00479  |
|    n_updates       | 1040     |
---------------------------------
Eval num_timesteps=214000, episode_reward=129.03 +/- 80.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 129      |
| time/              |          |
|    total_timesteps | 214000   |
---------------------------------
Eval num_timesteps=215000, episode_reward=143.03 +/- 26.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 215000   |
---------------------------------
Eval num_timesteps=216000, episode_reward=0.36 +/- 16.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.363    |
| time/              |          |
|    total_timesteps | 216000   |
| train/             |          |
|    actor_loss      | -3.31    |
|    critic_loss     | 0.0108   |
|    ent_coef        | 0.00807  |
|    ent_coef_loss   | -35.5    |
|    learning_rate   | 0.00478  |
|    n_updates       | 1050     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -207     |
| time/              |          |
|    episodes        | 216      |
|    fps             | 624      |
|    time_elapsed    | 346      |
|    total_timesteps | 216000   |
---------------------------------
Eval num_timesteps=217000, episode_reward=7.14 +/- 29.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 7.14     |
| time/              |          |
|    total_timesteps | 217000   |
---------------------------------
Eval num_timesteps=218000, episode_reward=-0.57 +/- 19.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.568   |
| time/              |          |
|    total_timesteps | 218000   |
| train/             |          |
|    actor_loss      | -3.49    |
|    critic_loss     | 0.0163   |
|    ent_coef        | 0.00776  |
|    ent_coef_loss   | -30.7    |
|    learning_rate   | 0.00478  |
|    n_updates       | 1060     |
---------------------------------
Eval num_timesteps=219000, episode_reward=-34.92 +/- 65.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -34.9    |
| time/              |          |
|    total_timesteps | 219000   |
---------------------------------
Eval num_timesteps=220000, episode_reward=-160.36 +/- 28.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 220000   |
| train/             |          |
|    actor_loss      | -3.32    |
|    critic_loss     | 0.0138   |
|    ent_coef        | 0.00748  |
|    ent_coef_loss   | -29.1    |
|    learning_rate   | 0.00478  |
|    n_updates       | 1070     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -206     |
| time/              |          |
|    episodes        | 220      |
|    fps             | 623      |
|    time_elapsed    | 352      |
|    total_timesteps | 220000   |
---------------------------------
Eval num_timesteps=221000, episode_reward=-133.85 +/- 31.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 221000   |
---------------------------------
Eval num_timesteps=222000, episode_reward=-23.55 +/- 75.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -23.5    |
| time/              |          |
|    total_timesteps | 222000   |
| train/             |          |
|    actor_loss      | -3.31    |
|    critic_loss     | 0.0156   |
|    ent_coef        | 0.00725  |
|    ent_coef_loss   | -23.6    |
|    learning_rate   | 0.00478  |
|    n_updates       | 1080     |
---------------------------------
Eval num_timesteps=223000, episode_reward=-12.80 +/- 60.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -12.8    |
| time/              |          |
|    total_timesteps | 223000   |
---------------------------------
Eval num_timesteps=224000, episode_reward=5.46 +/- 14.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 5.46     |
| time/              |          |
|    total_timesteps | 224000   |
| train/             |          |
|    actor_loss      | -3.26    |
|    critic_loss     | 0.0103   |
|    ent_coef        | 0.00703  |
|    ent_coef_loss   | -39.9    |
|    learning_rate   | 0.00478  |
|    n_updates       | 1090     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -208     |
| time/              |          |
|    episodes        | 224      |
|    fps             | 623      |
|    time_elapsed    | 359      |
|    total_timesteps | 224000   |
---------------------------------
Eval num_timesteps=225000, episode_reward=11.16 +/- 3.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 11.2     |
| time/              |          |
|    total_timesteps | 225000   |
---------------------------------
Eval num_timesteps=226000, episode_reward=-131.88 +/- 57.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 226000   |
| train/             |          |
|    actor_loss      | -3.24    |
|    critic_loss     | 0.00892  |
|    ent_coef        | 0.00677  |
|    ent_coef_loss   | -41.9    |
|    learning_rate   | 0.00477  |
|    n_updates       | 1100     |
---------------------------------
Eval num_timesteps=227000, episode_reward=-122.83 +/- 22.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 227000   |
---------------------------------
Eval num_timesteps=228000, episode_reward=-68.10 +/- 130.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -68.1    |
| time/              |          |
|    total_timesteps | 228000   |
| train/             |          |
|    actor_loss      | -3.25    |
|    critic_loss     | 0.00927  |
|    ent_coef        | 0.00649  |
|    ent_coef_loss   | -37.6    |
|    learning_rate   | 0.00477  |
|    n_updates       | 1110     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -210     |
| time/              |          |
|    episodes        | 228      |
|    fps             | 622      |
|    time_elapsed    | 366      |
|    total_timesteps | 228000   |
---------------------------------
Eval num_timesteps=229000, episode_reward=-90.92 +/- 70.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -90.9    |
| time/              |          |
|    total_timesteps | 229000   |
---------------------------------
Eval num_timesteps=230000, episode_reward=-159.74 +/- 59.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 230000   |
| train/             |          |
|    actor_loss      | -3.31    |
|    critic_loss     | 0.014    |
|    ent_coef        | 0.00623  |
|    ent_coef_loss   | -31.2    |
|    learning_rate   | 0.00477  |
|    n_updates       | 1120     |
---------------------------------
Eval num_timesteps=231000, episode_reward=-90.42 +/- 26.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -90.4    |
| time/              |          |
|    total_timesteps | 231000   |
---------------------------------
Eval num_timesteps=232000, episode_reward=-555.67 +/- 317.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -556     |
| time/              |          |
|    total_timesteps | 232000   |
| train/             |          |
|    actor_loss      | -3.28    |
|    critic_loss     | 0.0143   |
|    ent_coef        | 0.00602  |
|    ent_coef_loss   | -27.9    |
|    learning_rate   | 0.00477  |
|    n_updates       | 1130     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -210     |
| time/              |          |
|    episodes        | 232      |
|    fps             | 621      |
|    time_elapsed    | 373      |
|    total_timesteps | 232000   |
---------------------------------
Eval num_timesteps=233000, episode_reward=-762.00 +/- 287.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -762     |
| time/              |          |
|    total_timesteps | 233000   |
---------------------------------
Eval num_timesteps=234000, episode_reward=-97.39 +/- 105.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -97.4    |
| time/              |          |
|    total_timesteps | 234000   |
| train/             |          |
|    actor_loss      | -3.29    |
|    critic_loss     | 0.0117   |
|    ent_coef        | 0.00582  |
|    ent_coef_loss   | -33.9    |
|    learning_rate   | 0.00477  |
|    n_updates       | 1140     |
---------------------------------
Eval num_timesteps=235000, episode_reward=-125.79 +/- 195.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 235000   |
---------------------------------
Eval num_timesteps=236000, episode_reward=-298.13 +/- 240.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -298     |
| time/              |          |
|    total_timesteps | 236000   |
| train/             |          |
|    actor_loss      | -3.25    |
|    critic_loss     | 0.0101   |
|    ent_coef        | 0.00563  |
|    ent_coef_loss   | -33.3    |
|    learning_rate   | 0.00476  |
|    n_updates       | 1150     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -206     |
| time/              |          |
|    episodes        | 236      |
|    fps             | 621      |
|    time_elapsed    | 379      |
|    total_timesteps | 236000   |
---------------------------------
Eval num_timesteps=237000, episode_reward=25.37 +/- 39.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 25.4     |
| time/              |          |
|    total_timesteps | 237000   |
---------------------------------
Eval num_timesteps=238000, episode_reward=107.27 +/- 61.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 238000   |
| train/             |          |
|    actor_loss      | -3.28    |
|    critic_loss     | 0.00964  |
|    ent_coef        | 0.00543  |
|    ent_coef_loss   | -33.7    |
|    learning_rate   | 0.00476  |
|    n_updates       | 1160     |
---------------------------------
Eval num_timesteps=239000, episode_reward=91.17 +/- 30.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 91.2     |
| time/              |          |
|    total_timesteps | 239000   |
---------------------------------
Eval num_timesteps=240000, episode_reward=-238.71 +/- 157.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 240000   |
| train/             |          |
|    actor_loss      | -3.43    |
|    critic_loss     | 0.0128   |
|    ent_coef        | 0.00524  |
|    ent_coef_loss   | -35.9    |
|    learning_rate   | 0.00476  |
|    n_updates       | 1170     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -206     |
| time/              |          |
|    episodes        | 240      |
|    fps             | 620      |
|    time_elapsed    | 386      |
|    total_timesteps | 240000   |
---------------------------------
Eval num_timesteps=241000, episode_reward=-325.01 +/- 14.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -325     |
| time/              |          |
|    total_timesteps | 241000   |
---------------------------------
Eval num_timesteps=242000, episode_reward=-432.54 +/- 9.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 242000   |
| train/             |          |
|    actor_loss      | -3.24    |
|    critic_loss     | 0.00856  |
|    ent_coef        | 0.00506  |
|    ent_coef_loss   | -31.3    |
|    learning_rate   | 0.00476  |
|    n_updates       | 1180     |
---------------------------------
Eval num_timesteps=243000, episode_reward=-417.38 +/- 57.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -417     |
| time/              |          |
|    total_timesteps | 243000   |
---------------------------------
Eval num_timesteps=244000, episode_reward=-82.06 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -82.1    |
| time/              |          |
|    total_timesteps | 244000   |
| train/             |          |
|    actor_loss      | -3.26    |
|    critic_loss     | 0.0105   |
|    ent_coef        | 0.00489  |
|    ent_coef_loss   | -30.6    |
|    learning_rate   | 0.00476  |
|    n_updates       | 1190     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -202     |
| time/              |          |
|    episodes        | 244      |
|    fps             | 621      |
|    time_elapsed    | 392      |
|    total_timesteps | 244000   |
---------------------------------
Eval num_timesteps=245000, episode_reward=-80.66 +/- 3.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -80.7    |
| time/              |          |
|    total_timesteps | 245000   |
---------------------------------
Eval num_timesteps=246000, episode_reward=-65.90 +/- 1.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -65.9    |
| time/              |          |
|    total_timesteps | 246000   |
| train/             |          |
|    actor_loss      | -3.26    |
|    critic_loss     | 0.00911  |
|    ent_coef        | 0.00473  |
|    ent_coef_loss   | -35.7    |
|    learning_rate   | 0.00475  |
|    n_updates       | 1200     |
---------------------------------
Eval num_timesteps=247000, episode_reward=-65.29 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -65.3    |
| time/              |          |
|    total_timesteps | 247000   |
---------------------------------
Eval num_timesteps=248000, episode_reward=-66.45 +/- 190.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -66.5    |
| time/              |          |
|    total_timesteps | 248000   |
| train/             |          |
|    actor_loss      | -3.32    |
|    critic_loss     | 0.0102   |
|    ent_coef        | 0.00457  |
|    ent_coef_loss   | -36.1    |
|    learning_rate   | 0.00475  |
|    n_updates       | 1210     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -196     |
| time/              |          |
|    episodes        | 248      |
|    fps             | 621      |
|    time_elapsed    | 398      |
|    total_timesteps | 248000   |
---------------------------------
Eval num_timesteps=249000, episode_reward=8.61 +/- 76.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 8.61     |
| time/              |          |
|    total_timesteps | 249000   |
---------------------------------
Eval num_timesteps=250000, episode_reward=80.52 +/- 34.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 80.5     |
| time/              |          |
|    total_timesteps | 250000   |
| train/             |          |
|    actor_loss      | -3.23    |
|    critic_loss     | 0.00912  |
|    ent_coef        | 0.0044   |
|    ent_coef_loss   | -32.1    |
|    learning_rate   | 0.00475  |
|    n_updates       | 1220     |
---------------------------------
Eval num_timesteps=251000, episode_reward=43.04 +/- 75.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 43       |
| time/              |          |
|    total_timesteps | 251000   |
---------------------------------
Eval num_timesteps=252000, episode_reward=85.37 +/- 46.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 85.4     |
| time/              |          |
|    total_timesteps | 252000   |
| train/             |          |
|    actor_loss      | -3.23    |
|    critic_loss     | 0.00976  |
|    ent_coef        | 0.00426  |
|    ent_coef_loss   | -27.1    |
|    learning_rate   | 0.00475  |
|    n_updates       | 1230     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -195     |
| time/              |          |
|    episodes        | 252      |
|    fps             | 622      |
|    time_elapsed    | 405      |
|    total_timesteps | 252000   |
---------------------------------
Eval num_timesteps=253000, episode_reward=-125.55 +/- 153.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 253000   |
---------------------------------
Eval num_timesteps=254000, episode_reward=-501.86 +/- 115.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 254000   |
| train/             |          |
|    actor_loss      | -3.22    |
|    critic_loss     | 0.00985  |
|    ent_coef        | 0.00413  |
|    ent_coef_loss   | -28.5    |
|    learning_rate   | 0.00475  |
|    n_updates       | 1240     |
---------------------------------
Eval num_timesteps=255000, episode_reward=-514.07 +/- 243.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 255000   |
---------------------------------
Eval num_timesteps=256000, episode_reward=-559.57 +/- 124.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 256000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -193     |
| time/              |          |
|    episodes        | 256      |
|    fps             | 622      |
|    time_elapsed    | 411      |
|    total_timesteps | 256000   |
---------------------------------
Eval num_timesteps=257000, episode_reward=-64.37 +/- 133.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -64.4    |
| time/              |          |
|    total_timesteps | 257000   |
| train/             |          |
|    actor_loss      | -3.63    |
|    critic_loss     | 0.0379   |
|    ent_coef        | 0.00401  |
|    ent_coef_loss   | -32.7    |
|    learning_rate   | 0.00474  |
|    n_updates       | 1250     |
---------------------------------
Eval num_timesteps=258000, episode_reward=-8.09 +/- 56.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.09    |
| time/              |          |
|    total_timesteps | 258000   |
---------------------------------
Eval num_timesteps=259000, episode_reward=-71.99 +/- 116.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -72      |
| time/              |          |
|    total_timesteps | 259000   |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.0187   |
|    ent_coef        | 0.00388  |
|    ent_coef_loss   | -37.3    |
|    learning_rate   | 0.00474  |
|    n_updates       | 1260     |
---------------------------------
Eval num_timesteps=260000, episode_reward=-56.42 +/- 137.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -56.4    |
| time/              |          |
|    total_timesteps | 260000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -189     |
| time/              |          |
|    episodes        | 260      |
|    fps             | 622      |
|    time_elapsed    | 417      |
|    total_timesteps | 260000   |
---------------------------------
Eval num_timesteps=261000, episode_reward=69.17 +/- 170.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 69.2     |
| time/              |          |
|    total_timesteps | 261000   |
| train/             |          |
|    actor_loss      | -3.18    |
|    critic_loss     | 0.0141   |
|    ent_coef        | 0.00375  |
|    ent_coef_loss   | -29.5    |
|    learning_rate   | 0.00474  |
|    n_updates       | 1270     |
---------------------------------
Eval num_timesteps=262000, episode_reward=143.00 +/- 37.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 262000   |
---------------------------------
Eval num_timesteps=263000, episode_reward=112.02 +/- 37.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 112      |
| time/              |          |
|    total_timesteps | 263000   |
| train/             |          |
|    actor_loss      | -3.13    |
|    critic_loss     | 0.00946  |
|    ent_coef        | 0.00363  |
|    ent_coef_loss   | -29.1    |
|    learning_rate   | 0.00474  |
|    n_updates       | 1280     |
---------------------------------
Eval num_timesteps=264000, episode_reward=118.36 +/- 122.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 118      |
| time/              |          |
|    total_timesteps | 264000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -191     |
| time/              |          |
|    episodes        | 264      |
|    fps             | 622      |
|    time_elapsed    | 423      |
|    total_timesteps | 264000   |
---------------------------------
Eval num_timesteps=265000, episode_reward=166.01 +/- 72.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 166      |
| time/              |          |
|    total_timesteps | 265000   |
| train/             |          |
|    actor_loss      | -3.13    |
|    critic_loss     | 0.00973  |
|    ent_coef        | 0.00352  |
|    ent_coef_loss   | -29.6    |
|    learning_rate   | 0.00474  |
|    n_updates       | 1290     |
---------------------------------
Eval num_timesteps=266000, episode_reward=193.07 +/- 68.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 266000   |
---------------------------------
Eval num_timesteps=267000, episode_reward=375.47 +/- 91.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 375      |
| time/              |          |
|    total_timesteps | 267000   |
| train/             |          |
|    actor_loss      | -3.17    |
|    critic_loss     | 0.0135   |
|    ent_coef        | 0.00342  |
|    ent_coef_loss   | -31.8    |
|    learning_rate   | 0.00473  |
|    n_updates       | 1300     |
---------------------------------
Eval num_timesteps=268000, episode_reward=398.73 +/- 66.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 268000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -191     |
| time/              |          |
|    episodes        | 268      |
|    fps             | 622      |
|    time_elapsed    | 430      |
|    total_timesteps | 268000   |
---------------------------------
Eval num_timesteps=269000, episode_reward=302.66 +/- 268.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 269000   |
| train/             |          |
|    actor_loss      | -3.15    |
|    critic_loss     | 0.00939  |
|    ent_coef        | 0.00331  |
|    ent_coef_loss   | -30.5    |
|    learning_rate   | 0.00473  |
|    n_updates       | 1310     |
---------------------------------
Eval num_timesteps=270000, episode_reward=287.65 +/- 139.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 270000   |
---------------------------------
Eval num_timesteps=271000, episode_reward=91.15 +/- 145.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 91.2     |
| time/              |          |
|    total_timesteps | 271000   |
| train/             |          |
|    actor_loss      | -3.12    |
|    critic_loss     | 0.00728  |
|    ent_coef        | 0.00321  |
|    ent_coef_loss   | -26.9    |
|    learning_rate   | 0.00473  |
|    n_updates       | 1320     |
---------------------------------
Eval num_timesteps=272000, episode_reward=190.28 +/- 20.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 272000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -191     |
| time/              |          |
|    episodes        | 272      |
|    fps             | 621      |
|    time_elapsed    | 437      |
|    total_timesteps | 272000   |
---------------------------------
Eval num_timesteps=273000, episode_reward=41.47 +/- 90.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 41.5     |
| time/              |          |
|    total_timesteps | 273000   |
| train/             |          |
|    actor_loss      | -3.19    |
|    critic_loss     | 0.0085   |
|    ent_coef        | 0.00312  |
|    ent_coef_loss   | -25.3    |
|    learning_rate   | 0.00473  |
|    n_updates       | 1330     |
---------------------------------
Eval num_timesteps=274000, episode_reward=102.91 +/- 57.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 103      |
| time/              |          |
|    total_timesteps | 274000   |
---------------------------------
Eval num_timesteps=275000, episode_reward=-37.30 +/- 85.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -37.3    |
| time/              |          |
|    total_timesteps | 275000   |
| train/             |          |
|    actor_loss      | -3.15    |
|    critic_loss     | 0.00694  |
|    ent_coef        | 0.00304  |
|    ent_coef_loss   | -25.1    |
|    learning_rate   | 0.00473  |
|    n_updates       | 1340     |
---------------------------------
Eval num_timesteps=276000, episode_reward=-70.17 +/- 106.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -70.2    |
| time/              |          |
|    total_timesteps | 276000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -189     |
| time/              |          |
|    episodes        | 276      |
|    fps             | 620      |
|    time_elapsed    | 444      |
|    total_timesteps | 276000   |
---------------------------------
Eval num_timesteps=277000, episode_reward=110.53 +/- 26.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 111      |
| time/              |          |
|    total_timesteps | 277000   |
| train/             |          |
|    actor_loss      | -3.25    |
|    critic_loss     | 0.0119   |
|    ent_coef        | 0.00296  |
|    ent_coef_loss   | -25      |
|    learning_rate   | 0.00472  |
|    n_updates       | 1350     |
---------------------------------
Eval num_timesteps=278000, episode_reward=108.32 +/- 146.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 108      |
| time/              |          |
|    total_timesteps | 278000   |
---------------------------------
Eval num_timesteps=279000, episode_reward=68.25 +/- 34.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 68.3     |
| time/              |          |
|    total_timesteps | 279000   |
| train/             |          |
|    actor_loss      | -3.23    |
|    critic_loss     | 0.00806  |
|    ent_coef        | 0.00289  |
|    ent_coef_loss   | -24.4    |
|    learning_rate   | 0.00472  |
|    n_updates       | 1360     |
---------------------------------
Eval num_timesteps=280000, episode_reward=39.04 +/- 50.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 39       |
| time/              |          |
|    total_timesteps | 280000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -188     |
| time/              |          |
|    episodes        | 280      |
|    fps             | 619      |
|    time_elapsed    | 451      |
|    total_timesteps | 280000   |
---------------------------------
Eval num_timesteps=281000, episode_reward=9.06 +/- 111.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 9.06     |
| time/              |          |
|    total_timesteps | 281000   |
| train/             |          |
|    actor_loss      | -3.12    |
|    critic_loss     | 0.00692  |
|    ent_coef        | 0.00282  |
|    ent_coef_loss   | -34.9    |
|    learning_rate   | 0.00472  |
|    n_updates       | 1370     |
---------------------------------
Eval num_timesteps=282000, episode_reward=-21.48 +/- 74.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -21.5    |
| time/              |          |
|    total_timesteps | 282000   |
---------------------------------
Eval num_timesteps=283000, episode_reward=-122.39 +/- 33.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 283000   |
| train/             |          |
|    actor_loss      | -3.13    |
|    critic_loss     | 0.00599  |
|    ent_coef        | 0.00273  |
|    ent_coef_loss   | -27      |
|    learning_rate   | 0.00472  |
|    n_updates       | 1380     |
---------------------------------
Eval num_timesteps=284000, episode_reward=-73.33 +/- 77.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -73.3    |
| time/              |          |
|    total_timesteps | 284000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -188     |
| time/              |          |
|    episodes        | 284      |
|    fps             | 618      |
|    time_elapsed    | 459      |
|    total_timesteps | 284000   |
---------------------------------
Eval num_timesteps=285000, episode_reward=-190.30 +/- 205.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 285000   |
| train/             |          |
|    actor_loss      | -3.14    |
|    critic_loss     | 0.00838  |
|    ent_coef        | 0.00265  |
|    ent_coef_loss   | -29      |
|    learning_rate   | 0.00472  |
|    n_updates       | 1390     |
---------------------------------
Eval num_timesteps=286000, episode_reward=-50.15 +/- 59.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -50.1    |
| time/              |          |
|    total_timesteps | 286000   |
---------------------------------
Eval num_timesteps=287000, episode_reward=-260.08 +/- 129.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 287000   |
| train/             |          |
|    actor_loss      | -3.11    |
|    critic_loss     | 0.00676  |
|    ent_coef        | 0.00258  |
|    ent_coef_loss   | -29.3    |
|    learning_rate   | 0.00471  |
|    n_updates       | 1400     |
---------------------------------
Eval num_timesteps=288000, episode_reward=-98.27 +/- 101.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -98.3    |
| time/              |          |
|    total_timesteps | 288000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -187     |
| time/              |          |
|    episodes        | 288      |
|    fps             | 618      |
|    time_elapsed    | 465      |
|    total_timesteps | 288000   |
---------------------------------
Eval num_timesteps=289000, episode_reward=-81.08 +/- 50.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -81.1    |
| time/              |          |
|    total_timesteps | 289000   |
| train/             |          |
|    actor_loss      | -3.11    |
|    critic_loss     | 0.00765  |
|    ent_coef        | 0.00251  |
|    ent_coef_loss   | -26.3    |
|    learning_rate   | 0.00471  |
|    n_updates       | 1410     |
---------------------------------
Eval num_timesteps=290000, episode_reward=-133.56 +/- 64.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 290000   |
---------------------------------
Eval num_timesteps=291000, episode_reward=-35.01 +/- 61.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -35      |
| time/              |          |
|    total_timesteps | 291000   |
| train/             |          |
|    actor_loss      | -3.12    |
|    critic_loss     | 0.00663  |
|    ent_coef        | 0.00244  |
|    ent_coef_loss   | -23.3    |
|    learning_rate   | 0.00471  |
|    n_updates       | 1420     |
---------------------------------
Eval num_timesteps=292000, episode_reward=-63.69 +/- 36.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -63.7    |
| time/              |          |
|    total_timesteps | 292000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -185     |
| time/              |          |
|    episodes        | 292      |
|    fps             | 618      |
|    time_elapsed    | 471      |
|    total_timesteps | 292000   |
---------------------------------
Eval num_timesteps=293000, episode_reward=41.20 +/- 40.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 41.2     |
| time/              |          |
|    total_timesteps | 293000   |
| train/             |          |
|    actor_loss      | -3.12    |
|    critic_loss     | 0.00632  |
|    ent_coef        | 0.00238  |
|    ent_coef_loss   | -24.4    |
|    learning_rate   | 0.00471  |
|    n_updates       | 1430     |
---------------------------------
Eval num_timesteps=294000, episode_reward=40.88 +/- 32.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 40.9     |
| time/              |          |
|    total_timesteps | 294000   |
---------------------------------
Eval num_timesteps=295000, episode_reward=95.75 +/- 44.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 95.8     |
| time/              |          |
|    total_timesteps | 295000   |
| train/             |          |
|    actor_loss      | -3.12    |
|    critic_loss     | 0.00651  |
|    ent_coef        | 0.00232  |
|    ent_coef_loss   | -30.8    |
|    learning_rate   | 0.00471  |
|    n_updates       | 1440     |
---------------------------------
Eval num_timesteps=296000, episode_reward=43.65 +/- 36.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 43.6     |
| time/              |          |
|    total_timesteps | 296000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -183     |
| time/              |          |
|    episodes        | 296      |
|    fps             | 619      |
|    time_elapsed    | 478      |
|    total_timesteps | 296000   |
---------------------------------
Eval num_timesteps=297000, episode_reward=71.59 +/- 38.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 71.6     |
| time/              |          |
|    total_timesteps | 297000   |
| train/             |          |
|    actor_loss      | -3.59    |
|    critic_loss     | 0.0142   |
|    ent_coef        | 0.00225  |
|    ent_coef_loss   | -31.8    |
|    learning_rate   | 0.0047   |
|    n_updates       | 1450     |
---------------------------------
Eval num_timesteps=298000, episode_reward=189.55 +/- 102.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 190      |
| time/              |          |
|    total_timesteps | 298000   |
---------------------------------
Eval num_timesteps=299000, episode_reward=96.55 +/- 69.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 96.6     |
| time/              |          |
|    total_timesteps | 299000   |
---------------------------------
Eval num_timesteps=300000, episode_reward=124.74 +/- 174.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 125      |
| time/              |          |
|    total_timesteps | 300000   |
| train/             |          |
|    actor_loss      | -3.09    |
|    critic_loss     | 0.00818  |
|    ent_coef        | 0.00219  |
|    ent_coef_loss   | -28.2    |
|    learning_rate   | 0.0047   |
|    n_updates       | 1460     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -172     |
| time/              |          |
|    episodes        | 300      |
|    fps             | 619      |
|    time_elapsed    | 484      |
|    total_timesteps | 300000   |
---------------------------------
Eval num_timesteps=301000, episode_reward=164.14 +/- 101.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 164      |
| time/              |          |
|    total_timesteps | 301000   |
---------------------------------
Eval num_timesteps=302000, episode_reward=-33.38 +/- 89.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -33.4    |
| time/              |          |
|    total_timesteps | 302000   |
| train/             |          |
|    actor_loss      | -3.09    |
|    critic_loss     | 0.00734  |
|    ent_coef        | 0.00212  |
|    ent_coef_loss   | -30.8    |
|    learning_rate   | 0.0047   |
|    n_updates       | 1470     |
---------------------------------
Eval num_timesteps=303000, episode_reward=-50.04 +/- 134.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -50      |
| time/              |          |
|    total_timesteps | 303000   |
---------------------------------
Eval num_timesteps=304000, episode_reward=-18.42 +/- 396.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.4    |
| time/              |          |
|    total_timesteps | 304000   |
| train/             |          |
|    actor_loss      | -3.06    |
|    critic_loss     | 0.00612  |
|    ent_coef        | 0.00206  |
|    ent_coef_loss   | -28      |
|    learning_rate   | 0.0047   |
|    n_updates       | 1480     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -169     |
| time/              |          |
|    episodes        | 304      |
|    fps             | 619      |
|    time_elapsed    | 490      |
|    total_timesteps | 304000   |
---------------------------------
Eval num_timesteps=305000, episode_reward=78.57 +/- 144.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 78.6     |
| time/              |          |
|    total_timesteps | 305000   |
---------------------------------
Eval num_timesteps=306000, episode_reward=82.79 +/- 287.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 82.8     |
| time/              |          |
|    total_timesteps | 306000   |
| train/             |          |
|    actor_loss      | -3.08    |
|    critic_loss     | 0.006    |
|    ent_coef        | 0.002    |
|    ent_coef_loss   | -32.7    |
|    learning_rate   | 0.00469  |
|    n_updates       | 1490     |
---------------------------------
Eval num_timesteps=307000, episode_reward=197.73 +/- 206.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 198      |
| time/              |          |
|    total_timesteps | 307000   |
---------------------------------
Eval num_timesteps=308000, episode_reward=-117.68 +/- 325.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 308000   |
| train/             |          |
|    actor_loss      | -3.04    |
|    critic_loss     | 0.00598  |
|    ent_coef        | 0.00194  |
|    ent_coef_loss   | -36.9    |
|    learning_rate   | 0.00469  |
|    n_updates       | 1500     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -170     |
| time/              |          |
|    episodes        | 308      |
|    fps             | 620      |
|    time_elapsed    | 496      |
|    total_timesteps | 308000   |
---------------------------------
Eval num_timesteps=309000, episode_reward=-170.66 +/- 280.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 309000   |
---------------------------------
Eval num_timesteps=310000, episode_reward=-806.22 +/- 223.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -806     |
| time/              |          |
|    total_timesteps | 310000   |
| train/             |          |
|    actor_loss      | -3.05    |
|    critic_loss     | 0.00561  |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | -36.2    |
|    learning_rate   | 0.00469  |
|    n_updates       | 1510     |
---------------------------------
Eval num_timesteps=311000, episode_reward=-608.93 +/- 372.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -609     |
| time/              |          |
|    total_timesteps | 311000   |
---------------------------------
Eval num_timesteps=312000, episode_reward=-447.02 +/- 578.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -447     |
| time/              |          |
|    total_timesteps | 312000   |
| train/             |          |
|    actor_loss      | -3.03    |
|    critic_loss     | 0.00525  |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | -38      |
|    learning_rate   | 0.00469  |
|    n_updates       | 1520     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -169     |
| time/              |          |
|    episodes        | 312      |
|    fps             | 620      |
|    time_elapsed    | 502      |
|    total_timesteps | 312000   |
---------------------------------
Eval num_timesteps=313000, episode_reward=-107.80 +/- 242.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 313000   |
---------------------------------
Eval num_timesteps=314000, episode_reward=-152.00 +/- 380.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 314000   |
| train/             |          |
|    actor_loss      | -3.04    |
|    critic_loss     | 0.00605  |
|    ent_coef        | 0.00175  |
|    ent_coef_loss   | -36.5    |
|    learning_rate   | 0.00469  |
|    n_updates       | 1530     |
---------------------------------
Eval num_timesteps=315000, episode_reward=-127.38 +/- 251.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 315000   |
---------------------------------
Eval num_timesteps=316000, episode_reward=-87.29 +/- 241.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -87.3    |
| time/              |          |
|    total_timesteps | 316000   |
| train/             |          |
|    actor_loss      | -3.05    |
|    critic_loss     | 0.00525  |
|    ent_coef        | 0.00168  |
|    ent_coef_loss   | -35.1    |
|    learning_rate   | 0.00468  |
|    n_updates       | 1540     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -166     |
| time/              |          |
|    episodes        | 316      |
|    fps             | 620      |
|    time_elapsed    | 509      |
|    total_timesteps | 316000   |
---------------------------------
Eval num_timesteps=317000, episode_reward=53.70 +/- 207.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 53.7     |
| time/              |          |
|    total_timesteps | 317000   |
---------------------------------
Eval num_timesteps=318000, episode_reward=-14.40 +/- 135.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -14.4    |
| time/              |          |
|    total_timesteps | 318000   |
| train/             |          |
|    actor_loss      | -3.03    |
|    critic_loss     | 0.00531  |
|    ent_coef        | 0.00163  |
|    ent_coef_loss   | -33.8    |
|    learning_rate   | 0.00468  |
|    n_updates       | 1550     |
---------------------------------
Eval num_timesteps=319000, episode_reward=-37.72 +/- 220.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -37.7    |
| time/              |          |
|    total_timesteps | 319000   |
---------------------------------
Eval num_timesteps=320000, episode_reward=-94.80 +/- 31.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -94.8    |
| time/              |          |
|    total_timesteps | 320000   |
| train/             |          |
|    actor_loss      | -3.05    |
|    critic_loss     | 0.00515  |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | -26.8    |
|    learning_rate   | 0.00468  |
|    n_updates       | 1560     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -163     |
| time/              |          |
|    episodes        | 320      |
|    fps             | 620      |
|    time_elapsed    | 515      |
|    total_timesteps | 320000   |
---------------------------------
Eval num_timesteps=321000, episode_reward=-339.64 +/- 290.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 321000   |
---------------------------------
Eval num_timesteps=322000, episode_reward=-373.96 +/- 540.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 322000   |
| train/             |          |
|    actor_loss      | -3.07    |
|    critic_loss     | 0.0057   |
|    ent_coef        | 0.00153  |
|    ent_coef_loss   | -19.7    |
|    learning_rate   | 0.00468  |
|    n_updates       | 1570     |
---------------------------------
Eval num_timesteps=323000, episode_reward=-292.58 +/- 175.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 323000   |
---------------------------------
Eval num_timesteps=324000, episode_reward=-471.64 +/- 573.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -472     |
| time/              |          |
|    total_timesteps | 324000   |
| train/             |          |
|    actor_loss      | -3.05    |
|    critic_loss     | 0.00516  |
|    ent_coef        | 0.0015   |
|    ent_coef_loss   | -20.6    |
|    learning_rate   | 0.00468  |
|    n_updates       | 1580     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -155     |
| time/              |          |
|    episodes        | 324      |
|    fps             | 621      |
|    time_elapsed    | 521      |
|    total_timesteps | 324000   |
---------------------------------
Eval num_timesteps=325000, episode_reward=-519.01 +/- 538.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 325000   |
---------------------------------
Eval num_timesteps=326000, episode_reward=-427.73 +/- 625.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 326000   |
| train/             |          |
|    actor_loss      | -3.06    |
|    critic_loss     | 0.00511  |
|    ent_coef        | 0.00146  |
|    ent_coef_loss   | -31.2    |
|    learning_rate   | 0.00467  |
|    n_updates       | 1590     |
---------------------------------
Eval num_timesteps=327000, episode_reward=-656.22 +/- 714.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -656     |
| time/              |          |
|    total_timesteps | 327000   |
---------------------------------
Eval num_timesteps=328000, episode_reward=-84.97 +/- 302.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -85      |
| time/              |          |
|    total_timesteps | 328000   |
| train/             |          |
|    actor_loss      | -3.03    |
|    critic_loss     | 0.0045   |
|    ent_coef        | 0.00142  |
|    ent_coef_loss   | -33.5    |
|    learning_rate   | 0.00467  |
|    n_updates       | 1600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 328      |
|    fps             | 621      |
|    time_elapsed    | 527      |
|    total_timesteps | 328000   |
---------------------------------
Eval num_timesteps=329000, episode_reward=186.88 +/- 90.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 187      |
| time/              |          |
|    total_timesteps | 329000   |
---------------------------------
Eval num_timesteps=330000, episode_reward=-268.09 +/- 14.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 330000   |
| train/             |          |
|    actor_loss      | -3.5     |
|    critic_loss     | 0.0135   |
|    ent_coef        | 0.00138  |
|    ent_coef_loss   | -34.4    |
|    learning_rate   | 0.00467  |
|    n_updates       | 1610     |
---------------------------------
Eval num_timesteps=331000, episode_reward=-256.22 +/- 64.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 331000   |
---------------------------------
Eval num_timesteps=332000, episode_reward=-193.55 +/- 253.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 332000   |
| train/             |          |
|    actor_loss      | -3.05    |
|    critic_loss     | 0.00706  |
|    ent_coef        | 0.00134  |
|    ent_coef_loss   | -22.7    |
|    learning_rate   | 0.00467  |
|    n_updates       | 1620     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -144     |
| time/              |          |
|    episodes        | 332      |
|    fps             | 621      |
|    time_elapsed    | 534      |
|    total_timesteps | 332000   |
---------------------------------
Eval num_timesteps=333000, episode_reward=-159.07 +/- 218.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 333000   |
---------------------------------
Eval num_timesteps=334000, episode_reward=-7.74 +/- 182.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.74    |
| time/              |          |
|    total_timesteps | 334000   |
| train/             |          |
|    actor_loss      | -3.08    |
|    critic_loss     | 0.00483  |
|    ent_coef        | 0.00131  |
|    ent_coef_loss   | -20.7    |
|    learning_rate   | 0.00467  |
|    n_updates       | 1630     |
---------------------------------
Eval num_timesteps=335000, episode_reward=22.56 +/- 156.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 22.6     |
| time/              |          |
|    total_timesteps | 335000   |
---------------------------------
Eval num_timesteps=336000, episode_reward=-1098.68 +/- 31.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.1e+03 |
| time/              |          |
|    total_timesteps | 336000   |
| train/             |          |
|    actor_loss      | -3.05    |
|    critic_loss     | 0.00574  |
|    ent_coef        | 0.00128  |
|    ent_coef_loss   | -22.4    |
|    learning_rate   | 0.00466  |
|    n_updates       | 1640     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -141     |
| time/              |          |
|    episodes        | 336      |
|    fps             | 621      |
|    time_elapsed    | 540      |
|    total_timesteps | 336000   |
---------------------------------
Eval num_timesteps=337000, episode_reward=-1123.43 +/- 16.05
Episode length: 1000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1e+03     |
|    mean_reward     | -1.12e+03 |
| time/              |           |
|    total_timesteps | 337000    |
----------------------------------
Eval num_timesteps=338000, episode_reward=-511.13 +/- 428.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 338000   |
| train/             |          |
|    actor_loss      | -3.03    |
|    critic_loss     | 0.00738  |
|    ent_coef        | 0.00125  |
|    ent_coef_loss   | -20.8    |
|    learning_rate   | 0.00466  |
|    n_updates       | 1650     |
---------------------------------
Eval num_timesteps=339000, episode_reward=-377.22 +/- 432.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 339000   |
---------------------------------
Eval num_timesteps=340000, episode_reward=-118.83 +/- 12.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 340000   |
| train/             |          |
|    actor_loss      | -3.04    |
|    critic_loss     | 0.00699  |
|    ent_coef        | 0.00122  |
|    ent_coef_loss   | -23.8    |
|    learning_rate   | 0.00466  |
|    n_updates       | 1660     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -144     |
| time/              |          |
|    episodes        | 340      |
|    fps             | 622      |
|    time_elapsed    | 546      |
|    total_timesteps | 340000   |
---------------------------------
Eval num_timesteps=341000, episode_reward=-136.17 +/- 31.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 341000   |
---------------------------------
Eval num_timesteps=342000, episode_reward=-119.48 +/- 11.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 342000   |
---------------------------------
Eval num_timesteps=343000, episode_reward=-185.80 +/- 28.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 343000   |
| train/             |          |
|    actor_loss      | -2.98    |
|    critic_loss     | 0.00482  |
|    ent_coef        | 0.0012   |
|    ent_coef_loss   | -8.17    |
|    learning_rate   | 0.00466  |
|    n_updates       | 1670     |
---------------------------------
Eval num_timesteps=344000, episode_reward=-166.73 +/- 21.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 344000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 344      |
|    fps             | 622      |
|    time_elapsed    | 552      |
|    total_timesteps | 344000   |
---------------------------------
Eval num_timesteps=345000, episode_reward=-360.48 +/- 7.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 345000   |
| train/             |          |
|    actor_loss      | -3       |
|    critic_loss     | 0.00503  |
|    ent_coef        | 0.00118  |
|    ent_coef_loss   | -18.2    |
|    learning_rate   | 0.00466  |
|    n_updates       | 1680     |
---------------------------------
Eval num_timesteps=346000, episode_reward=-324.32 +/- 68.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -324     |
| time/              |          |
|    total_timesteps | 346000   |
---------------------------------
Eval num_timesteps=347000, episode_reward=-293.73 +/- 115.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 347000   |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.00515  |
|    ent_coef        | 0.00116  |
|    ent_coef_loss   | -21.3    |
|    learning_rate   | 0.00465  |
|    n_updates       | 1690     |
---------------------------------
Eval num_timesteps=348000, episode_reward=-268.56 +/- 164.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 348000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -153     |
| time/              |          |
|    episodes        | 348      |
|    fps             | 622      |
|    time_elapsed    | 559      |
|    total_timesteps | 348000   |
---------------------------------
Eval num_timesteps=349000, episode_reward=-470.39 +/- 236.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -470     |
| time/              |          |
|    total_timesteps | 349000   |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.00591  |
|    ent_coef        | 0.00114  |
|    ent_coef_loss   | -16.8    |
|    learning_rate   | 0.00465  |
|    n_updates       | 1700     |
---------------------------------
Eval num_timesteps=350000, episode_reward=-388.53 +/- 174.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -389     |
| time/              |          |
|    total_timesteps | 350000   |
---------------------------------
Eval num_timesteps=351000, episode_reward=-987.24 +/- 38.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -987     |
| time/              |          |
|    total_timesteps | 351000   |
| train/             |          |
|    actor_loss      | -3.05    |
|    critic_loss     | 0.00729  |
|    ent_coef        | 0.00112  |
|    ent_coef_loss   | -8.21    |
|    learning_rate   | 0.00465  |
|    n_updates       | 1710     |
---------------------------------
Eval num_timesteps=352000, episode_reward=-1003.93 +/- 39.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1e+03   |
| time/              |          |
|    total_timesteps | 352000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -162     |
| time/              |          |
|    episodes        | 352      |
|    fps             | 622      |
|    time_elapsed    | 565      |
|    total_timesteps | 352000   |
---------------------------------
Eval num_timesteps=353000, episode_reward=-659.11 +/- 45.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -659     |
| time/              |          |
|    total_timesteps | 353000   |
| train/             |          |
|    actor_loss      | -3.04    |
|    critic_loss     | 0.00724  |
|    ent_coef        | 0.00111  |
|    ent_coef_loss   | -6.96    |
|    learning_rate   | 0.00465  |
|    n_updates       | 1720     |
---------------------------------
Eval num_timesteps=354000, episode_reward=-630.82 +/- 28.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -631     |
| time/              |          |
|    total_timesteps | 354000   |
---------------------------------
Eval num_timesteps=355000, episode_reward=-822.66 +/- 9.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -823     |
| time/              |          |
|    total_timesteps | 355000   |
| train/             |          |
|    actor_loss      | -3.17    |
|    critic_loss     | 0.00763  |
|    ent_coef        | 0.0011   |
|    ent_coef_loss   | -15.2    |
|    learning_rate   | 0.00465  |
|    n_updates       | 1730     |
---------------------------------
Eval num_timesteps=356000, episode_reward=-818.60 +/- 13.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -819     |
| time/              |          |
|    total_timesteps | 356000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -165     |
| time/              |          |
|    episodes        | 356      |
|    fps             | 622      |
|    time_elapsed    | 571      |
|    total_timesteps | 356000   |
---------------------------------
Eval num_timesteps=357000, episode_reward=-1094.06 +/- 58.48
Episode length: 1000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1e+03     |
|    mean_reward     | -1.09e+03 |
| time/              |           |
|    total_timesteps | 357000    |
| train/             |           |
|    actor_loss      | -3.05     |
|    critic_loss     | 0.00535   |
|    ent_coef        | 0.00108   |
|    ent_coef_loss   | -17.7     |
|    learning_rate   | 0.00464   |
|    n_updates       | 1740      |
----------------------------------
Eval num_timesteps=358000, episode_reward=-1061.37 +/- 69.52
Episode length: 1000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1e+03     |
|    mean_reward     | -1.06e+03 |
| time/              |           |
|    total_timesteps | 358000    |
----------------------------------
Eval num_timesteps=359000, episode_reward=238.31 +/- 69.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 238      |
| time/              |          |
|    total_timesteps | 359000   |
| train/             |          |
|    actor_loss      | -3.08    |
|    critic_loss     | 0.00501  |
|    ent_coef        | 0.00107  |
|    ent_coef_loss   | -21.4    |
|    learning_rate   | 0.00464  |
|    n_updates       | 1750     |
---------------------------------
Eval num_timesteps=360000, episode_reward=240.45 +/- 122.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 360000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -168     |
| time/              |          |
|    episodes        | 360      |
|    fps             | 623      |
|    time_elapsed    | 577      |
|    total_timesteps | 360000   |
---------------------------------
Eval num_timesteps=361000, episode_reward=158.32 +/- 46.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 158      |
| time/              |          |
|    total_timesteps | 361000   |
| train/             |          |
|    actor_loss      | -2.98    |
|    critic_loss     | 0.00671  |
|    ent_coef        | 0.00105  |
|    ent_coef_loss   | -10.6    |
|    learning_rate   | 0.00464  |
|    n_updates       | 1760     |
---------------------------------
Eval num_timesteps=362000, episode_reward=201.66 +/- 38.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 362000   |
---------------------------------
Eval num_timesteps=363000, episode_reward=-11.30 +/- 39.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -11.3    |
| time/              |          |
|    total_timesteps | 363000   |
| train/             |          |
|    actor_loss      | -2.98    |
|    critic_loss     | 0.00491  |
|    ent_coef        | 0.00104  |
|    ent_coef_loss   | 1.84     |
|    learning_rate   | 0.00464  |
|    n_updates       | 1770     |
---------------------------------
Eval num_timesteps=364000, episode_reward=12.37 +/- 47.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 12.4     |
| time/              |          |
|    total_timesteps | 364000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -155     |
| time/              |          |
|    episodes        | 364      |
|    fps             | 623      |
|    time_elapsed    | 584      |
|    total_timesteps | 364000   |
---------------------------------
Eval num_timesteps=365000, episode_reward=256.39 +/- 46.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 256      |
| time/              |          |
|    total_timesteps | 365000   |
| train/             |          |
|    actor_loss      | -3.7     |
|    critic_loss     | 0.0279   |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | 2.2      |
|    learning_rate   | 0.00464  |
|    n_updates       | 1780     |
---------------------------------
Eval num_timesteps=366000, episode_reward=258.55 +/- 81.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 259      |
| time/              |          |
|    total_timesteps | 366000   |
---------------------------------
Eval num_timesteps=367000, episode_reward=197.43 +/- 55.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 197      |
| time/              |          |
|    total_timesteps | 367000   |
| train/             |          |
|    actor_loss      | -3.21    |
|    critic_loss     | 0.0129   |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | 2.88     |
|    learning_rate   | 0.00463  |
|    n_updates       | 1790     |
---------------------------------
Eval num_timesteps=368000, episode_reward=217.54 +/- 36.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 218      |
| time/              |          |
|    total_timesteps | 368000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -146     |
| time/              |          |
|    episodes        | 368      |
|    fps             | 623      |
|    time_elapsed    | 590      |
|    total_timesteps | 368000   |
---------------------------------
Eval num_timesteps=369000, episode_reward=157.31 +/- 76.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 157      |
| time/              |          |
|    total_timesteps | 369000   |
| train/             |          |
|    actor_loss      | -2.93    |
|    critic_loss     | 0.00798  |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | 2.64     |
|    learning_rate   | 0.00463  |
|    n_updates       | 1800     |
---------------------------------
Eval num_timesteps=370000, episode_reward=187.78 +/- 48.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 370000   |
---------------------------------
Eval num_timesteps=371000, episode_reward=370.40 +/- 48.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 370      |
| time/              |          |
|    total_timesteps | 371000   |
| train/             |          |
|    actor_loss      | -2.96    |
|    critic_loss     | 0.00554  |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -7.19    |
|    learning_rate   | 0.00463  |
|    n_updates       | 1810     |
---------------------------------
Eval num_timesteps=372000, episode_reward=187.78 +/- 290.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 372000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -139     |
| time/              |          |
|    episodes        | 372      |
|    fps             | 623      |
|    time_elapsed    | 596      |
|    total_timesteps | 372000   |
---------------------------------
Eval num_timesteps=373000, episode_reward=291.94 +/- 62.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 373000   |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.00544  |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -5.35    |
|    learning_rate   | 0.00463  |
|    n_updates       | 1820     |
---------------------------------
Eval num_timesteps=374000, episode_reward=296.11 +/- 63.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 374000   |
---------------------------------
Eval num_timesteps=375000, episode_reward=321.83 +/- 47.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 375000   |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.00605  |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -4.79    |
|    learning_rate   | 0.00463  |
|    n_updates       | 1830     |
---------------------------------
Eval num_timesteps=376000, episode_reward=312.65 +/- 51.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 313      |
| time/              |          |
|    total_timesteps | 376000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -134     |
| time/              |          |
|    episodes        | 376      |
|    fps             | 623      |
|    time_elapsed    | 602      |
|    total_timesteps | 376000   |
---------------------------------
Eval num_timesteps=377000, episode_reward=167.57 +/- 204.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 377000   |
| train/             |          |
|    actor_loss      | -2.97    |
|    critic_loss     | 0.00458  |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | -13.9    |
|    learning_rate   | 0.00462  |
|    n_updates       | 1840     |
---------------------------------
Eval num_timesteps=378000, episode_reward=128.89 +/- 179.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 129      |
| time/              |          |
|    total_timesteps | 378000   |
---------------------------------
Eval num_timesteps=379000, episode_reward=-98.36 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -98.4    |
| time/              |          |
|    total_timesteps | 379000   |
| train/             |          |
|    actor_loss      | -3.08    |
|    critic_loss     | 0.00582  |
|    ent_coef        | 0.00101  |
|    ent_coef_loss   | -26.5    |
|    learning_rate   | 0.00462  |
|    n_updates       | 1850     |
---------------------------------
Eval num_timesteps=380000, episode_reward=-99.10 +/- 1.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -99.1    |
| time/              |          |
|    total_timesteps | 380000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -125     |
| time/              |          |
|    episodes        | 380      |
|    fps             | 623      |
|    time_elapsed    | 609      |
|    total_timesteps | 380000   |
---------------------------------
Eval num_timesteps=381000, episode_reward=-88.57 +/- 10.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -88.6    |
| time/              |          |
|    total_timesteps | 381000   |
| train/             |          |
|    actor_loss      | -2.95    |
|    critic_loss     | 0.00417  |
|    ent_coef        | 0.000985 |
|    ent_coef_loss   | -30.1    |
|    learning_rate   | 0.00462  |
|    n_updates       | 1860     |
---------------------------------
Eval num_timesteps=382000, episode_reward=-97.43 +/- 9.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -97.4    |
| time/              |          |
|    total_timesteps | 382000   |
---------------------------------
Eval num_timesteps=383000, episode_reward=-131.85 +/- 56.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 383000   |
| train/             |          |
|    actor_loss      | -2.92    |
|    critic_loss     | 0.00329  |
|    ent_coef        | 0.000958 |
|    ent_coef_loss   | -25.1    |
|    learning_rate   | 0.00462  |
|    n_updates       | 1870     |
---------------------------------
Eval num_timesteps=384000, episode_reward=-87.31 +/- 26.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -87.3    |
| time/              |          |
|    total_timesteps | 384000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -125     |
| time/              |          |
|    episodes        | 384      |
|    fps             | 623      |
|    time_elapsed    | 615      |
|    total_timesteps | 384000   |
---------------------------------
Eval num_timesteps=385000, episode_reward=-125.76 +/- 61.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 385000   |
---------------------------------
Eval num_timesteps=386000, episode_reward=-431.79 +/- 19.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 386000   |
| train/             |          |
|    actor_loss      | -3.02    |
|    critic_loss     | 0.00493  |
|    ent_coef        | 0.000933 |
|    ent_coef_loss   | -26.3    |
|    learning_rate   | 0.00461  |
|    n_updates       | 1880     |
---------------------------------
Eval num_timesteps=387000, episode_reward=-421.58 +/- 39.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 387000   |
---------------------------------
Eval num_timesteps=388000, episode_reward=-1095.73 +/- 177.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.1e+03 |
| time/              |          |
|    total_timesteps | 388000   |
| train/             |          |
|    actor_loss      | -2.91    |
|    critic_loss     | 0.00436  |
|    ent_coef        | 0.000908 |
|    ent_coef_loss   | -23      |
|    learning_rate   | 0.00461  |
|    n_updates       | 1890     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -136     |
| time/              |          |
|    episodes        | 388      |
|    fps             | 624      |
|    time_elapsed    | 621      |
|    total_timesteps | 388000   |
---------------------------------
Eval num_timesteps=389000, episode_reward=-1069.03 +/- 398.99
Episode length: 1000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1e+03     |
|    mean_reward     | -1.07e+03 |
| time/              |           |
|    total_timesteps | 389000    |
----------------------------------
Eval num_timesteps=390000, episode_reward=-311.47 +/- 53.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 390000   |
| train/             |          |
|    actor_loss      | -3.02    |
|    critic_loss     | 0.0048   |
|    ent_coef        | 0.000885 |
|    ent_coef_loss   | -27.8    |
|    learning_rate   | 0.00461  |
|    n_updates       | 1900     |
---------------------------------
Eval num_timesteps=391000, episode_reward=-321.44 +/- 27.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -321     |
| time/              |          |
|    total_timesteps | 391000   |
---------------------------------
Eval num_timesteps=392000, episode_reward=-205.95 +/- 1.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 392000   |
| train/             |          |
|    actor_loss      | -2.84    |
|    critic_loss     | 0.00321  |
|    ent_coef        | 0.000862 |
|    ent_coef_loss   | -21.4    |
|    learning_rate   | 0.00461  |
|    n_updates       | 1910     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -148     |
| time/              |          |
|    episodes        | 392      |
|    fps             | 624      |
|    time_elapsed    | 628      |
|    total_timesteps | 392000   |
---------------------------------
Eval num_timesteps=393000, episode_reward=-207.63 +/- 0.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 393000   |
---------------------------------
Eval num_timesteps=394000, episode_reward=-235.58 +/- 3.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 394000   |
| train/             |          |
|    actor_loss      | -2.87    |
|    critic_loss     | 0.00368  |
|    ent_coef        | 0.000842 |
|    ent_coef_loss   | -18.5    |
|    learning_rate   | 0.00461  |
|    n_updates       | 1920     |
---------------------------------
Eval num_timesteps=395000, episode_reward=-235.52 +/- 2.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 395000   |
---------------------------------
Eval num_timesteps=396000, episode_reward=304.18 +/- 233.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 304      |
| time/              |          |
|    total_timesteps | 396000   |
| train/             |          |
|    actor_loss      | -2.95    |
|    critic_loss     | 0.00405  |
|    ent_coef        | 0.000826 |
|    ent_coef_loss   | -7.87    |
|    learning_rate   | 0.0046   |
|    n_updates       | 1930     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -145     |
| time/              |          |
|    episodes        | 396      |
|    fps             | 624      |
|    time_elapsed    | 634      |
|    total_timesteps | 396000   |
---------------------------------
Eval num_timesteps=397000, episode_reward=423.85 +/- 52.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 397000   |
---------------------------------
Eval num_timesteps=398000, episode_reward=179.81 +/- 70.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 398000   |
| train/             |          |
|    actor_loss      | -2.97    |
|    critic_loss     | 0.00465  |
|    ent_coef        | 0.000816 |
|    ent_coef_loss   | 4.74     |
|    learning_rate   | 0.0046   |
|    n_updates       | 1940     |
---------------------------------
Eval num_timesteps=399000, episode_reward=203.18 +/- 69.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 203      |
| time/              |          |
|    total_timesteps | 399000   |
---------------------------------
Eval num_timesteps=400000, episode_reward=-157.23 +/- 2.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 400000   |
| train/             |          |
|    actor_loss      | -2.99    |
|    critic_loss     | 0.00612  |
|    ent_coef        | 0.000816 |
|    ent_coef_loss   | 20.1     |
|    learning_rate   | 0.0046   |
|    n_updates       | 1950     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -139     |
| time/              |          |
|    episodes        | 400      |
|    fps             | 624      |
|    time_elapsed    | 640      |
|    total_timesteps | 400000   |
---------------------------------
Eval num_timesteps=401000, episode_reward=-156.89 +/- 1.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 401000   |
---------------------------------
Eval num_timesteps=402000, episode_reward=-111.63 +/- 3.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -112     |
| time/              |          |
|    total_timesteps | 402000   |
| train/             |          |
|    actor_loss      | -2.96    |
|    critic_loss     | 0.00505  |
|    ent_coef        | 0.000827 |
|    ent_coef_loss   | 16.7     |
|    learning_rate   | 0.0046   |
|    n_updates       | 1960     |
---------------------------------
Eval num_timesteps=403000, episode_reward=-114.83 +/- 3.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 403000   |
---------------------------------
Eval num_timesteps=404000, episode_reward=-94.52 +/- 2.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -94.5    |
| time/              |          |
|    total_timesteps | 404000   |
| train/             |          |
|    actor_loss      | -2.95    |
|    critic_loss     | 0.00332  |
|    ent_coef        | 0.000836 |
|    ent_coef_loss   | -18.9    |
|    learning_rate   | 0.0046   |
|    n_updates       | 1970     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -131     |
| time/              |          |
|    episodes        | 404      |
|    fps             | 624      |
|    time_elapsed    | 647      |
|    total_timesteps | 404000   |
---------------------------------
Eval num_timesteps=405000, episode_reward=-96.22 +/- 2.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -96.2    |
| time/              |          |
|    total_timesteps | 405000   |
---------------------------------
Eval num_timesteps=406000, episode_reward=-115.74 +/- 2.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 406000   |
| train/             |          |
|    actor_loss      | -2.89    |
|    critic_loss     | 0.00373  |
|    ent_coef        | 0.00083  |
|    ent_coef_loss   | -16.9    |
|    learning_rate   | 0.00459  |
|    n_updates       | 1980     |
---------------------------------
Eval num_timesteps=407000, episode_reward=-113.13 +/- 4.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 407000   |
---------------------------------
Eval num_timesteps=408000, episode_reward=-122.32 +/- 2.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 408000   |
| train/             |          |
|    actor_loss      | -2.91    |
|    critic_loss     | 0.00343  |
|    ent_coef        | 0.000818 |
|    ent_coef_loss   | -16.7    |
|    learning_rate   | 0.00459  |
|    n_updates       | 1990     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -131     |
| time/              |          |
|    episodes        | 408      |
|    fps             | 624      |
|    time_elapsed    | 653      |
|    total_timesteps | 408000   |
---------------------------------
Eval num_timesteps=409000, episode_reward=-121.67 +/- 1.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 409000   |
---------------------------------
Eval num_timesteps=410000, episode_reward=-108.74 +/- 1.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 410000   |
| train/             |          |
|    actor_loss      | -2.93    |
|    critic_loss     | 0.00267  |
|    ent_coef        | 0.000805 |
|    ent_coef_loss   | -20.6    |
|    learning_rate   | 0.00459  |
|    n_updates       | 2000     |
---------------------------------
Eval num_timesteps=411000, episode_reward=-109.64 +/- 2.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 411000   |
---------------------------------
Eval num_timesteps=412000, episode_reward=-379.49 +/- 28.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -379     |
| time/              |          |
|    total_timesteps | 412000   |
| train/             |          |
|    actor_loss      | -2.9     |
|    critic_loss     | 0.00254  |
|    ent_coef        | 0.00079  |
|    ent_coef_loss   | -14.6    |
|    learning_rate   | 0.00459  |
|    n_updates       | 2010     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -121     |
| time/              |          |
|    episodes        | 412      |
|    fps             | 624      |
|    time_elapsed    | 659      |
|    total_timesteps | 412000   |
---------------------------------
Eval num_timesteps=413000, episode_reward=-378.73 +/- 40.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -379     |
| time/              |          |
|    total_timesteps | 413000   |
---------------------------------
Eval num_timesteps=414000, episode_reward=-437.40 +/- 13.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 414000   |
| train/             |          |
|    actor_loss      | -2.93    |
|    critic_loss     | 0.00355  |
|    ent_coef        | 0.000775 |
|    ent_coef_loss   | -26.9    |
|    learning_rate   | 0.00459  |
|    n_updates       | 2020     |
---------------------------------
Eval num_timesteps=415000, episode_reward=-413.83 +/- 50.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 415000   |
---------------------------------
Eval num_timesteps=416000, episode_reward=-250.23 +/- 35.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 416000   |
| train/             |          |
|    actor_loss      | -2.86    |
|    critic_loss     | 0.0031   |
|    ent_coef        | 0.000756 |
|    ent_coef_loss   | -29.4    |
|    learning_rate   | 0.00458  |
|    n_updates       | 2030     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -128     |
| time/              |          |
|    episodes        | 416      |
|    fps             | 624      |
|    time_elapsed    | 665      |
|    total_timesteps | 416000   |
---------------------------------
Eval num_timesteps=417000, episode_reward=-225.43 +/- 42.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 417000   |
---------------------------------
Eval num_timesteps=418000, episode_reward=36.40 +/- 49.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 36.4     |
| time/              |          |
|    total_timesteps | 418000   |
| train/             |          |
|    actor_loss      | -2.85    |
|    critic_loss     | 0.00288  |
|    ent_coef        | 0.000734 |
|    ent_coef_loss   | -30.3    |
|    learning_rate   | 0.00458  |
|    n_updates       | 2040     |
---------------------------------
Eval num_timesteps=419000, episode_reward=76.32 +/- 20.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 76.3     |
| time/              |          |
|    total_timesteps | 419000   |
---------------------------------
Eval num_timesteps=420000, episode_reward=219.51 +/- 132.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 220      |
| time/              |          |
|    total_timesteps | 420000   |
| train/             |          |
|    actor_loss      | -2.84    |
|    critic_loss     | 0.00247  |
|    ent_coef        | 0.000713 |
|    ent_coef_loss   | -17.6    |
|    learning_rate   | 0.00458  |
|    n_updates       | 2050     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -133     |
| time/              |          |
|    episodes        | 420      |
|    fps             | 624      |
|    time_elapsed    | 672      |
|    total_timesteps | 420000   |
---------------------------------
Eval num_timesteps=421000, episode_reward=199.45 +/- 125.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 421000   |
---------------------------------
Eval num_timesteps=422000, episode_reward=-137.91 +/- 116.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 422000   |
| train/             |          |
|    actor_loss      | -2.82    |
|    critic_loss     | 0.00229  |
|    ent_coef        | 0.000695 |
|    ent_coef_loss   | -29.3    |
|    learning_rate   | 0.00458  |
|    n_updates       | 2060     |
---------------------------------
Eval num_timesteps=423000, episode_reward=-47.53 +/- 155.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -47.5    |
| time/              |          |
|    total_timesteps | 423000   |
---------------------------------
Eval num_timesteps=424000, episode_reward=-123.81 +/- 47.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 424000   |
| train/             |          |
|    actor_loss      | -2.79    |
|    critic_loss     | 0.0017   |
|    ent_coef        | 0.000676 |
|    ent_coef_loss   | -22.9    |
|    learning_rate   | 0.00458  |
|    n_updates       | 2070     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -142     |
| time/              |          |
|    episodes        | 424      |
|    fps             | 625      |
|    time_elapsed    | 678      |
|    total_timesteps | 424000   |
---------------------------------
Eval num_timesteps=425000, episode_reward=-105.12 +/- 45.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 425000   |
---------------------------------
Eval num_timesteps=426000, episode_reward=-224.28 +/- 41.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -224     |
| time/              |          |
|    total_timesteps | 426000   |
| train/             |          |
|    actor_loss      | -2.8     |
|    critic_loss     | 0.00119  |
|    ent_coef        | 0.000658 |
|    ent_coef_loss   | -28.5    |
|    learning_rate   | 0.00457  |
|    n_updates       | 2080     |
---------------------------------
Eval num_timesteps=427000, episode_reward=-192.46 +/- 46.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 427000   |
---------------------------------
Eval num_timesteps=428000, episode_reward=-164.60 +/- 35.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 428000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -144     |
| time/              |          |
|    episodes        | 428      |
|    fps             | 625      |
|    time_elapsed    | 684      |
|    total_timesteps | 428000   |
---------------------------------
Eval num_timesteps=429000, episode_reward=-125.37 +/- 21.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 429000   |
| train/             |          |
|    actor_loss      | -2.8     |
|    critic_loss     | 0.00156  |
|    ent_coef        | 0.000641 |
|    ent_coef_loss   | -11.6    |
|    learning_rate   | 0.00457  |
|    n_updates       | 2090     |
---------------------------------
Eval num_timesteps=430000, episode_reward=-124.92 +/- 10.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 430000   |
---------------------------------
Eval num_timesteps=431000, episode_reward=-269.42 +/- 40.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 431000   |
| train/             |          |
|    actor_loss      | -3.32    |
|    critic_loss     | 0.00742  |
|    ent_coef        | 0.000634 |
|    ent_coef_loss   | 25.1     |
|    learning_rate   | 0.00457  |
|    n_updates       | 2100     |
---------------------------------
Eval num_timesteps=432000, episode_reward=-197.81 +/- 40.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 432000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -152     |
| time/              |          |
|    episodes        | 432      |
|    fps             | 625      |
|    time_elapsed    | 690      |
|    total_timesteps | 432000   |
---------------------------------
Eval num_timesteps=433000, episode_reward=-280.27 +/- 212.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -280     |
| time/              |          |
|    total_timesteps | 433000   |
| train/             |          |
|    actor_loss      | -2.8     |
|    critic_loss     | 0.00217  |
|    ent_coef        | 0.000636 |
|    ent_coef_loss   | -15.5    |
|    learning_rate   | 0.00457  |
|    n_updates       | 2110     |
---------------------------------
Eval num_timesteps=434000, episode_reward=-418.62 +/- 47.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -419     |
| time/              |          |
|    total_timesteps | 434000   |
---------------------------------
Eval num_timesteps=435000, episode_reward=-303.82 +/- 48.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 435000   |
| train/             |          |
|    actor_loss      | -2.86    |
|    critic_loss     | 0.00205  |
|    ent_coef        | 0.000632 |
|    ent_coef_loss   | -23.4    |
|    learning_rate   | 0.00457  |
|    n_updates       | 2120     |
---------------------------------
Eval num_timesteps=436000, episode_reward=-370.96 +/- 36.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 436000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 436      |
|    fps             | 625      |
|    time_elapsed    | 696      |
|    total_timesteps | 436000   |
---------------------------------
Eval num_timesteps=437000, episode_reward=-121.44 +/- 87.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 437000   |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.00192  |
|    ent_coef        | 0.000619 |
|    ent_coef_loss   | -32.5    |
|    learning_rate   | 0.00456  |
|    n_updates       | 2130     |
---------------------------------
Eval num_timesteps=438000, episode_reward=-180.36 +/- 56.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 438000   |
---------------------------------
Eval num_timesteps=439000, episode_reward=-5.44 +/- 132.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.44    |
| time/              |          |
|    total_timesteps | 439000   |
| train/             |          |
|    actor_loss      | -3.3     |
|    critic_loss     | 0.00736  |
|    ent_coef        | 0.000603 |
|    ent_coef_loss   | -2.01    |
|    learning_rate   | 0.00456  |
|    n_updates       | 2140     |
---------------------------------
Eval num_timesteps=440000, episode_reward=-24.48 +/- 175.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -24.5    |
| time/              |          |
|    total_timesteps | 440000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -137     |
| time/              |          |
|    episodes        | 440      |
|    fps             | 625      |
|    time_elapsed    | 703      |
|    total_timesteps | 440000   |
---------------------------------
Eval num_timesteps=441000, episode_reward=261.99 +/- 147.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 441000   |
| train/             |          |
|    actor_loss      | -2.96    |
|    critic_loss     | 0.00725  |
|    ent_coef        | 0.000598 |
|    ent_coef_loss   | 21.6     |
|    learning_rate   | 0.00456  |
|    n_updates       | 2150     |
---------------------------------
Eval num_timesteps=442000, episode_reward=172.90 +/- 265.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 173      |
| time/              |          |
|    total_timesteps | 442000   |
---------------------------------
Eval num_timesteps=443000, episode_reward=-217.79 +/- 52.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 443000   |
| train/             |          |
|    actor_loss      | -3.23    |
|    critic_loss     | 0.00381  |
|    ent_coef        | 0.000607 |
|    ent_coef_loss   | 46.7     |
|    learning_rate   | 0.00456  |
|    n_updates       | 2160     |
---------------------------------
Eval num_timesteps=444000, episode_reward=-268.35 +/- 92.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 444000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -134     |
| time/              |          |
|    episodes        | 444      |
|    fps             | 625      |
|    time_elapsed    | 709      |
|    total_timesteps | 444000   |
---------------------------------
Eval num_timesteps=445000, episode_reward=212.73 +/- 177.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 445000   |
| train/             |          |
|    actor_loss      | -2.81    |
|    critic_loss     | 0.00221  |
|    ent_coef        | 0.000624 |
|    ent_coef_loss   | -11.9    |
|    learning_rate   | 0.00456  |
|    n_updates       | 2170     |
---------------------------------
Eval num_timesteps=446000, episode_reward=287.76 +/- 131.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 446000   |
---------------------------------
Eval num_timesteps=447000, episode_reward=-376.36 +/- 43.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 447000   |
| train/             |          |
|    actor_loss      | -2.87    |
|    critic_loss     | 0.00253  |
|    ent_coef        | 0.000629 |
|    ent_coef_loss   | 6.34     |
|    learning_rate   | 0.00455  |
|    n_updates       | 2180     |
---------------------------------
Eval num_timesteps=448000, episode_reward=-402.96 +/- 89.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 448000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -127     |
| time/              |          |
|    episodes        | 448      |
|    fps             | 625      |
|    time_elapsed    | 715      |
|    total_timesteps | 448000   |
---------------------------------
Eval num_timesteps=449000, episode_reward=179.56 +/- 96.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 180      |
| time/              |          |
|    total_timesteps | 449000   |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.00217  |
|    ent_coef        | 0.000632 |
|    ent_coef_loss   | 2.73     |
|    learning_rate   | 0.00455  |
|    n_updates       | 2190     |
---------------------------------
Eval num_timesteps=450000, episode_reward=141.18 +/- 263.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 141      |
| time/              |          |
|    total_timesteps | 450000   |
---------------------------------
Eval num_timesteps=451000, episode_reward=-625.52 +/- 67.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -626     |
| time/              |          |
|    total_timesteps | 451000   |
| train/             |          |
|    actor_loss      | -2.88    |
|    critic_loss     | 0.00297  |
|    ent_coef        | 0.000634 |
|    ent_coef_loss   | -3.55    |
|    learning_rate   | 0.00455  |
|    n_updates       | 2200     |
---------------------------------
Eval num_timesteps=452000, episode_reward=-601.18 +/- 155.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -601     |
| time/              |          |
|    total_timesteps | 452000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -112     |
| time/              |          |
|    episodes        | 452      |
|    fps             | 626      |
|    time_elapsed    | 721      |
|    total_timesteps | 452000   |
---------------------------------
Eval num_timesteps=453000, episode_reward=-180.03 +/- 288.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 453000   |
| train/             |          |
|    actor_loss      | -2.88    |
|    critic_loss     | 0.00308  |
|    ent_coef        | 0.000633 |
|    ent_coef_loss   | -11.4    |
|    learning_rate   | 0.00455  |
|    n_updates       | 2210     |
---------------------------------
Eval num_timesteps=454000, episode_reward=-556.71 +/- 41.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -557     |
| time/              |          |
|    total_timesteps | 454000   |
---------------------------------
Eval num_timesteps=455000, episode_reward=-16.88 +/- 43.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -16.9    |
| time/              |          |
|    total_timesteps | 455000   |
| train/             |          |
|    actor_loss      | -2.86    |
|    critic_loss     | 0.0032   |
|    ent_coef        | 0.000626 |
|    ent_coef_loss   | -35.8    |
|    learning_rate   | 0.00455  |
|    n_updates       | 2220     |
---------------------------------
Eval num_timesteps=456000, episode_reward=15.46 +/- 104.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 15.5     |
| time/              |          |
|    total_timesteps | 456000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -109     |
| time/              |          |
|    episodes        | 456      |
|    fps             | 626      |
|    time_elapsed    | 728      |
|    total_timesteps | 456000   |
---------------------------------
Eval num_timesteps=457000, episode_reward=224.02 +/- 46.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 224      |
| time/              |          |
|    total_timesteps | 457000   |
| train/             |          |
|    actor_loss      | -2.86    |
|    critic_loss     | 0.00262  |
|    ent_coef        | 0.000609 |
|    ent_coef_loss   | -20.3    |
|    learning_rate   | 0.00454  |
|    n_updates       | 2230     |
---------------------------------
Eval num_timesteps=458000, episode_reward=260.71 +/- 44.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 261      |
| time/              |          |
|    total_timesteps | 458000   |
---------------------------------
Eval num_timesteps=459000, episode_reward=346.27 +/- 74.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 346      |
| time/              |          |
|    total_timesteps | 459000   |
| train/             |          |
|    actor_loss      | -2.86    |
|    critic_loss     | 0.00167  |
|    ent_coef        | 0.000597 |
|    ent_coef_loss   | 7.5      |
|    learning_rate   | 0.00454  |
|    n_updates       | 2240     |
---------------------------------
Eval num_timesteps=460000, episode_reward=429.31 +/- 59.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 460000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -96.3    |
| time/              |          |
|    episodes        | 460      |
|    fps             | 626      |
|    time_elapsed    | 734      |
|    total_timesteps | 460000   |
---------------------------------
Eval num_timesteps=461000, episode_reward=13.81 +/- 100.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 13.8     |
| time/              |          |
|    total_timesteps | 461000   |
| train/             |          |
|    actor_loss      | -2.92    |
|    critic_loss     | 0.00236  |
|    ent_coef        | 0.000593 |
|    ent_coef_loss   | -14.1    |
|    learning_rate   | 0.00454  |
|    n_updates       | 2250     |
---------------------------------
Eval num_timesteps=462000, episode_reward=-104.50 +/- 53.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 462000   |
---------------------------------
Eval num_timesteps=463000, episode_reward=-19.26 +/- 190.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -19.3    |
| time/              |          |
|    total_timesteps | 463000   |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.0026   |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | 25.6     |
|    learning_rate   | 0.00454  |
|    n_updates       | 2260     |
---------------------------------
Eval num_timesteps=464000, episode_reward=124.91 +/- 267.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 125      |
| time/              |          |
|    total_timesteps | 464000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -93.1    |
| time/              |          |
|    episodes        | 464      |
|    fps             | 626      |
|    time_elapsed    | 740      |
|    total_timesteps | 464000   |
---------------------------------
Eval num_timesteps=465000, episode_reward=-101.46 +/- 189.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -101     |
| time/              |          |
|    total_timesteps | 465000   |
| train/             |          |
|    actor_loss      | -2.84    |
|    critic_loss     | 0.0021   |
|    ent_coef        | 0.000596 |
|    ent_coef_loss   | 4.4      |
|    learning_rate   | 0.00454  |
|    n_updates       | 2270     |
---------------------------------
Eval num_timesteps=466000, episode_reward=-63.27 +/- 161.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -63.3    |
| time/              |          |
|    total_timesteps | 466000   |
---------------------------------
Eval num_timesteps=467000, episode_reward=298.68 +/- 68.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 299      |
| time/              |          |
|    total_timesteps | 467000   |
| train/             |          |
|    actor_loss      | -2.83    |
|    critic_loss     | 0.00239  |
|    ent_coef        | 0.000599 |
|    ent_coef_loss   | -14.4    |
|    learning_rate   | 0.00453  |
|    n_updates       | 2280     |
---------------------------------
Eval num_timesteps=468000, episode_reward=265.95 +/- 62.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 266      |
| time/              |          |
|    total_timesteps | 468000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -92.3    |
| time/              |          |
|    episodes        | 468      |
|    fps             | 626      |
|    time_elapsed    | 746      |
|    total_timesteps | 468000   |
---------------------------------
Eval num_timesteps=469000, episode_reward=-86.59 +/- 83.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -86.6    |
| time/              |          |
|    total_timesteps | 469000   |
| train/             |          |
|    actor_loss      | -2.85    |
|    critic_loss     | 0.00237  |
|    ent_coef        | 0.000595 |
|    ent_coef_loss   | -8.24    |
|    learning_rate   | 0.00453  |
|    n_updates       | 2290     |
---------------------------------
Eval num_timesteps=470000, episode_reward=-134.70 +/- 27.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 470000   |
---------------------------------
Eval num_timesteps=471000, episode_reward=-105.47 +/- 56.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 471000   |
---------------------------------
Eval num_timesteps=472000, episode_reward=633.44 +/- 134.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 472000   |
| train/             |          |
|    actor_loss      | -2.87    |
|    critic_loss     | 0.00216  |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | 8.7      |
|    learning_rate   | 0.00453  |
|    n_updates       | 2300     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -96.2    |
| time/              |          |
|    episodes        | 472      |
|    fps             | 626      |
|    time_elapsed    | 753      |
|    total_timesteps | 472000   |
---------------------------------
Eval num_timesteps=473000, episode_reward=545.00 +/- 118.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 545      |
| time/              |          |
|    total_timesteps | 473000   |
---------------------------------
Eval num_timesteps=474000, episode_reward=-312.02 +/- 1.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 474000   |
| train/             |          |
|    actor_loss      | -2.93    |
|    critic_loss     | 0.00246  |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | -11.4    |
|    learning_rate   | 0.00453  |
|    n_updates       | 2310     |
---------------------------------
Eval num_timesteps=475000, episode_reward=-311.16 +/- 1.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 475000   |
---------------------------------
Eval num_timesteps=476000, episode_reward=-351.40 +/- 1.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -351     |
| time/              |          |
|    total_timesteps | 476000   |
| train/             |          |
|    actor_loss      | -2.96    |
|    critic_loss     | 0.00489  |
|    ent_coef        | 0.000588 |
|    ent_coef_loss   | 15.9     |
|    learning_rate   | 0.00452  |
|    n_updates       | 2320     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -99.8    |
| time/              |          |
|    episodes        | 476      |
|    fps             | 626      |
|    time_elapsed    | 759      |
|    total_timesteps | 476000   |
---------------------------------
Eval num_timesteps=477000, episode_reward=-350.38 +/- 1.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 477000   |
---------------------------------
Eval num_timesteps=478000, episode_reward=-366.69 +/- 1.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -367     |
| time/              |          |
|    total_timesteps | 478000   |
| train/             |          |
|    actor_loss      | -2.89    |
|    critic_loss     | 0.00369  |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | -10.7    |
|    learning_rate   | 0.00452  |
|    n_updates       | 2330     |
---------------------------------
Eval num_timesteps=479000, episode_reward=-363.78 +/- 2.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 479000   |
---------------------------------
Eval num_timesteps=480000, episode_reward=-478.01 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -478     |
| time/              |          |
|    total_timesteps | 480000   |
| train/             |          |
|    actor_loss      | -2.86    |
|    critic_loss     | 0.00474  |
|    ent_coef        | 0.000592 |
|    ent_coef_loss   | 36.1     |
|    learning_rate   | 0.00452  |
|    n_updates       | 2340     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 480      |
|    fps             | 626      |
|    time_elapsed    | 766      |
|    total_timesteps | 480000   |
---------------------------------
Eval num_timesteps=481000, episode_reward=-477.81 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -478     |
| time/              |          |
|    total_timesteps | 481000   |
---------------------------------
Eval num_timesteps=482000, episode_reward=-543.30 +/- 359.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 482000   |
| train/             |          |
|    actor_loss      | -2.68    |
|    critic_loss     | 0.326    |
|    ent_coef        | 0.000606 |
|    ent_coef_loss   | 12.7     |
|    learning_rate   | 0.00452  |
|    n_updates       | 2350     |
---------------------------------
Eval num_timesteps=483000, episode_reward=-341.82 +/- 256.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -342     |
| time/              |          |
|    total_timesteps | 483000   |
---------------------------------
Eval num_timesteps=484000, episode_reward=-163.60 +/- 122.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 484000   |
| train/             |          |
|    actor_loss      | -2.56    |
|    critic_loss     | 0.111    |
|    ent_coef        | 0.000617 |
|    ent_coef_loss   | 15.2     |
|    learning_rate   | 0.00452  |
|    n_updates       | 2360     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -110     |
| time/              |          |
|    episodes        | 484      |
|    fps             | 626      |
|    time_elapsed    | 772      |
|    total_timesteps | 484000   |
---------------------------------
Eval num_timesteps=485000, episode_reward=-345.04 +/- 167.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -345     |
| time/              |          |
|    total_timesteps | 485000   |
---------------------------------
Eval num_timesteps=486000, episode_reward=-757.30 +/- 396.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -757     |
| time/              |          |
|    total_timesteps | 486000   |
| train/             |          |
|    actor_loss      | -2.61    |
|    critic_loss     | 0.0536   |
|    ent_coef        | 0.000638 |
|    ent_coef_loss   | 146      |
|    learning_rate   | 0.00451  |
|    n_updates       | 2370     |
---------------------------------
Eval num_timesteps=487000, episode_reward=-912.21 +/- 298.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -912     |
| time/              |          |
|    total_timesteps | 487000   |
---------------------------------
Eval num_timesteps=488000, episode_reward=-1073.67 +/- 43.97
Episode length: 1000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1e+03     |
|    mean_reward     | -1.07e+03 |
| time/              |           |
|    total_timesteps | 488000    |
| train/             |           |
|    actor_loss      | -2.63     |
|    critic_loss     | 0.0249    |
|    ent_coef        | 0.000719  |
|    ent_coef_loss   | 263       |
|    learning_rate   | 0.00451   |
|    n_updates       | 2380      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 488      |
|    fps             | 626      |
|    time_elapsed    | 778      |
|    total_timesteps | 488000   |
---------------------------------
Eval num_timesteps=489000, episode_reward=-1100.03 +/- 49.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -1.1e+03 |
| time/              |          |
|    total_timesteps | 489000   |
---------------------------------
Eval num_timesteps=490000, episode_reward=-566.01 +/- 93.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 490000   |
| train/             |          |
|    actor_loss      | -2.85    |
|    critic_loss     | 0.0312   |
|    ent_coef        | 0.000866 |
|    ent_coef_loss   | 224      |
|    learning_rate   | 0.00451  |
|    n_updates       | 2390     |
---------------------------------
Eval num_timesteps=491000, episode_reward=-683.84 +/- 181.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -684     |
| time/              |          |
|    total_timesteps | 491000   |
---------------------------------
Eval num_timesteps=492000, episode_reward=-427.76 +/- 105.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 492000   |
| train/             |          |
|    actor_loss      | -2.84    |
|    critic_loss     | 0.0198   |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | 92.3     |
|    learning_rate   | 0.00451  |
|    n_updates       | 2400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -115     |
| time/              |          |
|    episodes        | 492      |
|    fps             | 626      |
|    time_elapsed    | 785      |
|    total_timesteps | 492000   |
---------------------------------
Eval num_timesteps=493000, episode_reward=-427.40 +/- 78.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 493000   |
---------------------------------
Eval num_timesteps=494000, episode_reward=-287.04 +/- 59.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 494000   |
| train/             |          |
|    actor_loss      | -2.89    |
|    critic_loss     | 0.00877  |
|    ent_coef        | 0.00116  |
|    ent_coef_loss   | 90.2     |
|    learning_rate   | 0.00451  |
|    n_updates       | 2410     |
---------------------------------
Eval num_timesteps=495000, episode_reward=-324.43 +/- 82.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -324     |
| time/              |          |
|    total_timesteps | 495000   |
---------------------------------
Eval num_timesteps=496000, episode_reward=-355.73 +/- 9.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -356     |
| time/              |          |
|    total_timesteps | 496000   |
| train/             |          |
|    actor_loss      | -2.74    |
|    critic_loss     | 0.00696  |
|    ent_coef        | 0.00126  |
|    ent_coef_loss   | 55.1     |
|    learning_rate   | 0.0045   |
|    n_updates       | 2420     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -134     |
| time/              |          |
|    episodes        | 496      |
|    fps             | 626      |
|    time_elapsed    | 791      |
|    total_timesteps | 496000   |
---------------------------------
Eval num_timesteps=497000, episode_reward=-368.10 +/- 43.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 497000   |
---------------------------------
Eval num_timesteps=498000, episode_reward=-468.25 +/- 101.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -468     |
| time/              |          |
|    total_timesteps | 498000   |
| train/             |          |
|    actor_loss      | -2.65    |
|    critic_loss     | 0.00696  |
|    ent_coef        | 0.00134  |
|    ent_coef_loss   | 10       |
|    learning_rate   | 0.0045   |
|    n_updates       | 2430     |
---------------------------------
Eval num_timesteps=499000, episode_reward=-522.55 +/- 194.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 499000   |
---------------------------------
Eval num_timesteps=500000, episode_reward=-364.22 +/- 1.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 500000   |
| train/             |          |
|    actor_loss      | -2.55    |
|    critic_loss     | 0.00542  |
|    ent_coef        | 0.00138  |
|    ent_coef_loss   | 0.281    |
|    learning_rate   | 0.0045   |
|    n_updates       | 2440     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -158     |
| time/              |          |
|    episodes        | 500      |
|    fps             | 627      |
|    time_elapsed    | 797      |
|    total_timesteps | 500000   |
---------------------------------
Eval num_timesteps=501000, episode_reward=-364.51 +/- 3.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -365     |
| time/              |          |
|    total_timesteps | 501000   |
---------------------------------
Eval num_timesteps=502000, episode_reward=-344.89 +/- 96.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -345     |
| time/              |          |
|    total_timesteps | 502000   |
| train/             |          |
|    actor_loss      | -2.85    |
|    critic_loss     | 0.00463  |
|    ent_coef        | 0.00139  |
|    ent_coef_loss   | 17.2     |
|    learning_rate   | 0.0045   |
|    n_updates       | 2450     |
---------------------------------
Eval num_timesteps=503000, episode_reward=-358.94 +/- 60.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -359     |
| time/              |          |
|    total_timesteps | 503000   |
---------------------------------
Eval num_timesteps=504000, episode_reward=-297.25 +/- 143.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 504000   |
| train/             |          |
|    actor_loss      | -2.52    |
|    critic_loss     | 0.00541  |
|    ent_coef        | 0.00141  |
|    ent_coef_loss   | 22.1     |
|    learning_rate   | 0.0045   |
|    n_updates       | 2460     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -182     |
| time/              |          |
|    episodes        | 504      |
|    fps             | 627      |
|    time_elapsed    | 803      |
|    total_timesteps | 504000   |
---------------------------------
Eval num_timesteps=505000, episode_reward=-635.41 +/- 310.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -635     |
| time/              |          |
|    total_timesteps | 505000   |
---------------------------------
Eval num_timesteps=506000, episode_reward=-897.91 +/- 316.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -898     |
| time/              |          |
|    total_timesteps | 506000   |
| train/             |          |
|    actor_loss      | -2.57    |
|    critic_loss     | 0.00713  |
|    ent_coef        | 0.00144  |
|    ent_coef_loss   | 25.2     |
|    learning_rate   | 0.00449  |
|    n_updates       | 2470     |
---------------------------------
Eval num_timesteps=507000, episode_reward=-922.08 +/- 261.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -922     |
| time/              |          |
|    total_timesteps | 507000   |
---------------------------------
Eval num_timesteps=508000, episode_reward=-512.52 +/- 192.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 508000   |
| train/             |          |
|    actor_loss      | -3.06    |
|    critic_loss     | 0.00744  |
|    ent_coef        | 0.00146  |
|    ent_coef_loss   | 12       |
|    learning_rate   | 0.00449  |
|    n_updates       | 2480     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -197     |
| time/              |          |
|    episodes        | 508      |
|    fps             | 627      |
|    time_elapsed    | 809      |
|    total_timesteps | 508000   |
---------------------------------
Eval num_timesteps=509000, episode_reward=-447.26 +/- 204.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -447     |
| time/              |          |
|    total_timesteps | 509000   |
---------------------------------
Eval num_timesteps=510000, episode_reward=-778.38 +/- 79.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -778     |
| time/              |          |
|    total_timesteps | 510000   |
| train/             |          |
|    actor_loss      | -2.59    |
|    critic_loss     | 0.00645  |
|    ent_coef        | 0.00149  |
|    ent_coef_loss   | 70.6     |
|    learning_rate   | 0.00449  |
|    n_updates       | 2490     |
---------------------------------
Eval num_timesteps=511000, episode_reward=-755.76 +/- 193.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -756     |
| time/              |          |
|    total_timesteps | 511000   |
---------------------------------
Eval num_timesteps=512000, episode_reward=-886.74 +/- 24.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -887     |
| time/              |          |
|    total_timesteps | 512000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -224     |
| time/              |          |
|    episodes        | 512      |
|    fps             | 627      |
|    time_elapsed    | 815      |
|    total_timesteps | 512000   |
---------------------------------
Eval num_timesteps=513000, episode_reward=-778.20 +/- 19.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -778     |
| time/              |          |
|    total_timesteps | 513000   |
| train/             |          |
|    actor_loss      | -2.53    |
|    critic_loss     | 0.00606  |
|    ent_coef        | 0.00156  |
|    ent_coef_loss   | 64.4     |
|    learning_rate   | 0.00449  |
|    n_updates       | 2500     |
---------------------------------
Eval num_timesteps=514000, episode_reward=-761.31 +/- 22.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -761     |
| time/              |          |
|    total_timesteps | 514000   |
---------------------------------
Eval num_timesteps=515000, episode_reward=-511.99 +/- 2.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 515000   |
| train/             |          |
|    actor_loss      | -2.59    |
|    critic_loss     | 0.00796  |
|    ent_coef        | 0.00165  |
|    ent_coef_loss   | 69.8     |
|    learning_rate   | 0.00449  |
|    n_updates       | 2510     |
---------------------------------
Eval num_timesteps=516000, episode_reward=-509.82 +/- 2.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 516000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -233     |
| time/              |          |
|    episodes        | 516      |
|    fps             | 627      |
|    time_elapsed    | 821      |
|    total_timesteps | 516000   |
---------------------------------
Eval num_timesteps=517000, episode_reward=-321.97 +/- 7.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -322     |
| time/              |          |
|    total_timesteps | 517000   |
| train/             |          |
|    actor_loss      | -2.41    |
|    critic_loss     | 0.0311   |
|    ent_coef        | 0.00174  |
|    ent_coef_loss   | -18.4    |
|    learning_rate   | 0.00448  |
|    n_updates       | 2520     |
---------------------------------
Eval num_timesteps=518000, episode_reward=-325.73 +/- 4.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -326     |
| time/              |          |
|    total_timesteps | 518000   |
---------------------------------
Eval num_timesteps=519000, episode_reward=-164.53 +/- 200.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 519000   |
| train/             |          |
|    actor_loss      | -2.54    |
|    critic_loss     | 0.00819  |
|    ent_coef        | 0.00177  |
|    ent_coef_loss   | 1.27     |
|    learning_rate   | 0.00448  |
|    n_updates       | 2530     |
---------------------------------
Eval num_timesteps=520000, episode_reward=-361.62 +/- 244.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -362     |
| time/              |          |
|    total_timesteps | 520000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -241     |
| time/              |          |
|    episodes        | 520      |
|    fps             | 628      |
|    time_elapsed    | 827      |
|    total_timesteps | 520000   |
---------------------------------
Eval num_timesteps=521000, episode_reward=-147.80 +/- 1.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 521000   |
| train/             |          |
|    actor_loss      | -2.55    |
|    critic_loss     | 0.0079   |
|    ent_coef        | 0.00178  |
|    ent_coef_loss   | 16       |
|    learning_rate   | 0.00448  |
|    n_updates       | 2540     |
---------------------------------
Eval num_timesteps=522000, episode_reward=-142.41 +/- 6.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 522000   |
---------------------------------
Eval num_timesteps=523000, episode_reward=-232.01 +/- 1.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 523000   |
| train/             |          |
|    actor_loss      | -2.58    |
|    critic_loss     | 0.00437  |
|    ent_coef        | 0.00179  |
|    ent_coef_loss   | -6.6     |
|    learning_rate   | 0.00448  |
|    n_updates       | 2550     |
---------------------------------
Eval num_timesteps=524000, episode_reward=-231.02 +/- 2.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 524000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -243     |
| time/              |          |
|    episodes        | 524      |
|    fps             | 628      |
|    time_elapsed    | 834      |
|    total_timesteps | 524000   |
---------------------------------
Eval num_timesteps=525000, episode_reward=-312.99 +/- 2.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 525000   |
| train/             |          |
|    actor_loss      | -2.63    |
|    critic_loss     | 0.00317  |
|    ent_coef        | 0.00178  |
|    ent_coef_loss   | -6.08    |
|    learning_rate   | 0.00448  |
|    n_updates       | 2560     |
---------------------------------
Eval num_timesteps=526000, episode_reward=-311.76 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 526000   |
---------------------------------
Eval num_timesteps=527000, episode_reward=43.40 +/- 12.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 43.4     |
| time/              |          |
|    total_timesteps | 527000   |
| train/             |          |
|    actor_loss      | -2.51    |
|    critic_loss     | 0.00718  |
|    ent_coef        | 0.00179  |
|    ent_coef_loss   | 18.2     |
|    learning_rate   | 0.00447  |
|    n_updates       | 2570     |
---------------------------------
Eval num_timesteps=528000, episode_reward=82.51 +/- 34.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 82.5     |
| time/              |          |
|    total_timesteps | 528000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -254     |
| time/              |          |
|    episodes        | 528      |
|    fps             | 628      |
|    time_elapsed    | 840      |
|    total_timesteps | 528000   |
---------------------------------
Eval num_timesteps=529000, episode_reward=-73.88 +/- 4.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -73.9    |
| time/              |          |
|    total_timesteps | 529000   |
| train/             |          |
|    actor_loss      | -2.54    |
|    critic_loss     | 0.00469  |
|    ent_coef        | 0.0018   |
|    ent_coef_loss   | 4.17     |
|    learning_rate   | 0.00447  |
|    n_updates       | 2580     |
---------------------------------
Eval num_timesteps=530000, episode_reward=-69.94 +/- 15.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -69.9    |
| time/              |          |
|    total_timesteps | 530000   |
---------------------------------
Eval num_timesteps=531000, episode_reward=-99.81 +/- 30.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -99.8    |
| time/              |          |
|    total_timesteps | 531000   |
| train/             |          |
|    actor_loss      | -2.55    |
|    critic_loss     | 0.00407  |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | -12.7    |
|    learning_rate   | 0.00447  |
|    n_updates       | 2590     |
---------------------------------
Eval num_timesteps=532000, episode_reward=-91.51 +/- 10.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -91.5    |
| time/              |          |
|    total_timesteps | 532000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -264     |
| time/              |          |
|    episodes        | 532      |
|    fps             | 628      |
|    time_elapsed    | 846      |
|    total_timesteps | 532000   |
---------------------------------
Eval num_timesteps=533000, episode_reward=-146.18 +/- 8.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 533000   |
| train/             |          |
|    actor_loss      | -2.54    |
|    critic_loss     | 0.00284  |
|    ent_coef        | 0.0018   |
|    ent_coef_loss   | -15.1    |
|    learning_rate   | 0.00447  |
|    n_updates       | 2600     |
---------------------------------
Eval num_timesteps=534000, episode_reward=-139.18 +/- 15.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 534000   |
---------------------------------
Eval num_timesteps=535000, episode_reward=-102.30 +/- 4.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -102     |
| time/              |          |
|    total_timesteps | 535000   |
| train/             |          |
|    actor_loss      | -2.52    |
|    critic_loss     | 0.00479  |
|    ent_coef        | 0.00178  |
|    ent_coef_loss   | 18.3     |
|    learning_rate   | 0.00447  |
|    n_updates       | 2610     |
---------------------------------
Eval num_timesteps=536000, episode_reward=-105.14 +/- 3.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 536000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -276     |
| time/              |          |
|    episodes        | 536      |
|    fps             | 628      |
|    time_elapsed    | 852      |
|    total_timesteps | 536000   |
---------------------------------
Eval num_timesteps=537000, episode_reward=-218.47 +/- 19.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 537000   |
| train/             |          |
|    actor_loss      | -2.57    |
|    critic_loss     | 0.00299  |
|    ent_coef        | 0.00179  |
|    ent_coef_loss   | -14.6    |
|    learning_rate   | 0.00446  |
|    n_updates       | 2620     |
---------------------------------
Eval num_timesteps=538000, episode_reward=-213.75 +/- 22.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 538000   |
---------------------------------
Eval num_timesteps=539000, episode_reward=-139.08 +/- 23.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 539000   |
| train/             |          |
|    actor_loss      | -2.56    |
|    critic_loss     | 0.00281  |
|    ent_coef        | 0.00178  |
|    ent_coef_loss   | -9.69    |
|    learning_rate   | 0.00446  |
|    n_updates       | 2630     |
---------------------------------
Eval num_timesteps=540000, episode_reward=-136.83 +/- 14.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 540000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -289     |
| time/              |          |
|    episodes        | 540      |
|    fps             | 628      |
|    time_elapsed    | 858      |
|    total_timesteps | 540000   |
---------------------------------
Eval num_timesteps=541000, episode_reward=-130.37 +/- 65.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 541000   |
| train/             |          |
|    actor_loss      | -2.56    |
|    critic_loss     | 0.00277  |
|    ent_coef        | 0.00176  |
|    ent_coef_loss   | -19.4    |
|    learning_rate   | 0.00446  |
|    n_updates       | 2640     |
---------------------------------
Eval num_timesteps=542000, episode_reward=-66.16 +/- 92.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -66.2    |
| time/              |          |
|    total_timesteps | 542000   |
---------------------------------
Eval num_timesteps=543000, episode_reward=31.80 +/- 54.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 31.8     |
| time/              |          |
|    total_timesteps | 543000   |
| train/             |          |
|    actor_loss      | -2.56    |
|    critic_loss     | 0.00269  |
|    ent_coef        | 0.00174  |
|    ent_coef_loss   | -5.28    |
|    learning_rate   | 0.00446  |
|    n_updates       | 2650     |
---------------------------------
Eval num_timesteps=544000, episode_reward=68.89 +/- 57.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 68.9     |
| time/              |          |
|    total_timesteps | 544000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -287     |
| time/              |          |
|    episodes        | 544      |
|    fps             | 629      |
|    time_elapsed    | 864      |
|    total_timesteps | 544000   |
---------------------------------
Eval num_timesteps=545000, episode_reward=-141.94 +/- 40.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 545000   |
| train/             |          |
|    actor_loss      | -2.55    |
|    critic_loss     | 0.00238  |
|    ent_coef        | 0.00173  |
|    ent_coef_loss   | 1.3      |
|    learning_rate   | 0.00446  |
|    n_updates       | 2660     |
---------------------------------
Eval num_timesteps=546000, episode_reward=-165.76 +/- 36.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 546000   |
---------------------------------
Eval num_timesteps=547000, episode_reward=-312.51 +/- 20.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 547000   |
| train/             |          |
|    actor_loss      | -2.52    |
|    critic_loss     | 0.0014   |
|    ent_coef        | 0.00172  |
|    ent_coef_loss   | -13.1    |
|    learning_rate   | 0.00445  |
|    n_updates       | 2670     |
---------------------------------
Eval num_timesteps=548000, episode_reward=-314.76 +/- 17.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 548000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -295     |
| time/              |          |
|    episodes        | 548      |
|    fps             | 629      |
|    time_elapsed    | 871      |
|    total_timesteps | 548000   |
---------------------------------
Eval num_timesteps=549000, episode_reward=118.70 +/- 143.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 119      |
| time/              |          |
|    total_timesteps | 549000   |
| train/             |          |
|    actor_loss      | -2.52    |
|    critic_loss     | 0.00217  |
|    ent_coef        | 0.00171  |
|    ent_coef_loss   | -2.36    |
|    learning_rate   | 0.00445  |
|    n_updates       | 2680     |
---------------------------------
Eval num_timesteps=550000, episode_reward=212.04 +/- 59.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 550000   |
---------------------------------
Eval num_timesteps=551000, episode_reward=-83.44 +/- 29.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -83.4    |
| time/              |          |
|    total_timesteps | 551000   |
| train/             |          |
|    actor_loss      | -2.58    |
|    critic_loss     | 0.00348  |
|    ent_coef        | 0.0017   |
|    ent_coef_loss   | -1.25    |
|    learning_rate   | 0.00445  |
|    n_updates       | 2690     |
---------------------------------
Eval num_timesteps=552000, episode_reward=-83.31 +/- 30.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -83.3    |
| time/              |          |
|    total_timesteps | 552000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -303     |
| time/              |          |
|    episodes        | 552      |
|    fps             | 629      |
|    time_elapsed    | 877      |
|    total_timesteps | 552000   |
---------------------------------
Eval num_timesteps=553000, episode_reward=75.08 +/- 39.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 75.1     |
| time/              |          |
|    total_timesteps | 553000   |
| train/             |          |
|    actor_loss      | -2.57    |
|    critic_loss     | 0.00284  |
|    ent_coef        | 0.00169  |
|    ent_coef_loss   | -23.8    |
|    learning_rate   | 0.00445  |
|    n_updates       | 2700     |
---------------------------------
Eval num_timesteps=554000, episode_reward=106.73 +/- 71.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 554000   |
---------------------------------
Eval num_timesteps=555000, episode_reward=115.10 +/- 76.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 115      |
| time/              |          |
|    total_timesteps | 555000   |
---------------------------------
Eval num_timesteps=556000, episode_reward=84.25 +/- 32.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 84.3     |
| time/              |          |
|    total_timesteps | 556000   |
| train/             |          |
|    actor_loss      | -2.58    |
|    critic_loss     | 0.00266  |
|    ent_coef        | 0.00167  |
|    ent_coef_loss   | -10.6    |
|    learning_rate   | 0.00444  |
|    n_updates       | 2710     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -300     |
| time/              |          |
|    episodes        | 556      |
|    fps             | 629      |
|    time_elapsed    | 883      |
|    total_timesteps | 556000   |
---------------------------------
Eval num_timesteps=557000, episode_reward=-49.92 +/- 245.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -49.9    |
| time/              |          |
|    total_timesteps | 557000   |
---------------------------------
Eval num_timesteps=558000, episode_reward=36.92 +/- 36.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 36.9     |
| time/              |          |
|    total_timesteps | 558000   |
| train/             |          |
|    actor_loss      | -2.54    |
|    critic_loss     | 0.00258  |
|    ent_coef        | 0.00165  |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.00444  |
|    n_updates       | 2720     |
---------------------------------
Eval num_timesteps=559000, episode_reward=55.70 +/- 18.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 55.7     |
| time/              |          |
|    total_timesteps | 559000   |
---------------------------------
Eval num_timesteps=560000, episode_reward=198.53 +/- 82.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 560000   |
| train/             |          |
|    actor_loss      | -2.54    |
|    critic_loss     | 0.00243  |
|    ent_coef        | 0.00163  |
|    ent_coef_loss   | -0.518   |
|    learning_rate   | 0.00444  |
|    n_updates       | 2730     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -309     |
| time/              |          |
|    episodes        | 560      |
|    fps             | 629      |
|    time_elapsed    | 889      |
|    total_timesteps | 560000   |
---------------------------------
Eval num_timesteps=561000, episode_reward=92.57 +/- 240.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 92.6     |
| time/              |          |
|    total_timesteps | 561000   |
---------------------------------
Eval num_timesteps=562000, episode_reward=8.13 +/- 118.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 8.13     |
| time/              |          |
|    total_timesteps | 562000   |
| train/             |          |
|    actor_loss      | -2.55    |
|    critic_loss     | 0.00237  |
|    ent_coef        | 0.00162  |
|    ent_coef_loss   | -4.65    |
|    learning_rate   | 0.00444  |
|    n_updates       | 2740     |
---------------------------------
Eval num_timesteps=563000, episode_reward=3.46 +/- 67.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.46     |
| time/              |          |
|    total_timesteps | 563000   |
---------------------------------
Eval num_timesteps=564000, episode_reward=24.62 +/- 34.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 24.6     |
| time/              |          |
|    total_timesteps | 564000   |
| train/             |          |
|    actor_loss      | -2.55    |
|    critic_loss     | 0.00257  |
|    ent_coef        | 0.00161  |
|    ent_coef_loss   | -11      |
|    learning_rate   | 0.00444  |
|    n_updates       | 2750     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -316     |
| time/              |          |
|    episodes        | 564      |
|    fps             | 629      |
|    time_elapsed    | 895      |
|    total_timesteps | 564000   |
---------------------------------
Eval num_timesteps=565000, episode_reward=31.75 +/- 32.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 31.7     |
| time/              |          |
|    total_timesteps | 565000   |
---------------------------------
Eval num_timesteps=566000, episode_reward=14.71 +/- 14.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 14.7     |
| time/              |          |
|    total_timesteps | 566000   |
| train/             |          |
|    actor_loss      | -2.53    |
|    critic_loss     | 0.00263  |
|    ent_coef        | 0.0016   |
|    ent_coef_loss   | -23.4    |
|    learning_rate   | 0.00443  |
|    n_updates       | 2760     |
---------------------------------
Eval num_timesteps=567000, episode_reward=4.96 +/- 16.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 4.96     |
| time/              |          |
|    total_timesteps | 567000   |
---------------------------------
Eval num_timesteps=568000, episode_reward=50.07 +/- 60.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 50.1     |
| time/              |          |
|    total_timesteps | 568000   |
| train/             |          |
|    actor_loss      | -2.58    |
|    critic_loss     | 0.00255  |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | -16.6    |
|    learning_rate   | 0.00443  |
|    n_updates       | 2770     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -325     |
| time/              |          |
|    episodes        | 568      |
|    fps             | 629      |
|    time_elapsed    | 901      |
|    total_timesteps | 568000   |
---------------------------------
Eval num_timesteps=569000, episode_reward=-8.47 +/- 23.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.47    |
| time/              |          |
|    total_timesteps | 569000   |
---------------------------------
Eval num_timesteps=570000, episode_reward=-29.44 +/- 33.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -29.4    |
| time/              |          |
|    total_timesteps | 570000   |
| train/             |          |
|    actor_loss      | -2.52    |
|    critic_loss     | 0.00306  |
|    ent_coef        | 0.00154  |
|    ent_coef_loss   | -23.8    |
|    learning_rate   | 0.00443  |
|    n_updates       | 2780     |
---------------------------------
Eval num_timesteps=571000, episode_reward=-27.01 +/- 19.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -27      |
| time/              |          |
|    total_timesteps | 571000   |
---------------------------------
Eval num_timesteps=572000, episode_reward=97.70 +/- 93.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 97.7     |
| time/              |          |
|    total_timesteps | 572000   |
| train/             |          |
|    actor_loss      | -2.54    |
|    critic_loss     | 0.00297  |
|    ent_coef        | 0.00151  |
|    ent_coef_loss   | -21.4    |
|    learning_rate   | 0.00443  |
|    n_updates       | 2790     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -330     |
| time/              |          |
|    episodes        | 572      |
|    fps             | 630      |
|    time_elapsed    | 907      |
|    total_timesteps | 572000   |
---------------------------------
Eval num_timesteps=573000, episode_reward=-39.04 +/- 61.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -39      |
| time/              |          |
|    total_timesteps | 573000   |
---------------------------------
Eval num_timesteps=574000, episode_reward=-57.86 +/- 59.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -57.9    |
| time/              |          |
|    total_timesteps | 574000   |
| train/             |          |
|    actor_loss      | -2.54    |
|    critic_loss     | 0.0026   |
|    ent_coef        | 0.00148  |
|    ent_coef_loss   | -14.8    |
|    learning_rate   | 0.00443  |
|    n_updates       | 2800     |
---------------------------------
Eval num_timesteps=575000, episode_reward=-52.28 +/- 101.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -52.3    |
| time/              |          |
|    total_timesteps | 575000   |
---------------------------------
Eval num_timesteps=576000, episode_reward=-7.32 +/- 200.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.32    |
| time/              |          |
|    total_timesteps | 576000   |
| train/             |          |
|    actor_loss      | -2.61    |
|    critic_loss     | 0.0128   |
|    ent_coef        | 0.00145  |
|    ent_coef_loss   | -7.22    |
|    learning_rate   | 0.00442  |
|    n_updates       | 2810     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -331     |
| time/              |          |
|    episodes        | 576      |
|    fps             | 630      |
|    time_elapsed    | 913      |
|    total_timesteps | 576000   |
---------------------------------
Eval num_timesteps=577000, episode_reward=-30.92 +/- 168.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -30.9    |
| time/              |          |
|    total_timesteps | 577000   |
---------------------------------
Eval num_timesteps=578000, episode_reward=-190.97 +/- 11.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 578000   |
| train/             |          |
|    actor_loss      | -2.51    |
|    critic_loss     | 0.00468  |
|    ent_coef        | 0.00144  |
|    ent_coef_loss   | 12       |
|    learning_rate   | 0.00442  |
|    n_updates       | 2820     |
---------------------------------
Eval num_timesteps=579000, episode_reward=-191.56 +/- 12.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 579000   |
---------------------------------
Eval num_timesteps=580000, episode_reward=-58.67 +/- 16.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -58.7    |
| time/              |          |
|    total_timesteps | 580000   |
| train/             |          |
|    actor_loss      | -2.51    |
|    critic_loss     | 0.00247  |
|    ent_coef        | 0.00144  |
|    ent_coef_loss   | -13.9    |
|    learning_rate   | 0.00442  |
|    n_updates       | 2830     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -329     |
| time/              |          |
|    episodes        | 580      |
|    fps             | 630      |
|    time_elapsed    | 919      |
|    total_timesteps | 580000   |
---------------------------------
Eval num_timesteps=581000, episode_reward=-53.10 +/- 14.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -53.1    |
| time/              |          |
|    total_timesteps | 581000   |
---------------------------------
Eval num_timesteps=582000, episode_reward=-361.75 +/- 18.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -362     |
| time/              |          |
|    total_timesteps | 582000   |
| train/             |          |
|    actor_loss      | -2.53    |
|    critic_loss     | 0.00248  |
|    ent_coef        | 0.00143  |
|    ent_coef_loss   | -30.7    |
|    learning_rate   | 0.00442  |
|    n_updates       | 2840     |
---------------------------------
Eval num_timesteps=583000, episode_reward=-327.63 +/- 55.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -328     |
| time/              |          |
|    total_timesteps | 583000   |
---------------------------------
Eval num_timesteps=584000, episode_reward=-454.94 +/- 144.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -455     |
| time/              |          |
|    total_timesteps | 584000   |
| train/             |          |
|    actor_loss      | -2.46    |
|    critic_loss     | 0.00241  |
|    ent_coef        | 0.00139  |
|    ent_coef_loss   | -27.3    |
|    learning_rate   | 0.00442  |
|    n_updates       | 2850     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -336     |
| time/              |          |
|    episodes        | 584      |
|    fps             | 630      |
|    time_elapsed    | 926      |
|    total_timesteps | 584000   |
---------------------------------
Eval num_timesteps=585000, episode_reward=-451.43 +/- 64.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -451     |
| time/              |          |
|    total_timesteps | 585000   |
---------------------------------
Eval num_timesteps=586000, episode_reward=-129.53 +/- 33.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 586000   |
| train/             |          |
|    actor_loss      | -2.44    |
|    critic_loss     | 0.00279  |
|    ent_coef        | 0.00136  |
|    ent_coef_loss   | -19      |
|    learning_rate   | 0.00441  |
|    n_updates       | 2860     |
---------------------------------
Eval num_timesteps=587000, episode_reward=-174.65 +/- 31.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 587000   |
---------------------------------
Eval num_timesteps=588000, episode_reward=141.22 +/- 37.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 141      |
| time/              |          |
|    total_timesteps | 588000   |
| train/             |          |
|    actor_loss      | -2.5     |
|    critic_loss     | 0.00325  |
|    ent_coef        | 0.00133  |
|    ent_coef_loss   | 4.09     |
|    learning_rate   | 0.00441  |
|    n_updates       | 2870     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -341     |
| time/              |          |
|    episodes        | 588      |
|    fps             | 630      |
|    time_elapsed    | 932      |
|    total_timesteps | 588000   |
---------------------------------
Eval num_timesteps=589000, episode_reward=130.87 +/- 54.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 131      |
| time/              |          |
|    total_timesteps | 589000   |
---------------------------------
Eval num_timesteps=590000, episode_reward=1.35 +/- 13.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35     |
| time/              |          |
|    total_timesteps | 590000   |
| train/             |          |
|    actor_loss      | -2.53    |
|    critic_loss     | 0.00249  |
|    ent_coef        | 0.00133  |
|    ent_coef_loss   | 8.79     |
|    learning_rate   | 0.00441  |
|    n_updates       | 2880     |
---------------------------------
Eval num_timesteps=591000, episode_reward=-5.95 +/- 11.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.95    |
| time/              |          |
|    total_timesteps | 591000   |
---------------------------------
Eval num_timesteps=592000, episode_reward=-272.11 +/- 472.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 592000   |
| train/             |          |
|    actor_loss      | -2.69    |
|    critic_loss     | 0.00866  |
|    ent_coef        | 0.00134  |
|    ent_coef_loss   | 17       |
|    learning_rate   | 0.00441  |
|    n_updates       | 2890     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -320     |
| time/              |          |
|    episodes        | 592      |
|    fps             | 630      |
|    time_elapsed    | 938      |
|    total_timesteps | 592000   |
---------------------------------
Eval num_timesteps=593000, episode_reward=-707.52 +/- 548.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -708     |
| time/              |          |
|    total_timesteps | 593000   |
---------------------------------
Eval num_timesteps=594000, episode_reward=-166.47 +/- 83.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 594000   |
| train/             |          |
|    actor_loss      | -2.61    |
|    critic_loss     | 0.00345  |
|    ent_coef        | 0.00135  |
|    ent_coef_loss   | -13.1    |
|    learning_rate   | 0.00441  |
|    n_updates       | 2900     |
---------------------------------
Eval num_timesteps=595000, episode_reward=-101.45 +/- 5.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -101     |
| time/              |          |
|    total_timesteps | 595000   |
---------------------------------
Eval num_timesteps=596000, episode_reward=-62.35 +/- 11.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -62.3    |
| time/              |          |
|    total_timesteps | 596000   |
| train/             |          |
|    actor_loss      | -2.49    |
|    critic_loss     | 0.00291  |
|    ent_coef        | 0.00134  |
|    ent_coef_loss   | -21.4    |
|    learning_rate   | 0.0044   |
|    n_updates       | 2910     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -315     |
| time/              |          |
|    episodes        | 596      |
|    fps             | 631      |
|    time_elapsed    | 944      |
|    total_timesteps | 596000   |
---------------------------------
Eval num_timesteps=597000, episode_reward=-64.67 +/- 8.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -64.7    |
| time/              |          |
|    total_timesteps | 597000   |
---------------------------------
Eval num_timesteps=598000, episode_reward=-42.76 +/- 11.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -42.8    |
| time/              |          |
|    total_timesteps | 598000   |
---------------------------------
Eval num_timesteps=599000, episode_reward=-63.41 +/- 10.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -63.4    |
| time/              |          |
|    total_timesteps | 599000   |
| train/             |          |
|    actor_loss      | -2.55    |
|    critic_loss     | 0.00264  |
|    ent_coef        | 0.00132  |
|    ent_coef_loss   | -22.7    |
|    learning_rate   | 0.0044   |
|    n_updates       | 2920     |
---------------------------------
Eval num_timesteps=600000, episode_reward=-48.95 +/- 22.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -48.9    |
| time/              |          |
|    total_timesteps | 600000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -301     |
| time/              |          |
|    episodes        | 600      |
|    fps             | 631      |
|    time_elapsed    | 950      |
|    total_timesteps | 600000   |
---------------------------------
Eval num_timesteps=601000, episode_reward=-143.85 +/- 8.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 601000   |
| train/             |          |
|    actor_loss      | -2.5     |
|    critic_loss     | 0.00195  |
|    ent_coef        | 0.00129  |
|    ent_coef_loss   | -21.7    |
|    learning_rate   | 0.0044   |
|    n_updates       | 2930     |
---------------------------------
Eval num_timesteps=602000, episode_reward=-139.44 +/- 16.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 602000   |
---------------------------------
Eval num_timesteps=603000, episode_reward=-175.37 +/- 50.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 603000   |
| train/             |          |
|    actor_loss      | -2.52    |
|    critic_loss     | 0.00182  |
|    ent_coef        | 0.00127  |
|    ent_coef_loss   | -22.2    |
|    learning_rate   | 0.0044   |
|    n_updates       | 2940     |
---------------------------------
Eval num_timesteps=604000, episode_reward=-153.33 +/- 34.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 604000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -286     |
| time/              |          |
|    episodes        | 604      |
|    fps             | 631      |
|    time_elapsed    | 956      |
|    total_timesteps | 604000   |
---------------------------------
Eval num_timesteps=605000, episode_reward=-158.17 +/- 31.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 605000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.00192  |
|    ent_coef        | 0.00124  |
|    ent_coef_loss   | -20.3    |
|    learning_rate   | 0.0044   |
|    n_updates       | 2950     |
---------------------------------
Eval num_timesteps=606000, episode_reward=-128.00 +/- 32.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 606000   |
---------------------------------
Eval num_timesteps=607000, episode_reward=-186.88 +/- 87.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 607000   |
| train/             |          |
|    actor_loss      | -2.44    |
|    critic_loss     | 0.0021   |
|    ent_coef        | 0.00121  |
|    ent_coef_loss   | -8.61    |
|    learning_rate   | 0.00439  |
|    n_updates       | 2960     |
---------------------------------
Eval num_timesteps=608000, episode_reward=-180.67 +/- 112.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 608000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -271     |
| time/              |          |
|    episodes        | 608      |
|    fps             | 631      |
|    time_elapsed    | 962      |
|    total_timesteps | 608000   |
---------------------------------
Eval num_timesteps=609000, episode_reward=-84.85 +/- 79.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -84.9    |
| time/              |          |
|    total_timesteps | 609000   |
| train/             |          |
|    actor_loss      | -2.52    |
|    critic_loss     | 0.00292  |
|    ent_coef        | 0.00119  |
|    ent_coef_loss   | -15.3    |
|    learning_rate   | 0.00439  |
|    n_updates       | 2970     |
---------------------------------
Eval num_timesteps=610000, episode_reward=-187.94 +/- 21.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -188     |
| time/              |          |
|    total_timesteps | 610000   |
---------------------------------
Eval num_timesteps=611000, episode_reward=-123.58 +/- 16.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 611000   |
| train/             |          |
|    actor_loss      | -2.5     |
|    critic_loss     | 0.00236  |
|    ent_coef        | 0.00118  |
|    ent_coef_loss   | -8.31    |
|    learning_rate   | 0.00439  |
|    n_updates       | 2980     |
---------------------------------
Eval num_timesteps=612000, episode_reward=-142.03 +/- 30.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 612000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -245     |
| time/              |          |
|    episodes        | 612      |
|    fps             | 631      |
|    time_elapsed    | 969      |
|    total_timesteps | 612000   |
---------------------------------
Eval num_timesteps=613000, episode_reward=-41.52 +/- 68.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -41.5    |
| time/              |          |
|    total_timesteps | 613000   |
| train/             |          |
|    actor_loss      | -2.5     |
|    critic_loss     | 0.00173  |
|    ent_coef        | 0.00117  |
|    ent_coef_loss   | -8.6     |
|    learning_rate   | 0.00439  |
|    n_updates       | 2990     |
---------------------------------
Eval num_timesteps=614000, episode_reward=-93.06 +/- 79.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -93.1    |
| time/              |          |
|    total_timesteps | 614000   |
---------------------------------
Eval num_timesteps=615000, episode_reward=-76.01 +/- 34.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -76      |
| time/              |          |
|    total_timesteps | 615000   |
| train/             |          |
|    actor_loss      | -2.76    |
|    critic_loss     | 0.00237  |
|    ent_coef        | 0.00115  |
|    ent_coef_loss   | -12.8    |
|    learning_rate   | 0.00439  |
|    n_updates       | 3000     |
---------------------------------
Eval num_timesteps=616000, episode_reward=-143.37 +/- 85.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 616000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -232     |
| time/              |          |
|    episodes        | 616      |
|    fps             | 631      |
|    time_elapsed    | 975      |
|    total_timesteps | 616000   |
---------------------------------
Eval num_timesteps=617000, episode_reward=-146.61 +/- 58.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 617000   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00224  |
|    ent_coef        | 0.00114  |
|    ent_coef_loss   | -12      |
|    learning_rate   | 0.00438  |
|    n_updates       | 3010     |
---------------------------------
Eval num_timesteps=618000, episode_reward=-67.47 +/- 56.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -67.5    |
| time/              |          |
|    total_timesteps | 618000   |
---------------------------------
Eval num_timesteps=619000, episode_reward=64.64 +/- 52.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 64.6     |
| time/              |          |
|    total_timesteps | 619000   |
| train/             |          |
|    actor_loss      | -2.58    |
|    critic_loss     | 0.00238  |
|    ent_coef        | 0.00112  |
|    ent_coef_loss   | -19.7    |
|    learning_rate   | 0.00438  |
|    n_updates       | 3020     |
---------------------------------
Eval num_timesteps=620000, episode_reward=63.74 +/- 52.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 63.7     |
| time/              |          |
|    total_timesteps | 620000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -220     |
| time/              |          |
|    episodes        | 620      |
|    fps             | 631      |
|    time_elapsed    | 981      |
|    total_timesteps | 620000   |
---------------------------------
Eval num_timesteps=621000, episode_reward=245.14 +/- 29.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 245      |
| time/              |          |
|    total_timesteps | 621000   |
| train/             |          |
|    actor_loss      | -2.46    |
|    critic_loss     | 0.00192  |
|    ent_coef        | 0.00111  |
|    ent_coef_loss   | -15.1    |
|    learning_rate   | 0.00438  |
|    n_updates       | 3030     |
---------------------------------
Eval num_timesteps=622000, episode_reward=276.00 +/- 36.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 276      |
| time/              |          |
|    total_timesteps | 622000   |
---------------------------------
Eval num_timesteps=623000, episode_reward=200.69 +/- 40.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 201      |
| time/              |          |
|    total_timesteps | 623000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.00185  |
|    ent_coef        | 0.00109  |
|    ent_coef_loss   | -16.6    |
|    learning_rate   | 0.00438  |
|    n_updates       | 3040     |
---------------------------------
Eval num_timesteps=624000, episode_reward=184.61 +/- 30.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 624000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -205     |
| time/              |          |
|    episodes        | 624      |
|    fps             | 632      |
|    time_elapsed    | 987      |
|    total_timesteps | 624000   |
---------------------------------
Eval num_timesteps=625000, episode_reward=132.76 +/- 29.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 133      |
| time/              |          |
|    total_timesteps | 625000   |
| train/             |          |
|    actor_loss      | -2.46    |
|    critic_loss     | 0.00175  |
|    ent_coef        | 0.00107  |
|    ent_coef_loss   | -17.8    |
|    learning_rate   | 0.00438  |
|    n_updates       | 3050     |
---------------------------------
Eval num_timesteps=626000, episode_reward=110.33 +/- 16.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 110      |
| time/              |          |
|    total_timesteps | 626000   |
---------------------------------
Eval num_timesteps=627000, episode_reward=168.07 +/- 25.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 168      |
| time/              |          |
|    total_timesteps | 627000   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00177  |
|    ent_coef        | 0.00105  |
|    ent_coef_loss   | -5.56    |
|    learning_rate   | 0.00437  |
|    n_updates       | 3060     |
---------------------------------
Eval num_timesteps=628000, episode_reward=193.30 +/- 46.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 628000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -180     |
| time/              |          |
|    episodes        | 628      |
|    fps             | 632      |
|    time_elapsed    | 993      |
|    total_timesteps | 628000   |
---------------------------------
Eval num_timesteps=629000, episode_reward=73.01 +/- 71.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 73       |
| time/              |          |
|    total_timesteps | 629000   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00148  |
|    ent_coef        | 0.00104  |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.00437  |
|    n_updates       | 3070     |
---------------------------------
Eval num_timesteps=630000, episode_reward=106.72 +/- 40.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 630000   |
---------------------------------
Eval num_timesteps=631000, episode_reward=167.16 +/- 40.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 167      |
| time/              |          |
|    total_timesteps | 631000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.00164  |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -13.3    |
|    learning_rate   | 0.00437  |
|    n_updates       | 3080     |
---------------------------------
Eval num_timesteps=632000, episode_reward=195.72 +/- 55.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 196      |
| time/              |          |
|    total_timesteps | 632000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -157     |
| time/              |          |
|    episodes        | 632      |
|    fps             | 632      |
|    time_elapsed    | 999      |
|    total_timesteps | 632000   |
---------------------------------
Eval num_timesteps=633000, episode_reward=217.07 +/- 29.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 633000   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00185  |
|    ent_coef        | 0.00101  |
|    ent_coef_loss   | -16.4    |
|    learning_rate   | 0.00437  |
|    n_updates       | 3090     |
---------------------------------
Eval num_timesteps=634000, episode_reward=226.66 +/- 12.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 227      |
| time/              |          |
|    total_timesteps | 634000   |
---------------------------------
Eval num_timesteps=635000, episode_reward=294.85 +/- 29.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 295      |
| time/              |          |
|    total_timesteps | 635000   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00182  |
|    ent_coef        | 0.000998 |
|    ent_coef_loss   | -12.7    |
|    learning_rate   | 0.00437  |
|    n_updates       | 3100     |
---------------------------------
Eval num_timesteps=636000, episode_reward=308.73 +/- 18.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 309      |
| time/              |          |
|    total_timesteps | 636000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -137     |
| time/              |          |
|    episodes        | 636      |
|    fps             | 632      |
|    time_elapsed    | 1005     |
|    total_timesteps | 636000   |
---------------------------------
Eval num_timesteps=637000, episode_reward=276.62 +/- 28.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 277      |
| time/              |          |
|    total_timesteps | 637000   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00183  |
|    ent_coef        | 0.000983 |
|    ent_coef_loss   | -12.5    |
|    learning_rate   | 0.00436  |
|    n_updates       | 3110     |
---------------------------------
Eval num_timesteps=638000, episode_reward=285.05 +/- 46.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 638000   |
---------------------------------
Eval num_timesteps=639000, episode_reward=461.52 +/- 29.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 462      |
| time/              |          |
|    total_timesteps | 639000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.00179  |
|    ent_coef        | 0.00097  |
|    ent_coef_loss   | -11.2    |
|    learning_rate   | 0.00436  |
|    n_updates       | 3120     |
---------------------------------
Eval num_timesteps=640000, episode_reward=416.57 +/- 29.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 640000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -119     |
| time/              |          |
|    episodes        | 640      |
|    fps             | 632      |
|    time_elapsed    | 1011     |
|    total_timesteps | 640000   |
---------------------------------
Eval num_timesteps=641000, episode_reward=466.45 +/- 17.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 641000   |
---------------------------------
Eval num_timesteps=642000, episode_reward=493.55 +/- 26.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 494      |
| time/              |          |
|    total_timesteps | 642000   |
| train/             |          |
|    actor_loss      | -2.49    |
|    critic_loss     | 0.0018   |
|    ent_coef        | 0.000958 |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.00436  |
|    n_updates       | 3130     |
---------------------------------
Eval num_timesteps=643000, episode_reward=511.78 +/- 14.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 512      |
| time/              |          |
|    total_timesteps | 643000   |
---------------------------------
Eval num_timesteps=644000, episode_reward=200.45 +/- 30.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 200      |
| time/              |          |
|    total_timesteps | 644000   |
| train/             |          |
|    actor_loss      | -2.5     |
|    critic_loss     | 0.00169  |
|    ent_coef        | 0.000947 |
|    ent_coef_loss   | -6.95    |
|    learning_rate   | 0.00436  |
|    n_updates       | 3140     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -102     |
| time/              |          |
|    episodes        | 644      |
|    fps             | 632      |
|    time_elapsed    | 1017     |
|    total_timesteps | 644000   |
---------------------------------
Eval num_timesteps=645000, episode_reward=193.04 +/- 21.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 193      |
| time/              |          |
|    total_timesteps | 645000   |
---------------------------------
Eval num_timesteps=646000, episode_reward=250.78 +/- 19.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 251      |
| time/              |          |
|    total_timesteps | 646000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.00153  |
|    ent_coef        | 0.000938 |
|    ent_coef_loss   | -12.4    |
|    learning_rate   | 0.00435  |
|    n_updates       | 3150     |
---------------------------------
Eval num_timesteps=647000, episode_reward=248.99 +/- 18.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 249      |
| time/              |          |
|    total_timesteps | 647000   |
---------------------------------
Eval num_timesteps=648000, episode_reward=258.11 +/- 54.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 258      |
| time/              |          |
|    total_timesteps | 648000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.00115  |
|    ent_coef        | 0.000927 |
|    ent_coef_loss   | -15.8    |
|    learning_rate   | 0.00435  |
|    n_updates       | 3160     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -89.1    |
| time/              |          |
|    episodes        | 648      |
|    fps             | 632      |
|    time_elapsed    | 1023     |
|    total_timesteps | 648000   |
---------------------------------
Eval num_timesteps=649000, episode_reward=236.30 +/- 51.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 236      |
| time/              |          |
|    total_timesteps | 649000   |
---------------------------------
Eval num_timesteps=650000, episode_reward=194.55 +/- 20.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 195      |
| time/              |          |
|    total_timesteps | 650000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.00121  |
|    ent_coef        | 0.000914 |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.00435  |
|    n_updates       | 3170     |
---------------------------------
Eval num_timesteps=651000, episode_reward=212.04 +/- 29.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 212      |
| time/              |          |
|    total_timesteps | 651000   |
---------------------------------
Eval num_timesteps=652000, episode_reward=223.00 +/- 27.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 223      |
| time/              |          |
|    total_timesteps | 652000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000901 |
|    ent_coef_loss   | -18.2    |
|    learning_rate   | 0.00435  |
|    n_updates       | 3180     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -73.8    |
| time/              |          |
|    episodes        | 652      |
|    fps             | 633      |
|    time_elapsed    | 1029     |
|    total_timesteps | 652000   |
---------------------------------
Eval num_timesteps=653000, episode_reward=240.00 +/- 24.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 240      |
| time/              |          |
|    total_timesteps | 653000   |
---------------------------------
Eval num_timesteps=654000, episode_reward=-290.91 +/- 5.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 654000   |
| train/             |          |
|    actor_loss      | -2.45    |
|    critic_loss     | 0.000752 |
|    ent_coef        | 0.000885 |
|    ent_coef_loss   | -20.6    |
|    learning_rate   | 0.00435  |
|    n_updates       | 3190     |
---------------------------------
Eval num_timesteps=655000, episode_reward=-289.43 +/- 6.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -289     |
| time/              |          |
|    total_timesteps | 655000   |
---------------------------------
Eval num_timesteps=656000, episode_reward=-360.40 +/- 9.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 656000   |
| train/             |          |
|    actor_loss      | -2.44    |
|    critic_loss     | 0.000388 |
|    ent_coef        | 0.000869 |
|    ent_coef_loss   | -1.12    |
|    learning_rate   | 0.00434  |
|    n_updates       | 3200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -73.1    |
| time/              |          |
|    episodes        | 656      |
|    fps             | 633      |
|    time_elapsed    | 1036     |
|    total_timesteps | 656000   |
---------------------------------
Eval num_timesteps=657000, episode_reward=-340.04 +/- 10.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 657000   |
---------------------------------
Eval num_timesteps=658000, episode_reward=-136.21 +/- 20.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 658000   |
| train/             |          |
|    actor_loss      | -2.41    |
|    critic_loss     | 0.000275 |
|    ent_coef        | 0.000862 |
|    ent_coef_loss   | -5.23    |
|    learning_rate   | 0.00434  |
|    n_updates       | 3210     |
---------------------------------
Eval num_timesteps=659000, episode_reward=-154.00 +/- 20.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 659000   |
---------------------------------
Eval num_timesteps=660000, episode_reward=-223.86 +/- 3.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -224     |
| time/              |          |
|    total_timesteps | 660000   |
| train/             |          |
|    actor_loss      | -2.41    |
|    critic_loss     | 0.000411 |
|    ent_coef        | 0.000857 |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.00434  |
|    n_updates       | 3220     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -82.4    |
| time/              |          |
|    episodes        | 660      |
|    fps             | 633      |
|    time_elapsed    | 1042     |
|    total_timesteps | 660000   |
---------------------------------
Eval num_timesteps=661000, episode_reward=-225.08 +/- 3.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 661000   |
---------------------------------
Eval num_timesteps=662000, episode_reward=-160.99 +/- 97.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 662000   |
| train/             |          |
|    actor_loss      | -2.44    |
|    critic_loss     | 0.000771 |
|    ent_coef        | 0.000853 |
|    ent_coef_loss   | -9.4     |
|    learning_rate   | 0.00434  |
|    n_updates       | 3230     |
---------------------------------
Eval num_timesteps=663000, episode_reward=-193.03 +/- 61.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 663000   |
---------------------------------
Eval num_timesteps=664000, episode_reward=-120.39 +/- 4.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 664000   |
| train/             |          |
|    actor_loss      | -2.5     |
|    critic_loss     | 0.000741 |
|    ent_coef        | 0.000848 |
|    ent_coef_loss   | -12.5    |
|    learning_rate   | 0.00434  |
|    n_updates       | 3240     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -87.3    |
| time/              |          |
|    episodes        | 664      |
|    fps             | 633      |
|    time_elapsed    | 1048     |
|    total_timesteps | 664000   |
---------------------------------
Eval num_timesteps=665000, episode_reward=-113.13 +/- 3.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 665000   |
---------------------------------
Eval num_timesteps=666000, episode_reward=-266.98 +/- 5.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 666000   |
| train/             |          |
|    actor_loss      | -2.39    |
|    critic_loss     | 0.000896 |
|    ent_coef        | 0.000838 |
|    ent_coef_loss   | -24.1    |
|    learning_rate   | 0.00433  |
|    n_updates       | 3250     |
---------------------------------
Eval num_timesteps=667000, episode_reward=-270.02 +/- 4.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 667000   |
---------------------------------
Eval num_timesteps=668000, episode_reward=-154.81 +/- 9.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 668000   |
| train/             |          |
|    actor_loss      | -2.49    |
|    critic_loss     | 0.00121  |
|    ent_coef        | 0.000822 |
|    ent_coef_loss   | -5.97    |
|    learning_rate   | 0.00433  |
|    n_updates       | 3260     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -91.3    |
| time/              |          |
|    episodes        | 668      |
|    fps             | 633      |
|    time_elapsed    | 1054     |
|    total_timesteps | 668000   |
---------------------------------
Eval num_timesteps=669000, episode_reward=-125.77 +/- 33.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 669000   |
---------------------------------
Eval num_timesteps=670000, episode_reward=382.71 +/- 25.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 383      |
| time/              |          |
|    total_timesteps | 670000   |
| train/             |          |
|    actor_loss      | -2.62    |
|    critic_loss     | 0.000957 |
|    ent_coef        | 0.000811 |
|    ent_coef_loss   | -16.6    |
|    learning_rate   | 0.00433  |
|    n_updates       | 3270     |
---------------------------------
Eval num_timesteps=671000, episode_reward=416.71 +/- 26.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 417      |
| time/              |          |
|    total_timesteps | 671000   |
---------------------------------
Eval num_timesteps=672000, episode_reward=230.17 +/- 8.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 230      |
| time/              |          |
|    total_timesteps | 672000   |
| train/             |          |
|    actor_loss      | -2.44    |
|    critic_loss     | 0.00157  |
|    ent_coef        | 0.000799 |
|    ent_coef_loss   | -5.8     |
|    learning_rate   | 0.00433  |
|    n_updates       | 3280     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -83.6    |
| time/              |          |
|    episodes        | 672      |
|    fps             | 633      |
|    time_elapsed    | 1060     |
|    total_timesteps | 672000   |
---------------------------------
Eval num_timesteps=673000, episode_reward=244.67 +/- 15.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 245      |
| time/              |          |
|    total_timesteps | 673000   |
---------------------------------
Eval num_timesteps=674000, episode_reward=-98.03 +/- 258.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -98      |
| time/              |          |
|    total_timesteps | 674000   |
| train/             |          |
|    actor_loss      | -2.42    |
|    critic_loss     | 0.0012   |
|    ent_coef        | 0.00079  |
|    ent_coef_loss   | -18.1    |
|    learning_rate   | 0.00433  |
|    n_updates       | 3290     |
---------------------------------
Eval num_timesteps=675000, episode_reward=-165.86 +/- 1.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 675000   |
---------------------------------
Eval num_timesteps=676000, episode_reward=360.10 +/- 33.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 360      |
| time/              |          |
|    total_timesteps | 676000   |
| train/             |          |
|    actor_loss      | -2.59    |
|    critic_loss     | 0.00122  |
|    ent_coef        | 0.000777 |
|    ent_coef_loss   | -17.9    |
|    learning_rate   | 0.00432  |
|    n_updates       | 3300     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -81.6    |
| time/              |          |
|    episodes        | 676      |
|    fps             | 633      |
|    time_elapsed    | 1066     |
|    total_timesteps | 676000   |
---------------------------------
Eval num_timesteps=677000, episode_reward=329.68 +/- 28.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 330      |
| time/              |          |
|    total_timesteps | 677000   |
---------------------------------
Eval num_timesteps=678000, episode_reward=-99.78 +/- 3.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -99.8    |
| time/              |          |
|    total_timesteps | 678000   |
| train/             |          |
|    actor_loss      | -2.43    |
|    critic_loss     | 0.00142  |
|    ent_coef        | 0.000761 |
|    ent_coef_loss   | -15      |
|    learning_rate   | 0.00432  |
|    n_updates       | 3310     |
---------------------------------
Eval num_timesteps=679000, episode_reward=-99.84 +/- 4.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -99.8    |
| time/              |          |
|    total_timesteps | 679000   |
---------------------------------
Eval num_timesteps=680000, episode_reward=-256.57 +/- 3.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 680000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.00183  |
|    ent_coef        | 0.000745 |
|    ent_coef_loss   | -28.4    |
|    learning_rate   | 0.00432  |
|    n_updates       | 3320     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -69.5    |
| time/              |          |
|    episodes        | 680      |
|    fps             | 633      |
|    time_elapsed    | 1073     |
|    total_timesteps | 680000   |
---------------------------------
Eval num_timesteps=681000, episode_reward=-257.88 +/- 1.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 681000   |
---------------------------------
Eval num_timesteps=682000, episode_reward=-92.36 +/- 7.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -92.4    |
| time/              |          |
|    total_timesteps | 682000   |
| train/             |          |
|    actor_loss      | -2.53    |
|    critic_loss     | 0.00072  |
|    ent_coef        | 0.000727 |
|    ent_coef_loss   | -25.9    |
|    learning_rate   | 0.00432  |
|    n_updates       | 3330     |
---------------------------------
Eval num_timesteps=683000, episode_reward=-89.16 +/- 7.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -89.2    |
| time/              |          |
|    total_timesteps | 683000   |
---------------------------------
Eval num_timesteps=684000, episode_reward=-90.43 +/- 2.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -90.4    |
| time/              |          |
|    total_timesteps | 684000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -67.6    |
| time/              |          |
|    episodes        | 684      |
|    fps             | 633      |
|    time_elapsed    | 1079     |
|    total_timesteps | 684000   |
---------------------------------
Eval num_timesteps=685000, episode_reward=-286.73 +/- 1.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 685000   |
| train/             |          |
|    actor_loss      | -2.45    |
|    critic_loss     | 0.000922 |
|    ent_coef        | 0.000711 |
|    ent_coef_loss   | 27.4     |
|    learning_rate   | 0.00432  |
|    n_updates       | 3340     |
---------------------------------
Eval num_timesteps=686000, episode_reward=-288.29 +/- 2.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 686000   |
---------------------------------
Eval num_timesteps=687000, episode_reward=-989.09 +/- 240.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -989     |
| time/              |          |
|    total_timesteps | 687000   |
| train/             |          |
|    actor_loss      | -2.46    |
|    critic_loss     | 0.00244  |
|    ent_coef        | 0.000715 |
|    ent_coef_loss   | 4.59     |
|    learning_rate   | 0.00431  |
|    n_updates       | 3350     |
---------------------------------
Eval num_timesteps=688000, episode_reward=-1122.76 +/- 70.24
Episode length: 1000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1e+03     |
|    mean_reward     | -1.12e+03 |
| time/              |           |
|    total_timesteps | 688000    |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -68.2    |
| time/              |          |
|    episodes        | 688      |
|    fps             | 633      |
|    time_elapsed    | 1085     |
|    total_timesteps | 688000   |
---------------------------------
Eval num_timesteps=689000, episode_reward=-194.06 +/- 2.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 689000   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00275  |
|    ent_coef        | 0.000722 |
|    ent_coef_loss   | 23.1     |
|    learning_rate   | 0.00431  |
|    n_updates       | 3360     |
---------------------------------
Eval num_timesteps=690000, episode_reward=-194.11 +/- 1.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 690000   |
---------------------------------
Eval num_timesteps=691000, episode_reward=297.73 +/- 22.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 298      |
| time/              |          |
|    total_timesteps | 691000   |
| train/             |          |
|    actor_loss      | -2.5     |
|    critic_loss     | 0.00212  |
|    ent_coef        | 0.000733 |
|    ent_coef_loss   | -10.4    |
|    learning_rate   | 0.00431  |
|    n_updates       | 3370     |
---------------------------------
Eval num_timesteps=692000, episode_reward=292.25 +/- 20.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 692000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -61.8    |
| time/              |          |
|    episodes        | 692      |
|    fps             | 633      |
|    time_elapsed    | 1091     |
|    total_timesteps | 692000   |
---------------------------------
Eval num_timesteps=693000, episode_reward=256.09 +/- 24.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 256      |
| time/              |          |
|    total_timesteps | 693000   |
| train/             |          |
|    actor_loss      | -2.43    |
|    critic_loss     | 0.0018   |
|    ent_coef        | 0.000735 |
|    ent_coef_loss   | -5.25    |
|    learning_rate   | 0.00431  |
|    n_updates       | 3380     |
---------------------------------
Eval num_timesteps=694000, episode_reward=274.09 +/- 50.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 274      |
| time/              |          |
|    total_timesteps | 694000   |
---------------------------------
Eval num_timesteps=695000, episode_reward=118.17 +/- 76.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 118      |
| time/              |          |
|    total_timesteps | 695000   |
| train/             |          |
|    actor_loss      | -2.41    |
|    critic_loss     | 0.00149  |
|    ent_coef        | 0.000731 |
|    ent_coef_loss   | -17.8    |
|    learning_rate   | 0.00431  |
|    n_updates       | 3390     |
---------------------------------
Eval num_timesteps=696000, episode_reward=182.21 +/- 45.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 182      |
| time/              |          |
|    total_timesteps | 696000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -37.2    |
| time/              |          |
|    episodes        | 696      |
|    fps             | 633      |
|    time_elapsed    | 1097     |
|    total_timesteps | 696000   |
---------------------------------
Eval num_timesteps=697000, episode_reward=57.29 +/- 120.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 57.3     |
| time/              |          |
|    total_timesteps | 697000   |
| train/             |          |
|    actor_loss      | -2.42    |
|    critic_loss     | 0.00115  |
|    ent_coef        | 0.00072  |
|    ent_coef_loss   | -15.9    |
|    learning_rate   | 0.0043   |
|    n_updates       | 3400     |
---------------------------------
Eval num_timesteps=698000, episode_reward=132.22 +/- 68.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 132      |
| time/              |          |
|    total_timesteps | 698000   |
---------------------------------
Eval num_timesteps=699000, episode_reward=294.83 +/- 73.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 295      |
| time/              |          |
|    total_timesteps | 699000   |
| train/             |          |
|    actor_loss      | -2.37    |
|    critic_loss     | 0.00157  |
|    ent_coef        | 0.000707 |
|    ent_coef_loss   | -26.1    |
|    learning_rate   | 0.0043   |
|    n_updates       | 3410     |
---------------------------------
Eval num_timesteps=700000, episode_reward=303.35 +/- 31.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 303      |
| time/              |          |
|    total_timesteps | 700000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -27      |
| time/              |          |
|    episodes        | 700      |
|    fps             | 634      |
|    time_elapsed    | 1103     |
|    total_timesteps | 700000   |
---------------------------------
Eval num_timesteps=701000, episode_reward=661.99 +/- 62.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 701000   |
| train/             |          |
|    actor_loss      | -2.41    |
|    critic_loss     | 0.00128  |
|    ent_coef        | 0.000691 |
|    ent_coef_loss   | -11.4    |
|    learning_rate   | 0.0043   |
|    n_updates       | 3420     |
---------------------------------
New best mean reward!
Eval num_timesteps=702000, episode_reward=721.45 +/- 16.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 702000   |
---------------------------------
New best mean reward!
Eval num_timesteps=703000, episode_reward=626.64 +/- 49.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 627      |
| time/              |          |
|    total_timesteps | 703000   |
| train/             |          |
|    actor_loss      | -2.44    |
|    critic_loss     | 0.00119  |
|    ent_coef        | 0.000679 |
|    ent_coef_loss   | -7.54    |
|    learning_rate   | 0.0043   |
|    n_updates       | 3430     |
---------------------------------
Eval num_timesteps=704000, episode_reward=635.82 +/- 20.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 704000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -5.77    |
| time/              |          |
|    episodes        | 704      |
|    fps             | 634      |
|    time_elapsed    | 1110     |
|    total_timesteps | 704000   |
---------------------------------
Eval num_timesteps=705000, episode_reward=660.72 +/- 22.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 705000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.000999 |
|    ent_coef        | 0.000674 |
|    ent_coef_loss   | 9.49     |
|    learning_rate   | 0.0043   |
|    n_updates       | 3440     |
---------------------------------
Eval num_timesteps=706000, episode_reward=618.53 +/- 36.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 619      |
| time/              |          |
|    total_timesteps | 706000   |
---------------------------------
Eval num_timesteps=707000, episode_reward=698.78 +/- 19.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 699      |
| time/              |          |
|    total_timesteps | 707000   |
| train/             |          |
|    actor_loss      | -2.47    |
|    critic_loss     | 0.0011   |
|    ent_coef        | 0.000674 |
|    ent_coef_loss   | 2.7      |
|    learning_rate   | 0.00429  |
|    n_updates       | 3450     |
---------------------------------
Eval num_timesteps=708000, episode_reward=702.11 +/- 36.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 702      |
| time/              |          |
|    total_timesteps | 708000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 22.4     |
| time/              |          |
|    episodes        | 708      |
|    fps             | 634      |
|    time_elapsed    | 1116     |
|    total_timesteps | 708000   |
---------------------------------
Eval num_timesteps=709000, episode_reward=661.09 +/- 40.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 661      |
| time/              |          |
|    total_timesteps | 709000   |
| train/             |          |
|    actor_loss      | -2.48    |
|    critic_loss     | 0.00105  |
|    ent_coef        | 0.000677 |
|    ent_coef_loss   | 3.12     |
|    learning_rate   | 0.00429  |
|    n_updates       | 3460     |
---------------------------------
Eval num_timesteps=710000, episode_reward=686.83 +/- 22.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 710000   |
---------------------------------
Eval num_timesteps=711000, episode_reward=517.46 +/- 16.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 517      |
| time/              |          |
|    total_timesteps | 711000   |
| train/             |          |
|    actor_loss      | -2.45    |
|    critic_loss     | 0.000918 |
|    ent_coef        | 0.000678 |
|    ent_coef_loss   | -7.08    |
|    learning_rate   | 0.00429  |
|    n_updates       | 3470     |
---------------------------------
Eval num_timesteps=712000, episode_reward=517.86 +/- 36.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 712000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 42.6     |
| time/              |          |
|    episodes        | 712      |
|    fps             | 634      |
|    time_elapsed    | 1122     |
|    total_timesteps | 712000   |
---------------------------------
Eval num_timesteps=713000, episode_reward=431.14 +/- 33.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 713000   |
| train/             |          |
|    actor_loss      | -2.43    |
|    critic_loss     | 0.00123  |
|    ent_coef        | 0.000675 |
|    ent_coef_loss   | -7.62    |
|    learning_rate   | 0.00429  |
|    n_updates       | 3480     |
---------------------------------
Eval num_timesteps=714000, episode_reward=422.78 +/- 25.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 714000   |
---------------------------------
Eval num_timesteps=715000, episode_reward=502.00 +/- 28.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 715000   |
| train/             |          |
|    actor_loss      | -2.4     |
|    critic_loss     | 0.00145  |
|    ent_coef        | 0.000671 |
|    ent_coef_loss   | -5.01    |
|    learning_rate   | 0.00429  |
|    n_updates       | 3490     |
---------------------------------
Eval num_timesteps=716000, episode_reward=498.89 +/- 36.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 499      |
| time/              |          |
|    total_timesteps | 716000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 64.1     |
| time/              |          |
|    episodes        | 716      |
|    fps             | 634      |
|    time_elapsed    | 1128     |
|    total_timesteps | 716000   |
---------------------------------
Eval num_timesteps=717000, episode_reward=538.91 +/- 23.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 539      |
| time/              |          |
|    total_timesteps | 717000   |
| train/             |          |
|    actor_loss      | -2.42    |
|    critic_loss     | 0.00116  |
|    ent_coef        | 0.000665 |
|    ent_coef_loss   | -12.7    |
|    learning_rate   | 0.00428  |
|    n_updates       | 3500     |
---------------------------------
Eval num_timesteps=718000, episode_reward=546.69 +/- 26.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 547      |
| time/              |          |
|    total_timesteps | 718000   |
---------------------------------
Eval num_timesteps=719000, episode_reward=602.66 +/- 183.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 603      |
| time/              |          |
|    total_timesteps | 719000   |
| train/             |          |
|    actor_loss      | -2.41    |
|    critic_loss     | 0.00103  |
|    ent_coef        | 0.000658 |
|    ent_coef_loss   | -9.36    |
|    learning_rate   | 0.00428  |
|    n_updates       | 3510     |
---------------------------------
Eval num_timesteps=720000, episode_reward=495.92 +/- 247.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 496      |
| time/              |          |
|    total_timesteps | 720000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 84.4     |
| time/              |          |
|    episodes        | 720      |
|    fps             | 634      |
|    time_elapsed    | 1134     |
|    total_timesteps | 720000   |
---------------------------------
Eval num_timesteps=721000, episode_reward=523.08 +/- 158.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 523      |
| time/              |          |
|    total_timesteps | 721000   |
| train/             |          |
|    actor_loss      | -2.4     |
|    critic_loss     | 0.0011   |
|    ent_coef        | 0.000649 |
|    ent_coef_loss   | -19.3    |
|    learning_rate   | 0.00428  |
|    n_updates       | 3520     |
---------------------------------
Eval num_timesteps=722000, episode_reward=593.08 +/- 20.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 722000   |
---------------------------------
Eval num_timesteps=723000, episode_reward=611.93 +/- 24.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 612      |
| time/              |          |
|    total_timesteps | 723000   |
| train/             |          |
|    actor_loss      | -2.43    |
|    critic_loss     | 0.00113  |
|    ent_coef        | 0.00064  |
|    ent_coef_loss   | 1.46     |
|    learning_rate   | 0.00428  |
|    n_updates       | 3530     |
---------------------------------
Eval num_timesteps=724000, episode_reward=414.13 +/- 366.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 414      |
| time/              |          |
|    total_timesteps | 724000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 98.6     |
| time/              |          |
|    episodes        | 724      |
|    fps             | 634      |
|    time_elapsed    | 1140     |
|    total_timesteps | 724000   |
---------------------------------
Eval num_timesteps=725000, episode_reward=472.80 +/- 27.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 725000   |
| train/             |          |
|    actor_loss      | -2.44    |
|    critic_loss     | 0.00123  |
|    ent_coef        | 0.000635 |
|    ent_coef_loss   | -4.96    |
|    learning_rate   | 0.00428  |
|    n_updates       | 3540     |
---------------------------------
Eval num_timesteps=726000, episode_reward=486.09 +/- 16.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 486      |
| time/              |          |
|    total_timesteps | 726000   |
---------------------------------
Eval num_timesteps=727000, episode_reward=491.32 +/- 24.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 491      |
| time/              |          |
|    total_timesteps | 727000   |
---------------------------------
Eval num_timesteps=728000, episode_reward=469.69 +/- 37.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 470      |
| time/              |          |
|    total_timesteps | 728000   |
| train/             |          |
|    actor_loss      | -2.42    |
|    critic_loss     | 0.0013   |
|    ent_coef        | 0.000631 |
|    ent_coef_loss   | -3.9     |
|    learning_rate   | 0.00427  |
|    n_updates       | 3550     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 113      |
| time/              |          |
|    episodes        | 728      |
|    fps             | 634      |
|    time_elapsed    | 1146     |
|    total_timesteps | 728000   |
---------------------------------
Eval num_timesteps=729000, episode_reward=476.12 +/- 16.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 729000   |
---------------------------------
Eval num_timesteps=730000, episode_reward=581.76 +/- 31.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 582      |
| time/              |          |
|    total_timesteps | 730000   |
| train/             |          |
|    actor_loss      | -2.4     |
|    critic_loss     | 0.00139  |
|    ent_coef        | 0.000628 |
|    ent_coef_loss   | -3.52    |
|    learning_rate   | 0.00427  |
|    n_updates       | 3560     |
---------------------------------
Eval num_timesteps=731000, episode_reward=426.33 +/- 248.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 731000   |
---------------------------------
Eval num_timesteps=732000, episode_reward=40.14 +/- 416.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 40.1     |
| time/              |          |
|    total_timesteps | 732000   |
| train/             |          |
|    actor_loss      | -2.43    |
|    critic_loss     | 0.001    |
|    ent_coef        | 0.000626 |
|    ent_coef_loss   | 7.03     |
|    learning_rate   | 0.00427  |
|    n_updates       | 3570     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 127      |
| time/              |          |
|    episodes        | 732      |
|    fps             | 634      |
|    time_elapsed    | 1153     |
|    total_timesteps | 732000   |
---------------------------------
Eval num_timesteps=733000, episode_reward=-34.85 +/- 346.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -34.8    |
| time/              |          |
|    total_timesteps | 733000   |
---------------------------------
Eval num_timesteps=734000, episode_reward=66.60 +/- 424.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 66.6     |
| time/              |          |
|    total_timesteps | 734000   |
| train/             |          |
|    actor_loss      | -2.43    |
|    critic_loss     | 0.000907 |
|    ent_coef        | 0.000628 |
|    ent_coef_loss   | -5.11    |
|    learning_rate   | 0.00427  |
|    n_updates       | 3580     |
---------------------------------
Eval num_timesteps=735000, episode_reward=236.93 +/- 427.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 237      |
| time/              |          |
|    total_timesteps | 735000   |
---------------------------------
Eval num_timesteps=736000, episode_reward=361.09 +/- 403.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 736000   |
| train/             |          |
|    actor_loss      | -2.41    |
|    critic_loss     | 0.000896 |
|    ent_coef        | 0.000625 |
|    ent_coef_loss   | -19.8    |
|    learning_rate   | 0.00426  |
|    n_updates       | 3590     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 137      |
| time/              |          |
|    episodes        | 736      |
|    fps             | 634      |
|    time_elapsed    | 1159     |
|    total_timesteps | 736000   |
---------------------------------
Eval num_timesteps=737000, episode_reward=369.40 +/- 452.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 369      |
| time/              |          |
|    total_timesteps | 737000   |
---------------------------------
Eval num_timesteps=738000, episode_reward=570.03 +/- 17.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 738000   |
| train/             |          |
|    actor_loss      | -2.38    |
|    critic_loss     | 0.00113  |
|    ent_coef        | 0.000615 |
|    ent_coef_loss   | -14.2    |
|    learning_rate   | 0.00426  |
|    n_updates       | 3600     |
---------------------------------
Eval num_timesteps=739000, episode_reward=611.60 +/- 27.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 612      |
| time/              |          |
|    total_timesteps | 739000   |
---------------------------------
Eval num_timesteps=740000, episode_reward=215.90 +/- 409.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 740000   |
| train/             |          |
|    actor_loss      | -2.42    |
|    critic_loss     | 0.00137  |
|    ent_coef        | 0.000607 |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.00426  |
|    n_updates       | 3610     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 144      |
| time/              |          |
|    episodes        | 740      |
|    fps             | 635      |
|    time_elapsed    | 1165     |
|    total_timesteps | 740000   |
---------------------------------
Eval num_timesteps=741000, episode_reward=362.18 +/- 321.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 362      |
| time/              |          |
|    total_timesteps | 741000   |
---------------------------------
Eval num_timesteps=742000, episode_reward=149.32 +/- 316.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 149      |
| time/              |          |
|    total_timesteps | 742000   |
| train/             |          |
|    actor_loss      | -2.4     |
|    critic_loss     | 0.00131  |
|    ent_coef        | 0.000603 |
|    ent_coef_loss   | -5.15    |
|    learning_rate   | 0.00426  |
|    n_updates       | 3620     |
---------------------------------
Eval num_timesteps=743000, episode_reward=342.35 +/- 122.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 342      |
| time/              |          |
|    total_timesteps | 743000   |
---------------------------------
Eval num_timesteps=744000, episode_reward=525.33 +/- 21.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 525      |
| time/              |          |
|    total_timesteps | 744000   |
| train/             |          |
|    actor_loss      | -2.39    |
|    critic_loss     | 0.00119  |
|    ent_coef        | 0.000599 |
|    ent_coef_loss   | -5.04    |
|    learning_rate   | 0.00426  |
|    n_updates       | 3630     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 147      |
| time/              |          |
|    episodes        | 744      |
|    fps             | 635      |
|    time_elapsed    | 1171     |
|    total_timesteps | 744000   |
---------------------------------
Eval num_timesteps=745000, episode_reward=500.97 +/- 55.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 501      |
| time/              |          |
|    total_timesteps | 745000   |
---------------------------------
Eval num_timesteps=746000, episode_reward=444.59 +/- 25.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 746000   |
| train/             |          |
|    actor_loss      | -2.39    |
|    critic_loss     | 0.00135  |
|    ent_coef        | 0.000596 |
|    ent_coef_loss   | -3.62    |
|    learning_rate   | 0.00425  |
|    n_updates       | 3640     |
---------------------------------
Eval num_timesteps=747000, episode_reward=472.75 +/- 61.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 747000   |
---------------------------------
Eval num_timesteps=748000, episode_reward=665.48 +/- 15.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 665      |
| time/              |          |
|    total_timesteps | 748000   |
| train/             |          |
|    actor_loss      | -2.39    |
|    critic_loss     | 0.00141  |
|    ent_coef        | 0.000594 |
|    ent_coef_loss   | 4.54     |
|    learning_rate   | 0.00425  |
|    n_updates       | 3650     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 158      |
| time/              |          |
|    episodes        | 748      |
|    fps             | 635      |
|    time_elapsed    | 1177     |
|    total_timesteps | 748000   |
---------------------------------
Eval num_timesteps=749000, episode_reward=651.64 +/- 15.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 749000   |
---------------------------------
Eval num_timesteps=750000, episode_reward=560.99 +/- 60.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 561      |
| time/              |          |
|    total_timesteps | 750000   |
| train/             |          |
|    actor_loss      | -2.39    |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.000594 |
|    ent_coef_loss   | -5.04    |
|    learning_rate   | 0.00425  |
|    n_updates       | 3660     |
---------------------------------
Eval num_timesteps=751000, episode_reward=585.41 +/- 85.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 585      |
| time/              |          |
|    total_timesteps | 751000   |
---------------------------------
Eval num_timesteps=752000, episode_reward=347.23 +/- 226.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 347      |
| time/              |          |
|    total_timesteps | 752000   |
| train/             |          |
|    actor_loss      | -2.38    |
|    critic_loss     | 0.00137  |
|    ent_coef        | 0.000592 |
|    ent_coef_loss   | 0.477    |
|    learning_rate   | 0.00425  |
|    n_updates       | 3670     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 170      |
| time/              |          |
|    episodes        | 752      |
|    fps             | 635      |
|    time_elapsed    | 1183     |
|    total_timesteps | 752000   |
---------------------------------
Eval num_timesteps=753000, episode_reward=361.08 +/- 89.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 361      |
| time/              |          |
|    total_timesteps | 753000   |
---------------------------------
Eval num_timesteps=754000, episode_reward=613.24 +/- 30.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 754000   |
| train/             |          |
|    actor_loss      | -2.36    |
|    critic_loss     | 0.00175  |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | -8.22    |
|    learning_rate   | 0.00425  |
|    n_updates       | 3680     |
---------------------------------
Eval num_timesteps=755000, episode_reward=610.30 +/- 32.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 610      |
| time/              |          |
|    total_timesteps | 755000   |
---------------------------------
Eval num_timesteps=756000, episode_reward=515.59 +/- 13.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 516      |
| time/              |          |
|    total_timesteps | 756000   |
| train/             |          |
|    actor_loss      | -2.4     |
|    critic_loss     | 0.00146  |
|    ent_coef        | 0.000587 |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.00424  |
|    n_updates       | 3690     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 193      |
| time/              |          |
|    episodes        | 756      |
|    fps             | 635      |
|    time_elapsed    | 1189     |
|    total_timesteps | 756000   |
---------------------------------
Eval num_timesteps=757000, episode_reward=499.28 +/- 7.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 499      |
| time/              |          |
|    total_timesteps | 757000   |
---------------------------------
Eval num_timesteps=758000, episode_reward=285.49 +/- 371.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 758000   |
| train/             |          |
|    actor_loss      | -2.37    |
|    critic_loss     | 0.00107  |
|    ent_coef        | 0.000586 |
|    ent_coef_loss   | -4.1     |
|    learning_rate   | 0.00424  |
|    n_updates       | 3700     |
---------------------------------
Eval num_timesteps=759000, episode_reward=267.11 +/- 352.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 267      |
| time/              |          |
|    total_timesteps | 759000   |
---------------------------------
Eval num_timesteps=760000, episode_reward=302.17 +/- 113.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 302      |
| time/              |          |
|    total_timesteps | 760000   |
| train/             |          |
|    actor_loss      | -2.37    |
|    critic_loss     | 0.00121  |
|    ent_coef        | 0.000583 |
|    ent_coef_loss   | -14.8    |
|    learning_rate   | 0.00424  |
|    n_updates       | 3710     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 223      |
| time/              |          |
|    episodes        | 760      |
|    fps             | 635      |
|    time_elapsed    | 1195     |
|    total_timesteps | 760000   |
---------------------------------
Eval num_timesteps=761000, episode_reward=152.27 +/- 174.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 152      |
| time/              |          |
|    total_timesteps | 761000   |
---------------------------------
Eval num_timesteps=762000, episode_reward=620.30 +/- 32.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 762000   |
| train/             |          |
|    actor_loss      | -2.37    |
|    critic_loss     | 0.00142  |
|    ent_coef        | 0.000576 |
|    ent_coef_loss   | -8.55    |
|    learning_rate   | 0.00424  |
|    n_updates       | 3720     |
---------------------------------
Eval num_timesteps=763000, episode_reward=106.64 +/- 387.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 107      |
| time/              |          |
|    total_timesteps | 763000   |
---------------------------------
Eval num_timesteps=764000, episode_reward=643.31 +/- 25.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 643      |
| time/              |          |
|    total_timesteps | 764000   |
| train/             |          |
|    actor_loss      | -2.38    |
|    critic_loss     | 0.00131  |
|    ent_coef        | 0.00057  |
|    ent_coef_loss   | 4.16     |
|    learning_rate   | 0.00424  |
|    n_updates       | 3730     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 250      |
| time/              |          |
|    episodes        | 764      |
|    fps             | 635      |
|    time_elapsed    | 1201     |
|    total_timesteps | 764000   |
---------------------------------
Eval num_timesteps=765000, episode_reward=589.43 +/- 154.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 765000   |
---------------------------------
Eval num_timesteps=766000, episode_reward=462.04 +/- 17.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 462      |
| time/              |          |
|    total_timesteps | 766000   |
| train/             |          |
|    actor_loss      | -2.4     |
|    critic_loss     | 0.00125  |
|    ent_coef        | 0.000568 |
|    ent_coef_loss   | -6.88    |
|    learning_rate   | 0.00423  |
|    n_updates       | 3740     |
---------------------------------
Eval num_timesteps=767000, episode_reward=464.86 +/- 52.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 767000   |
---------------------------------
Eval num_timesteps=768000, episode_reward=461.48 +/- 39.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 461      |
| time/              |          |
|    total_timesteps | 768000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 283      |
| time/              |          |
|    episodes        | 768      |
|    fps             | 635      |
|    time_elapsed    | 1207     |
|    total_timesteps | 768000   |
---------------------------------
Eval num_timesteps=769000, episode_reward=373.55 +/- 24.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 374      |
| time/              |          |
|    total_timesteps | 769000   |
| train/             |          |
|    actor_loss      | -2.35    |
|    critic_loss     | 0.00139  |
|    ent_coef        | 0.000565 |
|    ent_coef_loss   | -8.03    |
|    learning_rate   | 0.00423  |
|    n_updates       | 3750     |
---------------------------------
Eval num_timesteps=770000, episode_reward=381.57 +/- 13.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 382      |
| time/              |          |
|    total_timesteps | 770000   |
---------------------------------
Eval num_timesteps=771000, episode_reward=464.60 +/- 21.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 771000   |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.00131  |
|    ent_coef        | 0.000561 |
|    ent_coef_loss   | 6.4      |
|    learning_rate   | 0.00423  |
|    n_updates       | 3760     |
---------------------------------
Eval num_timesteps=772000, episode_reward=441.62 +/- 53.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 772000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 299      |
| time/              |          |
|    episodes        | 772      |
|    fps             | 635      |
|    time_elapsed    | 1213     |
|    total_timesteps | 772000   |
---------------------------------
Eval num_timesteps=773000, episode_reward=173.79 +/- 34.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 773000   |
| train/             |          |
|    actor_loss      | -2.36    |
|    critic_loss     | 0.00123  |
|    ent_coef        | 0.000563 |
|    ent_coef_loss   | 4.97     |
|    learning_rate   | 0.00423  |
|    n_updates       | 3770     |
---------------------------------
Eval num_timesteps=774000, episode_reward=185.69 +/- 32.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 186      |
| time/              |          |
|    total_timesteps | 774000   |
---------------------------------
Eval num_timesteps=775000, episode_reward=291.85 +/- 29.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 292      |
| time/              |          |
|    total_timesteps | 775000   |
| train/             |          |
|    actor_loss      | -2.31    |
|    critic_loss     | 0.0017   |
|    ent_coef        | 0.000567 |
|    ent_coef_loss   | 13.4     |
|    learning_rate   | 0.00423  |
|    n_updates       | 3780     |
---------------------------------
Eval num_timesteps=776000, episode_reward=249.68 +/- 39.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 250      |
| time/              |          |
|    total_timesteps | 776000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 313      |
| time/              |          |
|    episodes        | 776      |
|    fps             | 635      |
|    time_elapsed    | 1220     |
|    total_timesteps | 776000   |
---------------------------------
Eval num_timesteps=777000, episode_reward=527.55 +/- 20.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 777000   |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.00144  |
|    ent_coef        | 0.000574 |
|    ent_coef_loss   | 16.7     |
|    learning_rate   | 0.00422  |
|    n_updates       | 3790     |
---------------------------------
Eval num_timesteps=778000, episode_reward=517.88 +/- 33.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 778000   |
---------------------------------
Eval num_timesteps=779000, episode_reward=506.05 +/- 48.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 506      |
| time/              |          |
|    total_timesteps | 779000   |
| train/             |          |
|    actor_loss      | -2.36    |
|    critic_loss     | 0.00138  |
|    ent_coef        | 0.000583 |
|    ent_coef_loss   | 2.13     |
|    learning_rate   | 0.00422  |
|    n_updates       | 3800     |
---------------------------------
Eval num_timesteps=780000, episode_reward=466.94 +/- 26.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 467      |
| time/              |          |
|    total_timesteps | 780000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 332      |
| time/              |          |
|    episodes        | 780      |
|    fps             | 636      |
|    time_elapsed    | 1226     |
|    total_timesteps | 780000   |
---------------------------------
Eval num_timesteps=781000, episode_reward=509.69 +/- 36.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 510      |
| time/              |          |
|    total_timesteps | 781000   |
| train/             |          |
|    actor_loss      | -2.36    |
|    critic_loss     | 0.00158  |
|    ent_coef        | 0.000589 |
|    ent_coef_loss   | 7.99     |
|    learning_rate   | 0.00422  |
|    n_updates       | 3810     |
---------------------------------
Eval num_timesteps=782000, episode_reward=518.40 +/- 40.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 782000   |
---------------------------------
Eval num_timesteps=783000, episode_reward=387.04 +/- 22.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 387      |
| time/              |          |
|    total_timesteps | 783000   |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.00134  |
|    ent_coef        | 0.000594 |
|    ent_coef_loss   | 9.66     |
|    learning_rate   | 0.00422  |
|    n_updates       | 3820     |
---------------------------------
Eval num_timesteps=784000, episode_reward=336.71 +/- 56.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 337      |
| time/              |          |
|    total_timesteps | 784000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 359      |
| time/              |          |
|    episodes        | 784      |
|    fps             | 636      |
|    time_elapsed    | 1232     |
|    total_timesteps | 784000   |
---------------------------------
Eval num_timesteps=785000, episode_reward=623.59 +/- 41.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 624      |
| time/              |          |
|    total_timesteps | 785000   |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.0012   |
|    ent_coef        | 0.000601 |
|    ent_coef_loss   | 2.7      |
|    learning_rate   | 0.00422  |
|    n_updates       | 3830     |
---------------------------------
Eval num_timesteps=786000, episode_reward=625.30 +/- 45.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 625      |
| time/              |          |
|    total_timesteps | 786000   |
---------------------------------
Eval num_timesteps=787000, episode_reward=306.15 +/- 29.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 306      |
| time/              |          |
|    total_timesteps | 787000   |
| train/             |          |
|    actor_loss      | -2.36    |
|    critic_loss     | 0.00111  |
|    ent_coef        | 0.000604 |
|    ent_coef_loss   | -7.1     |
|    learning_rate   | 0.00421  |
|    n_updates       | 3840     |
---------------------------------
Eval num_timesteps=788000, episode_reward=343.14 +/- 42.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 343      |
| time/              |          |
|    total_timesteps | 788000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 389      |
| time/              |          |
|    episodes        | 788      |
|    fps             | 636      |
|    time_elapsed    | 1238     |
|    total_timesteps | 788000   |
---------------------------------
Eval num_timesteps=789000, episode_reward=392.05 +/- 46.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 392      |
| time/              |          |
|    total_timesteps | 789000   |
| train/             |          |
|    actor_loss      | -2.31    |
|    critic_loss     | 0.00143  |
|    ent_coef        | 0.000603 |
|    ent_coef_loss   | 3.13     |
|    learning_rate   | 0.00421  |
|    n_updates       | 3850     |
---------------------------------
Eval num_timesteps=790000, episode_reward=394.45 +/- 57.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 790000   |
---------------------------------
Eval num_timesteps=791000, episode_reward=630.35 +/- 13.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 630      |
| time/              |          |
|    total_timesteps | 791000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.00126  |
|    ent_coef        | 0.000603 |
|    ent_coef_loss   | -1.69    |
|    learning_rate   | 0.00421  |
|    n_updates       | 3860     |
---------------------------------
Eval num_timesteps=792000, episode_reward=558.76 +/- 42.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 559      |
| time/              |          |
|    total_timesteps | 792000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 408      |
| time/              |          |
|    episodes        | 792      |
|    fps             | 636      |
|    time_elapsed    | 1244     |
|    total_timesteps | 792000   |
---------------------------------
Eval num_timesteps=793000, episode_reward=478.76 +/- 313.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 479      |
| time/              |          |
|    total_timesteps | 793000   |
| train/             |          |
|    actor_loss      | -2.35    |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.000602 |
|    ent_coef_loss   | -11.9    |
|    learning_rate   | 0.00421  |
|    n_updates       | 3870     |
---------------------------------
Eval num_timesteps=794000, episode_reward=436.08 +/- 312.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 794000   |
---------------------------------
Eval num_timesteps=795000, episode_reward=497.04 +/- 41.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 795000   |
| train/             |          |
|    actor_loss      | -2.35    |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.000595 |
|    ent_coef_loss   | -11      |
|    learning_rate   | 0.00421  |
|    n_updates       | 3880     |
---------------------------------
Eval num_timesteps=796000, episode_reward=491.70 +/- 40.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 492      |
| time/              |          |
|    total_timesteps | 796000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 420      |
| time/              |          |
|    episodes        | 796      |
|    fps             | 636      |
|    time_elapsed    | 1250     |
|    total_timesteps | 796000   |
---------------------------------
Eval num_timesteps=797000, episode_reward=559.70 +/- 94.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 560      |
| time/              |          |
|    total_timesteps | 797000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.00121  |
|    ent_coef        | 0.000588 |
|    ent_coef_loss   | 0.386    |
|    learning_rate   | 0.0042   |
|    n_updates       | 3890     |
---------------------------------
Eval num_timesteps=798000, episode_reward=577.64 +/- 61.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 578      |
| time/              |          |
|    total_timesteps | 798000   |
---------------------------------
Eval num_timesteps=799000, episode_reward=557.41 +/- 33.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 557      |
| time/              |          |
|    total_timesteps | 799000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.0013   |
|    ent_coef        | 0.000586 |
|    ent_coef_loss   | 1.51     |
|    learning_rate   | 0.0042   |
|    n_updates       | 3900     |
---------------------------------
Eval num_timesteps=800000, episode_reward=491.48 +/- 44.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 491      |
| time/              |          |
|    total_timesteps | 800000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 434      |
| time/              |          |
|    episodes        | 800      |
|    fps             | 636      |
|    time_elapsed    | 1256     |
|    total_timesteps | 800000   |
---------------------------------
Eval num_timesteps=801000, episode_reward=696.74 +/- 38.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 697      |
| time/              |          |
|    total_timesteps | 801000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.00126  |
|    ent_coef        | 0.000585 |
|    ent_coef_loss   | -5.21    |
|    learning_rate   | 0.0042   |
|    n_updates       | 3910     |
---------------------------------
Eval num_timesteps=802000, episode_reward=661.85 +/- 27.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 802000   |
---------------------------------
Eval num_timesteps=803000, episode_reward=493.03 +/- 43.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 493      |
| time/              |          |
|    total_timesteps | 803000   |
| train/             |          |
|    actor_loss      | -2.36    |
|    critic_loss     | 0.00124  |
|    ent_coef        | 0.000583 |
|    ent_coef_loss   | 0.972    |
|    learning_rate   | 0.0042   |
|    n_updates       | 3920     |
---------------------------------
Eval num_timesteps=804000, episode_reward=514.85 +/- 33.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 515      |
| time/              |          |
|    total_timesteps | 804000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 440      |
| time/              |          |
|    episodes        | 804      |
|    fps             | 636      |
|    time_elapsed    | 1262     |
|    total_timesteps | 804000   |
---------------------------------
Eval num_timesteps=805000, episode_reward=205.21 +/- 53.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 205      |
| time/              |          |
|    total_timesteps | 805000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.00101  |
|    ent_coef        | 0.000582 |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.0042   |
|    n_updates       | 3930     |
---------------------------------
Eval num_timesteps=806000, episode_reward=215.52 +/- 56.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 806000   |
---------------------------------
Eval num_timesteps=807000, episode_reward=320.72 +/- 48.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 807000   |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 0.0018   |
|    ent_coef        | 0.000582 |
|    ent_coef_loss   | 13       |
|    learning_rate   | 0.00419  |
|    n_updates       | 3940     |
---------------------------------
Eval num_timesteps=808000, episode_reward=316.35 +/- 21.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 316      |
| time/              |          |
|    total_timesteps | 808000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 429      |
| time/              |          |
|    episodes        | 808      |
|    fps             | 636      |
|    time_elapsed    | 1268     |
|    total_timesteps | 808000   |
---------------------------------
Eval num_timesteps=809000, episode_reward=345.34 +/- 24.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 345      |
| time/              |          |
|    total_timesteps | 809000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 0.00182  |
|    ent_coef        | 0.000587 |
|    ent_coef_loss   | 7.25     |
|    learning_rate   | 0.00419  |
|    n_updates       | 3950     |
---------------------------------
Eval num_timesteps=810000, episode_reward=355.36 +/- 32.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 355      |
| time/              |          |
|    total_timesteps | 810000   |
---------------------------------
Eval num_timesteps=811000, episode_reward=351.86 +/- 28.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 352      |
| time/              |          |
|    total_timesteps | 811000   |
---------------------------------
Eval num_timesteps=812000, episode_reward=256.83 +/- 48.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 812000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 0.00155  |
|    ent_coef        | 0.000594 |
|    ent_coef_loss   | 6.21     |
|    learning_rate   | 0.00419  |
|    n_updates       | 3960     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 424      |
| time/              |          |
|    episodes        | 812      |
|    fps             | 636      |
|    time_elapsed    | 1274     |
|    total_timesteps | 812000   |
---------------------------------
Eval num_timesteps=813000, episode_reward=311.80 +/- 95.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 312      |
| time/              |          |
|    total_timesteps | 813000   |
---------------------------------
Eval num_timesteps=814000, episode_reward=164.69 +/- 32.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 165      |
| time/              |          |
|    total_timesteps | 814000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00159  |
|    ent_coef        | 0.000599 |
|    ent_coef_loss   | 6.54     |
|    learning_rate   | 0.00419  |
|    n_updates       | 3970     |
---------------------------------
Eval num_timesteps=815000, episode_reward=148.42 +/- 36.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 148      |
| time/              |          |
|    total_timesteps | 815000   |
---------------------------------
Eval num_timesteps=816000, episode_reward=399.01 +/- 35.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 816000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 0.00157  |
|    ent_coef        | 0.000605 |
|    ent_coef_loss   | 6.66     |
|    learning_rate   | 0.00418  |
|    n_updates       | 3980     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 420      |
| time/              |          |
|    episodes        | 816      |
|    fps             | 636      |
|    time_elapsed    | 1281     |
|    total_timesteps | 816000   |
---------------------------------
Eval num_timesteps=817000, episode_reward=408.65 +/- 36.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 409      |
| time/              |          |
|    total_timesteps | 817000   |
---------------------------------
Eval num_timesteps=818000, episode_reward=426.62 +/- 32.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 818000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00137  |
|    ent_coef        | 0.00061  |
|    ent_coef_loss   | -0.498   |
|    learning_rate   | 0.00418  |
|    n_updates       | 3990     |
---------------------------------
Eval num_timesteps=819000, episode_reward=409.81 +/- 18.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 410      |
| time/              |          |
|    total_timesteps | 819000   |
---------------------------------
Eval num_timesteps=820000, episode_reward=431.78 +/- 41.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 432      |
| time/              |          |
|    total_timesteps | 820000   |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 0.00141  |
|    ent_coef        | 0.000611 |
|    ent_coef_loss   | -11.4    |
|    learning_rate   | 0.00418  |
|    n_updates       | 4000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 421      |
| time/              |          |
|    episodes        | 820      |
|    fps             | 637      |
|    time_elapsed    | 1287     |
|    total_timesteps | 820000   |
---------------------------------
Eval num_timesteps=821000, episode_reward=390.44 +/- 17.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 821000   |
---------------------------------
Eval num_timesteps=822000, episode_reward=441.32 +/- 31.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 441      |
| time/              |          |
|    total_timesteps | 822000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 0.00168  |
|    ent_coef        | 0.000607 |
|    ent_coef_loss   | 3.43     |
|    learning_rate   | 0.00418  |
|    n_updates       | 4010     |
---------------------------------
Eval num_timesteps=823000, episode_reward=468.97 +/- 52.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 469      |
| time/              |          |
|    total_timesteps | 823000   |
---------------------------------
Eval num_timesteps=824000, episode_reward=668.28 +/- 30.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 824000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00166  |
|    ent_coef        | 0.000607 |
|    ent_coef_loss   | 6.64     |
|    learning_rate   | 0.00418  |
|    n_updates       | 4020     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 422      |
| time/              |          |
|    episodes        | 824      |
|    fps             | 637      |
|    time_elapsed    | 1293     |
|    total_timesteps | 824000   |
---------------------------------
Eval num_timesteps=825000, episode_reward=625.57 +/- 20.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 626      |
| time/              |          |
|    total_timesteps | 825000   |
---------------------------------
Eval num_timesteps=826000, episode_reward=548.93 +/- 25.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 549      |
| time/              |          |
|    total_timesteps | 826000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.00114  |
|    ent_coef        | 0.000608 |
|    ent_coef_loss   | -13.6    |
|    learning_rate   | 0.00417  |
|    n_updates       | 4030     |
---------------------------------
Eval num_timesteps=827000, episode_reward=539.53 +/- 26.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 540      |
| time/              |          |
|    total_timesteps | 827000   |
---------------------------------
Eval num_timesteps=828000, episode_reward=488.90 +/- 67.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 489      |
| time/              |          |
|    total_timesteps | 828000   |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 0.00114  |
|    ent_coef        | 0.000602 |
|    ent_coef_loss   | -6.7     |
|    learning_rate   | 0.00417  |
|    n_updates       | 4040     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 424      |
| time/              |          |
|    episodes        | 828      |
|    fps             | 637      |
|    time_elapsed    | 1299     |
|    total_timesteps | 828000   |
---------------------------------
Eval num_timesteps=829000, episode_reward=614.08 +/- 78.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 614      |
| time/              |          |
|    total_timesteps | 829000   |
---------------------------------
Eval num_timesteps=830000, episode_reward=879.59 +/- 65.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 830000   |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.000597 |
|    ent_coef_loss   | 3.27     |
|    learning_rate   | 0.00417  |
|    n_updates       | 4050     |
---------------------------------
New best mean reward!
Eval num_timesteps=831000, episode_reward=883.35 +/- 43.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 831000   |
---------------------------------
New best mean reward!
Eval num_timesteps=832000, episode_reward=690.43 +/- 7.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 690      |
| time/              |          |
|    total_timesteps | 832000   |
| train/             |          |
|    actor_loss      | -2.35    |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000597 |
|    ent_coef_loss   | 1.48     |
|    learning_rate   | 0.00417  |
|    n_updates       | 4060     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 433      |
| time/              |          |
|    episodes        | 832      |
|    fps             | 637      |
|    time_elapsed    | 1305     |
|    total_timesteps | 832000   |
---------------------------------
Eval num_timesteps=833000, episode_reward=693.30 +/- 18.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 833000   |
---------------------------------
Eval num_timesteps=834000, episode_reward=501.10 +/- 81.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 501      |
| time/              |          |
|    total_timesteps | 834000   |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.00102  |
|    ent_coef        | 0.000598 |
|    ent_coef_loss   | 1.81     |
|    learning_rate   | 0.00417  |
|    n_updates       | 4070     |
---------------------------------
Eval num_timesteps=835000, episode_reward=533.43 +/- 68.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 835000   |
---------------------------------
Eval num_timesteps=836000, episode_reward=473.19 +/- 42.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 473      |
| time/              |          |
|    total_timesteps | 836000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00141  |
|    ent_coef        | 0.000599 |
|    ent_coef_loss   | 3.03     |
|    learning_rate   | 0.00416  |
|    n_updates       | 4080     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 437      |
| time/              |          |
|    episodes        | 836      |
|    fps             | 637      |
|    time_elapsed    | 1311     |
|    total_timesteps | 836000   |
---------------------------------
Eval num_timesteps=837000, episode_reward=508.46 +/- 30.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 508      |
| time/              |          |
|    total_timesteps | 837000   |
---------------------------------
Eval num_timesteps=838000, episode_reward=586.25 +/- 49.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 586      |
| time/              |          |
|    total_timesteps | 838000   |
| train/             |          |
|    actor_loss      | -2.25    |
|    critic_loss     | 0.0014   |
|    ent_coef        | 0.000602 |
|    ent_coef_loss   | 13.4     |
|    learning_rate   | 0.00416  |
|    n_updates       | 4090     |
---------------------------------
Eval num_timesteps=839000, episode_reward=620.91 +/- 30.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 621      |
| time/              |          |
|    total_timesteps | 839000   |
---------------------------------
Eval num_timesteps=840000, episode_reward=605.65 +/- 70.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 840000   |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 0.00122  |
|    ent_coef        | 0.00061  |
|    ent_coef_loss   | 0.943    |
|    learning_rate   | 0.00416  |
|    n_updates       | 4100     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 439      |
| time/              |          |
|    episodes        | 840      |
|    fps             | 637      |
|    time_elapsed    | 1317     |
|    total_timesteps | 840000   |
---------------------------------
Eval num_timesteps=841000, episode_reward=505.16 +/- 70.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 505      |
| time/              |          |
|    total_timesteps | 841000   |
---------------------------------
Eval num_timesteps=842000, episode_reward=461.93 +/- 98.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 462      |
| time/              |          |
|    total_timesteps | 842000   |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 0.00151  |
|    ent_coef        | 0.000613 |
|    ent_coef_loss   | -3.48    |
|    learning_rate   | 0.00416  |
|    n_updates       | 4110     |
---------------------------------
Eval num_timesteps=843000, episode_reward=523.60 +/- 79.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 524      |
| time/              |          |
|    total_timesteps | 843000   |
---------------------------------
Eval num_timesteps=844000, episode_reward=820.69 +/- 13.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 844000   |
| train/             |          |
|    actor_loss      | -2.31    |
|    critic_loss     | 0.00127  |
|    ent_coef        | 0.000612 |
|    ent_coef_loss   | -6.29    |
|    learning_rate   | 0.00416  |
|    n_updates       | 4120     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 449      |
| time/              |          |
|    episodes        | 844      |
|    fps             | 637      |
|    time_elapsed    | 1324     |
|    total_timesteps | 844000   |
---------------------------------
Eval num_timesteps=845000, episode_reward=833.92 +/- 17.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 845000   |
---------------------------------
Eval num_timesteps=846000, episode_reward=706.73 +/- 25.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 846000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.00111  |
|    ent_coef        | 0.000608 |
|    ent_coef_loss   | -2.09    |
|    learning_rate   | 0.00415  |
|    n_updates       | 4130     |
---------------------------------
Eval num_timesteps=847000, episode_reward=714.21 +/- 25.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 847000   |
---------------------------------
Eval num_timesteps=848000, episode_reward=626.15 +/- 85.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 626      |
| time/              |          |
|    total_timesteps | 848000   |
| train/             |          |
|    actor_loss      | -2.32    |
|    critic_loss     | 0.00121  |
|    ent_coef        | 0.000605 |
|    ent_coef_loss   | -4.91    |
|    learning_rate   | 0.00415  |
|    n_updates       | 4140     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 459      |
| time/              |          |
|    episodes        | 848      |
|    fps             | 637      |
|    time_elapsed    | 1330     |
|    total_timesteps | 848000   |
---------------------------------
Eval num_timesteps=849000, episode_reward=587.37 +/- 52.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 587      |
| time/              |          |
|    total_timesteps | 849000   |
---------------------------------
Eval num_timesteps=850000, episode_reward=721.96 +/- 43.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 722      |
| time/              |          |
|    total_timesteps | 850000   |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 0.00158  |
|    ent_coef        | 0.000602 |
|    ent_coef_loss   | -3.08    |
|    learning_rate   | 0.00415  |
|    n_updates       | 4150     |
---------------------------------
Eval num_timesteps=851000, episode_reward=647.02 +/- 38.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 851000   |
---------------------------------
Eval num_timesteps=852000, episode_reward=844.12 +/- 20.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 852000   |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 0.00151  |
|    ent_coef        | 0.000599 |
|    ent_coef_loss   | 4.23     |
|    learning_rate   | 0.00415  |
|    n_updates       | 4160     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 466      |
| time/              |          |
|    episodes        | 852      |
|    fps             | 637      |
|    time_elapsed    | 1336     |
|    total_timesteps | 852000   |
---------------------------------
Eval num_timesteps=853000, episode_reward=853.88 +/- 17.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 853000   |
---------------------------------
Eval num_timesteps=854000, episode_reward=843.61 +/- 17.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 854000   |
---------------------------------
Eval num_timesteps=855000, episode_reward=695.25 +/- 46.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 695      |
| time/              |          |
|    total_timesteps | 855000   |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.00121  |
|    ent_coef        | 0.0006   |
|    ent_coef_loss   | 0.767    |
|    learning_rate   | 0.00415  |
|    n_updates       | 4170     |
---------------------------------
Eval num_timesteps=856000, episode_reward=636.34 +/- 41.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 636      |
| time/              |          |
|    total_timesteps | 856000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 475      |
| time/              |          |
|    episodes        | 856      |
|    fps             | 637      |
|    time_elapsed    | 1342     |
|    total_timesteps | 856000   |
---------------------------------
Eval num_timesteps=857000, episode_reward=318.92 +/- 291.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 319      |
| time/              |          |
|    total_timesteps | 857000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00145  |
|    ent_coef        | 0.000601 |
|    ent_coef_loss   | -3.99    |
|    learning_rate   | 0.00414  |
|    n_updates       | 4180     |
---------------------------------
Eval num_timesteps=858000, episode_reward=424.08 +/- 32.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 424      |
| time/              |          |
|    total_timesteps | 858000   |
---------------------------------
Eval num_timesteps=859000, episode_reward=594.03 +/- 28.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 594      |
| time/              |          |
|    total_timesteps | 859000   |
| train/             |          |
|    actor_loss      | -2.25    |
|    critic_loss     | 0.00132  |
|    ent_coef        | 0.000599 |
|    ent_coef_loss   | 2.58     |
|    learning_rate   | 0.00414  |
|    n_updates       | 4190     |
---------------------------------
Eval num_timesteps=860000, episode_reward=582.29 +/- 79.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 582      |
| time/              |          |
|    total_timesteps | 860000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 476      |
| time/              |          |
|    episodes        | 860      |
|    fps             | 637      |
|    time_elapsed    | 1348     |
|    total_timesteps | 860000   |
---------------------------------
Eval num_timesteps=861000, episode_reward=668.16 +/- 59.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 861000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 0.00119  |
|    ent_coef        | 0.0006   |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.00414  |
|    n_updates       | 4200     |
---------------------------------
Eval num_timesteps=862000, episode_reward=686.86 +/- 72.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 862000   |
---------------------------------
Eval num_timesteps=863000, episode_reward=740.52 +/- 54.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 863000   |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 0.00105  |
|    ent_coef        | 0.000601 |
|    ent_coef_loss   | -5.51    |
|    learning_rate   | 0.00414  |
|    n_updates       | 4210     |
---------------------------------
Eval num_timesteps=864000, episode_reward=717.41 +/- 59.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 864000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 481      |
| time/              |          |
|    episodes        | 864      |
|    fps             | 637      |
|    time_elapsed    | 1354     |
|    total_timesteps | 864000   |
---------------------------------
Eval num_timesteps=865000, episode_reward=769.04 +/- 17.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 865000   |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 0.00112  |
|    ent_coef        | 0.000598 |
|    ent_coef_loss   | -1.57    |
|    learning_rate   | 0.00414  |
|    n_updates       | 4220     |
---------------------------------
Eval num_timesteps=866000, episode_reward=792.25 +/- 32.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 866000   |
---------------------------------
Eval num_timesteps=867000, episode_reward=655.66 +/- 64.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 867000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 0.00137  |
|    ent_coef        | 0.000596 |
|    ent_coef_loss   | 2.46     |
|    learning_rate   | 0.00413  |
|    n_updates       | 4230     |
---------------------------------
Eval num_timesteps=868000, episode_reward=619.67 +/- 35.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 868000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 484      |
| time/              |          |
|    episodes        | 868      |
|    fps             | 637      |
|    time_elapsed    | 1360     |
|    total_timesteps | 868000   |
---------------------------------
Eval num_timesteps=869000, episode_reward=652.16 +/- 34.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 652      |
| time/              |          |
|    total_timesteps | 869000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00118  |
|    ent_coef        | 0.000597 |
|    ent_coef_loss   | 7.79     |
|    learning_rate   | 0.00413  |
|    n_updates       | 4240     |
---------------------------------
Eval num_timesteps=870000, episode_reward=646.15 +/- 102.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 646      |
| time/              |          |
|    total_timesteps | 870000   |
---------------------------------
Eval num_timesteps=871000, episode_reward=605.62 +/- 43.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 871000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 0.00125  |
|    ent_coef        | 0.000603 |
|    ent_coef_loss   | 13.1     |
|    learning_rate   | 0.00413  |
|    n_updates       | 4250     |
---------------------------------
Eval num_timesteps=872000, episode_reward=672.30 +/- 33.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 872000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 488      |
| time/              |          |
|    episodes        | 872      |
|    fps             | 638      |
|    time_elapsed    | 1366     |
|    total_timesteps | 872000   |
---------------------------------
Eval num_timesteps=873000, episode_reward=778.87 +/- 62.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 873000   |
| train/             |          |
|    actor_loss      | -2.25    |
|    critic_loss     | 0.00125  |
|    ent_coef        | 0.000612 |
|    ent_coef_loss   | 2.57     |
|    learning_rate   | 0.00413  |
|    n_updates       | 4260     |
---------------------------------
Eval num_timesteps=874000, episode_reward=760.35 +/- 69.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 760      |
| time/              |          |
|    total_timesteps | 874000   |
---------------------------------
Eval num_timesteps=875000, episode_reward=788.77 +/- 75.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 789      |
| time/              |          |
|    total_timesteps | 875000   |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 0.0013   |
|    ent_coef        | 0.000618 |
|    ent_coef_loss   | 3.16     |
|    learning_rate   | 0.00413  |
|    n_updates       | 4270     |
---------------------------------
Eval num_timesteps=876000, episode_reward=846.81 +/- 55.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 847      |
| time/              |          |
|    total_timesteps | 876000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 505      |
| time/              |          |
|    episodes        | 876      |
|    fps             | 638      |
|    time_elapsed    | 1372     |
|    total_timesteps | 876000   |
---------------------------------
Eval num_timesteps=877000, episode_reward=676.38 +/- 66.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 676      |
| time/              |          |
|    total_timesteps | 877000   |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 0.00136  |
|    ent_coef        | 0.000621 |
|    ent_coef_loss   | 3.48     |
|    learning_rate   | 0.00412  |
|    n_updates       | 4280     |
---------------------------------
Eval num_timesteps=878000, episode_reward=632.52 +/- 29.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 633      |
| time/              |          |
|    total_timesteps | 878000   |
---------------------------------
Eval num_timesteps=879000, episode_reward=608.46 +/- 65.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 879000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 0.00139  |
|    ent_coef        | 0.000626 |
|    ent_coef_loss   | 8.03     |
|    learning_rate   | 0.00412  |
|    n_updates       | 4290     |
---------------------------------
Eval num_timesteps=880000, episode_reward=567.12 +/- 94.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 567      |
| time/              |          |
|    total_timesteps | 880000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 507      |
| time/              |          |
|    episodes        | 880      |
|    fps             | 638      |
|    time_elapsed    | 1378     |
|    total_timesteps | 880000   |
---------------------------------
Eval num_timesteps=881000, episode_reward=676.19 +/- 75.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 676      |
| time/              |          |
|    total_timesteps | 881000   |
| train/             |          |
|    actor_loss      | -2.25    |
|    critic_loss     | 0.00127  |
|    ent_coef        | 0.000633 |
|    ent_coef_loss   | 8.58     |
|    learning_rate   | 0.00412  |
|    n_updates       | 4300     |
---------------------------------
Eval num_timesteps=882000, episode_reward=746.91 +/- 52.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 882000   |
---------------------------------
Eval num_timesteps=883000, episode_reward=812.97 +/- 20.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 883000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.00064  |
|    ent_coef_loss   | 5.09     |
|    learning_rate   | 0.00412  |
|    n_updates       | 4310     |
---------------------------------
Eval num_timesteps=884000, episode_reward=787.37 +/- 75.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 884000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 519      |
| time/              |          |
|    episodes        | 884      |
|    fps             | 638      |
|    time_elapsed    | 1384     |
|    total_timesteps | 884000   |
---------------------------------
Eval num_timesteps=885000, episode_reward=741.84 +/- 62.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 885000   |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 0.00112  |
|    ent_coef        | 0.000647 |
|    ent_coef_loss   | 4.13     |
|    learning_rate   | 0.00412  |
|    n_updates       | 4320     |
---------------------------------
Eval num_timesteps=886000, episode_reward=691.02 +/- 124.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 691      |
| time/              |          |
|    total_timesteps | 886000   |
---------------------------------
Eval num_timesteps=887000, episode_reward=524.93 +/- 60.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 525      |
| time/              |          |
|    total_timesteps | 887000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00124  |
|    ent_coef        | 0.000652 |
|    ent_coef_loss   | 3.34     |
|    learning_rate   | 0.00411  |
|    n_updates       | 4330     |
---------------------------------
Eval num_timesteps=888000, episode_reward=353.80 +/- 382.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 354      |
| time/              |          |
|    total_timesteps | 888000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 527      |
| time/              |          |
|    episodes        | 888      |
|    fps             | 638      |
|    time_elapsed    | 1390     |
|    total_timesteps | 888000   |
---------------------------------
Eval num_timesteps=889000, episode_reward=439.12 +/- 71.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 439      |
| time/              |          |
|    total_timesteps | 889000   |
| train/             |          |
|    actor_loss      | -2.23    |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.000656 |
|    ent_coef_loss   | 8.91     |
|    learning_rate   | 0.00411  |
|    n_updates       | 4340     |
---------------------------------
Eval num_timesteps=890000, episode_reward=554.62 +/- 20.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 555      |
| time/              |          |
|    total_timesteps | 890000   |
---------------------------------
Eval num_timesteps=891000, episode_reward=750.27 +/- 25.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 891000   |
| train/             |          |
|    actor_loss      | -2.22    |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000665 |
|    ent_coef_loss   | 9.55     |
|    learning_rate   | 0.00411  |
|    n_updates       | 4350     |
---------------------------------
Eval num_timesteps=892000, episode_reward=719.13 +/- 61.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 892000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 530      |
| time/              |          |
|    episodes        | 892      |
|    fps             | 638      |
|    time_elapsed    | 1396     |
|    total_timesteps | 892000   |
---------------------------------
Eval num_timesteps=893000, episode_reward=-63.26 +/- 195.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -63.3    |
| time/              |          |
|    total_timesteps | 893000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 0.00133  |
|    ent_coef        | 0.000673 |
|    ent_coef_loss   | -2.33    |
|    learning_rate   | 0.00411  |
|    n_updates       | 4360     |
---------------------------------
Eval num_timesteps=894000, episode_reward=-95.18 +/- 154.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -95.2    |
| time/              |          |
|    total_timesteps | 894000   |
---------------------------------
Eval num_timesteps=895000, episode_reward=367.87 +/- 282.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 368      |
| time/              |          |
|    total_timesteps | 895000   |
| train/             |          |
|    actor_loss      | -2.15    |
|    critic_loss     | 0.0015   |
|    ent_coef        | 0.000675 |
|    ent_coef_loss   | -1.15    |
|    learning_rate   | 0.00411  |
|    n_updates       | 4370     |
---------------------------------
Eval num_timesteps=896000, episode_reward=62.69 +/- 271.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 62.7     |
| time/              |          |
|    total_timesteps | 896000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 523      |
| time/              |          |
|    episodes        | 896      |
|    fps             | 638      |
|    time_elapsed    | 1403     |
|    total_timesteps | 896000   |
---------------------------------
Eval num_timesteps=897000, episode_reward=263.16 +/- 367.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 263      |
| time/              |          |
|    total_timesteps | 897000   |
---------------------------------
Eval num_timesteps=898000, episode_reward=81.28 +/- 362.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 81.3     |
| time/              |          |
|    total_timesteps | 898000   |
| train/             |          |
|    actor_loss      | -2.22    |
|    critic_loss     | 0.00156  |
|    ent_coef        | 0.000676 |
|    ent_coef_loss   | 13.4     |
|    learning_rate   | 0.0041   |
|    n_updates       | 4380     |
---------------------------------
Eval num_timesteps=899000, episode_reward=140.27 +/- 408.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 140      |
| time/              |          |
|    total_timesteps | 899000   |
---------------------------------
Eval num_timesteps=900000, episode_reward=-211.37 +/- 3.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 900000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 0.00142  |
|    ent_coef        | 0.000685 |
|    ent_coef_loss   | 3.26     |
|    learning_rate   | 0.0041   |
|    n_updates       | 4390     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 528      |
| time/              |          |
|    episodes        | 900      |
|    fps             | 638      |
|    time_elapsed    | 1409     |
|    total_timesteps | 900000   |
---------------------------------
Eval num_timesteps=901000, episode_reward=-212.82 +/- 5.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 901000   |
---------------------------------
Eval num_timesteps=902000, episode_reward=-117.95 +/- 4.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 902000   |
| train/             |          |
|    actor_loss      | -2.22    |
|    critic_loss     | 0.00128  |
|    ent_coef        | 0.000691 |
|    ent_coef_loss   | 1.31     |
|    learning_rate   | 0.0041   |
|    n_updates       | 4400     |
---------------------------------
Eval num_timesteps=903000, episode_reward=-117.07 +/- 4.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 903000   |
---------------------------------
Eval num_timesteps=904000, episode_reward=24.08 +/- 294.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 24.1     |
| time/              |          |
|    total_timesteps | 904000   |
| train/             |          |
|    actor_loss      | -2.23    |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.000693 |
|    ent_coef_loss   | -4.09    |
|    learning_rate   | 0.0041   |
|    n_updates       | 4410     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 526      |
| time/              |          |
|    episodes        | 904      |
|    fps             | 638      |
|    time_elapsed    | 1415     |
|    total_timesteps | 904000   |
---------------------------------
Eval num_timesteps=905000, episode_reward=-123.07 +/- 3.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 905000   |
---------------------------------
Eval num_timesteps=906000, episode_reward=-133.07 +/- 3.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 906000   |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 0.00115  |
|    ent_coef        | 0.000692 |
|    ent_coef_loss   | -3.83    |
|    learning_rate   | 0.00409  |
|    n_updates       | 4420     |
---------------------------------
Eval num_timesteps=907000, episode_reward=-130.21 +/- 5.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 907000   |
---------------------------------
Eval num_timesteps=908000, episode_reward=-103.79 +/- 4.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 908000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 0.00115  |
|    ent_coef        | 0.000689 |
|    ent_coef_loss   | 0.207    |
|    learning_rate   | 0.00409  |
|    n_updates       | 4430     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 543      |
| time/              |          |
|    episodes        | 908      |
|    fps             | 638      |
|    time_elapsed    | 1421     |
|    total_timesteps | 908000   |
---------------------------------
Eval num_timesteps=909000, episode_reward=-104.49 +/- 8.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 909000   |
---------------------------------
Eval num_timesteps=910000, episode_reward=0.35 +/- 131.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 0.354    |
| time/              |          |
|    total_timesteps | 910000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 0.00114  |
|    ent_coef        | 0.000686 |
|    ent_coef_loss   | -7.14    |
|    learning_rate   | 0.00409  |
|    n_updates       | 4440     |
---------------------------------
Eval num_timesteps=911000, episode_reward=-69.07 +/- 7.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -69.1    |
| time/              |          |
|    total_timesteps | 911000   |
---------------------------------
Eval num_timesteps=912000, episode_reward=800.46 +/- 77.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 800      |
| time/              |          |
|    total_timesteps | 912000   |
| train/             |          |
|    actor_loss      | -2.22    |
|    critic_loss     | 0.00122  |
|    ent_coef        | 0.000681 |
|    ent_coef_loss   | 1.15     |
|    learning_rate   | 0.00409  |
|    n_updates       | 4450     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 549      |
| time/              |          |
|    episodes        | 912      |
|    fps             | 638      |
|    time_elapsed    | 1427     |
|    total_timesteps | 912000   |
---------------------------------
Eval num_timesteps=913000, episode_reward=726.32 +/- 84.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 913000   |
---------------------------------
Eval num_timesteps=914000, episode_reward=720.86 +/- 34.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 914000   |
| train/             |          |
|    actor_loss      | -2.22    |
|    critic_loss     | 0.00126  |
|    ent_coef        | 0.00068  |
|    ent_coef_loss   | 5.38     |
|    learning_rate   | 0.00409  |
|    n_updates       | 4460     |
---------------------------------
Eval num_timesteps=915000, episode_reward=547.38 +/- 368.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 547      |
| time/              |          |
|    total_timesteps | 915000   |
---------------------------------
Eval num_timesteps=916000, episode_reward=-169.35 +/- 3.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 916000   |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 0.00124  |
|    ent_coef        | 0.000685 |
|    ent_coef_loss   | 7.33     |
|    learning_rate   | 0.00408  |
|    n_updates       | 4470     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 566      |
| time/              |          |
|    episodes        | 916      |
|    fps             | 638      |
|    time_elapsed    | 1433     |
|    total_timesteps | 916000   |
---------------------------------
Eval num_timesteps=917000, episode_reward=-170.21 +/- 4.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 917000   |
---------------------------------
Eval num_timesteps=918000, episode_reward=-382.03 +/- 1.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 918000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 0.000669 |
|    ent_coef        | 0.000689 |
|    ent_coef_loss   | -0.0354  |
|    learning_rate   | 0.00408  |
|    n_updates       | 4480     |
---------------------------------
Eval num_timesteps=919000, episode_reward=-380.69 +/- 2.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -381     |
| time/              |          |
|    total_timesteps | 919000   |
---------------------------------
Eval num_timesteps=920000, episode_reward=-488.07 +/- 13.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 920000   |
| train/             |          |
|    actor_loss      | -2.34    |
|    critic_loss     | 0.000233 |
|    ent_coef        | 0.000692 |
|    ent_coef_loss   | 11       |
|    learning_rate   | 0.00408  |
|    n_updates       | 4490     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 538      |
| time/              |          |
|    episodes        | 920      |
|    fps             | 638      |
|    time_elapsed    | 1440     |
|    total_timesteps | 920000   |
---------------------------------
Eval num_timesteps=921000, episode_reward=-492.01 +/- 14.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -492     |
| time/              |          |
|    total_timesteps | 921000   |
---------------------------------
Eval num_timesteps=922000, episode_reward=-513.63 +/- 11.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 922000   |
| train/             |          |
|    actor_loss      | -2.32    |
|    critic_loss     | 0.00033  |
|    ent_coef        | 0.000704 |
|    ent_coef_loss   | 26.6     |
|    learning_rate   | 0.00408  |
|    n_updates       | 4500     |
---------------------------------
Eval num_timesteps=923000, episode_reward=-504.47 +/- 8.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 923000   |
---------------------------------
Eval num_timesteps=924000, episode_reward=-550.73 +/- 1.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 924000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.000126 |
|    ent_coef        | 0.000732 |
|    ent_coef_loss   | 41.2     |
|    learning_rate   | 0.00408  |
|    n_updates       | 4510     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 502      |
| time/              |          |
|    episodes        | 924      |
|    fps             | 638      |
|    time_elapsed    | 1446     |
|    total_timesteps | 924000   |
---------------------------------
Eval num_timesteps=925000, episode_reward=-551.27 +/- 3.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 925000   |
---------------------------------
Eval num_timesteps=926000, episode_reward=-536.51 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 926000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 9.95e-05 |
|    ent_coef        | 0.000779 |
|    ent_coef_loss   | 36.6     |
|    learning_rate   | 0.00407  |
|    n_updates       | 4520     |
---------------------------------
Eval num_timesteps=927000, episode_reward=-533.47 +/- 3.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 927000   |
---------------------------------
Eval num_timesteps=928000, episode_reward=-500.45 +/- 1.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -500     |
| time/              |          |
|    total_timesteps | 928000   |
| train/             |          |
|    actor_loss      | -2.33    |
|    critic_loss     | 0.000114 |
|    ent_coef        | 0.000829 |
|    ent_coef_loss   | 29.9     |
|    learning_rate   | 0.00407  |
|    n_updates       | 4530     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 460      |
| time/              |          |
|    episodes        | 928      |
|    fps             | 638      |
|    time_elapsed    | 1452     |
|    total_timesteps | 928000   |
---------------------------------
Eval num_timesteps=929000, episode_reward=-500.51 +/- 2.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 929000   |
---------------------------------
Eval num_timesteps=930000, episode_reward=-462.95 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -463     |
| time/              |          |
|    total_timesteps | 930000   |
| train/             |          |
|    actor_loss      | -2.32    |
|    critic_loss     | 9.03e-05 |
|    ent_coef        | 0.000875 |
|    ent_coef_loss   | 20.7     |
|    learning_rate   | 0.00407  |
|    n_updates       | 4540     |
---------------------------------
Eval num_timesteps=931000, episode_reward=-462.53 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -463     |
| time/              |          |
|    total_timesteps | 931000   |
---------------------------------
Eval num_timesteps=932000, episode_reward=-462.22 +/- 1.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -462     |
| time/              |          |
|    total_timesteps | 932000   |
| train/             |          |
|    actor_loss      | -2.32    |
|    critic_loss     | 6.23e-05 |
|    ent_coef        | 0.000914 |
|    ent_coef_loss   | 18.2     |
|    learning_rate   | 0.00407  |
|    n_updates       | 4550     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 415      |
| time/              |          |
|    episodes        | 932      |
|    fps             | 638      |
|    time_elapsed    | 1458     |
|    total_timesteps | 932000   |
---------------------------------
Eval num_timesteps=933000, episode_reward=-462.73 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -463     |
| time/              |          |
|    total_timesteps | 933000   |
---------------------------------
Eval num_timesteps=934000, episode_reward=-460.01 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -460     |
| time/              |          |
|    total_timesteps | 934000   |
| train/             |          |
|    actor_loss      | -2.32    |
|    critic_loss     | 7.32e-05 |
|    ent_coef        | 0.000948 |
|    ent_coef_loss   | 14.9     |
|    learning_rate   | 0.00407  |
|    n_updates       | 4560     |
---------------------------------
Eval num_timesteps=935000, episode_reward=-459.67 +/- 2.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -460     |
| time/              |          |
|    total_timesteps | 935000   |
---------------------------------
Eval num_timesteps=936000, episode_reward=-448.91 +/- 1.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -449     |
| time/              |          |
|    total_timesteps | 936000   |
| train/             |          |
|    actor_loss      | -2.32    |
|    critic_loss     | 5.3e-05  |
|    ent_coef        | 0.000977 |
|    ent_coef_loss   | 12       |
|    learning_rate   | 0.00406  |
|    n_updates       | 4570     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 377      |
| time/              |          |
|    episodes        | 936      |
|    fps             | 638      |
|    time_elapsed    | 1464     |
|    total_timesteps | 936000   |
---------------------------------
Eval num_timesteps=937000, episode_reward=-448.82 +/- 1.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -449     |
| time/              |          |
|    total_timesteps | 937000   |
---------------------------------
Eval num_timesteps=938000, episode_reward=-439.19 +/- 1.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -439     |
| time/              |          |
|    total_timesteps | 938000   |
| train/             |          |
|    actor_loss      | -2.31    |
|    critic_loss     | 5.99e-05 |
|    ent_coef        | 0.001    |
|    ent_coef_loss   | 7.12     |
|    learning_rate   | 0.00406  |
|    n_updates       | 4580     |
---------------------------------
Eval num_timesteps=939000, episode_reward=-436.13 +/- 2.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 939000   |
---------------------------------
Eval num_timesteps=940000, episode_reward=-439.00 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -439     |
| time/              |          |
|    total_timesteps | 940000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 342      |
| time/              |          |
|    episodes        | 940      |
|    fps             | 638      |
|    time_elapsed    | 1471     |
|    total_timesteps | 940000   |
---------------------------------
Eval num_timesteps=941000, episode_reward=-422.18 +/- 1.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -422     |
| time/              |          |
|    total_timesteps | 941000   |
| train/             |          |
|    actor_loss      | -2.31    |
|    critic_loss     | 6.17e-05 |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | 4.35     |
|    learning_rate   | 0.00406  |
|    n_updates       | 4590     |
---------------------------------
Eval num_timesteps=942000, episode_reward=-423.67 +/- 2.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 942000   |
---------------------------------
Eval num_timesteps=943000, episode_reward=-399.55 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 943000   |
| train/             |          |
|    actor_loss      | -2.31    |
|    critic_loss     | 3.37e-05 |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.00406  |
|    n_updates       | 4600     |
---------------------------------
Eval num_timesteps=944000, episode_reward=-399.44 +/- 0.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -399     |
| time/              |          |
|    total_timesteps | 944000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 303      |
| time/              |          |
|    episodes        | 944      |
|    fps             | 638      |
|    time_elapsed    | 1477     |
|    total_timesteps | 944000   |
---------------------------------
Eval num_timesteps=945000, episode_reward=-355.84 +/- 0.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -356     |
| time/              |          |
|    total_timesteps | 945000   |
| train/             |          |
|    actor_loss      | -2.31    |
|    critic_loss     | 3.99e-05 |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -2.51    |
|    learning_rate   | 0.00406  |
|    n_updates       | 4610     |
---------------------------------
Eval num_timesteps=946000, episode_reward=-356.29 +/- 0.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -356     |
| time/              |          |
|    total_timesteps | 946000   |
---------------------------------
Eval num_timesteps=947000, episode_reward=-334.17 +/- 1.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -334     |
| time/              |          |
|    total_timesteps | 947000   |
| train/             |          |
|    actor_loss      | -2.31    |
|    critic_loss     | 5.52e-05 |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -3.71    |
|    learning_rate   | 0.00405  |
|    n_updates       | 4620     |
---------------------------------
Eval num_timesteps=948000, episode_reward=-332.47 +/- 2.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -332     |
| time/              |          |
|    total_timesteps | 948000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 261      |
| time/              |          |
|    episodes        | 948      |
|    fps             | 638      |
|    time_elapsed    | 1483     |
|    total_timesteps | 948000   |
---------------------------------
Eval num_timesteps=949000, episode_reward=-335.57 +/- 2.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 949000   |
| train/             |          |
|    actor_loss      | -2.31    |
|    critic_loss     | 6.14e-05 |
|    ent_coef        | 0.00103  |
|    ent_coef_loss   | -1.86    |
|    learning_rate   | 0.00405  |
|    n_updates       | 4630     |
---------------------------------
Eval num_timesteps=950000, episode_reward=-335.32 +/- 0.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 950000   |
---------------------------------
Eval num_timesteps=951000, episode_reward=-309.82 +/- 2.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 951000   |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 3.96e-05 |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | -4.52    |
|    learning_rate   | 0.00405  |
|    n_updates       | 4640     |
---------------------------------
Eval num_timesteps=952000, episode_reward=-310.78 +/- 1.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 952000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 224      |
| time/              |          |
|    episodes        | 952      |
|    fps             | 638      |
|    time_elapsed    | 1490     |
|    total_timesteps | 952000   |
---------------------------------
Eval num_timesteps=953000, episode_reward=-296.71 +/- 2.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 953000   |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 4.99e-05 |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | -6.08    |
|    learning_rate   | 0.00405  |
|    n_updates       | 4650     |
---------------------------------
Eval num_timesteps=954000, episode_reward=-297.49 +/- 2.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 954000   |
---------------------------------
Eval num_timesteps=955000, episode_reward=-287.04 +/- 2.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 955000   |
| train/             |          |
|    actor_loss      | -2.3     |
|    critic_loss     | 6.66e-05 |
|    ent_coef        | 0.001    |
|    ent_coef_loss   | -8.43    |
|    learning_rate   | 0.00405  |
|    n_updates       | 4660     |
---------------------------------
Eval num_timesteps=956000, episode_reward=-285.90 +/- 2.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 956000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 184      |
| time/              |          |
|    episodes        | 956      |
|    fps             | 638      |
|    time_elapsed    | 1496     |
|    total_timesteps | 956000   |
---------------------------------
Eval num_timesteps=957000, episode_reward=-265.56 +/- 1.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 957000   |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 6.87e-05 |
|    ent_coef        | 0.00099  |
|    ent_coef_loss   | -9.75    |
|    learning_rate   | 0.00404  |
|    n_updates       | 4670     |
---------------------------------
Eval num_timesteps=958000, episode_reward=-262.73 +/- 1.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 958000   |
---------------------------------
Eval num_timesteps=959000, episode_reward=-259.96 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 959000   |
| train/             |          |
|    actor_loss      | -2.29    |
|    critic_loss     | 6.17e-05 |
|    ent_coef        | 0.000973 |
|    ent_coef_loss   | -11.1    |
|    learning_rate   | 0.00404  |
|    n_updates       | 4680     |
---------------------------------
Eval num_timesteps=960000, episode_reward=-259.13 +/- 1.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 960000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 155      |
| time/              |          |
|    episodes        | 960      |
|    fps             | 638      |
|    time_elapsed    | 1502     |
|    total_timesteps | 960000   |
---------------------------------
Eval num_timesteps=961000, episode_reward=-245.22 +/- 3.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 961000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 6.99e-05 |
|    ent_coef        | 0.000954 |
|    ent_coef_loss   | -12.8    |
|    learning_rate   | 0.00404  |
|    n_updates       | 4690     |
---------------------------------
Eval num_timesteps=962000, episode_reward=-245.32 +/- 2.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 962000   |
---------------------------------
Eval num_timesteps=963000, episode_reward=-263.03 +/- 3.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 963000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 6.22e-05 |
|    ent_coef        | 0.000933 |
|    ent_coef_loss   | -9.37    |
|    learning_rate   | 0.00404  |
|    n_updates       | 4700     |
---------------------------------
Eval num_timesteps=964000, episode_reward=-261.86 +/- 3.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 964000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 120      |
| time/              |          |
|    episodes        | 964      |
|    fps             | 638      |
|    time_elapsed    | 1509     |
|    total_timesteps | 964000   |
---------------------------------
Eval num_timesteps=965000, episode_reward=-270.64 +/- 2.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 965000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 5.68e-05 |
|    ent_coef        | 0.000915 |
|    ent_coef_loss   | -9.37    |
|    learning_rate   | 0.00404  |
|    n_updates       | 4710     |
---------------------------------
Eval num_timesteps=966000, episode_reward=-270.30 +/- 2.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -270     |
| time/              |          |
|    total_timesteps | 966000   |
---------------------------------
Eval num_timesteps=967000, episode_reward=-258.95 +/- 3.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 967000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 6.61e-05 |
|    ent_coef        | 0.000898 |
|    ent_coef_loss   | -10.4    |
|    learning_rate   | 0.00403  |
|    n_updates       | 4720     |
---------------------------------
Eval num_timesteps=968000, episode_reward=-259.34 +/- 1.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 968000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 85.8     |
| time/              |          |
|    episodes        | 968      |
|    fps             | 638      |
|    time_elapsed    | 1515     |
|    total_timesteps | 968000   |
---------------------------------
Eval num_timesteps=969000, episode_reward=-258.48 +/- 3.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 969000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 4.89e-05 |
|    ent_coef        | 0.00088  |
|    ent_coef_loss   | -10.2    |
|    learning_rate   | 0.00403  |
|    n_updates       | 4730     |
---------------------------------
Eval num_timesteps=970000, episode_reward=-255.18 +/- 2.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 970000   |
---------------------------------
Eval num_timesteps=971000, episode_reward=-246.62 +/- 2.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 971000   |
| train/             |          |
|    actor_loss      | -2.28    |
|    critic_loss     | 4.22e-05 |
|    ent_coef        | 0.000864 |
|    ent_coef_loss   | -8.5     |
|    learning_rate   | 0.00403  |
|    n_updates       | 4740     |
---------------------------------
Eval num_timesteps=972000, episode_reward=-244.60 +/- 2.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 972000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 55.8     |
| time/              |          |
|    episodes        | 972      |
|    fps             | 638      |
|    time_elapsed    | 1521     |
|    total_timesteps | 972000   |
---------------------------------
Eval num_timesteps=973000, episode_reward=-233.80 +/- 2.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 973000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 4.39e-05 |
|    ent_coef        | 0.000848 |
|    ent_coef_loss   | -12      |
|    learning_rate   | 0.00403  |
|    n_updates       | 4750     |
---------------------------------
Eval num_timesteps=974000, episode_reward=-232.53 +/- 2.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 974000   |
---------------------------------
Eval num_timesteps=975000, episode_reward=-243.51 +/- 2.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 975000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 3.9e-05  |
|    ent_coef        | 0.000831 |
|    ent_coef_loss   | -9.62    |
|    learning_rate   | 0.00403  |
|    n_updates       | 4760     |
---------------------------------
Eval num_timesteps=976000, episode_reward=-241.14 +/- 2.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 976000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 18.4     |
| time/              |          |
|    episodes        | 976      |
|    fps             | 638      |
|    time_elapsed    | 1527     |
|    total_timesteps | 976000   |
---------------------------------
Eval num_timesteps=977000, episode_reward=-244.07 +/- 2.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 977000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 4.72e-05 |
|    ent_coef        | 0.000815 |
|    ent_coef_loss   | -6.73    |
|    learning_rate   | 0.00402  |
|    n_updates       | 4770     |
---------------------------------
Eval num_timesteps=978000, episode_reward=-243.03 +/- 1.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -243     |
| time/              |          |
|    total_timesteps | 978000   |
---------------------------------
Eval num_timesteps=979000, episode_reward=-220.65 +/- 2.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -221     |
| time/              |          |
|    total_timesteps | 979000   |
| train/             |          |
|    actor_loss      | -2.27    |
|    critic_loss     | 3.78e-05 |
|    ent_coef        | 0.000802 |
|    ent_coef_loss   | -8.83    |
|    learning_rate   | 0.00402  |
|    n_updates       | 4780     |
---------------------------------
Eval num_timesteps=980000, episode_reward=-222.69 +/- 1.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 980000   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -13.5    |
| time/              |          |
|    episodes        | 980      |
|    fps             | 638      |
|    time_elapsed    | 1534     |
|    total_timesteps | 980000   |
---------------------------------
Eval num_timesteps=981000, episode_reward=-217.61 +/- 1.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 981000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 4.9e-05  |
|    ent_coef        | 0.000789 |
|    ent_coef_loss   | -11.5    |
|    learning_rate   | 0.00402  |
|    n_updates       | 4790     |
---------------------------------
Eval num_timesteps=982000, episode_reward=-214.16 +/- 1.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 982000   |
---------------------------------
Eval num_timesteps=983000, episode_reward=-216.16 +/- 2.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 983000   |
---------------------------------
Eval num_timesteps=984000, episode_reward=-223.37 +/- 1.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 984000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 5.45e-05 |
|    ent_coef        | 0.000774 |
|    ent_coef_loss   | -11.7    |
|    learning_rate   | 0.00402  |
|    n_updates       | 4800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -50.4    |
| time/              |          |
|    episodes        | 984      |
|    fps             | 638      |
|    time_elapsed    | 1540     |
|    total_timesteps | 984000   |
---------------------------------
Eval num_timesteps=985000, episode_reward=-223.02 +/- 2.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 985000   |
---------------------------------
Eval num_timesteps=986000, episode_reward=-253.44 +/- 2.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 986000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 4.52e-05 |
|    ent_coef        | 0.000758 |
|    ent_coef_loss   | -5.35    |
|    learning_rate   | 0.00401  |
|    n_updates       | 4810     |
---------------------------------
Eval num_timesteps=987000, episode_reward=-251.35 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 987000   |
---------------------------------
Eval num_timesteps=988000, episode_reward=-256.64 +/- 3.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 988000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 3.94e-05 |
|    ent_coef        | 0.000746 |
|    ent_coef_loss   | -6.15    |
|    learning_rate   | 0.00401  |
|    n_updates       | 4820     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -84.7    |
| time/              |          |
|    episodes        | 988      |
|    fps             | 638      |
|    time_elapsed    | 1547     |
|    total_timesteps | 988000   |
---------------------------------
Eval num_timesteps=989000, episode_reward=-255.12 +/- 2.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 989000   |
---------------------------------
Eval num_timesteps=990000, episode_reward=-242.23 +/- 2.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 990000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 2.75e-05 |
|    ent_coef        | 0.000737 |
|    ent_coef_loss   | -5.98    |
|    learning_rate   | 0.00401  |
|    n_updates       | 4830     |
---------------------------------
Eval num_timesteps=991000, episode_reward=-241.60 +/- 3.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 991000   |
---------------------------------
Eval num_timesteps=992000, episode_reward=-229.34 +/- 3.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 992000   |
| train/             |          |
|    actor_loss      | -2.26    |
|    critic_loss     | 2e-05    |
|    ent_coef        | 0.000728 |
|    ent_coef_loss   | -6.45    |
|    learning_rate   | 0.00401  |
|    n_updates       | 4840     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -114     |
| time/              |          |
|    episodes        | 992      |
|    fps             | 638      |
|    time_elapsed    | 1553     |
|    total_timesteps | 992000   |
---------------------------------
Eval num_timesteps=993000, episode_reward=-228.41 +/- 1.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 993000   |
---------------------------------
Eval num_timesteps=994000, episode_reward=-219.40 +/- 3.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 994000   |
| train/             |          |
|    actor_loss      | -2.25    |
|    critic_loss     | 2.36e-05 |
|    ent_coef        | 0.000719 |
|    ent_coef_loss   | -9.31    |
|    learning_rate   | 0.00401  |
|    n_updates       | 4850     |
---------------------------------
Eval num_timesteps=995000, episode_reward=-217.29 +/- 2.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -217     |
| time/              |          |
|    total_timesteps | 995000   |
---------------------------------
Eval num_timesteps=996000, episode_reward=-192.57 +/- 1.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 996000   |
| train/             |          |
|    actor_loss      | -2.25    |
|    critic_loss     | 2.84e-05 |
|    ent_coef        | 0.000708 |
|    ent_coef_loss   | -9.71    |
|    learning_rate   | 0.004    |
|    n_updates       | 4860     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -136     |
| time/              |          |
|    episodes        | 996      |
|    fps             | 638      |
|    time_elapsed    | 1559     |
|    total_timesteps | 996000   |
---------------------------------
Eval num_timesteps=997000, episode_reward=-193.75 +/- 1.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 997000   |
---------------------------------
Eval num_timesteps=998000, episode_reward=-187.36 +/- 1.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 998000   |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 4.08e-05 |
|    ent_coef        | 0.000695 |
|    ent_coef_loss   | -11      |
|    learning_rate   | 0.004    |
|    n_updates       | 4870     |
---------------------------------
Eval num_timesteps=999000, episode_reward=-186.77 +/- 1.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 999000   |
---------------------------------
Eval num_timesteps=1000000, episode_reward=-231.89 +/- 1.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1000000  |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 2.63e-05 |
|    ent_coef        | 0.000681 |
|    ent_coef_loss   | -6.59    |
|    learning_rate   | 0.004    |
|    n_updates       | 4880     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -169     |
| time/              |          |
|    episodes        | 1000     |
|    fps             | 638      |
|    time_elapsed    | 1566     |
|    total_timesteps | 1000000  |
---------------------------------
Eval num_timesteps=1001000, episode_reward=-231.76 +/- 1.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1001000  |
---------------------------------
Eval num_timesteps=1002000, episode_reward=-247.12 +/- 1.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 1002000  |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 3.86e-05 |
|    ent_coef        | 0.00067  |
|    ent_coef_loss   | -6.24    |
|    learning_rate   | 0.004    |
|    n_updates       | 4890     |
---------------------------------
Eval num_timesteps=1003000, episode_reward=-247.53 +/- 1.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 1003000  |
---------------------------------
Eval num_timesteps=1004000, episode_reward=-229.47 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 1004000  |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 2.93e-05 |
|    ent_coef        | 0.000661 |
|    ent_coef_loss   | -9.29    |
|    learning_rate   | 0.004    |
|    n_updates       | 4900     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -198     |
| time/              |          |
|    episodes        | 1004     |
|    fps             | 638      |
|    time_elapsed    | 1572     |
|    total_timesteps | 1004000  |
---------------------------------
Eval num_timesteps=1005000, episode_reward=-229.07 +/- 1.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 1005000  |
---------------------------------
Eval num_timesteps=1006000, episode_reward=-212.88 +/- 2.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 1006000  |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 2.37e-05 |
|    ent_coef        | 0.000651 |
|    ent_coef_loss   | -8.44    |
|    learning_rate   | 0.00399  |
|    n_updates       | 4910     |
---------------------------------
Eval num_timesteps=1007000, episode_reward=-210.63 +/- 0.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 1007000  |
---------------------------------
Eval num_timesteps=1008000, episode_reward=-199.10 +/- 1.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 1008000  |
| train/             |          |
|    actor_loss      | -2.24    |
|    critic_loss     | 3.25e-05 |
|    ent_coef        | 0.00064  |
|    ent_coef_loss   | -9.12    |
|    learning_rate   | 0.00399  |
|    n_updates       | 4920     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -233     |
| time/              |          |
|    episodes        | 1008     |
|    fps             | 638      |
|    time_elapsed    | 1578     |
|    total_timesteps | 1008000  |
---------------------------------
Eval num_timesteps=1009000, episode_reward=-200.91 +/- 1.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 1009000  |
---------------------------------
Eval num_timesteps=1010000, episode_reward=-224.10 +/- 0.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -224     |
| time/              |          |
|    total_timesteps | 1010000  |
| train/             |          |
|    actor_loss      | -2.23    |
|    critic_loss     | 3.02e-05 |
|    ent_coef        | 0.000629 |
|    ent_coef_loss   | -7.26    |
|    learning_rate   | 0.00399  |
|    n_updates       | 4930     |
---------------------------------
Eval num_timesteps=1011000, episode_reward=-223.05 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 1011000  |
---------------------------------
Eval num_timesteps=1012000, episode_reward=-238.76 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 1012000  |
| train/             |          |
|    actor_loss      | -2.23    |
|    critic_loss     | 2.34e-05 |
|    ent_coef        | 0.000619 |
|    ent_coef_loss   | -7.35    |
|    learning_rate   | 0.00399  |
|    n_updates       | 4940     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -260     |
| time/              |          |
|    episodes        | 1012     |
|    fps             | 638      |
|    time_elapsed    | 1584     |
|    total_timesteps | 1012000  |
---------------------------------
Eval num_timesteps=1013000, episode_reward=-238.17 +/- 1.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 1013000  |
---------------------------------
Eval num_timesteps=1014000, episode_reward=-231.73 +/- 2.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1014000  |
| train/             |          |
|    actor_loss      | -2.23    |
|    critic_loss     | 2.82e-05 |
|    ent_coef        | 0.00061  |
|    ent_coef_loss   | -8.6     |
|    learning_rate   | 0.00399  |
|    n_updates       | 4950     |
---------------------------------
Eval num_timesteps=1015000, episode_reward=-231.61 +/- 1.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1015000  |
---------------------------------
Eval num_timesteps=1016000, episode_reward=-225.24 +/- 1.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 1016000  |
| train/             |          |
|    actor_loss      | -2.22    |
|    critic_loss     | 2.42e-05 |
|    ent_coef        | 0.0006   |
|    ent_coef_loss   | -8.95    |
|    learning_rate   | 0.00398  |
|    n_updates       | 4960     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -295     |
| time/              |          |
|    episodes        | 1016     |
|    fps             | 638      |
|    time_elapsed    | 1591     |
|    total_timesteps | 1016000  |
---------------------------------
Eval num_timesteps=1017000, episode_reward=-225.59 +/- 1.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 1017000  |
---------------------------------
Eval num_timesteps=1018000, episode_reward=-217.53 +/- 1.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 1018000  |
| train/             |          |
|    actor_loss      | -2.22    |
|    critic_loss     | 2.31e-05 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | -8.6     |
|    learning_rate   | 0.00398  |
|    n_updates       | 4970     |
---------------------------------
Eval num_timesteps=1019000, episode_reward=-217.17 +/- 1.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -217     |
| time/              |          |
|    total_timesteps | 1019000  |
---------------------------------
Eval num_timesteps=1020000, episode_reward=-211.98 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 1020000  |
| train/             |          |
|    actor_loss      | -2.22    |
|    critic_loss     | 2.45e-05 |
|    ent_coef        | 0.00058  |
|    ent_coef_loss   | -9.12    |
|    learning_rate   | 0.00398  |
|    n_updates       | 4980     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -291     |
| time/              |          |
|    episodes        | 1020     |
|    fps             | 638      |
|    time_elapsed    | 1597     |
|    total_timesteps | 1020000  |
---------------------------------
Eval num_timesteps=1021000, episode_reward=-211.13 +/- 2.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 1021000  |
---------------------------------
Eval num_timesteps=1022000, episode_reward=-236.36 +/- 2.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 1022000  |
| train/             |          |
|    actor_loss      | -2.21    |
|    critic_loss     | 2.61e-05 |
|    ent_coef        | 0.00057  |
|    ent_coef_loss   | -6.8     |
|    learning_rate   | 0.00398  |
|    n_updates       | 4990     |
---------------------------------
Eval num_timesteps=1023000, episode_reward=-234.00 +/- 1.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 1023000  |
---------------------------------
Eval num_timesteps=1024000, episode_reward=-232.04 +/- 0.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1024000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -280     |
| time/              |          |
|    episodes        | 1024     |
|    fps             | 638      |
|    time_elapsed    | 1603     |
|    total_timesteps | 1024000  |
---------------------------------
Eval num_timesteps=1025000, episode_reward=-248.14 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 1025000  |
| train/             |          |
|    actor_loss      | -2.21    |
|    critic_loss     | 2.25e-05 |
|    ent_coef        | 0.000561 |
|    ent_coef_loss   | -6.37    |
|    learning_rate   | 0.00398  |
|    n_updates       | 5000     |
---------------------------------
Eval num_timesteps=1026000, episode_reward=-248.26 +/- 1.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 1026000  |
---------------------------------
Eval num_timesteps=1027000, episode_reward=-254.98 +/- 1.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 1027000  |
| train/             |          |
|    actor_loss      | -2.21    |
|    critic_loss     | 2.62e-05 |
|    ent_coef        | 0.000554 |
|    ent_coef_loss   | -5.1     |
|    learning_rate   | 0.00397  |
|    n_updates       | 5010     |
---------------------------------
Eval num_timesteps=1028000, episode_reward=-253.79 +/- 2.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 1028000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -269     |
| time/              |          |
|    episodes        | 1028     |
|    fps             | 638      |
|    time_elapsed    | 1610     |
|    total_timesteps | 1028000  |
---------------------------------
Eval num_timesteps=1029000, episode_reward=-254.89 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 1029000  |
| train/             |          |
|    actor_loss      | -2.21    |
|    critic_loss     | 2.52e-05 |
|    ent_coef        | 0.000547 |
|    ent_coef_loss   | -6.6     |
|    learning_rate   | 0.00397  |
|    n_updates       | 5020     |
---------------------------------
Eval num_timesteps=1030000, episode_reward=-254.66 +/- 2.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 1030000  |
---------------------------------
Eval num_timesteps=1031000, episode_reward=-247.66 +/- 1.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 1031000  |
| train/             |          |
|    actor_loss      | -2.2     |
|    critic_loss     | 2.18e-05 |
|    ent_coef        | 0.000541 |
|    ent_coef_loss   | -6.89    |
|    learning_rate   | 0.00397  |
|    n_updates       | 5030     |
---------------------------------
Eval num_timesteps=1032000, episode_reward=-246.71 +/- 1.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -247     |
| time/              |          |
|    total_timesteps | 1032000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -260     |
| time/              |          |
|    episodes        | 1032     |
|    fps             | 638      |
|    time_elapsed    | 1616     |
|    total_timesteps | 1032000  |
---------------------------------
Eval num_timesteps=1033000, episode_reward=-234.96 +/- 1.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 1033000  |
| train/             |          |
|    actor_loss      | -2.2     |
|    critic_loss     | 2.6e-05  |
|    ent_coef        | 0.000534 |
|    ent_coef_loss   | -6.88    |
|    learning_rate   | 0.00397  |
|    n_updates       | 5040     |
---------------------------------
Eval num_timesteps=1034000, episode_reward=-231.70 +/- 1.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1034000  |
---------------------------------
Eval num_timesteps=1035000, episode_reward=-253.79 +/- 1.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 1035000  |
| train/             |          |
|    actor_loss      | -2.2     |
|    critic_loss     | 1.95e-05 |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | -6.18    |
|    learning_rate   | 0.00397  |
|    n_updates       | 5050     |
---------------------------------
Eval num_timesteps=1036000, episode_reward=-254.97 +/- 1.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 1036000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -252     |
| time/              |          |
|    episodes        | 1036     |
|    fps             | 638      |
|    time_elapsed    | 1622     |
|    total_timesteps | 1036000  |
---------------------------------
Eval num_timesteps=1037000, episode_reward=-249.39 +/- 3.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 1037000  |
| train/             |          |
|    actor_loss      | -2.2     |
|    critic_loss     | 1.8e-05  |
|    ent_coef        | 0.00052  |
|    ent_coef_loss   | -5.59    |
|    learning_rate   | 0.00396  |
|    n_updates       | 5060     |
---------------------------------
Eval num_timesteps=1038000, episode_reward=-250.67 +/- 2.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -251     |
| time/              |          |
|    total_timesteps | 1038000  |
---------------------------------
Eval num_timesteps=1039000, episode_reward=-233.93 +/- 1.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 1039000  |
| train/             |          |
|    actor_loss      | -2.2     |
|    critic_loss     | 2.13e-05 |
|    ent_coef        | 0.000514 |
|    ent_coef_loss   | -6.48    |
|    learning_rate   | 0.00396  |
|    n_updates       | 5070     |
---------------------------------
Eval num_timesteps=1040000, episode_reward=-232.79 +/- 2.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 1040000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -245     |
| time/              |          |
|    episodes        | 1040     |
|    fps             | 638      |
|    time_elapsed    | 1629     |
|    total_timesteps | 1040000  |
---------------------------------
Eval num_timesteps=1041000, episode_reward=-237.52 +/- 1.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 1041000  |
| train/             |          |
|    actor_loss      | -2.19    |
|    critic_loss     | 2.15e-05 |
|    ent_coef        | 0.000507 |
|    ent_coef_loss   | -7.91    |
|    learning_rate   | 0.00396  |
|    n_updates       | 5080     |
---------------------------------
Eval num_timesteps=1042000, episode_reward=-237.78 +/- 2.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 1042000  |
---------------------------------
Eval num_timesteps=1043000, episode_reward=-241.29 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 1043000  |
| train/             |          |
|    actor_loss      | -2.19    |
|    critic_loss     | 2.9e-05  |
|    ent_coef        | 0.000499 |
|    ent_coef_loss   | -6.69    |
|    learning_rate   | 0.00396  |
|    n_updates       | 5090     |
---------------------------------
Eval num_timesteps=1044000, episode_reward=-240.88 +/- 2.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 1044000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -238     |
| time/              |          |
|    episodes        | 1044     |
|    fps             | 638      |
|    time_elapsed    | 1635     |
|    total_timesteps | 1044000  |
---------------------------------
Eval num_timesteps=1045000, episode_reward=-231.41 +/- 2.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 1045000  |
| train/             |          |
|    actor_loss      | -2.19    |
|    critic_loss     | 3.43e-05 |
|    ent_coef        | 0.000493 |
|    ent_coef_loss   | -6.36    |
|    learning_rate   | 0.00396  |
|    n_updates       | 5100     |
---------------------------------
Eval num_timesteps=1046000, episode_reward=-230.50 +/- 3.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 1046000  |
---------------------------------
Eval num_timesteps=1047000, episode_reward=-223.43 +/- 1.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 1047000  |
| train/             |          |
|    actor_loss      | -2.18    |
|    critic_loss     | 3.04e-05 |
|    ent_coef        | 0.000486 |
|    ent_coef_loss   | -7.64    |
|    learning_rate   | 0.00395  |
|    n_updates       | 5110     |
---------------------------------
Eval num_timesteps=1048000, episode_reward=-223.96 +/- 1.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -224     |
| time/              |          |
|    total_timesteps | 1048000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -234     |
| time/              |          |
|    episodes        | 1048     |
|    fps             | 638      |
|    time_elapsed    | 1641     |
|    total_timesteps | 1048000  |
---------------------------------
Eval num_timesteps=1049000, episode_reward=-215.26 +/- 1.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 1049000  |
| train/             |          |
|    actor_loss      | -2.18    |
|    critic_loss     | 2.3e-05  |
|    ent_coef        | 0.000479 |
|    ent_coef_loss   | -6.12    |
|    learning_rate   | 0.00395  |
|    n_updates       | 5120     |
---------------------------------
Eval num_timesteps=1050000, episode_reward=-215.02 +/- 3.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 1050000  |
---------------------------------
Eval num_timesteps=1051000, episode_reward=-228.53 +/- 1.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 1051000  |
| train/             |          |
|    actor_loss      | -2.18    |
|    critic_loss     | 2.52e-05 |
|    ent_coef        | 0.000472 |
|    ent_coef_loss   | -7.19    |
|    learning_rate   | 0.00395  |
|    n_updates       | 5130     |
---------------------------------
Eval num_timesteps=1052000, episode_reward=-228.13 +/- 1.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 1052000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -230     |
| time/              |          |
|    episodes        | 1052     |
|    fps             | 638      |
|    time_elapsed    | 1648     |
|    total_timesteps | 1052000  |
---------------------------------
Eval num_timesteps=1053000, episode_reward=-242.81 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -243     |
| time/              |          |
|    total_timesteps | 1053000  |
| train/             |          |
|    actor_loss      | -2.18    |
|    critic_loss     | 1.74e-05 |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | -7.18    |
|    learning_rate   | 0.00395  |
|    n_updates       | 5140     |
---------------------------------
Eval num_timesteps=1054000, episode_reward=-242.71 +/- 2.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -243     |
| time/              |          |
|    total_timesteps | 1054000  |
---------------------------------
Eval num_timesteps=1055000, episode_reward=-252.28 +/- 0.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 1055000  |
| train/             |          |
|    actor_loss      | -2.18    |
|    critic_loss     | 2.43e-05 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | -5.32    |
|    learning_rate   | 0.00395  |
|    n_updates       | 5150     |
---------------------------------
Eval num_timesteps=1056000, episode_reward=-250.25 +/- 2.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 1056000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -228     |
| time/              |          |
|    episodes        | 1056     |
|    fps             | 638      |
|    time_elapsed    | 1654     |
|    total_timesteps | 1056000  |
---------------------------------
Eval num_timesteps=1057000, episode_reward=-236.16 +/- 2.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 1057000  |
| train/             |          |
|    actor_loss      | -2.18    |
|    critic_loss     | 2.76e-05 |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | -5.8     |
|    learning_rate   | 0.00394  |
|    n_updates       | 5160     |
---------------------------------
Eval num_timesteps=1058000, episode_reward=-234.56 +/- 1.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 1058000  |
---------------------------------
Eval num_timesteps=1059000, episode_reward=-234.28 +/- 2.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 1059000  |
| train/             |          |
|    actor_loss      | -2.17    |
|    critic_loss     | 2.5e-05  |
|    ent_coef        | 0.000448 |
|    ent_coef_loss   | -5.7     |
|    learning_rate   | 0.00394  |
|    n_updates       | 5170     |
---------------------------------
Eval num_timesteps=1060000, episode_reward=-234.13 +/- 2.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 1060000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -227     |
| time/              |          |
|    episodes        | 1060     |
|    fps             | 638      |
|    time_elapsed    | 1660     |
|    total_timesteps | 1060000  |
---------------------------------
Eval num_timesteps=1061000, episode_reward=-236.49 +/- 2.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 1061000  |
| train/             |          |
|    actor_loss      | -2.17    |
|    critic_loss     | 1.77e-05 |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | -6.32    |
|    learning_rate   | 0.00394  |
|    n_updates       | 5180     |
---------------------------------
Eval num_timesteps=1062000, episode_reward=-237.79 +/- 3.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 1062000  |
---------------------------------
Eval num_timesteps=1063000, episode_reward=-202.31 +/- 1.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 1063000  |
| train/             |          |
|    actor_loss      | -2.17    |
|    critic_loss     | 2.89e-05 |
|    ent_coef        | 0.000437 |
|    ent_coef_loss   | -7.63    |
|    learning_rate   | 0.00394  |
|    n_updates       | 5190     |
---------------------------------
Eval num_timesteps=1064000, episode_reward=-203.35 +/- 1.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 1064000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -225     |
| time/              |          |
|    episodes        | 1064     |
|    fps             | 638      |
|    time_elapsed    | 1667     |
|    total_timesteps | 1064000  |
---------------------------------
Eval num_timesteps=1065000, episode_reward=-228.03 +/- 1.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 1065000  |
| train/             |          |
|    actor_loss      | -2.16    |
|    critic_loss     | 2.67e-05 |
|    ent_coef        | 0.000431 |
|    ent_coef_loss   | -5.58    |
|    learning_rate   | 0.00394  |
|    n_updates       | 5200     |
---------------------------------
Eval num_timesteps=1066000, episode_reward=-225.79 +/- 1.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 1066000  |
---------------------------------
Eval num_timesteps=1067000, episode_reward=-226.78 +/- 2.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 1067000  |
---------------------------------
Eval num_timesteps=1068000, episode_reward=-252.43 +/- 2.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 1068000  |
| train/             |          |
|    actor_loss      | -2.17    |
|    critic_loss     | 2.41e-05 |
|    ent_coef        | 0.000425 |
|    ent_coef_loss   | -4.91    |
|    learning_rate   | 0.00393  |
|    n_updates       | 5210     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -224     |
| time/              |          |
|    episodes        | 1068     |
|    fps             | 637      |
|    time_elapsed    | 1674     |
|    total_timesteps | 1068000  |
---------------------------------
Eval num_timesteps=1069000, episode_reward=-250.08 +/- 3.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -250     |
| time/              |          |
|    total_timesteps | 1069000  |
---------------------------------
Eval num_timesteps=1070000, episode_reward=-247.73 +/- 1.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 1070000  |
| train/             |          |
|    actor_loss      | -2.16    |
|    critic_loss     | 2.58e-05 |
|    ent_coef        | 0.000421 |
|    ent_coef_loss   | -3.38    |
|    learning_rate   | 0.00393  |
|    n_updates       | 5220     |
---------------------------------
Eval num_timesteps=1071000, episode_reward=-247.98 +/- 2.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -248     |
| time/              |          |
|    total_timesteps | 1071000  |
---------------------------------
Eval num_timesteps=1072000, episode_reward=-232.36 +/- 2.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1072000  |
| train/             |          |
|    actor_loss      | -2.16    |
|    critic_loss     | 2.05e-05 |
|    ent_coef        | 0.000417 |
|    ent_coef_loss   | -6.76    |
|    learning_rate   | 0.00393  |
|    n_updates       | 5230     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -224     |
| time/              |          |
|    episodes        | 1072     |
|    fps             | 637      |
|    time_elapsed    | 1680     |
|    total_timesteps | 1072000  |
---------------------------------
Eval num_timesteps=1073000, episode_reward=-232.45 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1073000  |
---------------------------------
Eval num_timesteps=1074000, episode_reward=-218.87 +/- 1.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 1074000  |
| train/             |          |
|    actor_loss      | -2.16    |
|    critic_loss     | 2.35e-05 |
|    ent_coef        | 0.000412 |
|    ent_coef_loss   | -5.7     |
|    learning_rate   | 0.00393  |
|    n_updates       | 5240     |
---------------------------------
Eval num_timesteps=1075000, episode_reward=-220.04 +/- 2.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 1075000  |
---------------------------------
Eval num_timesteps=1076000, episode_reward=-237.79 +/- 1.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 1076000  |
| train/             |          |
|    actor_loss      | -2.15    |
|    critic_loss     | 2.01e-05 |
|    ent_coef        | 0.000407 |
|    ent_coef_loss   | -6.06    |
|    learning_rate   | 0.00392  |
|    n_updates       | 5250     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -224     |
| time/              |          |
|    episodes        | 1076     |
|    fps             | 637      |
|    time_elapsed    | 1686     |
|    total_timesteps | 1076000  |
---------------------------------
Eval num_timesteps=1077000, episode_reward=-236.44 +/- 3.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 1077000  |
---------------------------------
Eval num_timesteps=1078000, episode_reward=-259.50 +/- 1.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 1078000  |
| train/             |          |
|    actor_loss      | -2.16    |
|    critic_loss     | 1.62e-05 |
|    ent_coef        | 0.000402 |
|    ent_coef_loss   | -4.24    |
|    learning_rate   | 0.00392  |
|    n_updates       | 5260     |
---------------------------------
Eval num_timesteps=1079000, episode_reward=-258.27 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 1079000  |
---------------------------------
Eval num_timesteps=1080000, episode_reward=-249.13 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 1080000  |
| train/             |          |
|    actor_loss      | -2.16    |
|    critic_loss     | 1.5e-05  |
|    ent_coef        | 0.000398 |
|    ent_coef_loss   | -3.19    |
|    learning_rate   | 0.00392  |
|    n_updates       | 5270     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -224     |
| time/              |          |
|    episodes        | 1080     |
|    fps             | 637      |
|    time_elapsed    | 1693     |
|    total_timesteps | 1080000  |
---------------------------------
Eval num_timesteps=1081000, episode_reward=-252.21 +/- 1.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -252     |
| time/              |          |
|    total_timesteps | 1081000  |
---------------------------------
Eval num_timesteps=1082000, episode_reward=-214.30 +/- 1.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 1082000  |
| train/             |          |
|    actor_loss      | -2.15    |
|    critic_loss     | 2.03e-05 |
|    ent_coef        | 0.000395 |
|    ent_coef_loss   | -6.06    |
|    learning_rate   | 0.00392  |
|    n_updates       | 5280     |
---------------------------------
Eval num_timesteps=1083000, episode_reward=-213.19 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 1083000  |
---------------------------------
Eval num_timesteps=1084000, episode_reward=-214.64 +/- 1.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 1084000  |
| train/             |          |
|    actor_loss      | -2.15    |
|    critic_loss     | 1.95e-05 |
|    ent_coef        | 0.00039  |
|    ent_coef_loss   | -4.35    |
|    learning_rate   | 0.00392  |
|    n_updates       | 5290     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -225     |
| time/              |          |
|    episodes        | 1084     |
|    fps             | 637      |
|    time_elapsed    | 1699     |
|    total_timesteps | 1084000  |
---------------------------------
Eval num_timesteps=1085000, episode_reward=-217.72 +/- 2.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 1085000  |
---------------------------------
Eval num_timesteps=1086000, episode_reward=-232.45 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1086000  |
| train/             |          |
|    actor_loss      | -2.14    |
|    critic_loss     | 1.97e-05 |
|    ent_coef        | 0.000386 |
|    ent_coef_loss   | -4.95    |
|    learning_rate   | 0.00391  |
|    n_updates       | 5300     |
---------------------------------
Eval num_timesteps=1087000, episode_reward=-230.69 +/- 1.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 1087000  |
---------------------------------
Eval num_timesteps=1088000, episode_reward=-242.02 +/- 2.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 1088000  |
| train/             |          |
|    actor_loss      | -2.15    |
|    critic_loss     | 1.09e-05 |
|    ent_coef        | 0.000383 |
|    ent_coef_loss   | -4.55    |
|    learning_rate   | 0.00391  |
|    n_updates       | 5310     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -224     |
| time/              |          |
|    episodes        | 1088     |
|    fps             | 637      |
|    time_elapsed    | 1705     |
|    total_timesteps | 1088000  |
---------------------------------
Eval num_timesteps=1089000, episode_reward=-243.56 +/- 1.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 1089000  |
---------------------------------
Eval num_timesteps=1090000, episode_reward=-241.01 +/- 1.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 1090000  |
| train/             |          |
|    actor_loss      | -2.15    |
|    critic_loss     | 1.8e-05  |
|    ent_coef        | 0.000379 |
|    ent_coef_loss   | -2.98    |
|    learning_rate   | 0.00391  |
|    n_updates       | 5320     |
---------------------------------
Eval num_timesteps=1091000, episode_reward=-240.63 +/- 2.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 1091000  |
---------------------------------
Eval num_timesteps=1092000, episode_reward=-210.80 +/- 3.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 1092000  |
| train/             |          |
|    actor_loss      | -2.14    |
|    critic_loss     | 1.64e-05 |
|    ent_coef        | 0.000376 |
|    ent_coef_loss   | -7.37    |
|    learning_rate   | 0.00391  |
|    n_updates       | 5330     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -224     |
| time/              |          |
|    episodes        | 1092     |
|    fps             | 637      |
|    time_elapsed    | 1712     |
|    total_timesteps | 1092000  |
---------------------------------
Eval num_timesteps=1093000, episode_reward=-209.72 +/- 1.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 1093000  |
---------------------------------
Eval num_timesteps=1094000, episode_reward=-229.01 +/- 3.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 1094000  |
| train/             |          |
|    actor_loss      | -2.14    |
|    critic_loss     | 2.18e-05 |
|    ent_coef        | 0.000371 |
|    ent_coef_loss   | -3.91    |
|    learning_rate   | 0.00391  |
|    n_updates       | 5340     |
---------------------------------
Eval num_timesteps=1095000, episode_reward=-227.66 +/- 4.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 1095000  |
---------------------------------
Eval num_timesteps=1096000, episode_reward=-245.76 +/- 2.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -246     |
| time/              |          |
|    total_timesteps | 1096000  |
| train/             |          |
|    actor_loss      | -2.14    |
|    critic_loss     | 1.43e-05 |
|    ent_coef        | 0.000367 |
|    ent_coef_loss   | -4.2     |
|    learning_rate   | 0.0039   |
|    n_updates       | 5350     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -225     |
| time/              |          |
|    episodes        | 1096     |
|    fps             | 637      |
|    time_elapsed    | 1718     |
|    total_timesteps | 1096000  |
---------------------------------
Eval num_timesteps=1097000, episode_reward=-243.54 +/- 2.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 1097000  |
---------------------------------
Eval num_timesteps=1098000, episode_reward=-232.16 +/- 2.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 1098000  |
| train/             |          |
|    actor_loss      | -2.14    |
|    critic_loss     | 1.53e-05 |
|    ent_coef        | 0.000364 |
|    ent_coef_loss   | -4.72    |
|    learning_rate   | 0.0039   |
|    n_updates       | 5360     |
---------------------------------
Eval num_timesteps=1099000, episode_reward=-233.65 +/- 2.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 1099000  |
---------------------------------
Eval num_timesteps=1100000, episode_reward=-210.08 +/- 1.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 1100000  |
| train/             |          |
|    actor_loss      | -2.14    |
|    critic_loss     | 1.41e-05 |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | -6.46    |
|    learning_rate   | 0.0039   |
|    n_updates       | 5370     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -226     |
| time/              |          |
|    episodes        | 1100     |
|    fps             | 637      |
|    time_elapsed    | 1724     |
|    total_timesteps | 1100000  |
---------------------------------
Eval num_timesteps=1101000, episode_reward=-211.18 +/- 3.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 1101000  |
---------------------------------
Eval num_timesteps=1102000, episode_reward=-205.70 +/- 2.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 1102000  |
| train/             |          |
|    actor_loss      | -2.13    |
|    critic_loss     | 1.79e-05 |
|    ent_coef        | 0.000356 |
|    ent_coef_loss   | -3.56    |
|    learning_rate   | 0.0039   |
|    n_updates       | 5380     |
---------------------------------
Eval num_timesteps=1103000, episode_reward=-206.49 +/- 2.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 1103000  |
---------------------------------
Eval num_timesteps=1104000, episode_reward=-229.01 +/- 2.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 1104000  |
| train/             |          |
|    actor_loss      | -2.13    |
|    critic_loss     | 1.92e-05 |
|    ent_coef        | 0.000352 |
|    ent_coef_loss   | -5.39    |
|    learning_rate   | 0.0039   |
|    n_updates       | 5390     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -225     |
| time/              |          |
|    episodes        | 1104     |
|    fps             | 637      |
|    time_elapsed    | 1730     |
|    total_timesteps | 1104000  |
---------------------------------
Eval num_timesteps=1105000, episode_reward=-230.66 +/- 1.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 1105000  |
---------------------------------
Eval num_timesteps=1106000, episode_reward=-229.84 +/- 2.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 1106000  |
| train/             |          |
|    actor_loss      | -2.13    |
|    critic_loss     | 2.11e-05 |
|    ent_coef        | 0.000349 |
|    ent_coef_loss   | -4.25    |
|    learning_rate   | 0.00389  |
|    n_updates       | 5400     |
---------------------------------
Eval num_timesteps=1107000, episode_reward=-229.74 +/- 2.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 1107000  |
---------------------------------
Eval num_timesteps=1108000, episode_reward=-209.19 +/- 2.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 1108000  |
| train/             |          |
|    actor_loss      | -2.13    |
|    critic_loss     | 1.57e-05 |
|    ent_coef        | 0.000345 |
|    ent_coef_loss   | -6.95    |
|    learning_rate   | 0.00389  |
|    n_updates       | 5410     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -226     |
| time/              |          |
|    episodes        | 1108     |
|    fps             | 637      |
|    time_elapsed    | 1737     |
|    total_timesteps | 1108000  |
---------------------------------
Eval num_timesteps=1109000, episode_reward=-212.53 +/- 2.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -213     |
| time/              |          |
|    total_timesteps | 1109000  |
---------------------------------
Eval num_timesteps=1110000, episode_reward=-209.30 +/- 2.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 1110000  |
---------------------------------
Eval num_timesteps=1111000, episode_reward=-219.28 +/- 1.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 1111000  |
| train/             |          |
|    actor_loss      | -2.12    |
|    critic_loss     | 2.42e-05 |
|    ent_coef        | 0.00034  |
|    ent_coef_loss   | -5.12    |
|    learning_rate   | 0.00389  |
|    n_updates       | 5420     |
---------------------------------
Eval num_timesteps=1112000, episode_reward=-220.96 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -221     |
| time/              |          |
|    total_timesteps | 1112000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -227     |
| time/              |          |
|    episodes        | 1112     |
|    fps             | 637      |
|    time_elapsed    | 1743     |
|    total_timesteps | 1112000  |
---------------------------------
Eval num_timesteps=1113000, episode_reward=-222.05 +/- 1.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 1113000  |
| train/             |          |
|    actor_loss      | -2.12    |
|    critic_loss     | 1.54e-05 |
|    ent_coef        | 0.000336 |
|    ent_coef_loss   | -2.31    |
|    learning_rate   | 0.00389  |
|    n_updates       | 5430     |
---------------------------------
Eval num_timesteps=1114000, episode_reward=-221.89 +/- 3.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -222     |
| time/              |          |
|    total_timesteps | 1114000  |
---------------------------------
Eval num_timesteps=1115000, episode_reward=-194.91 +/- 2.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 1115000  |
| train/             |          |
|    actor_loss      | -2.12    |
|    critic_loss     | 1.94e-05 |
|    ent_coef        | 0.000334 |
|    ent_coef_loss   | -4.76    |
|    learning_rate   | 0.00389  |
|    n_updates       | 5440     |
---------------------------------
Eval num_timesteps=1116000, episode_reward=-196.46 +/- 2.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 1116000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -226     |
| time/              |          |
|    episodes        | 1116     |
|    fps             | 637      |
|    time_elapsed    | 1749     |
|    total_timesteps | 1116000  |
---------------------------------
Eval num_timesteps=1117000, episode_reward=-203.64 +/- 3.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 1117000  |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 1.8e-05  |
|    ent_coef        | 0.000331 |
|    ent_coef_loss   | -2.81    |
|    learning_rate   | 0.00388  |
|    n_updates       | 5450     |
---------------------------------
Eval num_timesteps=1118000, episode_reward=-202.57 +/- 2.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 1118000  |
---------------------------------
Eval num_timesteps=1119000, episode_reward=-225.95 +/- 3.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 1119000  |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 1.62e-05 |
|    ent_coef        | 0.000328 |
|    ent_coef_loss   | -4.87    |
|    learning_rate   | 0.00388  |
|    n_updates       | 5460     |
---------------------------------
Eval num_timesteps=1120000, episode_reward=-224.63 +/- 2.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 1120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -226     |
| time/              |          |
|    episodes        | 1120     |
|    fps             | 637      |
|    time_elapsed    | 1756     |
|    total_timesteps | 1120000  |
---------------------------------
Eval num_timesteps=1121000, episode_reward=-244.32 +/- 2.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 1121000  |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 1.8e-05  |
|    ent_coef        | 0.000325 |
|    ent_coef_loss   | -5.21    |
|    learning_rate   | 0.00388  |
|    n_updates       | 5470     |
---------------------------------
Eval num_timesteps=1122000, episode_reward=-240.95 +/- 3.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 1122000  |
---------------------------------
Eval num_timesteps=1123000, episode_reward=-233.18 +/- 2.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 1123000  |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 1.5e-05  |
|    ent_coef        | 0.000321 |
|    ent_coef_loss   | -6.82    |
|    learning_rate   | 0.00388  |
|    n_updates       | 5480     |
---------------------------------
Eval num_timesteps=1124000, episode_reward=-230.83 +/- 2.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 1124000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -227     |
| time/              |          |
|    episodes        | 1124     |
|    fps             | 637      |
|    time_elapsed    | 1762     |
|    total_timesteps | 1124000  |
---------------------------------
Eval num_timesteps=1125000, episode_reward=-197.96 +/- 2.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 1125000  |
| train/             |          |
|    actor_loss      | -2.12    |
|    critic_loss     | 1.76e-05 |
|    ent_coef        | 0.000317 |
|    ent_coef_loss   | -5.76    |
|    learning_rate   | 0.00388  |
|    n_updates       | 5490     |
---------------------------------
Eval num_timesteps=1126000, episode_reward=-200.09 +/- 2.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 1126000  |
---------------------------------
Eval num_timesteps=1127000, episode_reward=-202.22 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 1127000  |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 2.48e-05 |
|    ent_coef        | 0.000313 |
|    ent_coef_loss   | -3.88    |
|    learning_rate   | 0.00387  |
|    n_updates       | 5500     |
---------------------------------
Eval num_timesteps=1128000, episode_reward=-203.44 +/- 1.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 1128000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -226     |
| time/              |          |
|    episodes        | 1128     |
|    fps             | 637      |
|    time_elapsed    | 1768     |
|    total_timesteps | 1128000  |
---------------------------------
Eval num_timesteps=1129000, episode_reward=-225.75 +/- 2.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 1129000  |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 1.39e-05 |
|    ent_coef        | 0.00031  |
|    ent_coef_loss   | -4.28    |
|    learning_rate   | 0.00387  |
|    n_updates       | 5510     |
---------------------------------
Eval num_timesteps=1130000, episode_reward=-222.57 +/- 3.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -223     |
| time/              |          |
|    total_timesteps | 1130000  |
---------------------------------
Eval num_timesteps=1131000, episode_reward=-215.59 +/- 2.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 1131000  |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 1.53e-05 |
|    ent_coef        | 0.000306 |
|    ent_coef_loss   | -6.05    |
|    learning_rate   | 0.00387  |
|    n_updates       | 5520     |
---------------------------------
Eval num_timesteps=1132000, episode_reward=-218.36 +/- 2.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 1132000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -225     |
| time/              |          |
|    episodes        | 1132     |
|    fps             | 637      |
|    time_elapsed    | 1774     |
|    total_timesteps | 1132000  |
---------------------------------
Eval num_timesteps=1133000, episode_reward=-200.47 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 1133000  |
| train/             |          |
|    actor_loss      | -2.11    |
|    critic_loss     | 1.51e-05 |
|    ent_coef        | 0.000303 |
|    ent_coef_loss   | -5.95    |
|    learning_rate   | 0.00387  |
|    n_updates       | 5530     |
---------------------------------
Eval num_timesteps=1134000, episode_reward=-199.16 +/- 1.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 1134000  |
---------------------------------
Eval num_timesteps=1135000, episode_reward=-190.43 +/- 2.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1135000  |
| train/             |          |
|    actor_loss      | -2.1     |
|    critic_loss     | 1.69e-05 |
|    ent_coef        | 0.000299 |
|    ent_coef_loss   | -6.43    |
|    learning_rate   | 0.00387  |
|    n_updates       | 5540     |
---------------------------------
Eval num_timesteps=1136000, episode_reward=-188.56 +/- 1.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 1136000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -223     |
| time/              |          |
|    episodes        | 1136     |
|    fps             | 637      |
|    time_elapsed    | 1781     |
|    total_timesteps | 1136000  |
---------------------------------
Eval num_timesteps=1137000, episode_reward=-200.14 +/- 1.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 1137000  |
| train/             |          |
|    actor_loss      | -2.1     |
|    critic_loss     | 2.02e-05 |
|    ent_coef        | 0.000295 |
|    ent_coef_loss   | -6.43    |
|    learning_rate   | 0.00386  |
|    n_updates       | 5550     |
---------------------------------
Eval num_timesteps=1138000, episode_reward=-200.28 +/- 2.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -200     |
| time/              |          |
|    total_timesteps | 1138000  |
---------------------------------
Eval num_timesteps=1139000, episode_reward=-218.25 +/- 3.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 1139000  |
| train/             |          |
|    actor_loss      | -2.1     |
|    critic_loss     | 1.78e-05 |
|    ent_coef        | 0.00029  |
|    ent_coef_loss   | -3.71    |
|    learning_rate   | 0.00386  |
|    n_updates       | 5560     |
---------------------------------
Eval num_timesteps=1140000, episode_reward=-219.38 +/- 3.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 1140000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -222     |
| time/              |          |
|    episodes        | 1140     |
|    fps             | 637      |
|    time_elapsed    | 1787     |
|    total_timesteps | 1140000  |
---------------------------------
Eval num_timesteps=1141000, episode_reward=-218.39 +/- 2.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 1141000  |
| train/             |          |
|    actor_loss      | -2.1     |
|    critic_loss     | 1.12e-05 |
|    ent_coef        | 0.000287 |
|    ent_coef_loss   | -2.17    |
|    learning_rate   | 0.00386  |
|    n_updates       | 5570     |
---------------------------------
Eval num_timesteps=1142000, episode_reward=-218.34 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 1142000  |
---------------------------------
Eval num_timesteps=1143000, episode_reward=-204.70 +/- 3.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 1143000  |
| train/             |          |
|    actor_loss      | -2.09    |
|    critic_loss     | 1.59e-05 |
|    ent_coef        | 0.000285 |
|    ent_coef_loss   | -6.7     |
|    learning_rate   | 0.00386  |
|    n_updates       | 5580     |
---------------------------------
Eval num_timesteps=1144000, episode_reward=-205.88 +/- 2.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 1144000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -222     |
| time/              |          |
|    episodes        | 1144     |
|    fps             | 637      |
|    time_elapsed    | 1793     |
|    total_timesteps | 1144000  |
---------------------------------
Eval num_timesteps=1145000, episode_reward=-175.46 +/- 2.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 1145000  |
| train/             |          |
|    actor_loss      | -2.09    |
|    critic_loss     | 1.35e-05 |
|    ent_coef        | 0.000282 |
|    ent_coef_loss   | -6.73    |
|    learning_rate   | 0.00386  |
|    n_updates       | 5590     |
---------------------------------
Eval num_timesteps=1146000, episode_reward=-173.44 +/- 2.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 1146000  |
---------------------------------
Eval num_timesteps=1147000, episode_reward=-202.72 +/- 2.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 1147000  |
| train/             |          |
|    actor_loss      | -2.08    |
|    critic_loss     | 1.26e-05 |
|    ent_coef        | 0.000278 |
|    ent_coef_loss   | -4.45    |
|    learning_rate   | 0.00385  |
|    n_updates       | 5600     |
---------------------------------
Eval num_timesteps=1148000, episode_reward=-202.53 +/- 2.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 1148000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -221     |
| time/              |          |
|    episodes        | 1148     |
|    fps             | 637      |
|    time_elapsed    | 1800     |
|    total_timesteps | 1148000  |
---------------------------------
Eval num_timesteps=1149000, episode_reward=-230.86 +/- 1.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 1149000  |
| train/             |          |
|    actor_loss      | -2.09    |
|    critic_loss     | 1.82e-05 |
|    ent_coef        | 0.000275 |
|    ent_coef_loss   | -3.52    |
|    learning_rate   | 0.00385  |
|    n_updates       | 5610     |
---------------------------------
Eval num_timesteps=1150000, episode_reward=-231.03 +/- 2.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 1150000  |
---------------------------------
Eval num_timesteps=1151000, episode_reward=-206.85 +/- 3.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 1151000  |
| train/             |          |
|    actor_loss      | -2.09    |
|    critic_loss     | 2.59e-05 |
|    ent_coef        | 0.000272 |
|    ent_coef_loss   | -4.8     |
|    learning_rate   | 0.00385  |
|    n_updates       | 5620     |
---------------------------------
Eval num_timesteps=1152000, episode_reward=-204.99 +/- 2.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 1152000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -221     |
| time/              |          |
|    episodes        | 1152     |
|    fps             | 637      |
|    time_elapsed    | 1806     |
|    total_timesteps | 1152000  |
---------------------------------
Eval num_timesteps=1153000, episode_reward=-205.34 +/- 2.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 1153000  |
---------------------------------
Eval num_timesteps=1154000, episode_reward=-191.52 +/- 2.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 1154000  |
| train/             |          |
|    actor_loss      | -2.09    |
|    critic_loss     | 2.42e-05 |
|    ent_coef        | 0.000269 |
|    ent_coef_loss   | -6.1     |
|    learning_rate   | 0.00385  |
|    n_updates       | 5630     |
---------------------------------
Eval num_timesteps=1155000, episode_reward=-190.41 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1155000  |
---------------------------------
Eval num_timesteps=1156000, episode_reward=-224.62 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 1156000  |
| train/             |          |
|    actor_loss      | -2.08    |
|    critic_loss     | 1.39e-05 |
|    ent_coef        | 0.000265 |
|    ent_coef_loss   | -6.28    |
|    learning_rate   | 0.00384  |
|    n_updates       | 5640     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -219     |
| time/              |          |
|    episodes        | 1156     |
|    fps             | 637      |
|    time_elapsed    | 1812     |
|    total_timesteps | 1156000  |
---------------------------------
Eval num_timesteps=1157000, episode_reward=-225.65 +/- 1.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 1157000  |
---------------------------------
Eval num_timesteps=1158000, episode_reward=-227.90 +/- 1.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 1158000  |
| train/             |          |
|    actor_loss      | -2.09    |
|    critic_loss     | 1.23e-05 |
|    ent_coef        | 0.000262 |
|    ent_coef_loss   | -4.75    |
|    learning_rate   | 0.00384  |
|    n_updates       | 5650     |
---------------------------------
Eval num_timesteps=1159000, episode_reward=-230.02 +/- 2.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 1159000  |
---------------------------------
Eval num_timesteps=1160000, episode_reward=-198.18 +/- 1.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 1160000  |
| train/             |          |
|    actor_loss      | -2.09    |
|    critic_loss     | 2.16e-05 |
|    ent_coef        | 0.000259 |
|    ent_coef_loss   | -6.03    |
|    learning_rate   | 0.00384  |
|    n_updates       | 5660     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -219     |
| time/              |          |
|    episodes        | 1160     |
|    fps             | 637      |
|    time_elapsed    | 1818     |
|    total_timesteps | 1160000  |
---------------------------------
Eval num_timesteps=1161000, episode_reward=-197.98 +/- 2.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 1161000  |
---------------------------------
Eval num_timesteps=1162000, episode_reward=-181.41 +/- 1.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 1162000  |
| train/             |          |
|    actor_loss      | -2.08    |
|    critic_loss     | 1.36e-05 |
|    ent_coef        | 0.000255 |
|    ent_coef_loss   | -3.67    |
|    learning_rate   | 0.00384  |
|    n_updates       | 5670     |
---------------------------------
Eval num_timesteps=1163000, episode_reward=-179.90 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 1163000  |
---------------------------------
Eval num_timesteps=1164000, episode_reward=-194.46 +/- 2.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 1164000  |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 1.08e-05 |
|    ent_coef        | 0.000252 |
|    ent_coef_loss   | -3.11    |
|    learning_rate   | 0.00384  |
|    n_updates       | 5680     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -217     |
| time/              |          |
|    episodes        | 1164     |
|    fps             | 637      |
|    time_elapsed    | 1825     |
|    total_timesteps | 1164000  |
---------------------------------
Eval num_timesteps=1165000, episode_reward=-195.89 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 1165000  |
---------------------------------
Eval num_timesteps=1166000, episode_reward=-202.34 +/- 2.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 1166000  |
| train/             |          |
|    actor_loss      | -2.08    |
|    critic_loss     | 9.69e-06 |
|    ent_coef        | 0.00025  |
|    ent_coef_loss   | -6.02    |
|    learning_rate   | 0.00383  |
|    n_updates       | 5690     |
---------------------------------
Eval num_timesteps=1167000, episode_reward=-201.24 +/- 1.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 1167000  |
---------------------------------
Eval num_timesteps=1168000, episode_reward=-204.23 +/- 2.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -204     |
| time/              |          |
|    total_timesteps | 1168000  |
| train/             |          |
|    actor_loss      | -2.08    |
|    critic_loss     | 7.67e-06 |
|    ent_coef        | 0.000247 |
|    ent_coef_loss   | -5.95    |
|    learning_rate   | 0.00383  |
|    n_updates       | 5700     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -216     |
| time/              |          |
|    episodes        | 1168     |
|    fps             | 637      |
|    time_elapsed    | 1831     |
|    total_timesteps | 1168000  |
---------------------------------
Eval num_timesteps=1169000, episode_reward=-202.15 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -202     |
| time/              |          |
|    total_timesteps | 1169000  |
---------------------------------
Eval num_timesteps=1170000, episode_reward=-181.18 +/- 2.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 1170000  |
| train/             |          |
|    actor_loss      | -2.08    |
|    critic_loss     | 1.3e-05  |
|    ent_coef        | 0.000244 |
|    ent_coef_loss   | -3.56    |
|    learning_rate   | 0.00383  |
|    n_updates       | 5710     |
---------------------------------
Eval num_timesteps=1171000, episode_reward=-182.73 +/- 2.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 1171000  |
---------------------------------
Eval num_timesteps=1172000, episode_reward=-173.48 +/- 2.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 1172000  |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 1.13e-05 |
|    ent_coef        | 0.000242 |
|    ent_coef_loss   | -3.18    |
|    learning_rate   | 0.00383  |
|    n_updates       | 5720     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -214     |
| time/              |          |
|    episodes        | 1172     |
|    fps             | 637      |
|    time_elapsed    | 1837     |
|    total_timesteps | 1172000  |
---------------------------------
Eval num_timesteps=1173000, episode_reward=-172.21 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 1173000  |
---------------------------------
Eval num_timesteps=1174000, episode_reward=-170.32 +/- 2.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 1174000  |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 1.13e-05 |
|    ent_coef        | 0.00024  |
|    ent_coef_loss   | -0.481   |
|    learning_rate   | 0.00383  |
|    n_updates       | 5730     |
---------------------------------
Eval num_timesteps=1175000, episode_reward=-169.25 +/- 1.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 1175000  |
---------------------------------
Eval num_timesteps=1176000, episode_reward=-191.24 +/- 1.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 1176000  |
| train/             |          |
|    actor_loss      | -2.06    |
|    critic_loss     | 1.11e-05 |
|    ent_coef        | 0.000239 |
|    ent_coef_loss   | -3.17    |
|    learning_rate   | 0.00382  |
|    n_updates       | 5740     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -212     |
| time/              |          |
|    episodes        | 1176     |
|    fps             | 637      |
|    time_elapsed    | 1843     |
|    total_timesteps | 1176000  |
---------------------------------
Eval num_timesteps=1177000, episode_reward=-191.99 +/- 2.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 1177000  |
---------------------------------
Eval num_timesteps=1178000, episode_reward=-191.24 +/- 2.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 1178000  |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 1.26e-05 |
|    ent_coef        | 0.000237 |
|    ent_coef_loss   | -1.88    |
|    learning_rate   | 0.00382  |
|    n_updates       | 5750     |
---------------------------------
Eval num_timesteps=1179000, episode_reward=-189.78 +/- 0.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1179000  |
---------------------------------
Eval num_timesteps=1180000, episode_reward=-206.06 +/- 2.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 1180000  |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 1.63e-05 |
|    ent_coef        | 0.000236 |
|    ent_coef_loss   | -2.5     |
|    learning_rate   | 0.00382  |
|    n_updates       | 5760     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -210     |
| time/              |          |
|    episodes        | 1180     |
|    fps             | 637      |
|    time_elapsed    | 1850     |
|    total_timesteps | 1180000  |
---------------------------------
Eval num_timesteps=1181000, episode_reward=-206.79 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 1181000  |
---------------------------------
Eval num_timesteps=1182000, episode_reward=-187.30 +/- 1.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 1182000  |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 1.84e-05 |
|    ent_coef        | 0.000235 |
|    ent_coef_loss   | -4.69    |
|    learning_rate   | 0.00382  |
|    n_updates       | 5770     |
---------------------------------
Eval num_timesteps=1183000, episode_reward=-186.94 +/- 1.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 1183000  |
---------------------------------
Eval num_timesteps=1184000, episode_reward=-155.59 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 1184000  |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 1.21e-05 |
|    ent_coef        | 0.000232 |
|    ent_coef_loss   | -5.45    |
|    learning_rate   | 0.00382  |
|    n_updates       | 5780     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -209     |
| time/              |          |
|    episodes        | 1184     |
|    fps             | 637      |
|    time_elapsed    | 1856     |
|    total_timesteps | 1184000  |
---------------------------------
Eval num_timesteps=1185000, episode_reward=-157.18 +/- 2.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 1185000  |
---------------------------------
Eval num_timesteps=1186000, episode_reward=-188.70 +/- 1.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 1186000  |
| train/             |          |
|    actor_loss      | -2.05    |
|    critic_loss     | 1.62e-05 |
|    ent_coef        | 0.00023  |
|    ent_coef_loss   | 1.92     |
|    learning_rate   | 0.00381  |
|    n_updates       | 5790     |
---------------------------------
Eval num_timesteps=1187000, episode_reward=-187.07 +/- 1.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 1187000  |
---------------------------------
Eval num_timesteps=1188000, episode_reward=-207.75 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -208     |
| time/              |          |
|    total_timesteps | 1188000  |
| train/             |          |
|    actor_loss      | -2.06    |
|    critic_loss     | 2.79e-05 |
|    ent_coef        | 0.00023  |
|    ent_coef_loss   | 1.19     |
|    learning_rate   | 0.00381  |
|    n_updates       | 5800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -207     |
| time/              |          |
|    episodes        | 1188     |
|    fps             | 637      |
|    time_elapsed    | 1862     |
|    total_timesteps | 1188000  |
---------------------------------
Eval num_timesteps=1189000, episode_reward=-209.13 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 1189000  |
---------------------------------
Eval num_timesteps=1190000, episode_reward=-207.42 +/- 2.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 1190000  |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 2.01e-05 |
|    ent_coef        | 0.00023  |
|    ent_coef_loss   | -3.1     |
|    learning_rate   | 0.00381  |
|    n_updates       | 5810     |
---------------------------------
Eval num_timesteps=1191000, episode_reward=-207.05 +/- 1.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 1191000  |
---------------------------------
Eval num_timesteps=1192000, episode_reward=-184.43 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 1192000  |
| train/             |          |
|    actor_loss      | -2.07    |
|    critic_loss     | 1.27e-05 |
|    ent_coef        | 0.000229 |
|    ent_coef_loss   | -0.84    |
|    learning_rate   | 0.00381  |
|    n_updates       | 5820     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -206     |
| time/              |          |
|    episodes        | 1192     |
|    fps             | 637      |
|    time_elapsed    | 1869     |
|    total_timesteps | 1192000  |
---------------------------------
Eval num_timesteps=1193000, episode_reward=-184.08 +/- 1.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 1193000  |
---------------------------------
Eval num_timesteps=1194000, episode_reward=-161.51 +/- 1.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 1194000  |
| train/             |          |
|    actor_loss      | -2.06    |
|    critic_loss     | 2.1e-05  |
|    ent_coef        | 0.000228 |
|    ent_coef_loss   | -7.05    |
|    learning_rate   | 0.00381  |
|    n_updates       | 5830     |
---------------------------------
Eval num_timesteps=1195000, episode_reward=-161.70 +/- 1.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 1195000  |
---------------------------------
Eval num_timesteps=1196000, episode_reward=-162.04 +/- 1.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 1196000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -205     |
| time/              |          |
|    episodes        | 1196     |
|    fps             | 637      |
|    time_elapsed    | 1875     |
|    total_timesteps | 1196000  |
---------------------------------
Eval num_timesteps=1197000, episode_reward=-148.51 +/- 1.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 1197000  |
| train/             |          |
|    actor_loss      | -2.06    |
|    critic_loss     | 1.64e-05 |
|    ent_coef        | 0.000225 |
|    ent_coef_loss   | -4.81    |
|    learning_rate   | 0.0038   |
|    n_updates       | 5840     |
---------------------------------
Eval num_timesteps=1198000, episode_reward=-146.18 +/- 3.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 1198000  |
---------------------------------
Eval num_timesteps=1199000, episode_reward=-190.15 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1199000  |
| train/             |          |
|    actor_loss      | -2.05    |
|    critic_loss     | 1.48e-05 |
|    ent_coef        | 0.000223 |
|    ent_coef_loss   | -0.0698  |
|    learning_rate   | 0.0038   |
|    n_updates       | 5850     |
---------------------------------
Eval num_timesteps=1200000, episode_reward=-189.90 +/- 1.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -202     |
| time/              |          |
|    episodes        | 1200     |
|    fps             | 637      |
|    time_elapsed    | 1881     |
|    total_timesteps | 1200000  |
---------------------------------
Eval num_timesteps=1201000, episode_reward=-205.67 +/- 2.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 1201000  |
| train/             |          |
|    actor_loss      | -2.06    |
|    critic_loss     | 1.61e-05 |
|    ent_coef        | 0.000222 |
|    ent_coef_loss   | 2.33     |
|    learning_rate   | 0.0038   |
|    n_updates       | 5860     |
---------------------------------
Eval num_timesteps=1202000, episode_reward=-205.61 +/- 1.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 1202000  |
---------------------------------
Eval num_timesteps=1203000, episode_reward=-187.42 +/- 1.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 1203000  |
| train/             |          |
|    actor_loss      | -2.06    |
|    critic_loss     | 2.33e-05 |
|    ent_coef        | 0.000222 |
|    ent_coef_loss   | -3.28    |
|    learning_rate   | 0.0038   |
|    n_updates       | 5870     |
---------------------------------
Eval num_timesteps=1204000, episode_reward=-189.08 +/- 1.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -189     |
| time/              |          |
|    total_timesteps | 1204000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -201     |
| time/              |          |
|    episodes        | 1204     |
|    fps             | 637      |
|    time_elapsed    | 1887     |
|    total_timesteps | 1204000  |
---------------------------------
Eval num_timesteps=1205000, episode_reward=-157.57 +/- 1.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 1205000  |
| train/             |          |
|    actor_loss      | -2.06    |
|    critic_loss     | 8.49e-06 |
|    ent_coef        | 0.000221 |
|    ent_coef_loss   | -2.71    |
|    learning_rate   | 0.0038   |
|    n_updates       | 5880     |
---------------------------------
Eval num_timesteps=1206000, episode_reward=-156.45 +/- 1.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 1206000  |
---------------------------------
Eval num_timesteps=1207000, episode_reward=-159.00 +/- 1.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 1207000  |
| train/             |          |
|    actor_loss      | -2.05    |
|    critic_loss     | 1.96e-05 |
|    ent_coef        | 0.00022  |
|    ent_coef_loss   | -4.17    |
|    learning_rate   | 0.00379  |
|    n_updates       | 5890     |
---------------------------------
Eval num_timesteps=1208000, episode_reward=-158.69 +/- 1.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 1208000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -198     |
| time/              |          |
|    episodes        | 1208     |
|    fps             | 637      |
|    time_elapsed    | 1894     |
|    total_timesteps | 1208000  |
---------------------------------
Eval num_timesteps=1209000, episode_reward=-166.50 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 1209000  |
| train/             |          |
|    actor_loss      | -2.05    |
|    critic_loss     | 1.35e-05 |
|    ent_coef        | 0.000218 |
|    ent_coef_loss   | -2.81    |
|    learning_rate   | 0.00379  |
|    n_updates       | 5900     |
---------------------------------
Eval num_timesteps=1210000, episode_reward=-165.67 +/- 1.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 1210000  |
---------------------------------
Eval num_timesteps=1211000, episode_reward=-155.11 +/- 1.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 1211000  |
| train/             |          |
|    actor_loss      | -2.05    |
|    critic_loss     | 1.41e-05 |
|    ent_coef        | 0.000216 |
|    ent_coef_loss   | -4.82    |
|    learning_rate   | 0.00379  |
|    n_updates       | 5910     |
---------------------------------
Eval num_timesteps=1212000, episode_reward=-157.08 +/- 1.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 1212000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -196     |
| time/              |          |
|    episodes        | 1212     |
|    fps             | 637      |
|    time_elapsed    | 1900     |
|    total_timesteps | 1212000  |
---------------------------------
Eval num_timesteps=1213000, episode_reward=-147.23 +/- 2.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 1213000  |
| train/             |          |
|    actor_loss      | -2.05    |
|    critic_loss     | 1.36e-05 |
|    ent_coef        | 0.000214 |
|    ent_coef_loss   | -4.69    |
|    learning_rate   | 0.00379  |
|    n_updates       | 5920     |
---------------------------------
Eval num_timesteps=1214000, episode_reward=-148.13 +/- 2.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 1214000  |
---------------------------------
Eval num_timesteps=1215000, episode_reward=-151.09 +/- 1.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 1215000  |
| train/             |          |
|    actor_loss      | -2.04    |
|    critic_loss     | 2.69e-05 |
|    ent_coef        | 0.000211 |
|    ent_coef_loss   | -2.03    |
|    learning_rate   | 0.00379  |
|    n_updates       | 5930     |
---------------------------------
Eval num_timesteps=1216000, episode_reward=-151.73 +/- 2.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 1216000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -194     |
| time/              |          |
|    episodes        | 1216     |
|    fps             | 637      |
|    time_elapsed    | 1906     |
|    total_timesteps | 1216000  |
---------------------------------
Eval num_timesteps=1217000, episode_reward=-152.78 +/- 1.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 1217000  |
| train/             |          |
|    actor_loss      | -2.04    |
|    critic_loss     | 1.57e-05 |
|    ent_coef        | 0.00021  |
|    ent_coef_loss   | -4.37    |
|    learning_rate   | 0.00378  |
|    n_updates       | 5940     |
---------------------------------
Eval num_timesteps=1218000, episode_reward=-151.89 +/- 2.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 1218000  |
---------------------------------
Eval num_timesteps=1219000, episode_reward=-159.05 +/- 1.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 1219000  |
| train/             |          |
|    actor_loss      | -2.04    |
|    critic_loss     | 1.51e-05 |
|    ent_coef        | 0.000208 |
|    ent_coef_loss   | -3.36    |
|    learning_rate   | 0.00378  |
|    n_updates       | 5950     |
---------------------------------
Eval num_timesteps=1220000, episode_reward=-157.82 +/- 1.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 1220000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -191     |
| time/              |          |
|    episodes        | 1220     |
|    fps             | 637      |
|    time_elapsed    | 1912     |
|    total_timesteps | 1220000  |
---------------------------------
Eval num_timesteps=1221000, episode_reward=-151.23 +/- 1.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 1221000  |
| train/             |          |
|    actor_loss      | -2.04    |
|    critic_loss     | 1.31e-05 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | -0.00023 |
|    learning_rate   | 0.00378  |
|    n_updates       | 5960     |
---------------------------------
Eval num_timesteps=1222000, episode_reward=-149.37 +/- 2.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 1222000  |
---------------------------------
Eval num_timesteps=1223000, episode_reward=-173.28 +/- 2.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 1223000  |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 2.36e-05 |
|    ent_coef        | 0.000205 |
|    ent_coef_loss   | -4.29    |
|    learning_rate   | 0.00378  |
|    n_updates       | 5970     |
---------------------------------
Eval num_timesteps=1224000, episode_reward=-173.08 +/- 3.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 1224000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -188     |
| time/              |          |
|    episodes        | 1224     |
|    fps             | 637      |
|    time_elapsed    | 1919     |
|    total_timesteps | 1224000  |
---------------------------------
Eval num_timesteps=1225000, episode_reward=-162.21 +/- 1.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 1225000  |
| train/             |          |
|    actor_loss      | -2.04    |
|    critic_loss     | 2.45e-05 |
|    ent_coef        | 0.000203 |
|    ent_coef_loss   | -6.83    |
|    learning_rate   | 0.00378  |
|    n_updates       | 5980     |
---------------------------------
Eval num_timesteps=1226000, episode_reward=-162.58 +/- 2.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 1226000  |
---------------------------------
Eval num_timesteps=1227000, episode_reward=-145.33 +/- 1.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 1227000  |
| train/             |          |
|    actor_loss      | -2.04    |
|    critic_loss     | 2.76e-05 |
|    ent_coef        | 0.000201 |
|    ent_coef_loss   | -4.03    |
|    learning_rate   | 0.00377  |
|    n_updates       | 5990     |
---------------------------------
Eval num_timesteps=1228000, episode_reward=-148.77 +/- 2.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 1228000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -186     |
| time/              |          |
|    episodes        | 1228     |
|    fps             | 637      |
|    time_elapsed    | 1925     |
|    total_timesteps | 1228000  |
---------------------------------
Eval num_timesteps=1229000, episode_reward=-143.12 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 1229000  |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 2.53e-05 |
|    ent_coef        | 0.000198 |
|    ent_coef_loss   | -1.11    |
|    learning_rate   | 0.00377  |
|    n_updates       | 6000     |
---------------------------------
Eval num_timesteps=1230000, episode_reward=-145.35 +/- 3.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 1230000  |
---------------------------------
Eval num_timesteps=1231000, episode_reward=-136.89 +/- 1.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 1231000  |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 4.76e-05 |
|    ent_coef        | 0.000197 |
|    ent_coef_loss   | -0.566   |
|    learning_rate   | 0.00377  |
|    n_updates       | 6010     |
---------------------------------
Eval num_timesteps=1232000, episode_reward=-138.22 +/- 2.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 1232000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -183     |
| time/              |          |
|    episodes        | 1232     |
|    fps             | 637      |
|    time_elapsed    | 1931     |
|    total_timesteps | 1232000  |
---------------------------------
Eval num_timesteps=1233000, episode_reward=-143.27 +/- 1.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 1233000  |
| train/             |          |
|    actor_loss      | -2.04    |
|    critic_loss     | 3.41e-05 |
|    ent_coef        | 0.000197 |
|    ent_coef_loss   | -0.225   |
|    learning_rate   | 0.00377  |
|    n_updates       | 6020     |
---------------------------------
Eval num_timesteps=1234000, episode_reward=-141.94 +/- 2.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 1234000  |
---------------------------------
Eval num_timesteps=1235000, episode_reward=-121.79 +/- 5.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 1235000  |
| train/             |          |
|    actor_loss      | -2.04    |
|    critic_loss     | 2.66e-05 |
|    ent_coef        | 0.000196 |
|    ent_coef_loss   | -1.7     |
|    learning_rate   | 0.00377  |
|    n_updates       | 6030     |
---------------------------------
Eval num_timesteps=1236000, episode_reward=-120.91 +/- 2.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 1236000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -181     |
| time/              |          |
|    episodes        | 1236     |
|    fps             | 637      |
|    time_elapsed    | 1937     |
|    total_timesteps | 1236000  |
---------------------------------
Eval num_timesteps=1237000, episode_reward=-139.23 +/- 2.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 1237000  |
| train/             |          |
|    actor_loss      | -2.02    |
|    critic_loss     | 5.01e-05 |
|    ent_coef        | 0.000195 |
|    ent_coef_loss   | 1.1      |
|    learning_rate   | 0.00376  |
|    n_updates       | 6040     |
---------------------------------
Eval num_timesteps=1238000, episode_reward=-137.79 +/- 3.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 1238000  |
---------------------------------
Eval num_timesteps=1239000, episode_reward=-140.19 +/- 6.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 1239000  |
---------------------------------
Eval num_timesteps=1240000, episode_reward=-196.59 +/- 5.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 1240000  |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 6.42e-05 |
|    ent_coef        | 0.000196 |
|    ent_coef_loss   | 4.08     |
|    learning_rate   | 0.00376  |
|    n_updates       | 6050     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -179     |
| time/              |          |
|    episodes        | 1240     |
|    fps             | 637      |
|    time_elapsed    | 1944     |
|    total_timesteps | 1240000  |
---------------------------------
Eval num_timesteps=1241000, episode_reward=-194.85 +/- 7.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 1241000  |
---------------------------------
Eval num_timesteps=1242000, episode_reward=-198.74 +/- 5.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 1242000  |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 0.000173 |
|    ent_coef        | 0.000198 |
|    ent_coef_loss   | 4.79     |
|    learning_rate   | 0.00376  |
|    n_updates       | 6060     |
---------------------------------
Eval num_timesteps=1243000, episode_reward=-198.03 +/- 4.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 1243000  |
---------------------------------
Eval num_timesteps=1244000, episode_reward=-166.92 +/- 4.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 1244000  |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 0.000115 |
|    ent_coef        | 0.000199 |
|    ent_coef_loss   | -6.55    |
|    learning_rate   | 0.00376  |
|    n_updates       | 6070     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -178     |
| time/              |          |
|    episodes        | 1244     |
|    fps             | 637      |
|    time_elapsed    | 1950     |
|    total_timesteps | 1244000  |
---------------------------------
Eval num_timesteps=1245000, episode_reward=-171.29 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -171     |
| time/              |          |
|    total_timesteps | 1245000  |
---------------------------------
Eval num_timesteps=1246000, episode_reward=-182.06 +/- 1.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 1246000  |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 5.37e-05 |
|    ent_coef        | 0.000198 |
|    ent_coef_loss   | 6.01     |
|    learning_rate   | 0.00375  |
|    n_updates       | 6080     |
---------------------------------
Eval num_timesteps=1247000, episode_reward=-181.82 +/- 2.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 1247000  |
---------------------------------
Eval num_timesteps=1248000, episode_reward=-145.77 +/- 2.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 1248000  |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 5.17e-05 |
|    ent_coef        | 0.0002   |
|    ent_coef_loss   | 3.59     |
|    learning_rate   | 0.00375  |
|    n_updates       | 6090     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -177     |
| time/              |          |
|    episodes        | 1248     |
|    fps             | 637      |
|    time_elapsed    | 1956     |
|    total_timesteps | 1248000  |
---------------------------------
Eval num_timesteps=1249000, episode_reward=-147.05 +/- 3.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 1249000  |
---------------------------------
Eval num_timesteps=1250000, episode_reward=-156.63 +/- 3.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 1250000  |
| train/             |          |
|    actor_loss      | -2.03    |
|    critic_loss     | 4.21e-05 |
|    ent_coef        | 0.000201 |
|    ent_coef_loss   | 4.63     |
|    learning_rate   | 0.00375  |
|    n_updates       | 6100     |
---------------------------------
Eval num_timesteps=1251000, episode_reward=-196.33 +/- 51.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 1251000  |
---------------------------------
Eval num_timesteps=1252000, episode_reward=-432.42 +/- 2.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 1252000  |
| train/             |          |
|    actor_loss      | -2.02    |
|    critic_loss     | 8.28e-05 |
|    ent_coef        | 0.000204 |
|    ent_coef_loss   | 4.18     |
|    learning_rate   | 0.00375  |
|    n_updates       | 6110     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -174     |
| time/              |          |
|    episodes        | 1252     |
|    fps             | 637      |
|    time_elapsed    | 1963     |
|    total_timesteps | 1252000  |
---------------------------------
Eval num_timesteps=1253000, episode_reward=-431.47 +/- 2.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 1253000  |
---------------------------------
Eval num_timesteps=1254000, episode_reward=-583.33 +/- 3.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -583     |
| time/              |          |
|    total_timesteps | 1254000  |
| train/             |          |
|    actor_loss      | -1.94    |
|    critic_loss     | 0.0642   |
|    ent_coef        | 0.000212 |
|    ent_coef_loss   | 104      |
|    learning_rate   | 0.00375  |
|    n_updates       | 6120     |
---------------------------------
Eval num_timesteps=1255000, episode_reward=-585.62 +/- 1.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 1255000  |
---------------------------------
Eval num_timesteps=1256000, episode_reward=-336.23 +/- 3.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 1256000  |
| train/             |          |
|    actor_loss      | -1.88    |
|    critic_loss     | 0.0293   |
|    ent_coef        | 0.000246 |
|    ent_coef_loss   | 130      |
|    learning_rate   | 0.00374  |
|    n_updates       | 6130     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -186     |
| time/              |          |
|    episodes        | 1256     |
|    fps             | 637      |
|    time_elapsed    | 1969     |
|    total_timesteps | 1256000  |
---------------------------------
Eval num_timesteps=1257000, episode_reward=-335.88 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 1257000  |
---------------------------------
Eval num_timesteps=1258000, episode_reward=-586.16 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 1258000  |
| train/             |          |
|    actor_loss      | -1.89    |
|    critic_loss     | 0.0548   |
|    ent_coef        | 0.000297 |
|    ent_coef_loss   | 263      |
|    learning_rate   | 0.00374  |
|    n_updates       | 6140     |
---------------------------------
Eval num_timesteps=1259000, episode_reward=-585.74 +/- 0.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -586     |
| time/              |          |
|    total_timesteps | 1259000  |
---------------------------------
Eval num_timesteps=1260000, episode_reward=-589.82 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -590     |
| time/              |          |
|    total_timesteps | 1260000  |
| train/             |          |
|    actor_loss      | -1.78    |
|    critic_loss     | 0.0518   |
|    ent_coef        | 0.000374 |
|    ent_coef_loss   | 222      |
|    learning_rate   | 0.00374  |
|    n_updates       | 6150     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -197     |
| time/              |          |
|    episodes        | 1260     |
|    fps             | 637      |
|    time_elapsed    | 1975     |
|    total_timesteps | 1260000  |
---------------------------------
Eval num_timesteps=1261000, episode_reward=-590.01 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -590     |
| time/              |          |
|    total_timesteps | 1261000  |
---------------------------------
Eval num_timesteps=1262000, episode_reward=-562.53 +/- 12.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -563     |
| time/              |          |
|    total_timesteps | 1262000  |
| train/             |          |
|    actor_loss      | -1.88    |
|    critic_loss     | 0.016    |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | 291      |
|    learning_rate   | 0.00374  |
|    n_updates       | 6160     |
---------------------------------
Eval num_timesteps=1263000, episode_reward=-567.75 +/- 8.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 1263000  |
---------------------------------
Eval num_timesteps=1264000, episode_reward=-583.20 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -583     |
| time/              |          |
|    total_timesteps | 1264000  |
| train/             |          |
|    actor_loss      | -1.89    |
|    critic_loss     | 0.0094   |
|    ent_coef        | 0.000544 |
|    ent_coef_loss   | 153      |
|    learning_rate   | 0.00374  |
|    n_updates       | 6170     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -211     |
| time/              |          |
|    episodes        | 1264     |
|    fps             | 637      |
|    time_elapsed    | 1981     |
|    total_timesteps | 1264000  |
---------------------------------
Eval num_timesteps=1265000, episode_reward=-583.78 +/- 1.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -584     |
| time/              |          |
|    total_timesteps | 1265000  |
---------------------------------
Eval num_timesteps=1266000, episode_reward=-580.55 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -581     |
| time/              |          |
|    total_timesteps | 1266000  |
| train/             |          |
|    actor_loss      | -1.88    |
|    critic_loss     | 0.00228  |
|    ent_coef        | 0.00062  |
|    ent_coef_loss   | 110      |
|    learning_rate   | 0.00373  |
|    n_updates       | 6180     |
---------------------------------
Eval num_timesteps=1267000, episode_reward=-579.84 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -580     |
| time/              |          |
|    total_timesteps | 1267000  |
---------------------------------
Eval num_timesteps=1268000, episode_reward=-577.46 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 1268000  |
| train/             |          |
|    actor_loss      | -1.89    |
|    critic_loss     | 0.00101  |
|    ent_coef        | 0.000684 |
|    ent_coef_loss   | 107      |
|    learning_rate   | 0.00373  |
|    n_updates       | 6190     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -227     |
| time/              |          |
|    episodes        | 1268     |
|    fps             | 637      |
|    time_elapsed    | 1987     |
|    total_timesteps | 1268000  |
---------------------------------
Eval num_timesteps=1269000, episode_reward=-576.98 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 1269000  |
---------------------------------
Eval num_timesteps=1270000, episode_reward=-576.83 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 1270000  |
| train/             |          |
|    actor_loss      | -1.89    |
|    critic_loss     | 0.000386 |
|    ent_coef        | 0.000742 |
|    ent_coef_loss   | 94.1     |
|    learning_rate   | 0.00373  |
|    n_updates       | 6200     |
---------------------------------
Eval num_timesteps=1271000, episode_reward=-576.59 +/- 0.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 1271000  |
---------------------------------
Eval num_timesteps=1272000, episode_reward=-574.51 +/- 0.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -575     |
| time/              |          |
|    total_timesteps | 1272000  |
| train/             |          |
|    actor_loss      | -1.87    |
|    critic_loss     | 0.000202 |
|    ent_coef        | 0.000797 |
|    ent_coef_loss   | 90.8     |
|    learning_rate   | 0.00373  |
|    n_updates       | 6210     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -242     |
| time/              |          |
|    episodes        | 1272     |
|    fps             | 637      |
|    time_elapsed    | 1994     |
|    total_timesteps | 1272000  |
---------------------------------
Eval num_timesteps=1273000, episode_reward=-575.12 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -575     |
| time/              |          |
|    total_timesteps | 1273000  |
---------------------------------
Eval num_timesteps=1274000, episode_reward=-579.82 +/- 2.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -580     |
| time/              |          |
|    total_timesteps | 1274000  |
| train/             |          |
|    actor_loss      | -1.88    |
|    critic_loss     | 0.000149 |
|    ent_coef        | 0.000852 |
|    ent_coef_loss   | 87.1     |
|    learning_rate   | 0.00373  |
|    n_updates       | 6220     |
---------------------------------
Eval num_timesteps=1275000, episode_reward=-578.68 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -579     |
| time/              |          |
|    total_timesteps | 1275000  |
---------------------------------
Eval num_timesteps=1276000, episode_reward=-569.79 +/- 1.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 1276000  |
| train/             |          |
|    actor_loss      | -1.85    |
|    critic_loss     | 0.000129 |
|    ent_coef        | 0.000907 |
|    ent_coef_loss   | 83.8     |
|    learning_rate   | 0.00372  |
|    n_updates       | 6230     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -257     |
| time/              |          |
|    episodes        | 1276     |
|    fps             | 637      |
|    time_elapsed    | 2000     |
|    total_timesteps | 1276000  |
---------------------------------
Eval num_timesteps=1277000, episode_reward=-569.00 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 1277000  |
---------------------------------
Eval num_timesteps=1278000, episode_reward=-572.98 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -573     |
| time/              |          |
|    total_timesteps | 1278000  |
| train/             |          |
|    actor_loss      | -1.85    |
|    critic_loss     | 7.15e-05 |
|    ent_coef        | 0.000964 |
|    ent_coef_loss   | 80.2     |
|    learning_rate   | 0.00372  |
|    n_updates       | 6240     |
---------------------------------
Eval num_timesteps=1279000, episode_reward=-573.27 +/- 1.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -573     |
| time/              |          |
|    total_timesteps | 1279000  |
---------------------------------
Eval num_timesteps=1280000, episode_reward=-574.27 +/- 1.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -574     |
| time/              |          |
|    total_timesteps | 1280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -272     |
| time/              |          |
|    episodes        | 1280     |
|    fps             | 637      |
|    time_elapsed    | 2006     |
|    total_timesteps | 1280000  |
---------------------------------
Eval num_timesteps=1281000, episode_reward=-569.08 +/- 1.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 1281000  |
| train/             |          |
|    actor_loss      | -1.84    |
|    critic_loss     | 8.55e-05 |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | 75.5     |
|    learning_rate   | 0.00372  |
|    n_updates       | 6250     |
---------------------------------
Eval num_timesteps=1282000, episode_reward=-569.90 +/- 1.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 1282000  |
---------------------------------
Eval num_timesteps=1283000, episode_reward=-571.40 +/- 2.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 1283000  |
| train/             |          |
|    actor_loss      | -1.85    |
|    critic_loss     | 0.000104 |
|    ent_coef        | 0.00108  |
|    ent_coef_loss   | 72       |
|    learning_rate   | 0.00372  |
|    n_updates       | 6260     |
---------------------------------
Eval num_timesteps=1284000, episode_reward=-568.91 +/- 2.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 1284000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -287     |
| time/              |          |
|    episodes        | 1284     |
|    fps             | 637      |
|    time_elapsed    | 2012     |
|    total_timesteps | 1284000  |
---------------------------------
Eval num_timesteps=1285000, episode_reward=-565.32 +/- 0.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 1285000  |
| train/             |          |
|    actor_loss      | -1.82    |
|    critic_loss     | 9.98e-05 |
|    ent_coef        | 0.00114  |
|    ent_coef_loss   | 69.8     |
|    learning_rate   | 0.00372  |
|    n_updates       | 6270     |
---------------------------------
Eval num_timesteps=1286000, episode_reward=-568.82 +/- 2.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -569     |
| time/              |          |
|    total_timesteps | 1286000  |
---------------------------------
Eval num_timesteps=1287000, episode_reward=-564.23 +/- 0.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 1287000  |
| train/             |          |
|    actor_loss      | -1.8     |
|    critic_loss     | 8.68e-05 |
|    ent_coef        | 0.0012   |
|    ent_coef_loss   | 67.4     |
|    learning_rate   | 0.00371  |
|    n_updates       | 6280     |
---------------------------------
Eval num_timesteps=1288000, episode_reward=-564.33 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 1288000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -302     |
| time/              |          |
|    episodes        | 1288     |
|    fps             | 637      |
|    time_elapsed    | 2018     |
|    total_timesteps | 1288000  |
---------------------------------
Eval num_timesteps=1289000, episode_reward=-560.65 +/- 4.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 1289000  |
| train/             |          |
|    actor_loss      | -1.82    |
|    critic_loss     | 6.43e-05 |
|    ent_coef        | 0.00126  |
|    ent_coef_loss   | 65.1     |
|    learning_rate   | 0.00371  |
|    n_updates       | 6290     |
---------------------------------
Eval num_timesteps=1290000, episode_reward=-563.05 +/- 4.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -563     |
| time/              |          |
|    total_timesteps | 1290000  |
---------------------------------
Eval num_timesteps=1291000, episode_reward=-564.61 +/- 4.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 1291000  |
| train/             |          |
|    actor_loss      | -1.76    |
|    critic_loss     | 8.97e-05 |
|    ent_coef        | 0.00132  |
|    ent_coef_loss   | 60.6     |
|    learning_rate   | 0.00371  |
|    n_updates       | 6300     |
---------------------------------
Eval num_timesteps=1292000, episode_reward=-560.32 +/- 3.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 1292000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -315     |
| time/              |          |
|    episodes        | 1292     |
|    fps             | 637      |
|    time_elapsed    | 2025     |
|    total_timesteps | 1292000  |
---------------------------------
Eval num_timesteps=1293000, episode_reward=-558.45 +/- 5.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -558     |
| time/              |          |
|    total_timesteps | 1293000  |
| train/             |          |
|    actor_loss      | -1.8     |
|    critic_loss     | 9.41e-05 |
|    ent_coef        | 0.00138  |
|    ent_coef_loss   | 59.4     |
|    learning_rate   | 0.00371  |
|    n_updates       | 6310     |
---------------------------------
Eval num_timesteps=1294000, episode_reward=-560.79 +/- 5.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 1294000  |
---------------------------------
Eval num_timesteps=1295000, episode_reward=-552.98 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -553     |
| time/              |          |
|    total_timesteps | 1295000  |
| train/             |          |
|    actor_loss      | -1.74    |
|    critic_loss     | 0.000103 |
|    ent_coef        | 0.00144  |
|    ent_coef_loss   | 55       |
|    learning_rate   | 0.00371  |
|    n_updates       | 6320     |
---------------------------------
Eval num_timesteps=1296000, episode_reward=-557.20 +/- 5.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -557     |
| time/              |          |
|    total_timesteps | 1296000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -329     |
| time/              |          |
|    episodes        | 1296     |
|    fps             | 637      |
|    time_elapsed    | 2031     |
|    total_timesteps | 1296000  |
---------------------------------
Eval num_timesteps=1297000, episode_reward=-553.91 +/- 4.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -554     |
| time/              |          |
|    total_timesteps | 1297000  |
| train/             |          |
|    actor_loss      | -1.77    |
|    critic_loss     | 9.1e-05  |
|    ent_coef        | 0.0015   |
|    ent_coef_loss   | 54.8     |
|    learning_rate   | 0.0037   |
|    n_updates       | 6330     |
---------------------------------
Eval num_timesteps=1298000, episode_reward=-556.58 +/- 3.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -557     |
| time/              |          |
|    total_timesteps | 1298000  |
---------------------------------
Eval num_timesteps=1299000, episode_reward=-566.96 +/- 5.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 1299000  |
| train/             |          |
|    actor_loss      | -1.78    |
|    critic_loss     | 0.000124 |
|    ent_coef        | 0.00156  |
|    ent_coef_loss   | 53.4     |
|    learning_rate   | 0.0037   |
|    n_updates       | 6340     |
---------------------------------
Eval num_timesteps=1300000, episode_reward=-567.50 +/- 5.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 1300000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -344     |
| time/              |          |
|    episodes        | 1300     |
|    fps             | 638      |
|    time_elapsed    | 2037     |
|    total_timesteps | 1300000  |
---------------------------------
Eval num_timesteps=1301000, episode_reward=-559.56 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 1301000  |
| train/             |          |
|    actor_loss      | -1.77    |
|    critic_loss     | 8.08e-05 |
|    ent_coef        | 0.00163  |
|    ent_coef_loss   | 53.1     |
|    learning_rate   | 0.0037   |
|    n_updates       | 6350     |
---------------------------------
Eval num_timesteps=1302000, episode_reward=-559.65 +/- 0.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 1302000  |
---------------------------------
Eval num_timesteps=1303000, episode_reward=-559.98 +/- 1.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 1303000  |
| train/             |          |
|    actor_loss      | -1.75    |
|    critic_loss     | 0.00011  |
|    ent_coef        | 0.00169  |
|    ent_coef_loss   | 49.4     |
|    learning_rate   | 0.0037   |
|    n_updates       | 6360     |
---------------------------------
Eval num_timesteps=1304000, episode_reward=-559.49 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -559     |
| time/              |          |
|    total_timesteps | 1304000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -358     |
| time/              |          |
|    episodes        | 1304     |
|    fps             | 638      |
|    time_elapsed    | 2043     |
|    total_timesteps | 1304000  |
---------------------------------
Eval num_timesteps=1305000, episode_reward=-557.68 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -558     |
| time/              |          |
|    total_timesteps | 1305000  |
| train/             |          |
|    actor_loss      | -1.75    |
|    critic_loss     | 6.7e-05  |
|    ent_coef        | 0.00175  |
|    ent_coef_loss   | 48.5     |
|    learning_rate   | 0.0037   |
|    n_updates       | 6370     |
---------------------------------
Eval num_timesteps=1306000, episode_reward=-557.24 +/- 1.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -557     |
| time/              |          |
|    total_timesteps | 1306000  |
---------------------------------
Eval num_timesteps=1307000, episode_reward=-556.18 +/- 0.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -556     |
| time/              |          |
|    total_timesteps | 1307000  |
| train/             |          |
|    actor_loss      | -1.74    |
|    critic_loss     | 9.39e-05 |
|    ent_coef        | 0.00182  |
|    ent_coef_loss   | 47.2     |
|    learning_rate   | 0.00369  |
|    n_updates       | 6380     |
---------------------------------
Eval num_timesteps=1308000, episode_reward=-557.13 +/- 1.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -557     |
| time/              |          |
|    total_timesteps | 1308000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -373     |
| time/              |          |
|    episodes        | 1308     |
|    fps             | 638      |
|    time_elapsed    | 2049     |
|    total_timesteps | 1308000  |
---------------------------------
Eval num_timesteps=1309000, episode_reward=-562.60 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -563     |
| time/              |          |
|    total_timesteps | 1309000  |
| train/             |          |
|    actor_loss      | -1.73    |
|    critic_loss     | 0.000127 |
|    ent_coef        | 0.00189  |
|    ent_coef_loss   | 45.4     |
|    learning_rate   | 0.00369  |
|    n_updates       | 6390     |
---------------------------------
Eval num_timesteps=1310000, episode_reward=-561.58 +/- 1.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -562     |
| time/              |          |
|    total_timesteps | 1310000  |
---------------------------------
Eval num_timesteps=1311000, episode_reward=-555.74 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -556     |
| time/              |          |
|    total_timesteps | 1311000  |
| train/             |          |
|    actor_loss      | -1.73    |
|    critic_loss     | 8.24e-05 |
|    ent_coef        | 0.00195  |
|    ent_coef_loss   | 45       |
|    learning_rate   | 0.00369  |
|    n_updates       | 6400     |
---------------------------------
Eval num_timesteps=1312000, episode_reward=-555.21 +/- 0.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -555     |
| time/              |          |
|    total_timesteps | 1312000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -388     |
| time/              |          |
|    episodes        | 1312     |
|    fps             | 638      |
|    time_elapsed    | 2056     |
|    total_timesteps | 1312000  |
---------------------------------
Eval num_timesteps=1313000, episode_reward=-561.33 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 1313000  |
| train/             |          |
|    actor_loss      | -1.72    |
|    critic_loss     | 8.68e-05 |
|    ent_coef        | 0.00202  |
|    ent_coef_loss   | 42.8     |
|    learning_rate   | 0.00369  |
|    n_updates       | 6410     |
---------------------------------
Eval num_timesteps=1314000, episode_reward=-560.43 +/- 1.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -560     |
| time/              |          |
|    total_timesteps | 1314000  |
---------------------------------
Eval num_timesteps=1315000, episode_reward=-566.04 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -566     |
| time/              |          |
|    total_timesteps | 1315000  |
| train/             |          |
|    actor_loss      | -1.7     |
|    critic_loss     | 8.3e-05  |
|    ent_coef        | 0.00209  |
|    ent_coef_loss   | 42.5     |
|    learning_rate   | 0.00369  |
|    n_updates       | 6420     |
---------------------------------
Eval num_timesteps=1316000, episode_reward=-564.79 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -565     |
| time/              |          |
|    total_timesteps | 1316000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -403     |
| time/              |          |
|    episodes        | 1316     |
|    fps             | 638      |
|    time_elapsed    | 2062     |
|    total_timesteps | 1316000  |
---------------------------------
Eval num_timesteps=1317000, episode_reward=-554.05 +/- 0.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -554     |
| time/              |          |
|    total_timesteps | 1317000  |
| train/             |          |
|    actor_loss      | -1.72    |
|    critic_loss     | 6.15e-05 |
|    ent_coef        | 0.00216  |
|    ent_coef_loss   | 41.4     |
|    learning_rate   | 0.00368  |
|    n_updates       | 6430     |
---------------------------------
Eval num_timesteps=1318000, episode_reward=-554.31 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -554     |
| time/              |          |
|    total_timesteps | 1318000  |
---------------------------------
Eval num_timesteps=1319000, episode_reward=-560.62 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 1319000  |
| train/             |          |
|    actor_loss      | -1.67    |
|    critic_loss     | 8.12e-05 |
|    ent_coef        | 0.00223  |
|    ent_coef_loss   | 37.9     |
|    learning_rate   | 0.00368  |
|    n_updates       | 6440     |
---------------------------------
Eval num_timesteps=1320000, episode_reward=-561.43 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 1320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -419     |
| time/              |          |
|    episodes        | 1320     |
|    fps             | 638      |
|    time_elapsed    | 2068     |
|    total_timesteps | 1320000  |
---------------------------------
Eval num_timesteps=1321000, episode_reward=-551.67 +/- 2.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -552     |
| time/              |          |
|    total_timesteps | 1321000  |
| train/             |          |
|    actor_loss      | -1.68    |
|    critic_loss     | 6.5e-05  |
|    ent_coef        | 0.0023   |
|    ent_coef_loss   | 38.8     |
|    learning_rate   | 0.00368  |
|    n_updates       | 6450     |
---------------------------------
Eval num_timesteps=1322000, episode_reward=-551.79 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -552     |
| time/              |          |
|    total_timesteps | 1322000  |
---------------------------------
Eval num_timesteps=1323000, episode_reward=-552.24 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -552     |
| time/              |          |
|    total_timesteps | 1323000  |
---------------------------------
Eval num_timesteps=1324000, episode_reward=-552.48 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -552     |
| time/              |          |
|    total_timesteps | 1324000  |
| train/             |          |
|    actor_loss      | -1.65    |
|    critic_loss     | 0.000101 |
|    ent_coef        | 0.00237  |
|    ent_coef_loss   | 33.9     |
|    learning_rate   | 0.00368  |
|    n_updates       | 6460     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -433     |
| time/              |          |
|    episodes        | 1324     |
|    fps             | 638      |
|    time_elapsed    | 2074     |
|    total_timesteps | 1324000  |
---------------------------------
Eval num_timesteps=1325000, episode_reward=-551.35 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -551     |
| time/              |          |
|    total_timesteps | 1325000  |
---------------------------------
Eval num_timesteps=1326000, episode_reward=-550.13 +/- 0.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -550     |
| time/              |          |
|    total_timesteps | 1326000  |
| train/             |          |
|    actor_loss      | -1.65    |
|    critic_loss     | 7.03e-05 |
|    ent_coef        | 0.00244  |
|    ent_coef_loss   | 34.5     |
|    learning_rate   | 0.00367  |
|    n_updates       | 6470     |
---------------------------------
Eval num_timesteps=1327000, episode_reward=-548.83 +/- 0.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 1327000  |
---------------------------------
Eval num_timesteps=1328000, episode_reward=-549.85 +/- 1.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -550     |
| time/              |          |
|    total_timesteps | 1328000  |
| train/             |          |
|    actor_loss      | -1.65    |
|    critic_loss     | 7.46e-05 |
|    ent_coef        | 0.00251  |
|    ent_coef_loss   | 31.2     |
|    learning_rate   | 0.00367  |
|    n_updates       | 6480     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -447     |
| time/              |          |
|    episodes        | 1328     |
|    fps             | 638      |
|    time_elapsed    | 2080     |
|    total_timesteps | 1328000  |
---------------------------------
Eval num_timesteps=1329000, episode_reward=-548.88 +/- 2.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 1329000  |
---------------------------------
Eval num_timesteps=1330000, episode_reward=-548.34 +/- 1.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -548     |
| time/              |          |
|    total_timesteps | 1330000  |
| train/             |          |
|    actor_loss      | -1.64    |
|    critic_loss     | 6.65e-05 |
|    ent_coef        | 0.00257  |
|    ent_coef_loss   | 30.1     |
|    learning_rate   | 0.00367  |
|    n_updates       | 6490     |
---------------------------------
Eval num_timesteps=1331000, episode_reward=-548.00 +/- 1.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -548     |
| time/              |          |
|    total_timesteps | 1331000  |
---------------------------------
Eval num_timesteps=1332000, episode_reward=-541.23 +/- 2.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 1332000  |
| train/             |          |
|    actor_loss      | -1.65    |
|    critic_loss     | 8.14e-05 |
|    ent_coef        | 0.00264  |
|    ent_coef_loss   | 30.6     |
|    learning_rate   | 0.00367  |
|    n_updates       | 6500     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -462     |
| time/              |          |
|    episodes        | 1332     |
|    fps             | 638      |
|    time_elapsed    | 2086     |
|    total_timesteps | 1332000  |
---------------------------------
Eval num_timesteps=1333000, episode_reward=-540.95 +/- 0.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 1333000  |
---------------------------------
Eval num_timesteps=1334000, episode_reward=-541.40 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 1334000  |
| train/             |          |
|    actor_loss      | -1.62    |
|    critic_loss     | 8.16e-05 |
|    ent_coef        | 0.0027   |
|    ent_coef_loss   | 25.8     |
|    learning_rate   | 0.00367  |
|    n_updates       | 6510     |
---------------------------------
Eval num_timesteps=1335000, episode_reward=-539.89 +/- 3.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -540     |
| time/              |          |
|    total_timesteps | 1335000  |
---------------------------------
Eval num_timesteps=1336000, episode_reward=-547.21 +/- 1.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 1336000  |
| train/             |          |
|    actor_loss      | -1.62    |
|    critic_loss     | 7.36e-05 |
|    ent_coef        | 0.00277  |
|    ent_coef_loss   | 27.1     |
|    learning_rate   | 0.00366  |
|    n_updates       | 6520     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -477     |
| time/              |          |
|    episodes        | 1336     |
|    fps             | 638      |
|    time_elapsed    | 2093     |
|    total_timesteps | 1336000  |
---------------------------------
Eval num_timesteps=1337000, episode_reward=-547.47 +/- 1.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -547     |
| time/              |          |
|    total_timesteps | 1337000  |
---------------------------------
Eval num_timesteps=1338000, episode_reward=-536.68 +/- 1.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -537     |
| time/              |          |
|    total_timesteps | 1338000  |
| train/             |          |
|    actor_loss      | -1.62    |
|    critic_loss     | 8.42e-05 |
|    ent_coef        | 0.00283  |
|    ent_coef_loss   | 24.4     |
|    learning_rate   | 0.00366  |
|    n_updates       | 6530     |
---------------------------------
Eval num_timesteps=1339000, episode_reward=-535.03 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -535     |
| time/              |          |
|    total_timesteps | 1339000  |
---------------------------------
Eval num_timesteps=1340000, episode_reward=-537.67 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 1340000  |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 8.88e-05 |
|    ent_coef        | 0.00289  |
|    ent_coef_loss   | 22.3     |
|    learning_rate   | 0.00366  |
|    n_updates       | 6540     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -490     |
| time/              |          |
|    episodes        | 1340     |
|    fps             | 638      |
|    time_elapsed    | 2099     |
|    total_timesteps | 1340000  |
---------------------------------
Eval num_timesteps=1341000, episode_reward=-538.46 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -538     |
| time/              |          |
|    total_timesteps | 1341000  |
---------------------------------
Eval num_timesteps=1342000, episode_reward=-536.19 +/- 0.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -536     |
| time/              |          |
|    total_timesteps | 1342000  |
| train/             |          |
|    actor_loss      | -1.6     |
|    critic_loss     | 8.64e-05 |
|    ent_coef        | 0.00294  |
|    ent_coef_loss   | 20.4     |
|    learning_rate   | 0.00366  |
|    n_updates       | 6550     |
---------------------------------
Eval num_timesteps=1343000, episode_reward=-531.88 +/- 8.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 1343000  |
---------------------------------
Eval num_timesteps=1344000, episode_reward=-545.99 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -546     |
| time/              |          |
|    total_timesteps | 1344000  |
| train/             |          |
|    actor_loss      | -1.6     |
|    critic_loss     | 8.47e-05 |
|    ent_coef        | 0.003    |
|    ent_coef_loss   | 18.8     |
|    learning_rate   | 0.00366  |
|    n_updates       | 6560     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -502     |
| time/              |          |
|    episodes        | 1344     |
|    fps             | 638      |
|    time_elapsed    | 2105     |
|    total_timesteps | 1344000  |
---------------------------------
Eval num_timesteps=1345000, episode_reward=-542.05 +/- 8.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -542     |
| time/              |          |
|    total_timesteps | 1345000  |
---------------------------------
Eval num_timesteps=1346000, episode_reward=-532.21 +/- 1.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 1346000  |
| train/             |          |
|    actor_loss      | -1.6     |
|    critic_loss     | 8.09e-05 |
|    ent_coef        | 0.00305  |
|    ent_coef_loss   | 18.8     |
|    learning_rate   | 0.00365  |
|    n_updates       | 6570     |
---------------------------------
Eval num_timesteps=1347000, episode_reward=-532.17 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 1347000  |
---------------------------------
Eval num_timesteps=1348000, episode_reward=-529.97 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -530     |
| time/              |          |
|    total_timesteps | 1348000  |
| train/             |          |
|    actor_loss      | -1.59    |
|    critic_loss     | 9.47e-05 |
|    ent_coef        | 0.0031   |
|    ent_coef_loss   | 16.3     |
|    learning_rate   | 0.00365  |
|    n_updates       | 6580     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -515     |
| time/              |          |
|    episodes        | 1348     |
|    fps             | 638      |
|    time_elapsed    | 2111     |
|    total_timesteps | 1348000  |
---------------------------------
Eval num_timesteps=1349000, episode_reward=-526.36 +/- 9.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -526     |
| time/              |          |
|    total_timesteps | 1349000  |
---------------------------------
Eval num_timesteps=1350000, episode_reward=-522.79 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 1350000  |
| train/             |          |
|    actor_loss      | -1.59    |
|    critic_loss     | 0.000115 |
|    ent_coef        | 0.00314  |
|    ent_coef_loss   | 14.6     |
|    learning_rate   | 0.00365  |
|    n_updates       | 6590     |
---------------------------------
Eval num_timesteps=1351000, episode_reward=-511.81 +/- 11.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -512     |
| time/              |          |
|    total_timesteps | 1351000  |
---------------------------------
Eval num_timesteps=1352000, episode_reward=-504.01 +/- 10.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 1352000  |
| train/             |          |
|    actor_loss      | -1.58    |
|    critic_loss     | 0.000173 |
|    ent_coef        | 0.00318  |
|    ent_coef_loss   | 10.6     |
|    learning_rate   | 0.00365  |
|    n_updates       | 6600     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -527     |
| time/              |          |
|    episodes        | 1352     |
|    fps             | 638      |
|    time_elapsed    | 2117     |
|    total_timesteps | 1352000  |
---------------------------------
Eval num_timesteps=1353000, episode_reward=-513.30 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 1353000  |
---------------------------------
Eval num_timesteps=1354000, episode_reward=-509.51 +/- 10.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -510     |
| time/              |          |
|    total_timesteps | 1354000  |
| train/             |          |
|    actor_loss      | -1.58    |
|    critic_loss     | 0.000154 |
|    ent_coef        | 0.00322  |
|    ent_coef_loss   | 7.63     |
|    learning_rate   | 0.00365  |
|    n_updates       | 6610     |
---------------------------------
Eval num_timesteps=1355000, episode_reward=-509.34 +/- 10.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 1355000  |
---------------------------------
Eval num_timesteps=1356000, episode_reward=-503.67 +/- 11.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 1356000  |
| train/             |          |
|    actor_loss      | -1.57    |
|    critic_loss     | 0.000163 |
|    ent_coef        | 0.00324  |
|    ent_coef_loss   | 5.59     |
|    learning_rate   | 0.00364  |
|    n_updates       | 6620     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -525     |
| time/              |          |
|    episodes        | 1356     |
|    fps             | 638      |
|    time_elapsed    | 2124     |
|    total_timesteps | 1356000  |
---------------------------------
Eval num_timesteps=1357000, episode_reward=-509.40 +/- 1.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -509     |
| time/              |          |
|    total_timesteps | 1357000  |
---------------------------------
Eval num_timesteps=1358000, episode_reward=-492.24 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -492     |
| time/              |          |
|    total_timesteps | 1358000  |
| train/             |          |
|    actor_loss      | -1.57    |
|    critic_loss     | 0.000214 |
|    ent_coef        | 0.00326  |
|    ent_coef_loss   | 3.74     |
|    learning_rate   | 0.00364  |
|    n_updates       | 6630     |
---------------------------------
Eval num_timesteps=1359000, episode_reward=-492.15 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -492     |
| time/              |          |
|    total_timesteps | 1359000  |
---------------------------------
Eval num_timesteps=1360000, episode_reward=-454.80 +/- 1.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -455     |
| time/              |          |
|    total_timesteps | 1360000  |
| train/             |          |
|    actor_loss      | -1.56    |
|    critic_loss     | 0.00148  |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | 1.31     |
|    learning_rate   | 0.00364  |
|    n_updates       | 6640     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -520     |
| time/              |          |
|    episodes        | 1360     |
|    fps             | 638      |
|    time_elapsed    | 2130     |
|    total_timesteps | 1360000  |
---------------------------------
Eval num_timesteps=1361000, episode_reward=-455.18 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -455     |
| time/              |          |
|    total_timesteps | 1361000  |
---------------------------------
Eval num_timesteps=1362000, episode_reward=-458.78 +/- 13.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -459     |
| time/              |          |
|    total_timesteps | 1362000  |
| train/             |          |
|    actor_loss      | -1.57    |
|    critic_loss     | 0.00101  |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | -0.956   |
|    learning_rate   | 0.00364  |
|    n_updates       | 6650     |
---------------------------------
Eval num_timesteps=1363000, episode_reward=-451.95 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -452     |
| time/              |          |
|    total_timesteps | 1363000  |
---------------------------------
Eval num_timesteps=1364000, episode_reward=-468.25 +/- 0.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -468     |
| time/              |          |
|    total_timesteps | 1364000  |
| train/             |          |
|    actor_loss      | -1.57    |
|    critic_loss     | 0.000386 |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | 0.681    |
|    learning_rate   | 0.00364  |
|    n_updates       | 6660     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -513     |
| time/              |          |
|    episodes        | 1364     |
|    fps             | 638      |
|    time_elapsed    | 2136     |
|    total_timesteps | 1364000  |
---------------------------------
Eval num_timesteps=1365000, episode_reward=-473.88 +/- 9.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -474     |
| time/              |          |
|    total_timesteps | 1365000  |
---------------------------------
Eval num_timesteps=1366000, episode_reward=-472.12 +/- 9.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -472     |
| time/              |          |
|    total_timesteps | 1366000  |
---------------------------------
Eval num_timesteps=1367000, episode_reward=-438.15 +/- 13.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 1367000  |
| train/             |          |
|    actor_loss      | -1.55    |
|    critic_loss     | 0.000281 |
|    ent_coef        | 0.00329  |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.00363  |
|    n_updates       | 6670     |
---------------------------------
Eval num_timesteps=1368000, episode_reward=-427.07 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 1368000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -505     |
| time/              |          |
|    episodes        | 1368     |
|    fps             | 638      |
|    time_elapsed    | 2142     |
|    total_timesteps | 1368000  |
---------------------------------
Eval num_timesteps=1369000, episode_reward=-397.61 +/- 0.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 1369000  |
| train/             |          |
|    actor_loss      | -1.55    |
|    critic_loss     | 0.000386 |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | -6.46    |
|    learning_rate   | 0.00363  |
|    n_updates       | 6680     |
---------------------------------
Eval num_timesteps=1370000, episode_reward=-398.16 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 1370000  |
---------------------------------
Eval num_timesteps=1371000, episode_reward=-352.58 +/- 0.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -353     |
| time/              |          |
|    total_timesteps | 1371000  |
| train/             |          |
|    actor_loss      | -1.55    |
|    critic_loss     | 0.000507 |
|    ent_coef        | 0.00327  |
|    ent_coef_loss   | -9.66    |
|    learning_rate   | 0.00363  |
|    n_updates       | 6690     |
---------------------------------
Eval num_timesteps=1372000, episode_reward=-352.28 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -352     |
| time/              |          |
|    total_timesteps | 1372000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -496     |
| time/              |          |
|    episodes        | 1372     |
|    fps             | 638      |
|    time_elapsed    | 2148     |
|    total_timesteps | 1372000  |
---------------------------------
Eval num_timesteps=1373000, episode_reward=-332.18 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -332     |
| time/              |          |
|    total_timesteps | 1373000  |
| train/             |          |
|    actor_loss      | -1.55    |
|    critic_loss     | 0.000716 |
|    ent_coef        | 0.00324  |
|    ent_coef_loss   | -12.5    |
|    learning_rate   | 0.00363  |
|    n_updates       | 6700     |
---------------------------------
Eval num_timesteps=1374000, episode_reward=-332.63 +/- 1.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 1374000  |
---------------------------------
Eval num_timesteps=1375000, episode_reward=-291.06 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 1375000  |
| train/             |          |
|    actor_loss      | -1.54    |
|    critic_loss     | 0.000815 |
|    ent_coef        | 0.00321  |
|    ent_coef_loss   | -13.1    |
|    learning_rate   | 0.00363  |
|    n_updates       | 6710     |
---------------------------------
Eval num_timesteps=1376000, episode_reward=-292.19 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 1376000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -487     |
| time/              |          |
|    episodes        | 1376     |
|    fps             | 638      |
|    time_elapsed    | 2155     |
|    total_timesteps | 1376000  |
---------------------------------
Eval num_timesteps=1377000, episode_reward=-488.06 +/- 20.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 1377000  |
| train/             |          |
|    actor_loss      | -1.54    |
|    critic_loss     | 0.00158  |
|    ent_coef        | 0.00318  |
|    ent_coef_loss   | -8.6     |
|    learning_rate   | 0.00362  |
|    n_updates       | 6720     |
---------------------------------
Eval num_timesteps=1378000, episode_reward=-497.47 +/- 1.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 1378000  |
---------------------------------
Eval num_timesteps=1379000, episode_reward=-532.12 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -532     |
| time/              |          |
|    total_timesteps | 1379000  |
| train/             |          |
|    actor_loss      | -1.65    |
|    critic_loss     | 0.0066   |
|    ent_coef        | 0.00316  |
|    ent_coef_loss   | 12.1     |
|    learning_rate   | 0.00362  |
|    n_updates       | 6730     |
---------------------------------
Eval num_timesteps=1380000, episode_reward=-531.27 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -531     |
| time/              |          |
|    total_timesteps | 1380000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -483     |
| time/              |          |
|    episodes        | 1380     |
|    fps             | 638      |
|    time_elapsed    | 2161     |
|    total_timesteps | 1380000  |
---------------------------------
Eval num_timesteps=1381000, episode_reward=-442.39 +/- 1.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 1381000  |
| train/             |          |
|    actor_loss      | -1.65    |
|    critic_loss     | 0.00132  |
|    ent_coef        | 0.00317  |
|    ent_coef_loss   | 20.2     |
|    learning_rate   | 0.00362  |
|    n_updates       | 6740     |
---------------------------------
Eval num_timesteps=1382000, episode_reward=-441.42 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 1382000  |
---------------------------------
Eval num_timesteps=1383000, episode_reward=-429.00 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 1383000  |
| train/             |          |
|    actor_loss      | -1.54    |
|    critic_loss     | 0.00225  |
|    ent_coef        | 0.00321  |
|    ent_coef_loss   | 5.14     |
|    learning_rate   | 0.00362  |
|    n_updates       | 6750     |
---------------------------------
Eval num_timesteps=1384000, episode_reward=-429.10 +/- 1.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -429     |
| time/              |          |
|    total_timesteps | 1384000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -477     |
| time/              |          |
|    episodes        | 1384     |
|    fps             | 638      |
|    time_elapsed    | 2167     |
|    total_timesteps | 1384000  |
---------------------------------
Eval num_timesteps=1385000, episode_reward=-328.27 +/- 0.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -328     |
| time/              |          |
|    total_timesteps | 1385000  |
| train/             |          |
|    actor_loss      | -1.54    |
|    critic_loss     | 0.00392  |
|    ent_coef        | 0.00323  |
|    ent_coef_loss   | 2.1      |
|    learning_rate   | 0.00362  |
|    n_updates       | 6760     |
---------------------------------
Eval num_timesteps=1386000, episode_reward=-328.28 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -328     |
| time/              |          |
|    total_timesteps | 1386000  |
---------------------------------
Eval num_timesteps=1387000, episode_reward=-430.69 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -431     |
| time/              |          |
|    total_timesteps | 1387000  |
| train/             |          |
|    actor_loss      | -1.61    |
|    critic_loss     | 0.00302  |
|    ent_coef        | 0.00325  |
|    ent_coef_loss   | 16.4     |
|    learning_rate   | 0.00361  |
|    n_updates       | 6770     |
---------------------------------
Eval num_timesteps=1388000, episode_reward=-430.49 +/- 0.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -430     |
| time/              |          |
|    total_timesteps | 1388000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -470     |
| time/              |          |
|    episodes        | 1388     |
|    fps             | 638      |
|    time_elapsed    | 2173     |
|    total_timesteps | 1388000  |
---------------------------------
Eval num_timesteps=1389000, episode_reward=-476.16 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 1389000  |
| train/             |          |
|    actor_loss      | -1.64    |
|    critic_loss     | 0.00134  |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | 18.2     |
|    learning_rate   | 0.00361  |
|    n_updates       | 6780     |
---------------------------------
Eval num_timesteps=1390000, episode_reward=-476.17 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 1390000  |
---------------------------------
Eval num_timesteps=1391000, episode_reward=-384.80 +/- 1.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -385     |
| time/              |          |
|    total_timesteps | 1391000  |
| train/             |          |
|    actor_loss      | -1.59    |
|    critic_loss     | 0.00145  |
|    ent_coef        | 0.00333  |
|    ent_coef_loss   | 3.37     |
|    learning_rate   | 0.00361  |
|    n_updates       | 6790     |
---------------------------------
Eval num_timesteps=1392000, episode_reward=-384.48 +/- 0.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -384     |
| time/              |          |
|    total_timesteps | 1392000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -463     |
| time/              |          |
|    episodes        | 1392     |
|    fps             | 638      |
|    time_elapsed    | 2179     |
|    total_timesteps | 1392000  |
---------------------------------
Eval num_timesteps=1393000, episode_reward=-340.74 +/- 0.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -341     |
| time/              |          |
|    total_timesteps | 1393000  |
| train/             |          |
|    actor_loss      | -1.62    |
|    critic_loss     | 0.00659  |
|    ent_coef        | 0.00335  |
|    ent_coef_loss   | 9.83     |
|    learning_rate   | 0.00361  |
|    n_updates       | 6800     |
---------------------------------
Eval num_timesteps=1394000, episode_reward=-340.98 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -341     |
| time/              |          |
|    total_timesteps | 1394000  |
---------------------------------
Eval num_timesteps=1395000, episode_reward=-458.18 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -458     |
| time/              |          |
|    total_timesteps | 1395000  |
| train/             |          |
|    actor_loss      | -1.6     |
|    critic_loss     | 0.00317  |
|    ent_coef        | 0.00338  |
|    ent_coef_loss   | 10       |
|    learning_rate   | 0.00361  |
|    n_updates       | 6810     |
---------------------------------
Eval num_timesteps=1396000, episode_reward=-457.84 +/- 0.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -458     |
| time/              |          |
|    total_timesteps | 1396000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -455     |
| time/              |          |
|    episodes        | 1396     |
|    fps             | 638      |
|    time_elapsed    | 2185     |
|    total_timesteps | 1396000  |
---------------------------------
Eval num_timesteps=1397000, episode_reward=-319.12 +/- 1.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -319     |
| time/              |          |
|    total_timesteps | 1397000  |
| train/             |          |
|    actor_loss      | -1.56    |
|    critic_loss     | 0.00177  |
|    ent_coef        | 0.00341  |
|    ent_coef_loss   | -0.413   |
|    learning_rate   | 0.0036   |
|    n_updates       | 6820     |
---------------------------------
Eval num_timesteps=1398000, episode_reward=-320.05 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -320     |
| time/              |          |
|    total_timesteps | 1398000  |
---------------------------------
Eval num_timesteps=1399000, episode_reward=-561.42 +/- 51.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -561     |
| time/              |          |
|    total_timesteps | 1399000  |
| train/             |          |
|    actor_loss      | -1.51    |
|    critic_loss     | 0.0147   |
|    ent_coef        | 0.00342  |
|    ent_coef_loss   | 9.99     |
|    learning_rate   | 0.0036   |
|    n_updates       | 6830     |
---------------------------------
Eval num_timesteps=1400000, episode_reward=-468.58 +/- 115.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -469     |
| time/              |          |
|    total_timesteps | 1400000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -451     |
| time/              |          |
|    episodes        | 1400     |
|    fps             | 638      |
|    time_elapsed    | 2192     |
|    total_timesteps | 1400000  |
---------------------------------
Eval num_timesteps=1401000, episode_reward=-370.83 +/- 1.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 1401000  |
| train/             |          |
|    actor_loss      | -1.58    |
|    critic_loss     | 0.00542  |
|    ent_coef        | 0.00344  |
|    ent_coef_loss   | -0.67    |
|    learning_rate   | 0.0036   |
|    n_updates       | 6840     |
---------------------------------
Eval num_timesteps=1402000, episode_reward=-370.81 +/- 2.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 1402000  |
---------------------------------
Eval num_timesteps=1403000, episode_reward=-376.17 +/- 1.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1403000  |
| train/             |          |
|    actor_loss      | -1.55    |
|    critic_loss     | 0.0022   |
|    ent_coef        | 0.00345  |
|    ent_coef_loss   | -0.847   |
|    learning_rate   | 0.0036   |
|    n_updates       | 6850     |
---------------------------------
Eval num_timesteps=1404000, episode_reward=-375.95 +/- 1.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1404000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -446     |
| time/              |          |
|    episodes        | 1404     |
|    fps             | 638      |
|    time_elapsed    | 2198     |
|    total_timesteps | 1404000  |
---------------------------------
Eval num_timesteps=1405000, episode_reward=-402.94 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 1405000  |
| train/             |          |
|    actor_loss      | -1.56    |
|    critic_loss     | 0.00182  |
|    ent_coef        | 0.00345  |
|    ent_coef_loss   | -6.19    |
|    learning_rate   | 0.0036   |
|    n_updates       | 6860     |
---------------------------------
Eval num_timesteps=1406000, episode_reward=-404.14 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 1406000  |
---------------------------------
Eval num_timesteps=1407000, episode_reward=-452.87 +/- 1.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -453     |
| time/              |          |
|    total_timesteps | 1407000  |
| train/             |          |
|    actor_loss      | -1.5     |
|    critic_loss     | 0.00103  |
|    ent_coef        | 0.00345  |
|    ent_coef_loss   | 6.53     |
|    learning_rate   | 0.00359  |
|    n_updates       | 6870     |
---------------------------------
Eval num_timesteps=1408000, episode_reward=-453.03 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -453     |
| time/              |          |
|    total_timesteps | 1408000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -441     |
| time/              |          |
|    episodes        | 1408     |
|    fps             | 638      |
|    time_elapsed    | 2204     |
|    total_timesteps | 1408000  |
---------------------------------
Eval num_timesteps=1409000, episode_reward=-452.48 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -452     |
| time/              |          |
|    total_timesteps | 1409000  |
---------------------------------
Eval num_timesteps=1410000, episode_reward=-488.52 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -489     |
| time/              |          |
|    total_timesteps | 1410000  |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 0.000741 |
|    ent_coef        | 0.00345  |
|    ent_coef_loss   | 4.09     |
|    learning_rate   | 0.00359  |
|    n_updates       | 6880     |
---------------------------------
Eval num_timesteps=1411000, episode_reward=-487.62 +/- 0.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 1411000  |
---------------------------------
Eval num_timesteps=1412000, episode_reward=-469.24 +/- 1.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -469     |
| time/              |          |
|    total_timesteps | 1412000  |
| train/             |          |
|    actor_loss      | -1.45    |
|    critic_loss     | 0.000844 |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | 1.02     |
|    learning_rate   | 0.00359  |
|    n_updates       | 6890     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -439     |
| time/              |          |
|    episodes        | 1412     |
|    fps             | 638      |
|    time_elapsed    | 2210     |
|    total_timesteps | 1412000  |
---------------------------------
Eval num_timesteps=1413000, episode_reward=-468.78 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -469     |
| time/              |          |
|    total_timesteps | 1413000  |
---------------------------------
Eval num_timesteps=1414000, episode_reward=-420.53 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 1414000  |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 0.00118  |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | 0.0378   |
|    learning_rate   | 0.00359  |
|    n_updates       | 6900     |
---------------------------------
Eval num_timesteps=1415000, episode_reward=-419.55 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 1415000  |
---------------------------------
Eval num_timesteps=1416000, episode_reward=-420.38 +/- 1.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 1416000  |
| train/             |          |
|    actor_loss      | -1.45    |
|    critic_loss     | 0.0014   |
|    ent_coef        | 0.00348  |
|    ent_coef_loss   | -1.9     |
|    learning_rate   | 0.00358  |
|    n_updates       | 6910     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -432     |
| time/              |          |
|    episodes        | 1416     |
|    fps             | 638      |
|    time_elapsed    | 2216     |
|    total_timesteps | 1416000  |
---------------------------------
Eval num_timesteps=1417000, episode_reward=-420.65 +/- 1.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 1417000  |
---------------------------------
Eval num_timesteps=1418000, episode_reward=-433.31 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -433     |
| time/              |          |
|    total_timesteps | 1418000  |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 0.00184  |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -2.06    |
|    learning_rate   | 0.00358  |
|    n_updates       | 6920     |
---------------------------------
Eval num_timesteps=1419000, episode_reward=-432.22 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 1419000  |
---------------------------------
Eval num_timesteps=1420000, episode_reward=-329.16 +/- 0.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 1420000  |
| train/             |          |
|    actor_loss      | -1.47    |
|    critic_loss     | 0.00175  |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | 4.91     |
|    learning_rate   | 0.00358  |
|    n_updates       | 6930     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -427     |
| time/              |          |
|    episodes        | 1420     |
|    fps             | 638      |
|    time_elapsed    | 2222     |
|    total_timesteps | 1420000  |
---------------------------------
Eval num_timesteps=1421000, episode_reward=-328.54 +/- 1.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 1421000  |
---------------------------------
Eval num_timesteps=1422000, episode_reward=-340.81 +/- 1.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -341     |
| time/              |          |
|    total_timesteps | 1422000  |
| train/             |          |
|    actor_loss      | -1.52    |
|    critic_loss     | 0.00175  |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -2.95    |
|    learning_rate   | 0.00358  |
|    n_updates       | 6940     |
---------------------------------
Eval num_timesteps=1423000, episode_reward=-340.97 +/- 2.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -341     |
| time/              |          |
|    total_timesteps | 1423000  |
---------------------------------
Eval num_timesteps=1424000, episode_reward=-326.99 +/- 1.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -327     |
| time/              |          |
|    total_timesteps | 1424000  |
| train/             |          |
|    actor_loss      | -1.53    |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -3.38    |
|    learning_rate   | 0.00358  |
|    n_updates       | 6950     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -420     |
| time/              |          |
|    episodes        | 1424     |
|    fps             | 638      |
|    time_elapsed    | 2229     |
|    total_timesteps | 1424000  |
---------------------------------
Eval num_timesteps=1425000, episode_reward=-328.52 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 1425000  |
---------------------------------
Eval num_timesteps=1426000, episode_reward=-323.85 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -324     |
| time/              |          |
|    total_timesteps | 1426000  |
| train/             |          |
|    actor_loss      | -1.52    |
|    critic_loss     | 0.00223  |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | 12.6     |
|    learning_rate   | 0.00357  |
|    n_updates       | 6960     |
---------------------------------
Eval num_timesteps=1427000, episode_reward=-322.92 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -323     |
| time/              |          |
|    total_timesteps | 1427000  |
---------------------------------
Eval num_timesteps=1428000, episode_reward=-391.56 +/- 0.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 1428000  |
| train/             |          |
|    actor_loss      | -1.52    |
|    critic_loss     | 0.00163  |
|    ent_coef        | 0.00349  |
|    ent_coef_loss   | 0.703    |
|    learning_rate   | 0.00357  |
|    n_updates       | 6970     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -412     |
| time/              |          |
|    episodes        | 1428     |
|    fps             | 638      |
|    time_elapsed    | 2235     |
|    total_timesteps | 1428000  |
---------------------------------
Eval num_timesteps=1429000, episode_reward=-390.86 +/- 0.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 1429000  |
---------------------------------
Eval num_timesteps=1430000, episode_reward=-405.46 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 1430000  |
| train/             |          |
|    actor_loss      | -1.52    |
|    critic_loss     | 0.00097  |
|    ent_coef        | 0.0035   |
|    ent_coef_loss   | -0.297   |
|    learning_rate   | 0.00357  |
|    n_updates       | 6980     |
---------------------------------
Eval num_timesteps=1431000, episode_reward=-405.29 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 1431000  |
---------------------------------
Eval num_timesteps=1432000, episode_reward=-359.91 +/- 1.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 1432000  |
| train/             |          |
|    actor_loss      | -1.5     |
|    critic_loss     | 0.000695 |
|    ent_coef        | 0.0035   |
|    ent_coef_loss   | -7.53    |
|    learning_rate   | 0.00357  |
|    n_updates       | 6990     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -410     |
| time/              |          |
|    episodes        | 1432     |
|    fps             | 638      |
|    time_elapsed    | 2241     |
|    total_timesteps | 1432000  |
---------------------------------
Eval num_timesteps=1433000, episode_reward=-360.20 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 1433000  |
---------------------------------
Eval num_timesteps=1434000, episode_reward=-332.29 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -332     |
| time/              |          |
|    total_timesteps | 1434000  |
| train/             |          |
|    actor_loss      | -1.49    |
|    critic_loss     | 0.00176  |
|    ent_coef        | 0.00349  |
|    ent_coef_loss   | -5.22    |
|    learning_rate   | 0.00357  |
|    n_updates       | 7000     |
---------------------------------
Eval num_timesteps=1435000, episode_reward=-333.55 +/- 0.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -334     |
| time/              |          |
|    total_timesteps | 1435000  |
---------------------------------
Eval num_timesteps=1436000, episode_reward=-343.83 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 1436000  |
| train/             |          |
|    actor_loss      | -1.51    |
|    critic_loss     | 0.00116  |
|    ent_coef        | 0.00347  |
|    ent_coef_loss   | -7.48    |
|    learning_rate   | 0.00356  |
|    n_updates       | 7010     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -407     |
| time/              |          |
|    episodes        | 1436     |
|    fps             | 638      |
|    time_elapsed    | 2247     |
|    total_timesteps | 1436000  |
---------------------------------
Eval num_timesteps=1437000, episode_reward=-343.19 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -343     |
| time/              |          |
|    total_timesteps | 1437000  |
---------------------------------
Eval num_timesteps=1438000, episode_reward=-349.52 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 1438000  |
| train/             |          |
|    actor_loss      | -1.49    |
|    critic_loss     | 0.00159  |
|    ent_coef        | 0.00344  |
|    ent_coef_loss   | 0.424    |
|    learning_rate   | 0.00356  |
|    n_updates       | 7020     |
---------------------------------
Eval num_timesteps=1439000, episode_reward=-348.98 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -349     |
| time/              |          |
|    total_timesteps | 1439000  |
---------------------------------
Eval num_timesteps=1440000, episode_reward=-345.94 +/- 1.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -346     |
| time/              |          |
|    total_timesteps | 1440000  |
| train/             |          |
|    actor_loss      | -1.49    |
|    critic_loss     | 0.00106  |
|    ent_coef        | 0.00344  |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.00356  |
|    n_updates       | 7030     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -405     |
| time/              |          |
|    episodes        | 1440     |
|    fps             | 638      |
|    time_elapsed    | 2253     |
|    total_timesteps | 1440000  |
---------------------------------
Eval num_timesteps=1441000, episode_reward=-346.85 +/- 1.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 1441000  |
---------------------------------
Eval num_timesteps=1442000, episode_reward=-326.53 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -327     |
| time/              |          |
|    total_timesteps | 1442000  |
| train/             |          |
|    actor_loss      | -1.49    |
|    critic_loss     | 0.000842 |
|    ent_coef        | 0.00343  |
|    ent_coef_loss   | 0.976    |
|    learning_rate   | 0.00356  |
|    n_updates       | 7040     |
---------------------------------
Eval num_timesteps=1443000, episode_reward=-326.49 +/- 2.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -326     |
| time/              |          |
|    total_timesteps | 1443000  |
---------------------------------
Eval num_timesteps=1444000, episode_reward=-354.52 +/- 30.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -355     |
| time/              |          |
|    total_timesteps | 1444000  |
| train/             |          |
|    actor_loss      | -1.48    |
|    critic_loss     | 0.0015   |
|    ent_coef        | 0.00343  |
|    ent_coef_loss   | -0.986   |
|    learning_rate   | 0.00356  |
|    n_updates       | 7050     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -404     |
| time/              |          |
|    episodes        | 1444     |
|    fps             | 639      |
|    time_elapsed    | 2259     |
|    total_timesteps | 1444000  |
---------------------------------
Eval num_timesteps=1445000, episode_reward=-362.86 +/- 47.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -363     |
| time/              |          |
|    total_timesteps | 1445000  |
---------------------------------
Eval num_timesteps=1446000, episode_reward=-316.18 +/- 1.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 1446000  |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 0.00242  |
|    ent_coef        | 0.00343  |
|    ent_coef_loss   | -6.61    |
|    learning_rate   | 0.00355  |
|    n_updates       | 7060     |
---------------------------------
Eval num_timesteps=1447000, episode_reward=-316.62 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -317     |
| time/              |          |
|    total_timesteps | 1447000  |
---------------------------------
Eval num_timesteps=1448000, episode_reward=-306.44 +/- 0.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 1448000  |
| train/             |          |
|    actor_loss      | -1.47    |
|    critic_loss     | 0.00112  |
|    ent_coef        | 0.00341  |
|    ent_coef_loss   | -0.857   |
|    learning_rate   | 0.00355  |
|    n_updates       | 7070     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -399     |
| time/              |          |
|    episodes        | 1448     |
|    fps             | 639      |
|    time_elapsed    | 2265     |
|    total_timesteps | 1448000  |
---------------------------------
Eval num_timesteps=1449000, episode_reward=-306.83 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 1449000  |
---------------------------------
Eval num_timesteps=1450000, episode_reward=-316.42 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 1450000  |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 0.00114  |
|    ent_coef        | 0.0034   |
|    ent_coef_loss   | -6.17    |
|    learning_rate   | 0.00355  |
|    n_updates       | 7080     |
---------------------------------
Eval num_timesteps=1451000, episode_reward=-316.70 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -317     |
| time/              |          |
|    total_timesteps | 1451000  |
---------------------------------
Eval num_timesteps=1452000, episode_reward=-318.00 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -318     |
| time/              |          |
|    total_timesteps | 1452000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -399     |
| time/              |          |
|    episodes        | 1452     |
|    fps             | 639      |
|    time_elapsed    | 2272     |
|    total_timesteps | 1452000  |
---------------------------------
Eval num_timesteps=1453000, episode_reward=-315.45 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 1453000  |
| train/             |          |
|    actor_loss      | -1.45    |
|    critic_loss     | 0.00188  |
|    ent_coef        | 0.00339  |
|    ent_coef_loss   | -4.97    |
|    learning_rate   | 0.00355  |
|    n_updates       | 7090     |
---------------------------------
Eval num_timesteps=1454000, episode_reward=-315.02 +/- 0.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 1454000  |
---------------------------------
Eval num_timesteps=1455000, episode_reward=-308.37 +/- 1.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 1455000  |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 0.00186  |
|    ent_coef        | 0.00337  |
|    ent_coef_loss   | -10.6    |
|    learning_rate   | 0.00355  |
|    n_updates       | 7100     |
---------------------------------
Eval num_timesteps=1456000, episode_reward=-308.69 +/- 1.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 1456000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -394     |
| time/              |          |
|    episodes        | 1456     |
|    fps             | 639      |
|    time_elapsed    | 2278     |
|    total_timesteps | 1456000  |
---------------------------------
Eval num_timesteps=1457000, episode_reward=-302.70 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 1457000  |
| train/             |          |
|    actor_loss      | -1.46    |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.00334  |
|    ent_coef_loss   | -5.43    |
|    learning_rate   | 0.00354  |
|    n_updates       | 7110     |
---------------------------------
Eval num_timesteps=1458000, episode_reward=-302.81 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -303     |
| time/              |          |
|    total_timesteps | 1458000  |
---------------------------------
Eval num_timesteps=1459000, episode_reward=-296.10 +/- 1.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 1459000  |
| train/             |          |
|    actor_loss      | -1.45    |
|    critic_loss     | 0.00104  |
|    ent_coef        | 0.00331  |
|    ent_coef_loss   | -10      |
|    learning_rate   | 0.00354  |
|    n_updates       | 7120     |
---------------------------------
Eval num_timesteps=1460000, episode_reward=-296.53 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 1460000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -396     |
| time/              |          |
|    episodes        | 1460     |
|    fps             | 639      |
|    time_elapsed    | 2284     |
|    total_timesteps | 1460000  |
---------------------------------
Eval num_timesteps=1461000, episode_reward=-301.65 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 1461000  |
| train/             |          |
|    actor_loss      | -1.44    |
|    critic_loss     | 0.00102  |
|    ent_coef        | 0.00328  |
|    ent_coef_loss   | -7.19    |
|    learning_rate   | 0.00354  |
|    n_updates       | 7130     |
---------------------------------
Eval num_timesteps=1462000, episode_reward=-300.32 +/- 2.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 1462000  |
---------------------------------
Eval num_timesteps=1463000, episode_reward=-316.20 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 1463000  |
| train/             |          |
|    actor_loss      | -1.44    |
|    critic_loss     | 0.000956 |
|    ent_coef        | 0.00325  |
|    ent_coef_loss   | -8.15    |
|    learning_rate   | 0.00354  |
|    n_updates       | 7140     |
---------------------------------
Eval num_timesteps=1464000, episode_reward=-315.74 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 1464000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -396     |
| time/              |          |
|    episodes        | 1464     |
|    fps             | 639      |
|    time_elapsed    | 2290     |
|    total_timesteps | 1464000  |
---------------------------------
Eval num_timesteps=1465000, episode_reward=-329.99 +/- 1.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 1465000  |
| train/             |          |
|    actor_loss      | -1.44    |
|    critic_loss     | 0.00089  |
|    ent_coef        | 0.00323  |
|    ent_coef_loss   | -6.75    |
|    learning_rate   | 0.00354  |
|    n_updates       | 7150     |
---------------------------------
Eval num_timesteps=1466000, episode_reward=-331.24 +/- 1.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -331     |
| time/              |          |
|    total_timesteps | 1466000  |
---------------------------------
Eval num_timesteps=1467000, episode_reward=-315.63 +/- 2.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 1467000  |
| train/             |          |
|    actor_loss      | -1.44    |
|    critic_loss     | 0.000763 |
|    ent_coef        | 0.0032   |
|    ent_coef_loss   | -3.84    |
|    learning_rate   | 0.00353  |
|    n_updates       | 7160     |
---------------------------------
Eval num_timesteps=1468000, episode_reward=-314.81 +/- 2.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 1468000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -396     |
| time/              |          |
|    episodes        | 1468     |
|    fps             | 639      |
|    time_elapsed    | 2296     |
|    total_timesteps | 1468000  |
---------------------------------
Eval num_timesteps=1469000, episode_reward=-317.84 +/- 2.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -318     |
| time/              |          |
|    total_timesteps | 1469000  |
| train/             |          |
|    actor_loss      | -1.42    |
|    critic_loss     | 0.000779 |
|    ent_coef        | 0.00318  |
|    ent_coef_loss   | -9.38    |
|    learning_rate   | 0.00353  |
|    n_updates       | 7170     |
---------------------------------
Eval num_timesteps=1470000, episode_reward=-319.28 +/- 1.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -319     |
| time/              |          |
|    total_timesteps | 1470000  |
---------------------------------
Eval num_timesteps=1471000, episode_reward=-332.90 +/- 1.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 1471000  |
| train/             |          |
|    actor_loss      | -1.43    |
|    critic_loss     | 0.00111  |
|    ent_coef        | 0.00316  |
|    ent_coef_loss   | -13.8    |
|    learning_rate   | 0.00353  |
|    n_updates       | 7180     |
---------------------------------
Eval num_timesteps=1472000, episode_reward=-334.06 +/- 1.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -334     |
| time/              |          |
|    total_timesteps | 1472000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -396     |
| time/              |          |
|    episodes        | 1472     |
|    fps             | 639      |
|    time_elapsed    | 2302     |
|    total_timesteps | 1472000  |
---------------------------------
Eval num_timesteps=1473000, episode_reward=-335.09 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 1473000  |
| train/             |          |
|    actor_loss      | -1.42    |
|    critic_loss     | 0.000937 |
|    ent_coef        | 0.00312  |
|    ent_coef_loss   | -11.4    |
|    learning_rate   | 0.00353  |
|    n_updates       | 7190     |
---------------------------------
Eval num_timesteps=1474000, episode_reward=-335.85 +/- 1.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 1474000  |
---------------------------------
Eval num_timesteps=1475000, episode_reward=-338.84 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 1475000  |
| train/             |          |
|    actor_loss      | -1.41    |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.00308  |
|    ent_coef_loss   | -10.7    |
|    learning_rate   | 0.00353  |
|    n_updates       | 7200     |
---------------------------------
Eval num_timesteps=1476000, episode_reward=-339.73 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 1476000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -395     |
| time/              |          |
|    episodes        | 1476     |
|    fps             | 639      |
|    time_elapsed    | 2309     |
|    total_timesteps | 1476000  |
---------------------------------
Eval num_timesteps=1477000, episode_reward=-329.71 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 1477000  |
| train/             |          |
|    actor_loss      | -1.42    |
|    critic_loss     | 0.000746 |
|    ent_coef        | 0.00304  |
|    ent_coef_loss   | -13.1    |
|    learning_rate   | 0.00352  |
|    n_updates       | 7210     |
---------------------------------
Eval num_timesteps=1478000, episode_reward=-329.70 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 1478000  |
---------------------------------
Eval num_timesteps=1479000, episode_reward=-331.68 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -332     |
| time/              |          |
|    total_timesteps | 1479000  |
| train/             |          |
|    actor_loss      | -1.42    |
|    critic_loss     | 0.000902 |
|    ent_coef        | 0.003    |
|    ent_coef_loss   | -14.5    |
|    learning_rate   | 0.00352  |
|    n_updates       | 7220     |
---------------------------------
Eval num_timesteps=1480000, episode_reward=-331.64 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -332     |
| time/              |          |
|    total_timesteps | 1480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -390     |
| time/              |          |
|    episodes        | 1480     |
|    fps             | 639      |
|    time_elapsed    | 2315     |
|    total_timesteps | 1480000  |
---------------------------------
Eval num_timesteps=1481000, episode_reward=-146.94 +/- 311.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 1481000  |
| train/             |          |
|    actor_loss      | -1.41    |
|    critic_loss     | 0.000931 |
|    ent_coef        | 0.00295  |
|    ent_coef_loss   | -14.9    |
|    learning_rate   | 0.00352  |
|    n_updates       | 7230     |
---------------------------------
Eval num_timesteps=1482000, episode_reward=-304.48 +/- 0.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 1482000  |
---------------------------------
Eval num_timesteps=1483000, episode_reward=-42.24 +/- 5.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -42.2    |
| time/              |          |
|    total_timesteps | 1483000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.00112  |
|    ent_coef        | 0.0029   |
|    ent_coef_loss   | -16.2    |
|    learning_rate   | 0.00352  |
|    n_updates       | 7240     |
---------------------------------
Eval num_timesteps=1484000, episode_reward=-44.48 +/- 2.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -44.5    |
| time/              |          |
|    total_timesteps | 1484000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -388     |
| time/              |          |
|    episodes        | 1484     |
|    fps             | 639      |
|    time_elapsed    | 2321     |
|    total_timesteps | 1484000  |
---------------------------------
Eval num_timesteps=1485000, episode_reward=-313.37 +/- 28.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -313     |
| time/              |          |
|    total_timesteps | 1485000  |
| train/             |          |
|    actor_loss      | -1.41    |
|    critic_loss     | 0.00111  |
|    ent_coef        | 0.00285  |
|    ent_coef_loss   | -18.5    |
|    learning_rate   | 0.00352  |
|    n_updates       | 7250     |
---------------------------------
Eval num_timesteps=1486000, episode_reward=-317.69 +/- 27.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -318     |
| time/              |          |
|    total_timesteps | 1486000  |
---------------------------------
Eval num_timesteps=1487000, episode_reward=-403.42 +/- 124.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 1487000  |
| train/             |          |
|    actor_loss      | -1.41    |
|    critic_loss     | 0.0014   |
|    ent_coef        | 0.0028   |
|    ent_coef_loss   | -22.2    |
|    learning_rate   | 0.00351  |
|    n_updates       | 7260     |
---------------------------------
Eval num_timesteps=1488000, episode_reward=-397.73 +/- 89.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -398     |
| time/              |          |
|    total_timesteps | 1488000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -388     |
| time/              |          |
|    episodes        | 1488     |
|    fps             | 639      |
|    time_elapsed    | 2327     |
|    total_timesteps | 1488000  |
---------------------------------
Eval num_timesteps=1489000, episode_reward=-305.79 +/- 18.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 1489000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.00088  |
|    ent_coef        | 0.00274  |
|    ent_coef_loss   | -18.6    |
|    learning_rate   | 0.00351  |
|    n_updates       | 7270     |
---------------------------------
Eval num_timesteps=1490000, episode_reward=-316.78 +/- 21.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -317     |
| time/              |          |
|    total_timesteps | 1490000  |
---------------------------------
Eval num_timesteps=1491000, episode_reward=-479.05 +/- 265.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -479     |
| time/              |          |
|    total_timesteps | 1491000  |
| train/             |          |
|    actor_loss      | -1.4     |
|    critic_loss     | 0.00065  |
|    ent_coef        | 0.00268  |
|    ent_coef_loss   | -16      |
|    learning_rate   | 0.00351  |
|    n_updates       | 7280     |
---------------------------------
Eval num_timesteps=1492000, episode_reward=-341.37 +/- 18.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -341     |
| time/              |          |
|    total_timesteps | 1492000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -386     |
| time/              |          |
|    episodes        | 1492     |
|    fps             | 639      |
|    time_elapsed    | 2333     |
|    total_timesteps | 1492000  |
---------------------------------
Eval num_timesteps=1493000, episode_reward=-344.83 +/- 28.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -345     |
| time/              |          |
|    total_timesteps | 1493000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000821 |
|    ent_coef        | 0.00263  |
|    ent_coef_loss   | -15.2    |
|    learning_rate   | 0.00351  |
|    n_updates       | 7290     |
---------------------------------
Eval num_timesteps=1494000, episode_reward=-364.51 +/- 33.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -365     |
| time/              |          |
|    total_timesteps | 1494000  |
---------------------------------
Eval num_timesteps=1495000, episode_reward=-461.08 +/- 94.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -461     |
| time/              |          |
|    total_timesteps | 1495000  |
---------------------------------
Eval num_timesteps=1496000, episode_reward=-366.35 +/- 92.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -366     |
| time/              |          |
|    total_timesteps | 1496000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000902 |
|    ent_coef        | 0.00258  |
|    ent_coef_loss   | -21      |
|    learning_rate   | 0.0035   |
|    n_updates       | 7300     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -391     |
| time/              |          |
|    episodes        | 1496     |
|    fps             | 639      |
|    time_elapsed    | 2339     |
|    total_timesteps | 1496000  |
---------------------------------
Eval num_timesteps=1497000, episode_reward=-381.89 +/- 70.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 1497000  |
---------------------------------
Eval num_timesteps=1498000, episode_reward=-348.27 +/- 43.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -348     |
| time/              |          |
|    total_timesteps | 1498000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000802 |
|    ent_coef        | 0.00253  |
|    ent_coef_loss   | -16.4    |
|    learning_rate   | 0.0035   |
|    n_updates       | 7310     |
---------------------------------
Eval num_timesteps=1499000, episode_reward=-370.21 +/- 28.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -370     |
| time/              |          |
|    total_timesteps | 1499000  |
---------------------------------
Eval num_timesteps=1500000, episode_reward=-425.32 +/- 65.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -425     |
| time/              |          |
|    total_timesteps | 1500000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000583 |
|    ent_coef        | 0.00248  |
|    ent_coef_loss   | -13.9    |
|    learning_rate   | 0.0035   |
|    n_updates       | 7320     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -386     |
| time/              |          |
|    episodes        | 1500     |
|    fps             | 639      |
|    time_elapsed    | 2345     |
|    total_timesteps | 1500000  |
---------------------------------
Eval num_timesteps=1501000, episode_reward=-391.92 +/- 52.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 1501000  |
---------------------------------
Eval num_timesteps=1502000, episode_reward=-405.87 +/- 53.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 1502000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000547 |
|    ent_coef        | 0.00244  |
|    ent_coef_loss   | -13.7    |
|    learning_rate   | 0.0035   |
|    n_updates       | 7330     |
---------------------------------
Eval num_timesteps=1503000, episode_reward=-472.68 +/- 160.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -473     |
| time/              |          |
|    total_timesteps | 1503000  |
---------------------------------
Eval num_timesteps=1504000, episode_reward=-376.69 +/- 95.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 1504000  |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.000503 |
|    ent_coef        | 0.0024   |
|    ent_coef_loss   | -16      |
|    learning_rate   | 0.0035   |
|    n_updates       | 7340     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -386     |
| time/              |          |
|    episodes        | 1504     |
|    fps             | 639      |
|    time_elapsed    | 2352     |
|    total_timesteps | 1504000  |
---------------------------------
Eval num_timesteps=1505000, episode_reward=-333.27 +/- 14.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 1505000  |
---------------------------------
Eval num_timesteps=1506000, episode_reward=-347.38 +/- 44.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 1506000  |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.000668 |
|    ent_coef        | 0.00236  |
|    ent_coef_loss   | -15      |
|    learning_rate   | 0.00349  |
|    n_updates       | 7350     |
---------------------------------
Eval num_timesteps=1507000, episode_reward=-349.73 +/- 46.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 1507000  |
---------------------------------
Eval num_timesteps=1508000, episode_reward=-594.59 +/- 260.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -595     |
| time/              |          |
|    total_timesteps | 1508000  |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.000583 |
|    ent_coef        | 0.00232  |
|    ent_coef_loss   | -15.1    |
|    learning_rate   | 0.00349  |
|    n_updates       | 7360     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -385     |
| time/              |          |
|    episodes        | 1508     |
|    fps             | 639      |
|    time_elapsed    | 2358     |
|    total_timesteps | 1508000  |
---------------------------------
Eval num_timesteps=1509000, episode_reward=-390.05 +/- 29.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 1509000  |
---------------------------------
Eval num_timesteps=1510000, episode_reward=-333.43 +/- 12.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 1510000  |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.000563 |
|    ent_coef        | 0.00228  |
|    ent_coef_loss   | -13.6    |
|    learning_rate   | 0.00349  |
|    n_updates       | 7370     |
---------------------------------
Eval num_timesteps=1511000, episode_reward=-419.85 +/- 48.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 1511000  |
---------------------------------
Eval num_timesteps=1512000, episode_reward=-394.93 +/- 54.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 1512000  |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.000609 |
|    ent_coef        | 0.00225  |
|    ent_coef_loss   | -16.8    |
|    learning_rate   | 0.00349  |
|    n_updates       | 7380     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -384     |
| time/              |          |
|    episodes        | 1512     |
|    fps             | 639      |
|    time_elapsed    | 2364     |
|    total_timesteps | 1512000  |
---------------------------------
Eval num_timesteps=1513000, episode_reward=-382.65 +/- 46.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 1513000  |
---------------------------------
Eval num_timesteps=1514000, episode_reward=-333.31 +/- 24.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 1514000  |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.000606 |
|    ent_coef        | 0.00221  |
|    ent_coef_loss   | -12.3    |
|    learning_rate   | 0.00349  |
|    n_updates       | 7390     |
---------------------------------
Eval num_timesteps=1515000, episode_reward=-338.09 +/- 38.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -338     |
| time/              |          |
|    total_timesteps | 1515000  |
---------------------------------
Eval num_timesteps=1516000, episode_reward=-347.18 +/- 21.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 1516000  |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.000429 |
|    ent_coef        | 0.00218  |
|    ent_coef_loss   | -15.8    |
|    learning_rate   | 0.00348  |
|    n_updates       | 7400     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -383     |
| time/              |          |
|    episodes        | 1516     |
|    fps             | 639      |
|    time_elapsed    | 2370     |
|    total_timesteps | 1516000  |
---------------------------------
Eval num_timesteps=1517000, episode_reward=-402.05 +/- 63.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 1517000  |
---------------------------------
Eval num_timesteps=1518000, episode_reward=-363.16 +/- 20.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -363     |
| time/              |          |
|    total_timesteps | 1518000  |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.000451 |
|    ent_coef        | 0.00214  |
|    ent_coef_loss   | -15.3    |
|    learning_rate   | 0.00348  |
|    n_updates       | 7410     |
---------------------------------
Eval num_timesteps=1519000, episode_reward=-378.32 +/- 31.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 1519000  |
---------------------------------
Eval num_timesteps=1520000, episode_reward=-354.20 +/- 22.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 1520000  |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.000521 |
|    ent_coef        | 0.0021   |
|    ent_coef_loss   | -16.8    |
|    learning_rate   | 0.00348  |
|    n_updates       | 7420     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -383     |
| time/              |          |
|    episodes        | 1520     |
|    fps             | 639      |
|    time_elapsed    | 2376     |
|    total_timesteps | 1520000  |
---------------------------------
Eval num_timesteps=1521000, episode_reward=-369.86 +/- 52.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -370     |
| time/              |          |
|    total_timesteps | 1521000  |
---------------------------------
Eval num_timesteps=1522000, episode_reward=-380.95 +/- 60.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -381     |
| time/              |          |
|    total_timesteps | 1522000  |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.000342 |
|    ent_coef        | 0.00207  |
|    ent_coef_loss   | -16.7    |
|    learning_rate   | 0.00348  |
|    n_updates       | 7430     |
---------------------------------
Eval num_timesteps=1523000, episode_reward=-361.77 +/- 30.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -362     |
| time/              |          |
|    total_timesteps | 1523000  |
---------------------------------
Eval num_timesteps=1524000, episode_reward=-354.35 +/- 21.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 1524000  |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.00057  |
|    ent_coef        | 0.00203  |
|    ent_coef_loss   | -19.4    |
|    learning_rate   | 0.00348  |
|    n_updates       | 7440     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -384     |
| time/              |          |
|    episodes        | 1524     |
|    fps             | 639      |
|    time_elapsed    | 2382     |
|    total_timesteps | 1524000  |
---------------------------------
Eval num_timesteps=1525000, episode_reward=-342.58 +/- 10.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -343     |
| time/              |          |
|    total_timesteps | 1525000  |
---------------------------------
Eval num_timesteps=1526000, episode_reward=-326.12 +/- 12.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -326     |
| time/              |          |
|    total_timesteps | 1526000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.000686 |
|    ent_coef        | 0.00199  |
|    ent_coef_loss   | -18.5    |
|    learning_rate   | 0.00347  |
|    n_updates       | 7450     |
---------------------------------
Eval num_timesteps=1527000, episode_reward=-329.00 +/- 7.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 1527000  |
---------------------------------
Eval num_timesteps=1528000, episode_reward=-388.54 +/- 70.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -389     |
| time/              |          |
|    total_timesteps | 1528000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.000627 |
|    ent_coef        | 0.00195  |
|    ent_coef_loss   | -15.7    |
|    learning_rate   | 0.00347  |
|    n_updates       | 7460     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -391     |
| time/              |          |
|    episodes        | 1528     |
|    fps             | 639      |
|    time_elapsed    | 2389     |
|    total_timesteps | 1528000  |
---------------------------------
Eval num_timesteps=1529000, episode_reward=-386.54 +/- 74.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 1529000  |
---------------------------------
Eval num_timesteps=1530000, episode_reward=-372.60 +/- 9.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 1530000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.000495 |
|    ent_coef        | 0.00191  |
|    ent_coef_loss   | -16      |
|    learning_rate   | 0.00347  |
|    n_updates       | 7470     |
---------------------------------
Eval num_timesteps=1531000, episode_reward=-375.54 +/- 48.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1531000  |
---------------------------------
Eval num_timesteps=1532000, episode_reward=-356.92 +/- 15.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -357     |
| time/              |          |
|    total_timesteps | 1532000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.000439 |
|    ent_coef        | 0.00188  |
|    ent_coef_loss   | -18.1    |
|    learning_rate   | 0.00347  |
|    n_updates       | 7480     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -391     |
| time/              |          |
|    episodes        | 1532     |
|    fps             | 639      |
|    time_elapsed    | 2395     |
|    total_timesteps | 1532000  |
---------------------------------
Eval num_timesteps=1533000, episode_reward=-353.39 +/- 14.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -353     |
| time/              |          |
|    total_timesteps | 1533000  |
---------------------------------
Eval num_timesteps=1534000, episode_reward=-348.19 +/- 71.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -348     |
| time/              |          |
|    total_timesteps | 1534000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000415 |
|    ent_coef        | 0.00184  |
|    ent_coef_loss   | -19.4    |
|    learning_rate   | 0.00347  |
|    n_updates       | 7490     |
---------------------------------
Eval num_timesteps=1535000, episode_reward=-357.34 +/- 78.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -357     |
| time/              |          |
|    total_timesteps | 1535000  |
---------------------------------
Eval num_timesteps=1536000, episode_reward=-360.02 +/- 66.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 1536000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -391     |
| time/              |          |
|    episodes        | 1536     |
|    fps             | 639      |
|    time_elapsed    | 2401     |
|    total_timesteps | 1536000  |
---------------------------------
Eval num_timesteps=1537000, episode_reward=-327.60 +/- 91.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -328     |
| time/              |          |
|    total_timesteps | 1537000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000625 |
|    ent_coef        | 0.00181  |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.00346  |
|    n_updates       | 7500     |
---------------------------------
Eval num_timesteps=1538000, episode_reward=-336.48 +/- 108.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 1538000  |
---------------------------------
Eval num_timesteps=1539000, episode_reward=-77.38 +/- 224.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -77.4    |
| time/              |          |
|    total_timesteps | 1539000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.000839 |
|    ent_coef        | 0.00178  |
|    ent_coef_loss   | -23.3    |
|    learning_rate   | 0.00346  |
|    n_updates       | 7510     |
---------------------------------
Eval num_timesteps=1540000, episode_reward=-82.97 +/- 196.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -83      |
| time/              |          |
|    total_timesteps | 1540000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -389     |
| time/              |          |
|    episodes        | 1540     |
|    fps             | 639      |
|    time_elapsed    | 2407     |
|    total_timesteps | 1540000  |
---------------------------------
Eval num_timesteps=1541000, episode_reward=-401.05 +/- 96.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 1541000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.00174  |
|    ent_coef_loss   | -21.4    |
|    learning_rate   | 0.00346  |
|    n_updates       | 7520     |
---------------------------------
Eval num_timesteps=1542000, episode_reward=-397.22 +/- 109.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -397     |
| time/              |          |
|    total_timesteps | 1542000  |
---------------------------------
Eval num_timesteps=1543000, episode_reward=-382.83 +/- 125.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 1543000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000961 |
|    ent_coef        | 0.0017   |
|    ent_coef_loss   | -18.2    |
|    learning_rate   | 0.00346  |
|    n_updates       | 7530     |
---------------------------------
Eval num_timesteps=1544000, episode_reward=-377.86 +/- 56.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 1544000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -381     |
| time/              |          |
|    episodes        | 1544     |
|    fps             | 639      |
|    time_elapsed    | 2413     |
|    total_timesteps | 1544000  |
---------------------------------
Eval num_timesteps=1545000, episode_reward=-309.39 +/- 99.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 1545000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.00074  |
|    ent_coef        | 0.00166  |
|    ent_coef_loss   | -18.5    |
|    learning_rate   | 0.00346  |
|    n_updates       | 7540     |
---------------------------------
Eval num_timesteps=1546000, episode_reward=-244.96 +/- 44.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 1546000  |
---------------------------------
Eval num_timesteps=1547000, episode_reward=-379.77 +/- 72.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -380     |
| time/              |          |
|    total_timesteps | 1547000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.00101  |
|    ent_coef        | 0.00163  |
|    ent_coef_loss   | -20.9    |
|    learning_rate   | 0.00345  |
|    n_updates       | 7550     |
---------------------------------
Eval num_timesteps=1548000, episode_reward=-376.09 +/- 40.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1548000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -380     |
| time/              |          |
|    episodes        | 1548     |
|    fps             | 639      |
|    time_elapsed    | 2419     |
|    total_timesteps | 1548000  |
---------------------------------
Eval num_timesteps=1549000, episode_reward=-243.49 +/- 20.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -243     |
| time/              |          |
|    total_timesteps | 1549000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000957 |
|    ent_coef        | 0.0016   |
|    ent_coef_loss   | -15.7    |
|    learning_rate   | 0.00345  |
|    n_updates       | 7560     |
---------------------------------
Eval num_timesteps=1550000, episode_reward=-336.77 +/- 145.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -337     |
| time/              |          |
|    total_timesteps | 1550000  |
---------------------------------
Eval num_timesteps=1551000, episode_reward=-387.60 +/- 132.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 1551000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.00114  |
|    ent_coef        | 0.00157  |
|    ent_coef_loss   | -15.8    |
|    learning_rate   | 0.00345  |
|    n_updates       | 7570     |
---------------------------------
Eval num_timesteps=1552000, episode_reward=-558.96 +/- 348.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -559     |
| time/              |          |
|    total_timesteps | 1552000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -372     |
| time/              |          |
|    episodes        | 1552     |
|    fps             | 639      |
|    time_elapsed    | 2426     |
|    total_timesteps | 1552000  |
---------------------------------
Eval num_timesteps=1553000, episode_reward=-641.85 +/- 309.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -642     |
| time/              |          |
|    total_timesteps | 1553000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.000974 |
|    ent_coef        | 0.00154  |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.00345  |
|    n_updates       | 7580     |
---------------------------------
Eval num_timesteps=1554000, episode_reward=-511.20 +/- 275.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -511     |
| time/              |          |
|    total_timesteps | 1554000  |
---------------------------------
Eval num_timesteps=1555000, episode_reward=-286.86 +/- 47.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 1555000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.000657 |
|    ent_coef        | 0.00152  |
|    ent_coef_loss   | -20.3    |
|    learning_rate   | 0.00345  |
|    n_updates       | 7590     |
---------------------------------
Eval num_timesteps=1556000, episode_reward=-381.16 +/- 163.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -381     |
| time/              |          |
|    total_timesteps | 1556000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -370     |
| time/              |          |
|    episodes        | 1556     |
|    fps             | 639      |
|    time_elapsed    | 2432     |
|    total_timesteps | 1556000  |
---------------------------------
Eval num_timesteps=1557000, episode_reward=-479.78 +/- 291.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -480     |
| time/              |          |
|    total_timesteps | 1557000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000677 |
|    ent_coef        | 0.00149  |
|    ent_coef_loss   | -15.9    |
|    learning_rate   | 0.00344  |
|    n_updates       | 7600     |
---------------------------------
Eval num_timesteps=1558000, episode_reward=-401.67 +/- 100.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 1558000  |
---------------------------------
Eval num_timesteps=1559000, episode_reward=-285.80 +/- 17.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -286     |
| time/              |          |
|    total_timesteps | 1559000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000608 |
|    ent_coef        | 0.00146  |
|    ent_coef_loss   | -19      |
|    learning_rate   | 0.00344  |
|    n_updates       | 7610     |
---------------------------------
Eval num_timesteps=1560000, episode_reward=-311.00 +/- 48.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 1560000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -367     |
| time/              |          |
|    episodes        | 1560     |
|    fps             | 639      |
|    time_elapsed    | 2438     |
|    total_timesteps | 1560000  |
---------------------------------
Eval num_timesteps=1561000, episode_reward=-255.40 +/- 77.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 1561000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.000669 |
|    ent_coef        | 0.00143  |
|    ent_coef_loss   | -9.91    |
|    learning_rate   | 0.00344  |
|    n_updates       | 7620     |
---------------------------------
Eval num_timesteps=1562000, episode_reward=-291.13 +/- 68.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -291     |
| time/              |          |
|    total_timesteps | 1562000  |
---------------------------------
Eval num_timesteps=1563000, episode_reward=-316.60 +/- 13.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -317     |
| time/              |          |
|    total_timesteps | 1563000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.00063  |
|    ent_coef        | 0.00141  |
|    ent_coef_loss   | -5.02    |
|    learning_rate   | 0.00344  |
|    n_updates       | 7630     |
---------------------------------
Eval num_timesteps=1564000, episode_reward=-324.81 +/- 17.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -325     |
| time/              |          |
|    total_timesteps | 1564000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -362     |
| time/              |          |
|    episodes        | 1564     |
|    fps             | 639      |
|    time_elapsed    | 2444     |
|    total_timesteps | 1564000  |
---------------------------------
Eval num_timesteps=1565000, episode_reward=-301.44 +/- 41.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -301     |
| time/              |          |
|    total_timesteps | 1565000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.000527 |
|    ent_coef        | 0.0014   |
|    ent_coef_loss   | -11.2    |
|    learning_rate   | 0.00344  |
|    n_updates       | 7640     |
---------------------------------
Eval num_timesteps=1566000, episode_reward=-259.04 +/- 14.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 1566000  |
---------------------------------
Eval num_timesteps=1567000, episode_reward=-17.15 +/- 138.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -17.1    |
| time/              |          |
|    total_timesteps | 1567000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000514 |
|    ent_coef        | 0.00138  |
|    ent_coef_loss   | -13.8    |
|    learning_rate   | 0.00343  |
|    n_updates       | 7650     |
---------------------------------
Eval num_timesteps=1568000, episode_reward=-241.47 +/- 116.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 1568000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -356     |
| time/              |          |
|    episodes        | 1568     |
|    fps             | 639      |
|    time_elapsed    | 2450     |
|    total_timesteps | 1568000  |
---------------------------------
Eval num_timesteps=1569000, episode_reward=-10.30 +/- 49.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -10.3    |
| time/              |          |
|    total_timesteps | 1569000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.000589 |
|    ent_coef        | 0.00136  |
|    ent_coef_loss   | -17.3    |
|    learning_rate   | 0.00343  |
|    n_updates       | 7660     |
---------------------------------
Eval num_timesteps=1570000, episode_reward=-48.52 +/- 43.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -48.5    |
| time/              |          |
|    total_timesteps | 1570000  |
---------------------------------
Eval num_timesteps=1571000, episode_reward=-21.93 +/- 15.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -21.9    |
| time/              |          |
|    total_timesteps | 1571000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000557 |
|    ent_coef        | 0.00134  |
|    ent_coef_loss   | -7       |
|    learning_rate   | 0.00343  |
|    n_updates       | 7670     |
---------------------------------
Eval num_timesteps=1572000, episode_reward=-8.75 +/- 21.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -8.75    |
| time/              |          |
|    total_timesteps | 1572000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -345     |
| time/              |          |
|    episodes        | 1572     |
|    fps             | 639      |
|    time_elapsed    | 2456     |
|    total_timesteps | 1572000  |
---------------------------------
Eval num_timesteps=1573000, episode_reward=-121.40 +/- 7.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 1573000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000448 |
|    ent_coef        | 0.00133  |
|    ent_coef_loss   | -9.62    |
|    learning_rate   | 0.00343  |
|    n_updates       | 7680     |
---------------------------------
Eval num_timesteps=1574000, episode_reward=-119.39 +/- 16.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 1574000  |
---------------------------------
Eval num_timesteps=1575000, episode_reward=14.89 +/- 8.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 14.9     |
| time/              |          |
|    total_timesteps | 1575000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.000367 |
|    ent_coef        | 0.00131  |
|    ent_coef_loss   | -17.8    |
|    learning_rate   | 0.00343  |
|    n_updates       | 7690     |
---------------------------------
Eval num_timesteps=1576000, episode_reward=24.04 +/- 8.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 1576000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -336     |
| time/              |          |
|    episodes        | 1576     |
|    fps             | 639      |
|    time_elapsed    | 2462     |
|    total_timesteps | 1576000  |
---------------------------------
Eval num_timesteps=1577000, episode_reward=187.61 +/- 31.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 188      |
| time/              |          |
|    total_timesteps | 1577000  |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.000333 |
|    ent_coef        | 0.00129  |
|    ent_coef_loss   | -20.1    |
|    learning_rate   | 0.00342  |
|    n_updates       | 7700     |
---------------------------------
Eval num_timesteps=1578000, episode_reward=185.30 +/- 40.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 1578000  |
---------------------------------
Eval num_timesteps=1579000, episode_reward=209.03 +/- 5.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 209      |
| time/              |          |
|    total_timesteps | 1579000  |
---------------------------------
Eval num_timesteps=1580000, episode_reward=-5.03 +/- 13.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -5.03    |
| time/              |          |
|    total_timesteps | 1580000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000366 |
|    ent_coef        | 0.00126  |
|    ent_coef_loss   | -16      |
|    learning_rate   | 0.00342  |
|    n_updates       | 7710     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -323     |
| time/              |          |
|    episodes        | 1580     |
|    fps             | 639      |
|    time_elapsed    | 2468     |
|    total_timesteps | 1580000  |
---------------------------------
Eval num_timesteps=1581000, episode_reward=-7.98 +/- 22.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.98    |
| time/              |          |
|    total_timesteps | 1581000  |
---------------------------------
Eval num_timesteps=1582000, episode_reward=234.99 +/- 10.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 1582000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000439 |
|    ent_coef        | 0.00124  |
|    ent_coef_loss   | -9.16    |
|    learning_rate   | 0.00342  |
|    n_updates       | 7720     |
---------------------------------
Eval num_timesteps=1583000, episode_reward=159.81 +/- 196.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 160      |
| time/              |          |
|    total_timesteps | 1583000  |
---------------------------------
Eval num_timesteps=1584000, episode_reward=-95.81 +/- 10.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -95.8    |
| time/              |          |
|    total_timesteps | 1584000  |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.000398 |
|    ent_coef        | 0.00123  |
|    ent_coef_loss   | -7.57    |
|    learning_rate   | 0.00342  |
|    n_updates       | 7730     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -311     |
| time/              |          |
|    episodes        | 1584     |
|    fps             | 639      |
|    time_elapsed    | 2475     |
|    total_timesteps | 1584000  |
---------------------------------
Eval num_timesteps=1585000, episode_reward=-135.14 +/- 57.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 1585000  |
---------------------------------
Eval num_timesteps=1586000, episode_reward=5.51 +/- 20.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 5.51     |
| time/              |          |
|    total_timesteps | 1586000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.000611 |
|    ent_coef        | 0.00121  |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.00341  |
|    n_updates       | 7740     |
---------------------------------
Eval num_timesteps=1587000, episode_reward=-48.63 +/- 92.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -48.6    |
| time/              |          |
|    total_timesteps | 1587000  |
---------------------------------
Eval num_timesteps=1588000, episode_reward=291.28 +/- 3.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 291      |
| time/              |          |
|    total_timesteps | 1588000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000351 |
|    ent_coef        | 0.0012   |
|    ent_coef_loss   | -14.7    |
|    learning_rate   | 0.00341  |
|    n_updates       | 7750     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -299     |
| time/              |          |
|    episodes        | 1588     |
|    fps             | 640      |
|    time_elapsed    | 2481     |
|    total_timesteps | 1588000  |
---------------------------------
Eval num_timesteps=1589000, episode_reward=284.97 +/- 9.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 1589000  |
---------------------------------
Eval num_timesteps=1590000, episode_reward=277.52 +/- 11.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 278      |
| time/              |          |
|    total_timesteps | 1590000  |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.000324 |
|    ent_coef        | 0.00118  |
|    ent_coef_loss   | -7.73    |
|    learning_rate   | 0.00341  |
|    n_updates       | 7760     |
---------------------------------
Eval num_timesteps=1591000, episode_reward=293.01 +/- 12.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 293      |
| time/              |          |
|    total_timesteps | 1591000  |
---------------------------------
Eval num_timesteps=1592000, episode_reward=381.29 +/- 3.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 381      |
| time/              |          |
|    total_timesteps | 1592000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000487 |
|    ent_coef        | 0.00117  |
|    ent_coef_loss   | -12.1    |
|    learning_rate   | 0.00341  |
|    n_updates       | 7770     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -278     |
| time/              |          |
|    episodes        | 1592     |
|    fps             | 640      |
|    time_elapsed    | 2487     |
|    total_timesteps | 1592000  |
---------------------------------
Eval num_timesteps=1593000, episode_reward=370.73 +/- 7.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 371      |
| time/              |          |
|    total_timesteps | 1593000  |
---------------------------------
Eval num_timesteps=1594000, episode_reward=512.51 +/- 20.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 1594000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000424 |
|    ent_coef        | 0.00115  |
|    ent_coef_loss   | -19.8    |
|    learning_rate   | 0.00341  |
|    n_updates       | 7780     |
---------------------------------
Eval num_timesteps=1595000, episode_reward=512.88 +/- 11.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 1595000  |
---------------------------------
Eval num_timesteps=1596000, episode_reward=256.52 +/- 9.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 257      |
| time/              |          |
|    total_timesteps | 1596000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000622 |
|    ent_coef        | 0.00113  |
|    ent_coef_loss   | -14.9    |
|    learning_rate   | 0.0034   |
|    n_updates       | 7790     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -259     |
| time/              |          |
|    episodes        | 1596     |
|    fps             | 640      |
|    time_elapsed    | 2493     |
|    total_timesteps | 1596000  |
---------------------------------
Eval num_timesteps=1597000, episode_reward=227.73 +/- 21.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 228      |
| time/              |          |
|    total_timesteps | 1597000  |
---------------------------------
Eval num_timesteps=1598000, episode_reward=497.44 +/- 25.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 497      |
| time/              |          |
|    total_timesteps | 1598000  |
| train/             |          |
|    actor_loss      | -1.37    |
|    critic_loss     | 0.000456 |
|    ent_coef        | 0.00111  |
|    ent_coef_loss   | -15.7    |
|    learning_rate   | 0.0034   |
|    n_updates       | 7800     |
---------------------------------
Eval num_timesteps=1599000, episode_reward=286.71 +/- 416.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 287      |
| time/              |          |
|    total_timesteps | 1599000  |
---------------------------------
Eval num_timesteps=1600000, episode_reward=407.60 +/- 16.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 408      |
| time/              |          |
|    total_timesteps | 1600000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000526 |
|    ent_coef        | 0.00109  |
|    ent_coef_loss   | -13.4    |
|    learning_rate   | 0.0034   |
|    n_updates       | 7810     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -236     |
| time/              |          |
|    episodes        | 1600     |
|    fps             | 640      |
|    time_elapsed    | 2499     |
|    total_timesteps | 1600000  |
---------------------------------
Eval num_timesteps=1601000, episode_reward=401.24 +/- 10.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 401      |
| time/              |          |
|    total_timesteps | 1601000  |
---------------------------------
Eval num_timesteps=1602000, episode_reward=362.50 +/- 13.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 1602000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000552 |
|    ent_coef        | 0.00108  |
|    ent_coef_loss   | -18.4    |
|    learning_rate   | 0.0034   |
|    n_updates       | 7820     |
---------------------------------
Eval num_timesteps=1603000, episode_reward=356.17 +/- 12.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 356      |
| time/              |          |
|    total_timesteps | 1603000  |
---------------------------------
Eval num_timesteps=1604000, episode_reward=453.79 +/- 12.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 454      |
| time/              |          |
|    total_timesteps | 1604000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000564 |
|    ent_coef        | 0.00106  |
|    ent_coef_loss   | -12.9    |
|    learning_rate   | 0.0034   |
|    n_updates       | 7830     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -208     |
| time/              |          |
|    episodes        | 1604     |
|    fps             | 640      |
|    time_elapsed    | 2505     |
|    total_timesteps | 1604000  |
---------------------------------
Eval num_timesteps=1605000, episode_reward=452.50 +/- 15.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 452      |
| time/              |          |
|    total_timesteps | 1605000  |
---------------------------------
Eval num_timesteps=1606000, episode_reward=399.75 +/- 43.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 400      |
| time/              |          |
|    total_timesteps | 1606000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.000793 |
|    ent_coef        | 0.00104  |
|    ent_coef_loss   | -21.4    |
|    learning_rate   | 0.00339  |
|    n_updates       | 7840     |
---------------------------------
Eval num_timesteps=1607000, episode_reward=406.55 +/- 22.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 1607000  |
---------------------------------
Eval num_timesteps=1608000, episode_reward=285.08 +/- 24.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 1608000  |
| train/             |          |
|    actor_loss      | -1.39    |
|    critic_loss     | 0.00111  |
|    ent_coef        | 0.00102  |
|    ent_coef_loss   | -16.6    |
|    learning_rate   | 0.00339  |
|    n_updates       | 7850     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -179     |
| time/              |          |
|    episodes        | 1608     |
|    fps             | 640      |
|    time_elapsed    | 2511     |
|    total_timesteps | 1608000  |
---------------------------------
Eval num_timesteps=1609000, episode_reward=301.48 +/- 35.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 301      |
| time/              |          |
|    total_timesteps | 1609000  |
---------------------------------
Eval num_timesteps=1610000, episode_reward=-239.27 +/- 41.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 1610000  |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.00113  |
|    ent_coef        | 0.000997 |
|    ent_coef_loss   | -6.69    |
|    learning_rate   | 0.00339  |
|    n_updates       | 7860     |
---------------------------------
Eval num_timesteps=1611000, episode_reward=-277.57 +/- 13.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 1611000  |
---------------------------------
Eval num_timesteps=1612000, episode_reward=-22.80 +/- 30.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.8    |
| time/              |          |
|    total_timesteps | 1612000  |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 0.00212  |
|    ent_coef        | 0.000985 |
|    ent_coef_loss   | -7.37    |
|    learning_rate   | 0.00339  |
|    n_updates       | 7870     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -161     |
| time/              |          |
|    episodes        | 1612     |
|    fps             | 640      |
|    time_elapsed    | 2517     |
|    total_timesteps | 1612000  |
---------------------------------
Eval num_timesteps=1613000, episode_reward=-36.22 +/- 46.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -36.2    |
| time/              |          |
|    total_timesteps | 1613000  |
---------------------------------
Eval num_timesteps=1614000, episode_reward=-328.44 +/- 8.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -328     |
| time/              |          |
|    total_timesteps | 1614000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.00203  |
|    ent_coef        | 0.000978 |
|    ent_coef_loss   | -0.269   |
|    learning_rate   | 0.00339  |
|    n_updates       | 7880     |
---------------------------------
Eval num_timesteps=1615000, episode_reward=-315.48 +/- 14.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 1615000  |
---------------------------------
Eval num_timesteps=1616000, episode_reward=-317.08 +/- 4.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -317     |
| time/              |          |
|    total_timesteps | 1616000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.00158  |
|    ent_coef        | 0.000973 |
|    ent_coef_loss   | -14.8    |
|    learning_rate   | 0.00338  |
|    n_updates       | 7890     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -156     |
| time/              |          |
|    episodes        | 1616     |
|    fps             | 640      |
|    time_elapsed    | 2523     |
|    total_timesteps | 1616000  |
---------------------------------
Eval num_timesteps=1617000, episode_reward=-321.54 +/- 2.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -322     |
| time/              |          |
|    total_timesteps | 1617000  |
---------------------------------
Eval num_timesteps=1618000, episode_reward=-371.23 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 1618000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.000603 |
|    ent_coef        | 0.000963 |
|    ent_coef_loss   | 0.823    |
|    learning_rate   | 0.00338  |
|    n_updates       | 7900     |
---------------------------------
Eval num_timesteps=1619000, episode_reward=-371.54 +/- 1.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -372     |
| time/              |          |
|    total_timesteps | 1619000  |
---------------------------------
Eval num_timesteps=1620000, episode_reward=-369.01 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 1620000  |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 0.000613 |
|    ent_coef        | 0.000958 |
|    ent_coef_loss   | -6.21    |
|    learning_rate   | 0.00338  |
|    n_updates       | 7910     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -146     |
| time/              |          |
|    episodes        | 1620     |
|    fps             | 640      |
|    time_elapsed    | 2529     |
|    total_timesteps | 1620000  |
---------------------------------
Eval num_timesteps=1621000, episode_reward=-368.88 +/- 0.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 1621000  |
---------------------------------
Eval num_timesteps=1622000, episode_reward=-368.64 +/- 0.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 1622000  |
---------------------------------
Eval num_timesteps=1623000, episode_reward=-364.68 +/- 0.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -365     |
| time/              |          |
|    total_timesteps | 1623000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.00135  |
|    ent_coef        | 0.000955 |
|    ent_coef_loss   | -2.25    |
|    learning_rate   | 0.00338  |
|    n_updates       | 7920     |
---------------------------------
Eval num_timesteps=1624000, episode_reward=-364.45 +/- 0.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 1624000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -136     |
| time/              |          |
|    episodes        | 1624     |
|    fps             | 640      |
|    time_elapsed    | 2535     |
|    total_timesteps | 1624000  |
---------------------------------
Eval num_timesteps=1625000, episode_reward=-189.56 +/- 17.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1625000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.00137  |
|    ent_coef        | 0.000949 |
|    ent_coef_loss   | -10.5    |
|    learning_rate   | 0.00338  |
|    n_updates       | 7930     |
---------------------------------
Eval num_timesteps=1626000, episode_reward=-196.66 +/- 44.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 1626000  |
---------------------------------
Eval num_timesteps=1627000, episode_reward=-158.26 +/- 20.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 1627000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.0013   |
|    ent_coef        | 0.000941 |
|    ent_coef_loss   | -3.95    |
|    learning_rate   | 0.00337  |
|    n_updates       | 7940     |
---------------------------------
Eval num_timesteps=1628000, episode_reward=-155.13 +/- 19.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 1628000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -125     |
| time/              |          |
|    episodes        | 1628     |
|    fps             | 640      |
|    time_elapsed    | 2541     |
|    total_timesteps | 1628000  |
---------------------------------
Eval num_timesteps=1629000, episode_reward=-319.04 +/- 126.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -319     |
| time/              |          |
|    total_timesteps | 1629000  |
| train/             |          |
|    actor_loss      | -1.31    |
|    critic_loss     | 0.000808 |
|    ent_coef        | 0.000934 |
|    ent_coef_loss   | -7.36    |
|    learning_rate   | 0.00337  |
|    n_updates       | 7950     |
---------------------------------
Eval num_timesteps=1630000, episode_reward=-381.30 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -381     |
| time/              |          |
|    total_timesteps | 1630000  |
---------------------------------
Eval num_timesteps=1631000, episode_reward=-382.42 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 1631000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000579 |
|    ent_coef        | 0.000925 |
|    ent_coef_loss   | -18.5    |
|    learning_rate   | 0.00337  |
|    n_updates       | 7960     |
---------------------------------
Eval num_timesteps=1632000, episode_reward=-382.69 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 1632000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 1632     |
|    fps             | 640      |
|    time_elapsed    | 2548     |
|    total_timesteps | 1632000  |
---------------------------------
Eval num_timesteps=1633000, episode_reward=-76.79 +/- 238.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -76.8    |
| time/              |          |
|    total_timesteps | 1633000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.000599 |
|    ent_coef        | 0.000911 |
|    ent_coef_loss   | -18.2    |
|    learning_rate   | 0.00337  |
|    n_updates       | 7970     |
---------------------------------
Eval num_timesteps=1634000, episode_reward=-176.94 +/- 234.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 1634000  |
---------------------------------
Eval num_timesteps=1635000, episode_reward=-293.11 +/- 162.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 1635000  |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 0.000652 |
|    ent_coef        | 0.000893 |
|    ent_coef_loss   | -13.8    |
|    learning_rate   | 0.00337  |
|    n_updates       | 7980     |
---------------------------------
Eval num_timesteps=1636000, episode_reward=-190.23 +/- 224.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -190     |
| time/              |          |
|    total_timesteps | 1636000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -89.9    |
| time/              |          |
|    episodes        | 1636     |
|    fps             | 640      |
|    time_elapsed    | 2554     |
|    total_timesteps | 1636000  |
---------------------------------
Eval num_timesteps=1637000, episode_reward=-305.53 +/- 121.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 1637000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000651 |
|    ent_coef        | 0.000879 |
|    ent_coef_loss   | -5.73    |
|    learning_rate   | 0.00336  |
|    n_updates       | 7990     |
---------------------------------
Eval num_timesteps=1638000, episode_reward=-230.97 +/- 131.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 1638000  |
---------------------------------
Eval num_timesteps=1639000, episode_reward=-383.35 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 1639000  |
| train/             |          |
|    actor_loss      | -1.32    |
|    critic_loss     | 0.000301 |
|    ent_coef        | 0.00087  |
|    ent_coef_loss   | -0.719   |
|    learning_rate   | 0.00336  |
|    n_updates       | 8000     |
---------------------------------
Eval num_timesteps=1640000, episode_reward=-382.81 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 1640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -77.1    |
| time/              |          |
|    episodes        | 1640     |
|    fps             | 640      |
|    time_elapsed    | 2560     |
|    total_timesteps | 1640000  |
---------------------------------
Eval num_timesteps=1641000, episode_reward=-401.92 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 1641000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000482 |
|    ent_coef        | 0.000867 |
|    ent_coef_loss   | -2.03    |
|    learning_rate   | 0.00336  |
|    n_updates       | 8010     |
---------------------------------
Eval num_timesteps=1642000, episode_reward=-402.50 +/- 1.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 1642000  |
---------------------------------
Eval num_timesteps=1643000, episode_reward=-455.16 +/- 0.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -455     |
| time/              |          |
|    total_timesteps | 1643000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.00047  |
|    ent_coef        | 0.000864 |
|    ent_coef_loss   | -3.93    |
|    learning_rate   | 0.00336  |
|    n_updates       | 8020     |
---------------------------------
Eval num_timesteps=1644000, episode_reward=-454.19 +/- 1.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -454     |
| time/              |          |
|    total_timesteps | 1644000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -69.3    |
| time/              |          |
|    episodes        | 1644     |
|    fps             | 640      |
|    time_elapsed    | 2566     |
|    total_timesteps | 1644000  |
---------------------------------
Eval num_timesteps=1645000, episode_reward=-424.23 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 1645000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000444 |
|    ent_coef        | 0.000861 |
|    ent_coef_loss   | -5.3     |
|    learning_rate   | 0.00336  |
|    n_updates       | 8030     |
---------------------------------
Eval num_timesteps=1646000, episode_reward=-424.45 +/- 1.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 1646000  |
---------------------------------
Eval num_timesteps=1647000, episode_reward=-52.36 +/- 201.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -52.4    |
| time/              |          |
|    total_timesteps | 1647000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.000851 |
|    ent_coef        | 0.000856 |
|    ent_coef_loss   | -6.66    |
|    learning_rate   | 0.00335  |
|    n_updates       | 8040     |
---------------------------------
Eval num_timesteps=1648000, episode_reward=191.71 +/- 11.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 192      |
| time/              |          |
|    total_timesteps | 1648000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -51.9    |
| time/              |          |
|    episodes        | 1648     |
|    fps             | 640      |
|    time_elapsed    | 2572     |
|    total_timesteps | 1648000  |
---------------------------------
Eval num_timesteps=1649000, episode_reward=207.18 +/- 357.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 207      |
| time/              |          |
|    total_timesteps | 1649000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000687 |
|    ent_coef        | 0.000848 |
|    ent_coef_loss   | -23.1    |
|    learning_rate   | 0.00335  |
|    n_updates       | 8050     |
---------------------------------
Eval num_timesteps=1650000, episode_reward=216.10 +/- 363.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 216      |
| time/              |          |
|    total_timesteps | 1650000  |
---------------------------------
Eval num_timesteps=1651000, episode_reward=-77.67 +/- 320.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -77.7    |
| time/              |          |
|    total_timesteps | 1651000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000645 |
|    ent_coef        | 0.000832 |
|    ent_coef_loss   | -20.4    |
|    learning_rate   | 0.00335  |
|    n_updates       | 8060     |
---------------------------------
Eval num_timesteps=1652000, episode_reward=328.69 +/- 9.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 329      |
| time/              |          |
|    total_timesteps | 1652000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -41.1    |
| time/              |          |
|    episodes        | 1652     |
|    fps             | 640      |
|    time_elapsed    | 2579     |
|    total_timesteps | 1652000  |
---------------------------------
Eval num_timesteps=1653000, episode_reward=497.98 +/- 35.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 498      |
| time/              |          |
|    total_timesteps | 1653000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000539 |
|    ent_coef        | 0.000812 |
|    ent_coef_loss   | -22.3    |
|    learning_rate   | 0.00335  |
|    n_updates       | 8070     |
---------------------------------
Eval num_timesteps=1654000, episode_reward=338.53 +/- 340.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 339      |
| time/              |          |
|    total_timesteps | 1654000  |
---------------------------------
Eval num_timesteps=1655000, episode_reward=436.73 +/- 10.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 1655000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.00082  |
|    ent_coef        | 0.000793 |
|    ent_coef_loss   | -15.5    |
|    learning_rate   | 0.00335  |
|    n_updates       | 8080     |
---------------------------------
Eval num_timesteps=1656000, episode_reward=426.46 +/- 17.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 1656000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -22      |
| time/              |          |
|    episodes        | 1656     |
|    fps             | 640      |
|    time_elapsed    | 2585     |
|    total_timesteps | 1656000  |
---------------------------------
Eval num_timesteps=1657000, episode_reward=465.35 +/- 25.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 465      |
| time/              |          |
|    total_timesteps | 1657000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000756 |
|    ent_coef        | 0.000776 |
|    ent_coef_loss   | -21      |
|    learning_rate   | 0.00334  |
|    n_updates       | 8090     |
---------------------------------
Eval num_timesteps=1658000, episode_reward=481.11 +/- 20.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 481      |
| time/              |          |
|    total_timesteps | 1658000  |
---------------------------------
Eval num_timesteps=1659000, episode_reward=210.95 +/- 476.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 211      |
| time/              |          |
|    total_timesteps | 1659000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.000951 |
|    ent_coef        | 0.000759 |
|    ent_coef_loss   | -20.5    |
|    learning_rate   | 0.00334  |
|    n_updates       | 8100     |
---------------------------------
Eval num_timesteps=1660000, episode_reward=208.30 +/- 476.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 208      |
| time/              |          |
|    total_timesteps | 1660000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 5.8      |
| time/              |          |
|    episodes        | 1660     |
|    fps             | 640      |
|    time_elapsed    | 2591     |
|    total_timesteps | 1660000  |
---------------------------------
Eval num_timesteps=1661000, episode_reward=-87.33 +/- 513.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -87.3    |
| time/              |          |
|    total_timesteps | 1661000  |
| train/             |          |
|    actor_loss      | -1.35    |
|    critic_loss     | 0.000753 |
|    ent_coef        | 0.00074  |
|    ent_coef_loss   | -27.9    |
|    learning_rate   | 0.00334  |
|    n_updates       | 8110     |
---------------------------------
Eval num_timesteps=1662000, episode_reward=-93.61 +/- 490.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -93.6    |
| time/              |          |
|    total_timesteps | 1662000  |
---------------------------------
Eval num_timesteps=1663000, episode_reward=-310.97 +/- 292.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 1663000  |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.000687 |
|    ent_coef        | 0.00072  |
|    ent_coef_loss   | -9.55    |
|    learning_rate   | 0.00334  |
|    n_updates       | 8120     |
---------------------------------
Eval num_timesteps=1664000, episode_reward=-127.55 +/- 403.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 1664000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 30.4     |
| time/              |          |
|    episodes        | 1664     |
|    fps             | 640      |
|    time_elapsed    | 2597     |
|    total_timesteps | 1664000  |
---------------------------------
Eval num_timesteps=1665000, episode_reward=-152.41 +/- 372.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 1665000  |
---------------------------------
Eval num_timesteps=1666000, episode_reward=-240.40 +/- 321.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -240     |
| time/              |          |
|    total_timesteps | 1666000  |
| train/             |          |
|    actor_loss      | -1.36    |
|    critic_loss     | 0.00139  |
|    ent_coef        | 0.000708 |
|    ent_coef_loss   | -6.8     |
|    learning_rate   | 0.00333  |
|    n_updates       | 8130     |
---------------------------------
Eval num_timesteps=1667000, episode_reward=69.68 +/- 387.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 69.7     |
| time/              |          |
|    total_timesteps | 1667000  |
---------------------------------
Eval num_timesteps=1668000, episode_reward=201.59 +/- 350.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 202      |
| time/              |          |
|    total_timesteps | 1668000  |
| train/             |          |
|    actor_loss      | -1.31    |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.000699 |
|    ent_coef_loss   | -18.5    |
|    learning_rate   | 0.00333  |
|    n_updates       | 8140     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 49.1     |
| time/              |          |
|    episodes        | 1668     |
|    fps             | 640      |
|    time_elapsed    | 2603     |
|    total_timesteps | 1668000  |
---------------------------------
Eval num_timesteps=1669000, episode_reward=71.07 +/- 368.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 71.1     |
| time/              |          |
|    total_timesteps | 1669000  |
---------------------------------
Eval num_timesteps=1670000, episode_reward=-153.35 +/- 267.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 1670000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.000789 |
|    ent_coef        | 0.000685 |
|    ent_coef_loss   | -29.5    |
|    learning_rate   | 0.00333  |
|    n_updates       | 8150     |
---------------------------------
Eval num_timesteps=1671000, episode_reward=-11.95 +/- 335.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -12      |
| time/              |          |
|    total_timesteps | 1671000  |
---------------------------------
Eval num_timesteps=1672000, episode_reward=-154.54 +/- 213.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 1672000  |
| train/             |          |
|    actor_loss      | -1.34    |
|    critic_loss     | 0.000753 |
|    ent_coef        | 0.000666 |
|    ent_coef_loss   | -18.1    |
|    learning_rate   | 0.00333  |
|    n_updates       | 8160     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 61.9     |
| time/              |          |
|    episodes        | 1672     |
|    fps             | 640      |
|    time_elapsed    | 2609     |
|    total_timesteps | 1672000  |
---------------------------------
Eval num_timesteps=1673000, episode_reward=-262.59 +/- 6.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 1673000  |
---------------------------------
Eval num_timesteps=1674000, episode_reward=-28.67 +/- 304.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -28.7    |
| time/              |          |
|    total_timesteps | 1674000  |
| train/             |          |
|    actor_loss      | -1.33    |
|    critic_loss     | 0.00102  |
|    ent_coef        | 0.000652 |
|    ent_coef_loss   | -14.4    |
|    learning_rate   | 0.00333  |
|    n_updates       | 8170     |
---------------------------------
Eval num_timesteps=1675000, episode_reward=-167.11 +/- 243.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 1675000  |
---------------------------------
Eval num_timesteps=1676000, episode_reward=32.38 +/- 250.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 32.4     |
| time/              |          |
|    total_timesteps | 1676000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.00059  |
|    ent_coef        | 0.000638 |
|    ent_coef_loss   | -21      |
|    learning_rate   | 0.00332  |
|    n_updates       | 8180     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 60.8     |
| time/              |          |
|    episodes        | 1676     |
|    fps             | 640      |
|    time_elapsed    | 2615     |
|    total_timesteps | 1676000  |
---------------------------------
Eval num_timesteps=1677000, episode_reward=-177.18 +/- 7.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 1677000  |
---------------------------------
Eval num_timesteps=1678000, episode_reward=77.30 +/- 178.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 77.3     |
| time/              |          |
|    total_timesteps | 1678000  |
| train/             |          |
|    actor_loss      | -1.2     |
|    critic_loss     | 0.000566 |
|    ent_coef        | 0.000625 |
|    ent_coef_loss   | -6.05    |
|    learning_rate   | 0.00332  |
|    n_updates       | 8190     |
---------------------------------
Eval num_timesteps=1679000, episode_reward=147.91 +/- 146.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 148      |
| time/              |          |
|    total_timesteps | 1679000  |
---------------------------------
Eval num_timesteps=1680000, episode_reward=270.99 +/- 38.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 1680000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.000502 |
|    ent_coef        | 0.000616 |
|    ent_coef_loss   | -18.3    |
|    learning_rate   | 0.00332  |
|    n_updates       | 8200     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 58       |
| time/              |          |
|    episodes        | 1680     |
|    fps             | 640      |
|    time_elapsed    | 2622     |
|    total_timesteps | 1680000  |
---------------------------------
Eval num_timesteps=1681000, episode_reward=131.06 +/- 269.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 131      |
| time/              |          |
|    total_timesteps | 1681000  |
---------------------------------
Eval num_timesteps=1682000, episode_reward=-68.36 +/- 4.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -68.4    |
| time/              |          |
|    total_timesteps | 1682000  |
| train/             |          |
|    actor_loss      | -1.38    |
|    critic_loss     | 0.00725  |
|    ent_coef        | 0.000606 |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.00332  |
|    n_updates       | 8210     |
---------------------------------
Eval num_timesteps=1683000, episode_reward=-78.83 +/- 10.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -78.8    |
| time/              |          |
|    total_timesteps | 1683000  |
---------------------------------
Eval num_timesteps=1684000, episode_reward=-303.62 +/- 411.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 1684000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.00175  |
|    ent_coef        | 0.0006   |
|    ent_coef_loss   | -11.3    |
|    learning_rate   | 0.00332  |
|    n_updates       | 8220     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 54.6     |
| time/              |          |
|    episodes        | 1684     |
|    fps             | 640      |
|    time_elapsed    | 2628     |
|    total_timesteps | 1684000  |
---------------------------------
Eval num_timesteps=1685000, episode_reward=-126.36 +/- 274.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 1685000  |
---------------------------------
Eval num_timesteps=1686000, episode_reward=-125.75 +/- 412.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 1686000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.00237  |
|    ent_coef        | 0.000596 |
|    ent_coef_loss   | 0.158    |
|    learning_rate   | 0.00331  |
|    n_updates       | 8230     |
---------------------------------
Eval num_timesteps=1687000, episode_reward=7.72 +/- 364.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 7.72     |
| time/              |          |
|    total_timesteps | 1687000  |
---------------------------------
Eval num_timesteps=1688000, episode_reward=-216.44 +/- 43.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -216     |
| time/              |          |
|    total_timesteps | 1688000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.00123  |
|    ent_coef        | 0.000595 |
|    ent_coef_loss   | 19.1     |
|    learning_rate   | 0.00331  |
|    n_updates       | 8240     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 53.4     |
| time/              |          |
|    episodes        | 1688     |
|    fps             | 640      |
|    time_elapsed    | 2634     |
|    total_timesteps | 1688000  |
---------------------------------
Eval num_timesteps=1689000, episode_reward=-219.15 +/- 38.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 1689000  |
---------------------------------
Eval num_timesteps=1690000, episode_reward=-192.48 +/- 35.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 1690000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.00116  |
|    ent_coef        | 0.000601 |
|    ent_coef_loss   | -3.51    |
|    learning_rate   | 0.00331  |
|    n_updates       | 8250     |
---------------------------------
Eval num_timesteps=1691000, episode_reward=-182.80 +/- 31.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 1691000  |
---------------------------------
Eval num_timesteps=1692000, episode_reward=-295.07 +/- 10.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 1692000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000992 |
|    ent_coef        | 0.000602 |
|    ent_coef_loss   | -16.5    |
|    learning_rate   | 0.00331  |
|    n_updates       | 8260     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 36.5     |
| time/              |          |
|    episodes        | 1692     |
|    fps             | 640      |
|    time_elapsed    | 2640     |
|    total_timesteps | 1692000  |
---------------------------------
Eval num_timesteps=1693000, episode_reward=-293.26 +/- 19.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 1693000  |
---------------------------------
Eval num_timesteps=1694000, episode_reward=-435.55 +/- 1.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 1694000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.0016   |
|    ent_coef        | 0.000597 |
|    ent_coef_loss   | 0.168    |
|    learning_rate   | 0.00331  |
|    n_updates       | 8270     |
---------------------------------
Eval num_timesteps=1695000, episode_reward=-435.80 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -436     |
| time/              |          |
|    total_timesteps | 1695000  |
---------------------------------
Eval num_timesteps=1696000, episode_reward=-201.24 +/- 109.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 1696000  |
| train/             |          |
|    actor_loss      | -1.2     |
|    critic_loss     | 0.000666 |
|    ent_coef        | 0.000595 |
|    ent_coef_loss   | 1.6      |
|    learning_rate   | 0.0033   |
|    n_updates       | 8280     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 24.7     |
| time/              |          |
|    episodes        | 1696     |
|    fps             | 640      |
|    time_elapsed    | 2646     |
|    total_timesteps | 1696000  |
---------------------------------
Eval num_timesteps=1697000, episode_reward=-224.74 +/- 119.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 1697000  |
---------------------------------
Eval num_timesteps=1698000, episode_reward=-181.48 +/- 177.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -181     |
| time/              |          |
|    total_timesteps | 1698000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.00173  |
|    ent_coef        | 0.000593 |
|    ent_coef_loss   | -7.02    |
|    learning_rate   | 0.0033   |
|    n_updates       | 8290     |
---------------------------------
Eval num_timesteps=1699000, episode_reward=-128.07 +/- 148.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 1699000  |
---------------------------------
Eval num_timesteps=1700000, episode_reward=-421.00 +/- 42.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 1700000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.00112  |
|    ent_coef        | 0.000592 |
|    ent_coef_loss   | 4.73     |
|    learning_rate   | 0.0033   |
|    n_updates       | 8300     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 9.86     |
| time/              |          |
|    episodes        | 1700     |
|    fps             | 640      |
|    time_elapsed    | 2652     |
|    total_timesteps | 1700000  |
---------------------------------
Eval num_timesteps=1701000, episode_reward=-456.40 +/- 31.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -456     |
| time/              |          |
|    total_timesteps | 1701000  |
---------------------------------
Eval num_timesteps=1702000, episode_reward=103.20 +/- 72.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 103      |
| time/              |          |
|    total_timesteps | 1702000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.00086  |
|    ent_coef        | 0.000593 |
|    ent_coef_loss   | 10.4     |
|    learning_rate   | 0.0033   |
|    n_updates       | 8310     |
---------------------------------
Eval num_timesteps=1703000, episode_reward=-110.73 +/- 280.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 1703000  |
---------------------------------
Eval num_timesteps=1704000, episode_reward=-344.39 +/- 120.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 1704000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.00122  |
|    ent_coef        | 0.000599 |
|    ent_coef_loss   | 12.4     |
|    learning_rate   | 0.0033   |
|    n_updates       | 8320     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -9.51    |
| time/              |          |
|    episodes        | 1704     |
|    fps             | 640      |
|    time_elapsed    | 2658     |
|    total_timesteps | 1704000  |
---------------------------------
Eval num_timesteps=1705000, episode_reward=-277.87 +/- 99.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 1705000  |
---------------------------------
Eval num_timesteps=1706000, episode_reward=-195.67 +/- 327.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -196     |
| time/              |          |
|    total_timesteps | 1706000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.000832 |
|    ent_coef        | 0.000605 |
|    ent_coef_loss   | -8.35    |
|    learning_rate   | 0.00329  |
|    n_updates       | 8330     |
---------------------------------
Eval num_timesteps=1707000, episode_reward=-191.44 +/- 332.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 1707000  |
---------------------------------
Eval num_timesteps=1708000, episode_reward=-464.83 +/- 2.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -465     |
| time/              |          |
|    total_timesteps | 1708000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -25.9    |
| time/              |          |
|    episodes        | 1708     |
|    fps             | 640      |
|    time_elapsed    | 2664     |
|    total_timesteps | 1708000  |
---------------------------------
Eval num_timesteps=1709000, episode_reward=-311.43 +/- 244.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 1709000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000653 |
|    ent_coef        | 0.000603 |
|    ent_coef_loss   | -11.6    |
|    learning_rate   | 0.00329  |
|    n_updates       | 8340     |
---------------------------------
Eval num_timesteps=1710000, episode_reward=-301.65 +/- 287.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 1710000  |
---------------------------------
Eval num_timesteps=1711000, episode_reward=-245.06 +/- 236.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 1711000  |
| train/             |          |
|    actor_loss      | -1.18    |
|    critic_loss     | 0.000543 |
|    ent_coef        | 0.000598 |
|    ent_coef_loss   | -6.45    |
|    learning_rate   | 0.00329  |
|    n_updates       | 8350     |
---------------------------------
Eval num_timesteps=1712000, episode_reward=-364.81 +/- 6.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -365     |
| time/              |          |
|    total_timesteps | 1712000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -33.6    |
| time/              |          |
|    episodes        | 1712     |
|    fps             | 640      |
|    time_elapsed    | 2671     |
|    total_timesteps | 1712000  |
---------------------------------
Eval num_timesteps=1713000, episode_reward=-98.34 +/- 277.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -98.3    |
| time/              |          |
|    total_timesteps | 1713000  |
| train/             |          |
|    actor_loss      | -1.19    |
|    critic_loss     | 0.00076  |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | -24      |
|    learning_rate   | 0.00329  |
|    n_updates       | 8360     |
---------------------------------
Eval num_timesteps=1714000, episode_reward=-109.30 +/- 264.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 1714000  |
---------------------------------
Eval num_timesteps=1715000, episode_reward=-75.76 +/- 297.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -75.8    |
| time/              |          |
|    total_timesteps | 1715000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.000748 |
|    ent_coef        | 0.000579 |
|    ent_coef_loss   | -13.3    |
|    learning_rate   | 0.00329  |
|    n_updates       | 8370     |
---------------------------------
Eval num_timesteps=1716000, episode_reward=295.68 +/- 134.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 296      |
| time/              |          |
|    total_timesteps | 1716000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -27.3    |
| time/              |          |
|    episodes        | 1716     |
|    fps             | 640      |
|    time_elapsed    | 2677     |
|    total_timesteps | 1716000  |
---------------------------------
Eval num_timesteps=1717000, episode_reward=-231.23 +/- 177.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 1717000  |
| train/             |          |
|    actor_loss      | -1.17    |
|    critic_loss     | 0.000547 |
|    ent_coef        | 0.000569 |
|    ent_coef_loss   | -9.84    |
|    learning_rate   | 0.00328  |
|    n_updates       | 8380     |
---------------------------------
Eval num_timesteps=1718000, episode_reward=82.14 +/- 205.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 82.1     |
| time/              |          |
|    total_timesteps | 1718000  |
---------------------------------
Eval num_timesteps=1719000, episode_reward=-182.25 +/- 321.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -182     |
| time/              |          |
|    total_timesteps | 1719000  |
| train/             |          |
|    actor_loss      | -1.19    |
|    critic_loss     | 0.000563 |
|    ent_coef        | 0.00056  |
|    ent_coef_loss   | -8.27    |
|    learning_rate   | 0.00328  |
|    n_updates       | 8390     |
---------------------------------
Eval num_timesteps=1720000, episode_reward=-495.78 +/- 465.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -496     |
| time/              |          |
|    total_timesteps | 1720000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -21.4    |
| time/              |          |
|    episodes        | 1720     |
|    fps             | 640      |
|    time_elapsed    | 2683     |
|    total_timesteps | 1720000  |
---------------------------------
Eval num_timesteps=1721000, episode_reward=-62.35 +/- 370.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -62.4    |
| time/              |          |
|    total_timesteps | 1721000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000744 |
|    ent_coef        | 0.000555 |
|    ent_coef_loss   | 1.33     |
|    learning_rate   | 0.00328  |
|    n_updates       | 8400     |
---------------------------------
Eval num_timesteps=1722000, episode_reward=40.05 +/- 335.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 40.1     |
| time/              |          |
|    total_timesteps | 1722000  |
---------------------------------
Eval num_timesteps=1723000, episode_reward=347.11 +/- 347.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 347      |
| time/              |          |
|    total_timesteps | 1723000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000971 |
|    ent_coef        | 0.000554 |
|    ent_coef_loss   | 0.248    |
|    learning_rate   | 0.00328  |
|    n_updates       | 8410     |
---------------------------------
Eval num_timesteps=1724000, episode_reward=506.89 +/- 55.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 507      |
| time/              |          |
|    total_timesteps | 1724000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -1.18    |
| time/              |          |
|    episodes        | 1724     |
|    fps             | 641      |
|    time_elapsed    | 2689     |
|    total_timesteps | 1724000  |
---------------------------------
Eval num_timesteps=1725000, episode_reward=465.88 +/- 59.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 1725000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.00099  |
|    ent_coef        | 0.000553 |
|    ent_coef_loss   | -6.64    |
|    learning_rate   | 0.00328  |
|    n_updates       | 8420     |
---------------------------------
Eval num_timesteps=1726000, episode_reward=288.20 +/- 326.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 288      |
| time/              |          |
|    total_timesteps | 1726000  |
---------------------------------
Eval num_timesteps=1727000, episode_reward=180.60 +/- 294.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 181      |
| time/              |          |
|    total_timesteps | 1727000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.000915 |
|    ent_coef        | 0.00055  |
|    ent_coef_loss   | -2.57    |
|    learning_rate   | 0.00327  |
|    n_updates       | 8430     |
---------------------------------
Eval num_timesteps=1728000, episode_reward=308.46 +/- 34.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 1728000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 11.9     |
| time/              |          |
|    episodes        | 1728     |
|    fps             | 641      |
|    time_elapsed    | 2695     |
|    total_timesteps | 1728000  |
---------------------------------
Eval num_timesteps=1729000, episode_reward=219.00 +/- 270.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 219      |
| time/              |          |
|    total_timesteps | 1729000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.000786 |
|    ent_coef        | 0.000548 |
|    ent_coef_loss   | 0.17     |
|    learning_rate   | 0.00327  |
|    n_updates       | 8440     |
---------------------------------
Eval num_timesteps=1730000, episode_reward=89.43 +/- 225.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 89.4     |
| time/              |          |
|    total_timesteps | 1730000  |
---------------------------------
Eval num_timesteps=1731000, episode_reward=517.95 +/- 11.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 1731000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.000905 |
|    ent_coef        | 0.000547 |
|    ent_coef_loss   | -3.3     |
|    learning_rate   | 0.00327  |
|    n_updates       | 8450     |
---------------------------------
Eval num_timesteps=1732000, episode_reward=519.33 +/- 21.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 519      |
| time/              |          |
|    total_timesteps | 1732000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 29.1     |
| time/              |          |
|    episodes        | 1732     |
|    fps             | 641      |
|    time_elapsed    | 2701     |
|    total_timesteps | 1732000  |
---------------------------------
Eval num_timesteps=1733000, episode_reward=390.66 +/- 18.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 391      |
| time/              |          |
|    total_timesteps | 1733000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.000731 |
|    ent_coef        | 0.000544 |
|    ent_coef_loss   | -10.4    |
|    learning_rate   | 0.00327  |
|    n_updates       | 8460     |
---------------------------------
Eval num_timesteps=1734000, episode_reward=375.63 +/- 25.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 376      |
| time/              |          |
|    total_timesteps | 1734000  |
---------------------------------
Eval num_timesteps=1735000, episode_reward=300.72 +/- 35.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 301      |
| time/              |          |
|    total_timesteps | 1735000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.000683 |
|    ent_coef        | 0.000539 |
|    ent_coef_loss   | -14.1    |
|    learning_rate   | 0.00327  |
|    n_updates       | 8470     |
---------------------------------
Eval num_timesteps=1736000, episode_reward=293.95 +/- 13.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 1736000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 23.6     |
| time/              |          |
|    episodes        | 1736     |
|    fps             | 641      |
|    time_elapsed    | 2707     |
|    total_timesteps | 1736000  |
---------------------------------
Eval num_timesteps=1737000, episode_reward=541.49 +/- 34.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 541      |
| time/              |          |
|    total_timesteps | 1737000  |
| train/             |          |
|    actor_loss      | -1.18    |
|    critic_loss     | 0.000493 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | -5       |
|    learning_rate   | 0.00326  |
|    n_updates       | 8480     |
---------------------------------
Eval num_timesteps=1738000, episode_reward=546.48 +/- 37.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 546      |
| time/              |          |
|    total_timesteps | 1738000  |
---------------------------------
Eval num_timesteps=1739000, episode_reward=598.82 +/- 26.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 599      |
| time/              |          |
|    total_timesteps | 1739000  |
| train/             |          |
|    actor_loss      | -1.19    |
|    critic_loss     | 0.00058  |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | -1.75    |
|    learning_rate   | 0.00326  |
|    n_updates       | 8490     |
---------------------------------
Eval num_timesteps=1740000, episode_reward=552.89 +/- 22.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 553      |
| time/              |          |
|    total_timesteps | 1740000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 33.8     |
| time/              |          |
|    episodes        | 1740     |
|    fps             | 641      |
|    time_elapsed    | 2713     |
|    total_timesteps | 1740000  |
---------------------------------
Eval num_timesteps=1741000, episode_reward=320.77 +/- 361.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 321      |
| time/              |          |
|    total_timesteps | 1741000  |
| train/             |          |
|    actor_loss      | -1.22    |
|    critic_loss     | 0.000681 |
|    ent_coef        | 0.000524 |
|    ent_coef_loss   | -8.55    |
|    learning_rate   | 0.00326  |
|    n_updates       | 8500     |
---------------------------------
Eval num_timesteps=1742000, episode_reward=322.54 +/- 362.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 323      |
| time/              |          |
|    total_timesteps | 1742000  |
---------------------------------
Eval num_timesteps=1743000, episode_reward=-137.11 +/- 318.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 1743000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000659 |
|    ent_coef        | 0.000519 |
|    ent_coef_loss   | -14.5    |
|    learning_rate   | 0.00326  |
|    n_updates       | 8510     |
---------------------------------
Eval num_timesteps=1744000, episode_reward=453.18 +/- 38.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 453      |
| time/              |          |
|    total_timesteps | 1744000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 44.6     |
| time/              |          |
|    episodes        | 1744     |
|    fps             | 641      |
|    time_elapsed    | 2720     |
|    total_timesteps | 1744000  |
---------------------------------
Eval num_timesteps=1745000, episode_reward=592.85 +/- 28.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 1745000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.000897 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.00326  |
|    n_updates       | 8520     |
---------------------------------
Eval num_timesteps=1746000, episode_reward=619.50 +/- 36.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 620      |
| time/              |          |
|    total_timesteps | 1746000  |
---------------------------------
Eval num_timesteps=1747000, episode_reward=-378.54 +/- 2.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -379     |
| time/              |          |
|    total_timesteps | 1747000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.00106  |
|    ent_coef        | 0.000505 |
|    ent_coef_loss   | -16.4    |
|    learning_rate   | 0.00325  |
|    n_updates       | 8530     |
---------------------------------
Eval num_timesteps=1748000, episode_reward=-377.33 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 1748000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 59.3     |
| time/              |          |
|    episodes        | 1748     |
|    fps             | 641      |
|    time_elapsed    | 2726     |
|    total_timesteps | 1748000  |
---------------------------------
Eval num_timesteps=1749000, episode_reward=-406.24 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 1749000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000787 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | -7.26    |
|    learning_rate   | 0.00325  |
|    n_updates       | 8540     |
---------------------------------
Eval num_timesteps=1750000, episode_reward=-406.27 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 1750000  |
---------------------------------
Eval num_timesteps=1751000, episode_reward=-406.43 +/- 1.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 1751000  |
---------------------------------
Eval num_timesteps=1752000, episode_reward=-422.58 +/- 1.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 1752000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000791 |
|    ent_coef        | 0.000489 |
|    ent_coef_loss   | -11.6    |
|    learning_rate   | 0.00325  |
|    n_updates       | 8550     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 75.1     |
| time/              |          |
|    episodes        | 1752     |
|    fps             | 641      |
|    time_elapsed    | 2732     |
|    total_timesteps | 1752000  |
---------------------------------
Eval num_timesteps=1753000, episode_reward=-423.12 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 1753000  |
---------------------------------
Eval num_timesteps=1754000, episode_reward=-414.74 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 1754000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000613 |
|    ent_coef        | 0.000483 |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.00325  |
|    n_updates       | 8560     |
---------------------------------
Eval num_timesteps=1755000, episode_reward=-415.25 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 1755000  |
---------------------------------
Eval num_timesteps=1756000, episode_reward=-413.81 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -414     |
| time/              |          |
|    total_timesteps | 1756000  |
| train/             |          |
|    actor_loss      | -1.18    |
|    critic_loss     | 0.000362 |
|    ent_coef        | 0.000478 |
|    ent_coef_loss   | -0.666   |
|    learning_rate   | 0.00324  |
|    n_updates       | 8570     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 69.8     |
| time/              |          |
|    episodes        | 1756     |
|    fps             | 641      |
|    time_elapsed    | 2738     |
|    total_timesteps | 1756000  |
---------------------------------
Eval num_timesteps=1757000, episode_reward=-413.00 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 1757000  |
---------------------------------
Eval num_timesteps=1758000, episode_reward=-34.37 +/- 483.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -34.4    |
| time/              |          |
|    total_timesteps | 1758000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.000528 |
|    ent_coef        | 0.000473 |
|    ent_coef_loss   | -17.5    |
|    learning_rate   | 0.00324  |
|    n_updates       | 8580     |
---------------------------------
Eval num_timesteps=1759000, episode_reward=-219.97 +/- 433.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 1759000  |
---------------------------------
Eval num_timesteps=1760000, episode_reward=-209.46 +/- 293.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 1760000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000769 |
|    ent_coef        | 0.000467 |
|    ent_coef_loss   | -12.1    |
|    learning_rate   | 0.00324  |
|    n_updates       | 8590     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 70.9     |
| time/              |          |
|    episodes        | 1760     |
|    fps             | 641      |
|    time_elapsed    | 2744     |
|    total_timesteps | 1760000  |
---------------------------------
Eval num_timesteps=1761000, episode_reward=52.20 +/- 397.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 52.2     |
| time/              |          |
|    total_timesteps | 1761000  |
---------------------------------
Eval num_timesteps=1762000, episode_reward=436.11 +/- 426.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 436      |
| time/              |          |
|    total_timesteps | 1762000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000654 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | -19.5    |
|    learning_rate   | 0.00324  |
|    n_updates       | 8600     |
---------------------------------
Eval num_timesteps=1763000, episode_reward=431.44 +/- 423.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 1763000  |
---------------------------------
Eval num_timesteps=1764000, episode_reward=5.40 +/- 428.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 5.4      |
| time/              |          |
|    total_timesteps | 1764000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.000873 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | -7.03    |
|    learning_rate   | 0.00324  |
|    n_updates       | 8610     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 74.8     |
| time/              |          |
|    episodes        | 1764     |
|    fps             | 641      |
|    time_elapsed    | 2750     |
|    total_timesteps | 1764000  |
---------------------------------
Eval num_timesteps=1765000, episode_reward=-170.26 +/- 358.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -170     |
| time/              |          |
|    total_timesteps | 1765000  |
---------------------------------
Eval num_timesteps=1766000, episode_reward=294.14 +/- 586.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 294      |
| time/              |          |
|    total_timesteps | 1766000  |
| train/             |          |
|    actor_loss      | -1.17    |
|    critic_loss     | 0.000572 |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | -6.27    |
|    learning_rate   | 0.00323  |
|    n_updates       | 8620     |
---------------------------------
Eval num_timesteps=1767000, episode_reward=523.71 +/- 473.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 524      |
| time/              |          |
|    total_timesteps | 1767000  |
---------------------------------
Eval num_timesteps=1768000, episode_reward=444.99 +/- 423.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 445      |
| time/              |          |
|    total_timesteps | 1768000  |
| train/             |          |
|    actor_loss      | -1.19    |
|    critic_loss     | 0.00059  |
|    ent_coef        | 0.000439 |
|    ent_coef_loss   | -1.56    |
|    learning_rate   | 0.00323  |
|    n_updates       | 8630     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 70.2     |
| time/              |          |
|    episodes        | 1768     |
|    fps             | 641      |
|    time_elapsed    | 2756     |
|    total_timesteps | 1768000  |
---------------------------------
Eval num_timesteps=1769000, episode_reward=17.78 +/- 499.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 17.8     |
| time/              |          |
|    total_timesteps | 1769000  |
---------------------------------
Eval num_timesteps=1770000, episode_reward=601.21 +/- 33.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 601      |
| time/              |          |
|    total_timesteps | 1770000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.00092  |
|    ent_coef        | 0.000437 |
|    ent_coef_loss   | -8.44    |
|    learning_rate   | 0.00323  |
|    n_updates       | 8640     |
---------------------------------
Eval num_timesteps=1771000, episode_reward=557.03 +/- 17.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 557      |
| time/              |          |
|    total_timesteps | 1771000  |
---------------------------------
Eval num_timesteps=1772000, episode_reward=683.28 +/- 52.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 1772000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000869 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -15.6    |
|    learning_rate   | 0.00323  |
|    n_updates       | 8650     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 83.2     |
| time/              |          |
|    episodes        | 1772     |
|    fps             | 641      |
|    time_elapsed    | 2763     |
|    total_timesteps | 1772000  |
---------------------------------
Eval num_timesteps=1773000, episode_reward=665.99 +/- 16.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 666      |
| time/              |          |
|    total_timesteps | 1773000  |
---------------------------------
Eval num_timesteps=1774000, episode_reward=591.49 +/- 458.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 591      |
| time/              |          |
|    total_timesteps | 1774000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000783 |
|    ent_coef        | 0.000426 |
|    ent_coef_loss   | -9.47    |
|    learning_rate   | 0.00323  |
|    n_updates       | 8660     |
---------------------------------
Eval num_timesteps=1775000, episode_reward=605.13 +/- 466.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 605      |
| time/              |          |
|    total_timesteps | 1775000  |
---------------------------------
Eval num_timesteps=1776000, episode_reward=102.13 +/- 535.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 102      |
| time/              |          |
|    total_timesteps | 1776000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.000841 |
|    ent_coef        | 0.00042  |
|    ent_coef_loss   | -12.5    |
|    learning_rate   | 0.00322  |
|    n_updates       | 8670     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 115      |
| time/              |          |
|    episodes        | 1776     |
|    fps             | 641      |
|    time_elapsed    | 2769     |
|    total_timesteps | 1776000  |
---------------------------------
Eval num_timesteps=1777000, episode_reward=341.18 +/- 541.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 1777000  |
---------------------------------
Eval num_timesteps=1778000, episode_reward=-322.81 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -323     |
| time/              |          |
|    total_timesteps | 1778000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.000695 |
|    ent_coef        | 0.000414 |
|    ent_coef_loss   | -8.67    |
|    learning_rate   | 0.00322  |
|    n_updates       | 8680     |
---------------------------------
Eval num_timesteps=1779000, episode_reward=-323.72 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -324     |
| time/              |          |
|    total_timesteps | 1779000  |
---------------------------------
Eval num_timesteps=1780000, episode_reward=-323.29 +/- 1.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -323     |
| time/              |          |
|    total_timesteps | 1780000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000969 |
|    ent_coef        | 0.000409 |
|    ent_coef_loss   | 1.14     |
|    learning_rate   | 0.00322  |
|    n_updates       | 8690     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 139      |
| time/              |          |
|    episodes        | 1780     |
|    fps             | 641      |
|    time_elapsed    | 2775     |
|    total_timesteps | 1780000  |
---------------------------------
Eval num_timesteps=1781000, episode_reward=-323.01 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -323     |
| time/              |          |
|    total_timesteps | 1781000  |
---------------------------------
Eval num_timesteps=1782000, episode_reward=-132.13 +/- 410.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 1782000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000998 |
|    ent_coef        | 0.000407 |
|    ent_coef_loss   | -7.39    |
|    learning_rate   | 0.00322  |
|    n_updates       | 8700     |
---------------------------------
Eval num_timesteps=1783000, episode_reward=73.92 +/- 498.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 73.9     |
| time/              |          |
|    total_timesteps | 1783000  |
---------------------------------
Eval num_timesteps=1784000, episode_reward=-118.48 +/- 443.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 1784000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.000879 |
|    ent_coef        | 0.000405 |
|    ent_coef_loss   | -0.749   |
|    learning_rate   | 0.00322  |
|    n_updates       | 8710     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 170      |
| time/              |          |
|    episodes        | 1784     |
|    fps             | 641      |
|    time_elapsed    | 2781     |
|    total_timesteps | 1784000  |
---------------------------------
Eval num_timesteps=1785000, episode_reward=-117.73 +/- 445.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 1785000  |
---------------------------------
Eval num_timesteps=1786000, episode_reward=466.03 +/- 416.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 1786000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.000799 |
|    ent_coef        | 0.000403 |
|    ent_coef_loss   | -8.12    |
|    learning_rate   | 0.00321  |
|    n_updates       | 8720     |
---------------------------------
Eval num_timesteps=1787000, episode_reward=-160.34 +/- 385.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 1787000  |
---------------------------------
Eval num_timesteps=1788000, episode_reward=-26.82 +/- 432.91
Episode length: 1000.00 +/- 0.00
----------------------------------
| eval/              |           |
|    mean_ep_length  | 1e+03     |
|    mean_reward     | -26.8     |
| time/              |           |
|    total_timesteps | 1788000   |
| train/             |           |
|    actor_loss      | -1.29     |
|    critic_loss     | 0.000902  |
|    ent_coef        | 0.0004    |
|    ent_coef_loss   | -0.000119 |
|    learning_rate   | 0.00321   |
|    n_updates       | 8730      |
----------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 201      |
| time/              |          |
|    episodes        | 1788     |
|    fps             | 641      |
|    time_elapsed    | 2787     |
|    total_timesteps | 1788000  |
---------------------------------
Eval num_timesteps=1789000, episode_reward=-22.75 +/- 433.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.7    |
| time/              |          |
|    total_timesteps | 1789000  |
---------------------------------
Eval num_timesteps=1790000, episode_reward=-381.59 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 1790000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.00073  |
|    ent_coef        | 0.000399 |
|    ent_coef_loss   | -3.94    |
|    learning_rate   | 0.00321  |
|    n_updates       | 8740     |
---------------------------------
Eval num_timesteps=1791000, episode_reward=-173.18 +/- 415.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -173     |
| time/              |          |
|    total_timesteps | 1791000  |
---------------------------------
Eval num_timesteps=1792000, episode_reward=-382.43 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 1792000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 231      |
| time/              |          |
|    episodes        | 1792     |
|    fps             | 641      |
|    time_elapsed    | 2793     |
|    total_timesteps | 1792000  |
---------------------------------
Eval num_timesteps=1793000, episode_reward=-391.95 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 1793000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000666 |
|    ent_coef        | 0.000397 |
|    ent_coef_loss   | -3.86    |
|    learning_rate   | 0.00321  |
|    n_updates       | 8750     |
---------------------------------
Eval num_timesteps=1794000, episode_reward=-392.96 +/- 0.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 1794000  |
---------------------------------
Eval num_timesteps=1795000, episode_reward=583.34 +/- 17.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 583      |
| time/              |          |
|    total_timesteps | 1795000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000883 |
|    ent_coef        | 0.000395 |
|    ent_coef_loss   | -3.52    |
|    learning_rate   | 0.00321  |
|    n_updates       | 8760     |
---------------------------------
Eval num_timesteps=1796000, episode_reward=398.57 +/- 387.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 1796000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 264      |
| time/              |          |
|    episodes        | 1796     |
|    fps             | 641      |
|    time_elapsed    | 2799     |
|    total_timesteps | 1796000  |
---------------------------------
Eval num_timesteps=1797000, episode_reward=616.53 +/- 62.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 617      |
| time/              |          |
|    total_timesteps | 1797000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000914 |
|    ent_coef        | 0.000393 |
|    ent_coef_loss   | -6.32    |
|    learning_rate   | 0.0032   |
|    n_updates       | 8770     |
---------------------------------
Eval num_timesteps=1798000, episode_reward=631.50 +/- 27.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 632      |
| time/              |          |
|    total_timesteps | 1798000  |
---------------------------------
Eval num_timesteps=1799000, episode_reward=646.79 +/- 21.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 1799000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000873 |
|    ent_coef        | 0.00039  |
|    ent_coef_loss   | -3.4     |
|    learning_rate   | 0.0032   |
|    n_updates       | 8780     |
---------------------------------
Eval num_timesteps=1800000, episode_reward=642.39 +/- 18.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 1800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 293      |
| time/              |          |
|    episodes        | 1800     |
|    fps             | 641      |
|    time_elapsed    | 2806     |
|    total_timesteps | 1800000  |
---------------------------------
Eval num_timesteps=1801000, episode_reward=700.84 +/- 20.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 1801000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000977 |
|    ent_coef        | 0.000388 |
|    ent_coef_loss   | -2.32    |
|    learning_rate   | 0.0032   |
|    n_updates       | 8790     |
---------------------------------
Eval num_timesteps=1802000, episode_reward=671.73 +/- 49.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 1802000  |
---------------------------------
Eval num_timesteps=1803000, episode_reward=803.79 +/- 40.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 1803000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000979 |
|    ent_coef        | 0.000387 |
|    ent_coef_loss   | -4.7     |
|    learning_rate   | 0.0032   |
|    n_updates       | 8800     |
---------------------------------
Eval num_timesteps=1804000, episode_reward=821.17 +/- 50.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 821      |
| time/              |          |
|    total_timesteps | 1804000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 327      |
| time/              |          |
|    episodes        | 1804     |
|    fps             | 641      |
|    time_elapsed    | 2812     |
|    total_timesteps | 1804000  |
---------------------------------
Eval num_timesteps=1805000, episode_reward=538.33 +/- 461.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 538      |
| time/              |          |
|    total_timesteps | 1805000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.000876 |
|    ent_coef        | 0.000384 |
|    ent_coef_loss   | -11.8    |
|    learning_rate   | 0.0032   |
|    n_updates       | 8810     |
---------------------------------
Eval num_timesteps=1806000, episode_reward=791.70 +/- 40.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 1806000  |
---------------------------------
Eval num_timesteps=1807000, episode_reward=-395.95 +/- 1.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 1807000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000931 |
|    ent_coef        | 0.00038  |
|    ent_coef_loss   | -3.82    |
|    learning_rate   | 0.00319  |
|    n_updates       | 8820     |
---------------------------------
Eval num_timesteps=1808000, episode_reward=-395.54 +/- 1.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 1808000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 359      |
| time/              |          |
|    episodes        | 1808     |
|    fps             | 641      |
|    time_elapsed    | 2818     |
|    total_timesteps | 1808000  |
---------------------------------
Eval num_timesteps=1809000, episode_reward=-391.73 +/- 1.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -392     |
| time/              |          |
|    total_timesteps | 1809000  |
| train/             |          |
|    actor_loss      | -1.3     |
|    critic_loss     | 0.000796 |
|    ent_coef        | 0.000377 |
|    ent_coef_loss   | -2.47    |
|    learning_rate   | 0.00319  |
|    n_updates       | 8830     |
---------------------------------
Eval num_timesteps=1810000, episode_reward=-393.13 +/- 1.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -393     |
| time/              |          |
|    total_timesteps | 1810000  |
---------------------------------
Eval num_timesteps=1811000, episode_reward=-158.31 +/- 453.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 1811000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000684 |
|    ent_coef        | 0.000375 |
|    ent_coef_loss   | -6.3     |
|    learning_rate   | 0.00319  |
|    n_updates       | 8840     |
---------------------------------
Eval num_timesteps=1812000, episode_reward=-160.48 +/- 453.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 1812000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 395      |
| time/              |          |
|    episodes        | 1812     |
|    fps             | 641      |
|    time_elapsed    | 2824     |
|    total_timesteps | 1812000  |
---------------------------------
Eval num_timesteps=1813000, episode_reward=-389.91 +/- 0.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 1813000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000707 |
|    ent_coef        | 0.000373 |
|    ent_coef_loss   | -4.92    |
|    learning_rate   | 0.00319  |
|    n_updates       | 8850     |
---------------------------------
Eval num_timesteps=1814000, episode_reward=-155.39 +/- 470.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 1814000  |
---------------------------------
Eval num_timesteps=1815000, episode_reward=-395.74 +/- 0.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -396     |
| time/              |          |
|    total_timesteps | 1815000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000773 |
|    ent_coef        | 0.00037  |
|    ent_coef_loss   | -5.44    |
|    learning_rate   | 0.00319  |
|    n_updates       | 8860     |
---------------------------------
Eval num_timesteps=1816000, episode_reward=-394.64 +/- 1.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -395     |
| time/              |          |
|    total_timesteps | 1816000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 421      |
| time/              |          |
|    episodes        | 1816     |
|    fps             | 641      |
|    time_elapsed    | 2830     |
|    total_timesteps | 1816000  |
---------------------------------
Eval num_timesteps=1817000, episode_reward=-407.27 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 1817000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000657 |
|    ent_coef        | 0.000367 |
|    ent_coef_loss   | -3.53    |
|    learning_rate   | 0.00318  |
|    n_updates       | 8870     |
---------------------------------
Eval num_timesteps=1818000, episode_reward=-406.45 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 1818000  |
---------------------------------
Eval num_timesteps=1819000, episode_reward=-404.67 +/- 1.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 1819000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000712 |
|    ent_coef        | 0.000365 |
|    ent_coef_loss   | -3.77    |
|    learning_rate   | 0.00318  |
|    n_updates       | 8880     |
---------------------------------
Eval num_timesteps=1820000, episode_reward=70.07 +/- 581.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 70.1     |
| time/              |          |
|    total_timesteps | 1820000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 449      |
| time/              |          |
|    episodes        | 1820     |
|    fps             | 641      |
|    time_elapsed    | 2836     |
|    total_timesteps | 1820000  |
---------------------------------
Eval num_timesteps=1821000, episode_reward=322.45 +/- 596.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 322      |
| time/              |          |
|    total_timesteps | 1821000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000864 |
|    ent_coef        | 0.000363 |
|    ent_coef_loss   | -2.86    |
|    learning_rate   | 0.00318  |
|    n_updates       | 8890     |
---------------------------------
Eval num_timesteps=1822000, episode_reward=-167.08 +/- 480.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -167     |
| time/              |          |
|    total_timesteps | 1822000  |
---------------------------------
Eval num_timesteps=1823000, episode_reward=790.05 +/- 21.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 1823000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000794 |
|    ent_coef        | 0.000362 |
|    ent_coef_loss   | -3.51    |
|    learning_rate   | 0.00318  |
|    n_updates       | 8900     |
---------------------------------
Eval num_timesteps=1824000, episode_reward=783.20 +/- 28.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 783      |
| time/              |          |
|    total_timesteps | 1824000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 462      |
| time/              |          |
|    episodes        | 1824     |
|    fps             | 641      |
|    time_elapsed    | 2843     |
|    total_timesteps | 1824000  |
---------------------------------
Eval num_timesteps=1825000, episode_reward=758.31 +/- 29.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 1825000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000763 |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | 0.979    |
|    learning_rate   | 0.00318  |
|    n_updates       | 8910     |
---------------------------------
Eval num_timesteps=1826000, episode_reward=710.63 +/- 9.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 711      |
| time/              |          |
|    total_timesteps | 1826000  |
---------------------------------
Eval num_timesteps=1827000, episode_reward=47.49 +/- 552.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 47.5     |
| time/              |          |
|    total_timesteps | 1827000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000727 |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | 2.72     |
|    learning_rate   | 0.00317  |
|    n_updates       | 8920     |
---------------------------------
Eval num_timesteps=1828000, episode_reward=50.52 +/- 556.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 50.5     |
| time/              |          |
|    total_timesteps | 1828000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 486      |
| time/              |          |
|    episodes        | 1828     |
|    fps             | 641      |
|    time_elapsed    | 2849     |
|    total_timesteps | 1828000  |
---------------------------------
Eval num_timesteps=1829000, episode_reward=-402.98 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 1829000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000764 |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.00317  |
|    n_updates       | 8930     |
---------------------------------
Eval num_timesteps=1830000, episode_reward=-165.23 +/- 478.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 1830000  |
---------------------------------
Eval num_timesteps=1831000, episode_reward=-423.74 +/- 1.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 1831000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000686 |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | -2.91    |
|    learning_rate   | 0.00317  |
|    n_updates       | 8940     |
---------------------------------
Eval num_timesteps=1832000, episode_reward=-426.11 +/- 1.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -426     |
| time/              |          |
|    total_timesteps | 1832000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 500      |
| time/              |          |
|    episodes        | 1832     |
|    fps             | 641      |
|    time_elapsed    | 2855     |
|    total_timesteps | 1832000  |
---------------------------------
Eval num_timesteps=1833000, episode_reward=-412.45 +/- 1.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 1833000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000804 |
|    ent_coef        | 0.000359 |
|    ent_coef_loss   | 0.825    |
|    learning_rate   | 0.00317  |
|    n_updates       | 8950     |
---------------------------------
Eval num_timesteps=1834000, episode_reward=-412.24 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 1834000  |
---------------------------------
Eval num_timesteps=1835000, episode_reward=-412.20 +/- 1.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 1835000  |
---------------------------------
Eval num_timesteps=1836000, episode_reward=706.59 +/- 30.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 707      |
| time/              |          |
|    total_timesteps | 1836000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.000764 |
|    ent_coef        | 0.000359 |
|    ent_coef_loss   | 3.08     |
|    learning_rate   | 0.00316  |
|    n_updates       | 8960     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 528      |
| time/              |          |
|    episodes        | 1836     |
|    fps             | 641      |
|    time_elapsed    | 2861     |
|    total_timesteps | 1836000  |
---------------------------------
Eval num_timesteps=1837000, episode_reward=707.89 +/- 23.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 708      |
| time/              |          |
|    total_timesteps | 1837000  |
---------------------------------
Eval num_timesteps=1838000, episode_reward=538.78 +/- 447.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 539      |
| time/              |          |
|    total_timesteps | 1838000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000682 |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | -0.77    |
|    learning_rate   | 0.00316  |
|    n_updates       | 8970     |
---------------------------------
Eval num_timesteps=1839000, episode_reward=758.66 +/- 60.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 1839000  |
---------------------------------
Eval num_timesteps=1840000, episode_reward=663.08 +/- 44.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 663      |
| time/              |          |
|    total_timesteps | 1840000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.0009   |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | -2.82    |
|    learning_rate   | 0.00316  |
|    n_updates       | 8980     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 549      |
| time/              |          |
|    episodes        | 1840     |
|    fps             | 641      |
|    time_elapsed    | 2867     |
|    total_timesteps | 1840000  |
---------------------------------
Eval num_timesteps=1841000, episode_reward=713.67 +/- 24.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 714      |
| time/              |          |
|    total_timesteps | 1841000  |
---------------------------------
Eval num_timesteps=1842000, episode_reward=838.79 +/- 27.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 839      |
| time/              |          |
|    total_timesteps | 1842000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000709 |
|    ent_coef        | 0.000359 |
|    ent_coef_loss   | -0.806   |
|    learning_rate   | 0.00316  |
|    n_updates       | 8990     |
---------------------------------
Eval num_timesteps=1843000, episode_reward=858.84 +/- 30.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 859      |
| time/              |          |
|    total_timesteps | 1843000  |
---------------------------------
Eval num_timesteps=1844000, episode_reward=766.40 +/- 26.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 1844000  |
| train/             |          |
|    actor_loss      | -1.28    |
|    critic_loss     | 0.000895 |
|    ent_coef        | 0.000358 |
|    ent_coef_loss   | 1.12     |
|    learning_rate   | 0.00316  |
|    n_updates       | 9000     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 570      |
| time/              |          |
|    episodes        | 1844     |
|    fps             | 641      |
|    time_elapsed    | 2873     |
|    total_timesteps | 1844000  |
---------------------------------
Eval num_timesteps=1845000, episode_reward=783.85 +/- 13.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 784      |
| time/              |          |
|    total_timesteps | 1845000  |
---------------------------------
Eval num_timesteps=1846000, episode_reward=808.10 +/- 24.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 1846000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000809 |
|    ent_coef        | 0.000359 |
|    ent_coef_loss   | 0.212    |
|    learning_rate   | 0.00315  |
|    n_updates       | 9010     |
---------------------------------
Eval num_timesteps=1847000, episode_reward=786.35 +/- 25.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 1847000  |
---------------------------------
Eval num_timesteps=1848000, episode_reward=896.06 +/- 20.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 1848000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000754 |
|    ent_coef        | 0.000358 |
|    ent_coef_loss   | -3.98    |
|    learning_rate   | 0.00315  |
|    n_updates       | 9020     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 580      |
| time/              |          |
|    episodes        | 1848     |
|    fps             | 641      |
|    time_elapsed    | 2879     |
|    total_timesteps | 1848000  |
---------------------------------
Eval num_timesteps=1849000, episode_reward=898.57 +/- 19.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 1849000  |
---------------------------------
New best mean reward!
Eval num_timesteps=1850000, episode_reward=719.28 +/- 54.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 719      |
| time/              |          |
|    total_timesteps | 1850000  |
| train/             |          |
|    actor_loss      | -1.29    |
|    critic_loss     | 0.000771 |
|    ent_coef        | 0.000357 |
|    ent_coef_loss   | -7.06    |
|    learning_rate   | 0.00315  |
|    n_updates       | 9030     |
---------------------------------
Eval num_timesteps=1851000, episode_reward=755.01 +/- 31.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 755      |
| time/              |          |
|    total_timesteps | 1851000  |
---------------------------------
Eval num_timesteps=1852000, episode_reward=641.29 +/- 29.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 641      |
| time/              |          |
|    total_timesteps | 1852000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000662 |
|    ent_coef        | 0.000355 |
|    ent_coef_loss   | 8.45     |
|    learning_rate   | 0.00315  |
|    n_updates       | 9040     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 594      |
| time/              |          |
|    episodes        | 1852     |
|    fps             | 641      |
|    time_elapsed    | 2886     |
|    total_timesteps | 1852000  |
---------------------------------
Eval num_timesteps=1853000, episode_reward=668.89 +/- 50.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 669      |
| time/              |          |
|    total_timesteps | 1853000  |
---------------------------------
Eval num_timesteps=1854000, episode_reward=630.22 +/- 22.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 630      |
| time/              |          |
|    total_timesteps | 1854000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000835 |
|    ent_coef        | 0.000356 |
|    ent_coef_loss   | 0.243    |
|    learning_rate   | 0.00315  |
|    n_updates       | 9050     |
---------------------------------
Eval num_timesteps=1855000, episode_reward=674.97 +/- 38.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 675      |
| time/              |          |
|    total_timesteps | 1855000  |
---------------------------------
Eval num_timesteps=1856000, episode_reward=790.46 +/- 22.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 1856000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.000732 |
|    ent_coef        | 0.000357 |
|    ent_coef_loss   | -2.57    |
|    learning_rate   | 0.00314  |
|    n_updates       | 9060     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 618      |
| time/              |          |
|    episodes        | 1856     |
|    fps             | 641      |
|    time_elapsed    | 2892     |
|    total_timesteps | 1856000  |
---------------------------------
Eval num_timesteps=1857000, episode_reward=791.27 +/- 24.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 1857000  |
---------------------------------
Eval num_timesteps=1858000, episode_reward=776.29 +/- 25.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 776      |
| time/              |          |
|    total_timesteps | 1858000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000759 |
|    ent_coef        | 0.000356 |
|    ent_coef_loss   | -2.86    |
|    learning_rate   | 0.00314  |
|    n_updates       | 9070     |
---------------------------------
Eval num_timesteps=1859000, episode_reward=764.32 +/- 23.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 1859000  |
---------------------------------
Eval num_timesteps=1860000, episode_reward=743.72 +/- 45.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 1860000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000765 |
|    ent_coef        | 0.000355 |
|    ent_coef_loss   | -0.804   |
|    learning_rate   | 0.00314  |
|    n_updates       | 9080     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 632      |
| time/              |          |
|    episodes        | 1860     |
|    fps             | 641      |
|    time_elapsed    | 2898     |
|    total_timesteps | 1860000  |
---------------------------------
Eval num_timesteps=1861000, episode_reward=748.19 +/- 23.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 748      |
| time/              |          |
|    total_timesteps | 1861000  |
---------------------------------
Eval num_timesteps=1862000, episode_reward=865.08 +/- 28.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 1862000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.000833 |
|    ent_coef        | 0.000354 |
|    ent_coef_loss   | -2.16    |
|    learning_rate   | 0.00314  |
|    n_updates       | 9090     |
---------------------------------
Eval num_timesteps=1863000, episode_reward=849.32 +/- 24.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 1863000  |
---------------------------------
Eval num_timesteps=1864000, episode_reward=825.11 +/- 15.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 1864000  |
| train/             |          |
|    actor_loss      | -1.27    |
|    critic_loss     | 0.000762 |
|    ent_coef        | 0.000353 |
|    ent_coef_loss   | -4.77    |
|    learning_rate   | 0.00314  |
|    n_updates       | 9100     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 642      |
| time/              |          |
|    episodes        | 1864     |
|    fps             | 641      |
|    time_elapsed    | 2904     |
|    total_timesteps | 1864000  |
---------------------------------
Eval num_timesteps=1865000, episode_reward=856.01 +/- 27.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 856      |
| time/              |          |
|    total_timesteps | 1865000  |
---------------------------------
Eval num_timesteps=1866000, episode_reward=785.54 +/- 60.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 1866000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000666 |
|    ent_coef        | 0.000351 |
|    ent_coef_loss   | -7.16    |
|    learning_rate   | 0.00313  |
|    n_updates       | 9110     |
---------------------------------
Eval num_timesteps=1867000, episode_reward=762.26 +/- 62.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 762      |
| time/              |          |
|    total_timesteps | 1867000  |
---------------------------------
Eval num_timesteps=1868000, episode_reward=794.25 +/- 43.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 794      |
| time/              |          |
|    total_timesteps | 1868000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.000857 |
|    ent_coef        | 0.000348 |
|    ent_coef_loss   | -2.38    |
|    learning_rate   | 0.00313  |
|    n_updates       | 9120     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 667      |
| time/              |          |
|    episodes        | 1868     |
|    fps             | 641      |
|    time_elapsed    | 2910     |
|    total_timesteps | 1868000  |
---------------------------------
Eval num_timesteps=1869000, episode_reward=789.99 +/- 24.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 1869000  |
---------------------------------
Eval num_timesteps=1870000, episode_reward=811.35 +/- 40.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 1870000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000901 |
|    ent_coef        | 0.000347 |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.00313  |
|    n_updates       | 9130     |
---------------------------------
Eval num_timesteps=1871000, episode_reward=798.20 +/- 32.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 798      |
| time/              |          |
|    total_timesteps | 1871000  |
---------------------------------
Eval num_timesteps=1872000, episode_reward=809.18 +/- 27.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 1872000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000939 |
|    ent_coef        | 0.000346 |
|    ent_coef_loss   | 0.623    |
|    learning_rate   | 0.00313  |
|    n_updates       | 9140     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 675      |
| time/              |          |
|    episodes        | 1872     |
|    fps             | 641      |
|    time_elapsed    | 2916     |
|    total_timesteps | 1872000  |
---------------------------------
Eval num_timesteps=1873000, episode_reward=810.91 +/- 27.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 1873000  |
---------------------------------
Eval num_timesteps=1874000, episode_reward=821.72 +/- 35.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 1874000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.000893 |
|    ent_coef        | 0.000346 |
|    ent_coef_loss   | -3.99    |
|    learning_rate   | 0.00313  |
|    n_updates       | 9150     |
---------------------------------
Eval num_timesteps=1875000, episode_reward=808.99 +/- 52.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 1875000  |
---------------------------------
Eval num_timesteps=1876000, episode_reward=865.38 +/- 52.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 865      |
| time/              |          |
|    total_timesteps | 1876000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.00077  |
|    ent_coef        | 0.000344 |
|    ent_coef_loss   | -8.55    |
|    learning_rate   | 0.00312  |
|    n_updates       | 9160     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 678      |
| time/              |          |
|    episodes        | 1876     |
|    fps             | 641      |
|    time_elapsed    | 2922     |
|    total_timesteps | 1876000  |
---------------------------------
Eval num_timesteps=1877000, episode_reward=850.57 +/- 17.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 1877000  |
---------------------------------
Eval num_timesteps=1878000, episode_reward=883.23 +/- 20.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 1878000  |
---------------------------------
Eval num_timesteps=1879000, episode_reward=741.20 +/- 35.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 741      |
| time/              |          |
|    total_timesteps | 1879000  |
| train/             |          |
|    actor_loss      | -1.26    |
|    critic_loss     | 0.00091  |
|    ent_coef        | 0.000341 |
|    ent_coef_loss   | -2.55    |
|    learning_rate   | 0.00312  |
|    n_updates       | 9170     |
---------------------------------
Eval num_timesteps=1880000, episode_reward=367.46 +/- 517.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 367      |
| time/              |          |
|    total_timesteps | 1880000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 688      |
| time/              |          |
|    episodes        | 1880     |
|    fps             | 641      |
|    time_elapsed    | 2928     |
|    total_timesteps | 1880000  |
---------------------------------
Eval num_timesteps=1881000, episode_reward=780.50 +/- 25.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 1881000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.000934 |
|    ent_coef        | 0.000339 |
|    ent_coef_loss   | -2.37    |
|    learning_rate   | 0.00312  |
|    n_updates       | 9180     |
---------------------------------
Eval num_timesteps=1882000, episode_reward=775.49 +/- 23.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 1882000  |
---------------------------------
Eval num_timesteps=1883000, episode_reward=-298.64 +/- 2.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -299     |
| time/              |          |
|    total_timesteps | 1883000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.000742 |
|    ent_coef        | 0.000338 |
|    ent_coef_loss   | -4.75    |
|    learning_rate   | 0.00312  |
|    n_updates       | 9190     |
---------------------------------
Eval num_timesteps=1884000, episode_reward=-299.45 +/- 3.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -299     |
| time/              |          |
|    total_timesteps | 1884000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 690      |
| time/              |          |
|    episodes        | 1884     |
|    fps             | 641      |
|    time_elapsed    | 2934     |
|    total_timesteps | 1884000  |
---------------------------------
Eval num_timesteps=1885000, episode_reward=738.79 +/- 15.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 739      |
| time/              |          |
|    total_timesteps | 1885000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.000613 |
|    ent_coef        | 0.000335 |
|    ent_coef_loss   | -8.14    |
|    learning_rate   | 0.00312  |
|    n_updates       | 9200     |
---------------------------------
Eval num_timesteps=1886000, episode_reward=735.69 +/- 34.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 736      |
| time/              |          |
|    total_timesteps | 1886000  |
---------------------------------
Eval num_timesteps=1887000, episode_reward=742.63 +/- 7.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 1887000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.000927 |
|    ent_coef        | 0.000333 |
|    ent_coef_loss   | 2.58     |
|    learning_rate   | 0.00311  |
|    n_updates       | 9210     |
---------------------------------
Eval num_timesteps=1888000, episode_reward=781.45 +/- 30.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 1888000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 690      |
| time/              |          |
|    episodes        | 1888     |
|    fps             | 641      |
|    time_elapsed    | 2941     |
|    total_timesteps | 1888000  |
---------------------------------
Eval num_timesteps=1889000, episode_reward=702.94 +/- 42.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 703      |
| time/              |          |
|    total_timesteps | 1889000  |
| train/             |          |
|    actor_loss      | -1.25    |
|    critic_loss     | 0.000884 |
|    ent_coef        | 0.000332 |
|    ent_coef_loss   | 3        |
|    learning_rate   | 0.00311  |
|    n_updates       | 9220     |
---------------------------------
Eval num_timesteps=1890000, episode_reward=709.35 +/- 36.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 1890000  |
---------------------------------
Eval num_timesteps=1891000, episode_reward=773.60 +/- 18.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 774      |
| time/              |          |
|    total_timesteps | 1891000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.000719 |
|    ent_coef        | 0.000333 |
|    ent_coef_loss   | -1.62    |
|    learning_rate   | 0.00311  |
|    n_updates       | 9230     |
---------------------------------
Eval num_timesteps=1892000, episode_reward=805.24 +/- 39.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 1892000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 696      |
| time/              |          |
|    episodes        | 1892     |
|    fps             | 641      |
|    time_elapsed    | 2947     |
|    total_timesteps | 1892000  |
---------------------------------
Eval num_timesteps=1893000, episode_reward=667.85 +/- 30.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 668      |
| time/              |          |
|    total_timesteps | 1893000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.000804 |
|    ent_coef        | 0.000333 |
|    ent_coef_loss   | -3.16    |
|    learning_rate   | 0.00311  |
|    n_updates       | 9240     |
---------------------------------
Eval num_timesteps=1894000, episode_reward=650.64 +/- 50.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 1894000  |
---------------------------------
Eval num_timesteps=1895000, episode_reward=782.39 +/- 18.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 782      |
| time/              |          |
|    total_timesteps | 1895000  |
| train/             |          |
|    actor_loss      | -1.22    |
|    critic_loss     | 0.000871 |
|    ent_coef        | 0.000332 |
|    ent_coef_loss   | -0.143   |
|    learning_rate   | 0.00311  |
|    n_updates       | 9250     |
---------------------------------
Eval num_timesteps=1896000, episode_reward=749.83 +/- 32.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 1896000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 701      |
| time/              |          |
|    episodes        | 1896     |
|    fps             | 641      |
|    time_elapsed    | 2953     |
|    total_timesteps | 1896000  |
---------------------------------
Eval num_timesteps=1897000, episode_reward=807.45 +/- 31.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 1897000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.000842 |
|    ent_coef        | 0.000331 |
|    ent_coef_loss   | -1.71    |
|    learning_rate   | 0.0031   |
|    n_updates       | 9260     |
---------------------------------
Eval num_timesteps=1898000, episode_reward=798.85 +/- 25.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 799      |
| time/              |          |
|    total_timesteps | 1898000  |
---------------------------------
Eval num_timesteps=1899000, episode_reward=849.22 +/- 40.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 849      |
| time/              |          |
|    total_timesteps | 1899000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.000951 |
|    ent_coef        | 0.00033  |
|    ent_coef_loss   | -4.49    |
|    learning_rate   | 0.0031   |
|    n_updates       | 9270     |
---------------------------------
Eval num_timesteps=1900000, episode_reward=810.56 +/- 38.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 1900000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 707      |
| time/              |          |
|    episodes        | 1900     |
|    fps             | 642      |
|    time_elapsed    | 2959     |
|    total_timesteps | 1900000  |
---------------------------------
Eval num_timesteps=1901000, episode_reward=764.72 +/- 38.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 765      |
| time/              |          |
|    total_timesteps | 1901000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.000853 |
|    ent_coef        | 0.000328 |
|    ent_coef_loss   | -5.21    |
|    learning_rate   | 0.0031   |
|    n_updates       | 9280     |
---------------------------------
Eval num_timesteps=1902000, episode_reward=756.01 +/- 28.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 1902000  |
---------------------------------
Eval num_timesteps=1903000, episode_reward=721.36 +/- 16.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 721      |
| time/              |          |
|    total_timesteps | 1903000  |
| train/             |          |
|    actor_loss      | -1.23    |
|    critic_loss     | 0.000755 |
|    ent_coef        | 0.000326 |
|    ent_coef_loss   | -2.95    |
|    learning_rate   | 0.0031   |
|    n_updates       | 9290     |
---------------------------------
Eval num_timesteps=1904000, episode_reward=684.02 +/- 29.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 1904000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 706      |
| time/              |          |
|    episodes        | 1904     |
|    fps             | 642      |
|    time_elapsed    | 2965     |
|    total_timesteps | 1904000  |
---------------------------------
Eval num_timesteps=1905000, episode_reward=785.82 +/- 17.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 786      |
| time/              |          |
|    total_timesteps | 1905000  |
| train/             |          |
|    actor_loss      | -1.22    |
|    critic_loss     | 0.000877 |
|    ent_coef        | 0.000324 |
|    ent_coef_loss   | 2.85     |
|    learning_rate   | 0.0031   |
|    n_updates       | 9300     |
---------------------------------
Eval num_timesteps=1906000, episode_reward=801.28 +/- 16.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 1906000  |
---------------------------------
Eval num_timesteps=1907000, episode_reward=635.00 +/- 32.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 635      |
| time/              |          |
|    total_timesteps | 1907000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.000781 |
|    ent_coef        | 0.000325 |
|    ent_coef_loss   | 6.09     |
|    learning_rate   | 0.00309  |
|    n_updates       | 9310     |
---------------------------------
Eval num_timesteps=1908000, episode_reward=615.69 +/- 68.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 616      |
| time/              |          |
|    total_timesteps | 1908000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 706      |
| time/              |          |
|    episodes        | 1908     |
|    fps             | 642      |
|    time_elapsed    | 2971     |
|    total_timesteps | 1908000  |
---------------------------------
Eval num_timesteps=1909000, episode_reward=825.64 +/- 36.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 826      |
| time/              |          |
|    total_timesteps | 1909000  |
| train/             |          |
|    actor_loss      | -1.2     |
|    critic_loss     | 0.000898 |
|    ent_coef        | 0.000327 |
|    ent_coef_loss   | 5.3      |
|    learning_rate   | 0.00309  |
|    n_updates       | 9320     |
---------------------------------
Eval num_timesteps=1910000, episode_reward=800.55 +/- 19.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 1910000  |
---------------------------------
Eval num_timesteps=1911000, episode_reward=923.33 +/- 327.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 1911000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.000879 |
|    ent_coef        | 0.000329 |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.00309  |
|    n_updates       | 9330     |
---------------------------------
New best mean reward!
Eval num_timesteps=1912000, episode_reward=1254.60 +/- 400.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 1912000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 706      |
| time/              |          |
|    episodes        | 1912     |
|    fps             | 642      |
|    time_elapsed    | 2977     |
|    total_timesteps | 1912000  |
---------------------------------
Eval num_timesteps=1913000, episode_reward=589.93 +/- 10.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 1913000  |
| train/             |          |
|    actor_loss      | -1.24    |
|    critic_loss     | 0.00364  |
|    ent_coef        | 0.000332 |
|    ent_coef_loss   | 22.8     |
|    learning_rate   | 0.00309  |
|    n_updates       | 9340     |
---------------------------------
Eval num_timesteps=1914000, episode_reward=594.81 +/- 18.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 595      |
| time/              |          |
|    total_timesteps | 1914000  |
---------------------------------
Eval num_timesteps=1915000, episode_reward=-235.57 +/- 4.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 1915000  |
| train/             |          |
|    actor_loss      | -1.2     |
|    critic_loss     | 0.00242  |
|    ent_coef        | 0.000341 |
|    ent_coef_loss   | 36.1     |
|    learning_rate   | 0.00309  |
|    n_updates       | 9350     |
---------------------------------
Eval num_timesteps=1916000, episode_reward=-238.91 +/- 3.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -239     |
| time/              |          |
|    total_timesteps | 1916000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 710      |
| time/              |          |
|    episodes        | 1916     |
|    fps             | 642      |
|    time_elapsed    | 2984     |
|    total_timesteps | 1916000  |
---------------------------------
Eval num_timesteps=1917000, episode_reward=279.71 +/- 412.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 280      |
| time/              |          |
|    total_timesteps | 1917000  |
| train/             |          |
|    actor_loss      | -1.2     |
|    critic_loss     | 0.0011   |
|    ent_coef        | 0.000354 |
|    ent_coef_loss   | 4.81     |
|    learning_rate   | 0.00308  |
|    n_updates       | 9360     |
---------------------------------
Eval num_timesteps=1918000, episode_reward=142.51 +/- 395.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 143      |
| time/              |          |
|    total_timesteps | 1918000  |
---------------------------------
Eval num_timesteps=1919000, episode_reward=-415.11 +/- 55.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 1919000  |
| train/             |          |
|    actor_loss      | -1.21    |
|    critic_loss     | 0.00126  |
|    ent_coef        | 0.000363 |
|    ent_coef_loss   | 17.6     |
|    learning_rate   | 0.00308  |
|    n_updates       | 9370     |
---------------------------------
Eval num_timesteps=1920000, episode_reward=-387.11 +/- 68.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 1920000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 697      |
| time/              |          |
|    episodes        | 1920     |
|    fps             | 642      |
|    time_elapsed    | 2990     |
|    total_timesteps | 1920000  |
---------------------------------
Eval num_timesteps=1921000, episode_reward=-442.72 +/- 0.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -443     |
| time/              |          |
|    total_timesteps | 1921000  |
---------------------------------
Eval num_timesteps=1922000, episode_reward=-374.79 +/- 3.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -375     |
| time/              |          |
|    total_timesteps | 1922000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000293 |
|    ent_coef        | 0.000373 |
|    ent_coef_loss   | 25.7     |
|    learning_rate   | 0.00308  |
|    n_updates       | 9380     |
---------------------------------
Eval num_timesteps=1923000, episode_reward=-374.46 +/- 3.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 1923000  |
---------------------------------
Eval num_timesteps=1924000, episode_reward=-403.12 +/- 1.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 1924000  |
| train/             |          |
|    actor_loss      | -1.13    |
|    critic_loss     | 0.00295  |
|    ent_coef        | 0.000386 |
|    ent_coef_loss   | 17.7     |
|    learning_rate   | 0.00308  |
|    n_updates       | 9390     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 665      |
| time/              |          |
|    episodes        | 1924     |
|    fps             | 642      |
|    time_elapsed    | 2996     |
|    total_timesteps | 1924000  |
---------------------------------
Eval num_timesteps=1925000, episode_reward=-403.94 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 1925000  |
---------------------------------
Eval num_timesteps=1926000, episode_reward=-503.59 +/- 1.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 1926000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000233 |
|    ent_coef        | 0.000398 |
|    ent_coef_loss   | 25.2     |
|    learning_rate   | 0.00307  |
|    n_updates       | 9400     |
---------------------------------
Eval num_timesteps=1927000, episode_reward=-504.47 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 1927000  |
---------------------------------
Eval num_timesteps=1928000, episode_reward=509.86 +/- 49.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 510      |
| time/              |          |
|    total_timesteps | 1928000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000235 |
|    ent_coef        | 0.000415 |
|    ent_coef_loss   | 28.6     |
|    learning_rate   | 0.00307  |
|    n_updates       | 9410     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 626      |
| time/              |          |
|    episodes        | 1928     |
|    fps             | 642      |
|    time_elapsed    | 3003     |
|    total_timesteps | 1928000  |
---------------------------------
Eval num_timesteps=1929000, episode_reward=501.54 +/- 33.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 502      |
| time/              |          |
|    total_timesteps | 1929000  |
---------------------------------
Eval num_timesteps=1930000, episode_reward=677.22 +/- 21.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 677      |
| time/              |          |
|    total_timesteps | 1930000  |
| train/             |          |
|    actor_loss      | -1.18    |
|    critic_loss     | 0.0236   |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 21.3     |
|    learning_rate   | 0.00307  |
|    n_updates       | 9420     |
---------------------------------
Eval num_timesteps=1931000, episode_reward=672.91 +/- 42.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 673      |
| time/              |          |
|    total_timesteps | 1931000  |
---------------------------------
Eval num_timesteps=1932000, episode_reward=-405.38 +/- 1.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 1932000  |
| train/             |          |
|    actor_loss      | -1.15    |
|    critic_loss     | 0.0062   |
|    ent_coef        | 0.000448 |
|    ent_coef_loss   | 8.69     |
|    learning_rate   | 0.00307  |
|    n_updates       | 9430     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 619      |
| time/              |          |
|    episodes        | 1932     |
|    fps             | 642      |
|    time_elapsed    | 3009     |
|    total_timesteps | 1932000  |
---------------------------------
Eval num_timesteps=1933000, episode_reward=-404.18 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 1933000  |
---------------------------------
Eval num_timesteps=1934000, episode_reward=-266.24 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 1934000  |
| train/             |          |
|    actor_loss      | -1.04    |
|    critic_loss     | 0.00124  |
|    ent_coef        | 0.000461 |
|    ent_coef_loss   | 25.2     |
|    learning_rate   | 0.00307  |
|    n_updates       | 9440     |
---------------------------------
Eval num_timesteps=1935000, episode_reward=-266.66 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 1935000  |
---------------------------------
Eval num_timesteps=1936000, episode_reward=-305.81 +/- 1.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 1936000  |
| train/             |          |
|    actor_loss      | -1.07    |
|    critic_loss     | 0.00168  |
|    ent_coef        | 0.000472 |
|    ent_coef_loss   | -0.763   |
|    learning_rate   | 0.00306  |
|    n_updates       | 9450     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 591      |
| time/              |          |
|    episodes        | 1936     |
|    fps             | 642      |
|    time_elapsed    | 3015     |
|    total_timesteps | 1936000  |
---------------------------------
Eval num_timesteps=1937000, episode_reward=-304.45 +/- 0.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -304     |
| time/              |          |
|    total_timesteps | 1937000  |
---------------------------------
Eval num_timesteps=1938000, episode_reward=-584.41 +/- 0.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -584     |
| time/              |          |
|    total_timesteps | 1938000  |
| train/             |          |
|    actor_loss      | -1.09    |
|    critic_loss     | 0.00168  |
|    ent_coef        | 0.000482 |
|    ent_coef_loss   | 64.6     |
|    learning_rate   | 0.00306  |
|    n_updates       | 9460     |
---------------------------------
Eval num_timesteps=1939000, episode_reward=-584.27 +/- 1.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -584     |
| time/              |          |
|    total_timesteps | 1939000  |
---------------------------------
Eval num_timesteps=1940000, episode_reward=-475.94 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 1940000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000155 |
|    ent_coef        | 0.000521 |
|    ent_coef_loss   | 56.7     |
|    learning_rate   | 0.00306  |
|    n_updates       | 9470     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 552      |
| time/              |          |
|    episodes        | 1940     |
|    fps             | 642      |
|    time_elapsed    | 3021     |
|    total_timesteps | 1940000  |
---------------------------------
Eval num_timesteps=1941000, episode_reward=-476.53 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -477     |
| time/              |          |
|    total_timesteps | 1941000  |
---------------------------------
Eval num_timesteps=1942000, episode_reward=-456.29 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -456     |
| time/              |          |
|    total_timesteps | 1942000  |
| train/             |          |
|    actor_loss      | -1.04    |
|    critic_loss     | 0.00024  |
|    ent_coef        | 0.000557 |
|    ent_coef_loss   | 17.3     |
|    learning_rate   | 0.00306  |
|    n_updates       | 9480     |
---------------------------------
Eval num_timesteps=1943000, episode_reward=-457.02 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -457     |
| time/              |          |
|    total_timesteps | 1943000  |
---------------------------------
Eval num_timesteps=1944000, episode_reward=-479.97 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -480     |
| time/              |          |
|    total_timesteps | 1944000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000163 |
|    ent_coef        | 0.000578 |
|    ent_coef_loss   | 22.8     |
|    learning_rate   | 0.00306  |
|    n_updates       | 9490     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 505      |
| time/              |          |
|    episodes        | 1944     |
|    fps             | 642      |
|    time_elapsed    | 3027     |
|    total_timesteps | 1944000  |
---------------------------------
Eval num_timesteps=1945000, episode_reward=-480.26 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -480     |
| time/              |          |
|    total_timesteps | 1945000  |
---------------------------------
Eval num_timesteps=1946000, episode_reward=-489.40 +/- 1.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -489     |
| time/              |          |
|    total_timesteps | 1946000  |
| train/             |          |
|    actor_loss      | -1.04    |
|    critic_loss     | 9.98e-05 |
|    ent_coef        | 0.0006   |
|    ent_coef_loss   | 39.3     |
|    learning_rate   | 0.00305  |
|    n_updates       | 9500     |
---------------------------------
Eval num_timesteps=1947000, episode_reward=-489.97 +/- 1.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -490     |
| time/              |          |
|    total_timesteps | 1947000  |
---------------------------------
Eval num_timesteps=1948000, episode_reward=-542.51 +/- 1.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 1948000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000101 |
|    ent_coef        | 0.000628 |
|    ent_coef_loss   | 37.5     |
|    learning_rate   | 0.00305  |
|    n_updates       | 9510     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 459      |
| time/              |          |
|    episodes        | 1948     |
|    fps             | 642      |
|    time_elapsed    | 3034     |
|    total_timesteps | 1948000  |
---------------------------------
Eval num_timesteps=1949000, episode_reward=-542.49 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -542     |
| time/              |          |
|    total_timesteps | 1949000  |
---------------------------------
Eval num_timesteps=1950000, episode_reward=-512.87 +/- 1.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 1950000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 5.58e-05 |
|    ent_coef        | 0.00066  |
|    ent_coef_loss   | 21.5     |
|    learning_rate   | 0.00305  |
|    n_updates       | 9520     |
---------------------------------
Eval num_timesteps=1951000, episode_reward=-513.49 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 1951000  |
---------------------------------
Eval num_timesteps=1952000, episode_reward=-487.84 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 1952000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 4.14e-05 |
|    ent_coef        | 0.000686 |
|    ent_coef_loss   | 14.9     |
|    learning_rate   | 0.00305  |
|    n_updates       | 9530     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 409      |
| time/              |          |
|    episodes        | 1952     |
|    fps             | 642      |
|    time_elapsed    | 3040     |
|    total_timesteps | 1952000  |
---------------------------------
Eval num_timesteps=1953000, episode_reward=-487.46 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -487     |
| time/              |          |
|    total_timesteps | 1953000  |
---------------------------------
Eval num_timesteps=1954000, episode_reward=-462.03 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -462     |
| time/              |          |
|    total_timesteps | 1954000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 5.69e-05 |
|    ent_coef        | 0.000705 |
|    ent_coef_loss   | 14       |
|    learning_rate   | 0.00305  |
|    n_updates       | 9540     |
---------------------------------
Eval num_timesteps=1955000, episode_reward=-461.31 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -461     |
| time/              |          |
|    total_timesteps | 1955000  |
---------------------------------
Eval num_timesteps=1956000, episode_reward=-460.89 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -461     |
| time/              |          |
|    total_timesteps | 1956000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 5.76e-05 |
|    ent_coef        | 0.00072  |
|    ent_coef_loss   | 13.9     |
|    learning_rate   | 0.00304  |
|    n_updates       | 9550     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 366      |
| time/              |          |
|    episodes        | 1956     |
|    fps             | 642      |
|    time_elapsed    | 3046     |
|    total_timesteps | 1956000  |
---------------------------------
Eval num_timesteps=1957000, episode_reward=-461.12 +/- 1.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -461     |
| time/              |          |
|    total_timesteps | 1957000  |
---------------------------------
Eval num_timesteps=1958000, episode_reward=-487.18 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -487     |
| time/              |          |
|    total_timesteps | 1958000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 5.24e-05 |
|    ent_coef        | 0.000736 |
|    ent_coef_loss   | 14.5     |
|    learning_rate   | 0.00304  |
|    n_updates       | 9560     |
---------------------------------
Eval num_timesteps=1959000, episode_reward=-486.34 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 1959000  |
---------------------------------
Eval num_timesteps=1960000, episode_reward=-498.32 +/- 1.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -498     |
| time/              |          |
|    total_timesteps | 1960000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 3.63e-05 |
|    ent_coef        | 0.000752 |
|    ent_coef_loss   | 16.2     |
|    learning_rate   | 0.00304  |
|    n_updates       | 9570     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 320      |
| time/              |          |
|    episodes        | 1960     |
|    fps             | 642      |
|    time_elapsed    | 3052     |
|    total_timesteps | 1960000  |
---------------------------------
Eval num_timesteps=1961000, episode_reward=-498.35 +/- 0.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -498     |
| time/              |          |
|    total_timesteps | 1961000  |
---------------------------------
Eval num_timesteps=1962000, episode_reward=-497.39 +/- 1.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 1962000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 3.39e-05 |
|    ent_coef        | 0.000769 |
|    ent_coef_loss   | 12.8     |
|    learning_rate   | 0.00304  |
|    n_updates       | 9580     |
---------------------------------
Eval num_timesteps=1963000, episode_reward=-497.31 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 1963000  |
---------------------------------
Eval num_timesteps=1964000, episode_reward=-497.49 +/- 1.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -497     |
| time/              |          |
|    total_timesteps | 1964000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 272      |
| time/              |          |
|    episodes        | 1964     |
|    fps             | 642      |
|    time_elapsed    | 3058     |
|    total_timesteps | 1964000  |
---------------------------------
Eval num_timesteps=1965000, episode_reward=-479.74 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -480     |
| time/              |          |
|    total_timesteps | 1965000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 4.23e-05 |
|    ent_coef        | 0.000784 |
|    ent_coef_loss   | 8.7      |
|    learning_rate   | 0.00304  |
|    n_updates       | 9590     |
---------------------------------
Eval num_timesteps=1966000, episode_reward=-479.82 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -480     |
| time/              |          |
|    total_timesteps | 1966000  |
---------------------------------
Eval num_timesteps=1967000, episode_reward=-468.45 +/- 0.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -468     |
| time/              |          |
|    total_timesteps | 1967000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 4e-05    |
|    ent_coef        | 0.000796 |
|    ent_coef_loss   | 5.59     |
|    learning_rate   | 0.00303  |
|    n_updates       | 9600     |
---------------------------------
Eval num_timesteps=1968000, episode_reward=-467.55 +/- 1.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -468     |
| time/              |          |
|    total_timesteps | 1968000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 226      |
| time/              |          |
|    episodes        | 1968     |
|    fps             | 642      |
|    time_elapsed    | 3064     |
|    total_timesteps | 1968000  |
---------------------------------
Eval num_timesteps=1969000, episode_reward=-460.27 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -460     |
| time/              |          |
|    total_timesteps | 1969000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 4.21e-05 |
|    ent_coef        | 0.000804 |
|    ent_coef_loss   | 2.49     |
|    learning_rate   | 0.00303  |
|    n_updates       | 9610     |
---------------------------------
Eval num_timesteps=1970000, episode_reward=-459.87 +/- 0.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -460     |
| time/              |          |
|    total_timesteps | 1970000  |
---------------------------------
Eval num_timesteps=1971000, episode_reward=-448.16 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -448     |
| time/              |          |
|    total_timesteps | 1971000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 4.63e-05 |
|    ent_coef        | 0.000809 |
|    ent_coef_loss   | 1.07     |
|    learning_rate   | 0.00303  |
|    n_updates       | 9620     |
---------------------------------
Eval num_timesteps=1972000, episode_reward=-448.05 +/- 0.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -448     |
| time/              |          |
|    total_timesteps | 1972000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 179      |
| time/              |          |
|    episodes        | 1972     |
|    fps             | 642      |
|    time_elapsed    | 3071     |
|    total_timesteps | 1972000  |
---------------------------------
Eval num_timesteps=1973000, episode_reward=-437.99 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 1973000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 5.68e-05 |
|    ent_coef        | 0.000811 |
|    ent_coef_loss   | -1.2     |
|    learning_rate   | 0.00303  |
|    n_updates       | 9630     |
---------------------------------
Eval num_timesteps=1974000, episode_reward=-437.66 +/- 0.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 1974000  |
---------------------------------
Eval num_timesteps=1975000, episode_reward=-420.43 +/- 1.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -420     |
| time/              |          |
|    total_timesteps | 1975000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 6.21e-05 |
|    ent_coef        | 0.000811 |
|    ent_coef_loss   | -4.59    |
|    learning_rate   | 0.00303  |
|    n_updates       | 9640     |
---------------------------------
Eval num_timesteps=1976000, episode_reward=-421.05 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -421     |
| time/              |          |
|    total_timesteps | 1976000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 135      |
| time/              |          |
|    episodes        | 1976     |
|    fps             | 642      |
|    time_elapsed    | 3077     |
|    total_timesteps | 1976000  |
---------------------------------
Eval num_timesteps=1977000, episode_reward=-376.42 +/- 0.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1977000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 7.14e-05 |
|    ent_coef        | 0.000807 |
|    ent_coef_loss   | -8.74    |
|    learning_rate   | 0.00302  |
|    n_updates       | 9650     |
---------------------------------
Eval num_timesteps=1978000, episode_reward=-375.57 +/- 0.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1978000  |
---------------------------------
Eval num_timesteps=1979000, episode_reward=-363.32 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -363     |
| time/              |          |
|    total_timesteps | 1979000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 9.62e-05 |
|    ent_coef        | 0.000799 |
|    ent_coef_loss   | -10.2    |
|    learning_rate   | 0.00302  |
|    n_updates       | 9660     |
---------------------------------
Eval num_timesteps=1980000, episode_reward=-363.34 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -363     |
| time/              |          |
|    total_timesteps | 1980000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 93.2     |
| time/              |          |
|    episodes        | 1980     |
|    fps             | 642      |
|    time_elapsed    | 3083     |
|    total_timesteps | 1980000  |
---------------------------------
Eval num_timesteps=1981000, episode_reward=-375.98 +/- 1.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1981000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 9.58e-05 |
|    ent_coef        | 0.000788 |
|    ent_coef_loss   | -9.92    |
|    learning_rate   | 0.00302  |
|    n_updates       | 9670     |
---------------------------------
Eval num_timesteps=1982000, episode_reward=-374.96 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -375     |
| time/              |          |
|    total_timesteps | 1982000  |
---------------------------------
Eval num_timesteps=1983000, episode_reward=-367.61 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 1983000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 8.39e-05 |
|    ent_coef        | 0.000776 |
|    ent_coef_loss   | -10.4    |
|    learning_rate   | 0.00302  |
|    n_updates       | 9680     |
---------------------------------
Eval num_timesteps=1984000, episode_reward=-366.79 +/- 1.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -367     |
| time/              |          |
|    total_timesteps | 1984000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 56.2     |
| time/              |          |
|    episodes        | 1984     |
|    fps             | 642      |
|    time_elapsed    | 3090     |
|    total_timesteps | 1984000  |
---------------------------------
Eval num_timesteps=1985000, episode_reward=-329.54 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 1985000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 9.71e-05 |
|    ent_coef        | 0.000765 |
|    ent_coef_loss   | -8.43    |
|    learning_rate   | 0.00302  |
|    n_updates       | 9690     |
---------------------------------
Eval num_timesteps=1986000, episode_reward=-329.73 +/- 1.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 1986000  |
---------------------------------
Eval num_timesteps=1987000, episode_reward=-315.34 +/- 1.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 1987000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.000103 |
|    ent_coef        | 0.000754 |
|    ent_coef_loss   | -9.25    |
|    learning_rate   | 0.00301  |
|    n_updates       | 9700     |
---------------------------------
Eval num_timesteps=1988000, episode_reward=-315.39 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 1988000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 19.9     |
| time/              |          |
|    episodes        | 1988     |
|    fps             | 642      |
|    time_elapsed    | 3096     |
|    total_timesteps | 1988000  |
---------------------------------
Eval num_timesteps=1989000, episode_reward=-376.08 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1989000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 8.9e-05  |
|    ent_coef        | 0.000744 |
|    ent_coef_loss   | -8.57    |
|    learning_rate   | 0.00301  |
|    n_updates       | 9710     |
---------------------------------
Eval num_timesteps=1990000, episode_reward=-375.85 +/- 1.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 1990000  |
---------------------------------
Eval num_timesteps=1991000, episode_reward=-360.09 +/- 0.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 1991000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 8.82e-05 |
|    ent_coef        | 0.000734 |
|    ent_coef_loss   | -9.02    |
|    learning_rate   | 0.00301  |
|    n_updates       | 9720     |
---------------------------------
Eval num_timesteps=1992000, episode_reward=-359.84 +/- 2.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 1992000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -15.4    |
| time/              |          |
|    episodes        | 1992     |
|    fps             | 642      |
|    time_elapsed    | 3102     |
|    total_timesteps | 1992000  |
---------------------------------
Eval num_timesteps=1993000, episode_reward=-363.39 +/- 1.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -363     |
| time/              |          |
|    total_timesteps | 1993000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.000101 |
|    ent_coef        | 0.000724 |
|    ent_coef_loss   | -6.68    |
|    learning_rate   | 0.00301  |
|    n_updates       | 9730     |
---------------------------------
Eval num_timesteps=1994000, episode_reward=-364.26 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 1994000  |
---------------------------------
Eval num_timesteps=1995000, episode_reward=-354.04 +/- 2.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 1995000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 8.78e-05 |
|    ent_coef        | 0.000715 |
|    ent_coef_loss   | -7.94    |
|    learning_rate   | 0.00301  |
|    n_updates       | 9740     |
---------------------------------
Eval num_timesteps=1996000, episode_reward=-354.55 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -355     |
| time/              |          |
|    total_timesteps | 1996000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -49.8    |
| time/              |          |
|    episodes        | 1996     |
|    fps             | 642      |
|    time_elapsed    | 3108     |
|    total_timesteps | 1996000  |
---------------------------------
Eval num_timesteps=1997000, episode_reward=-241.86 +/- 2.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 1997000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.000102 |
|    ent_coef        | 0.000706 |
|    ent_coef_loss   | -10.7    |
|    learning_rate   | 0.003    |
|    n_updates       | 9750     |
---------------------------------
Eval num_timesteps=1998000, episode_reward=-242.09 +/- 2.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 1998000  |
---------------------------------
Eval num_timesteps=1999000, episode_reward=-183.19 +/- 6.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 1999000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.00012  |
|    ent_coef        | 0.000696 |
|    ent_coef_loss   | -13.1    |
|    learning_rate   | 0.003    |
|    n_updates       | 9760     |
---------------------------------
Eval num_timesteps=2000000, episode_reward=-185.62 +/- 3.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -186     |
| time/              |          |
|    total_timesteps | 2000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -80.4    |
| time/              |          |
|    episodes        | 2000     |
|    fps             | 642      |
|    time_elapsed    | 3114     |
|    total_timesteps | 2000000  |
---------------------------------
Eval num_timesteps=2001000, episode_reward=88.19 +/- 22.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 88.2     |
| time/              |          |
|    total_timesteps | 2001000  |
| train/             |          |
|    actor_loss      | -0.996   |
|    critic_loss     | 0.000244 |
|    ent_coef        | 0.000682 |
|    ent_coef_loss   | -22.1    |
|    learning_rate   | 0.003    |
|    n_updates       | 9770     |
---------------------------------
Eval num_timesteps=2002000, episode_reward=106.28 +/- 34.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 106      |
| time/              |          |
|    total_timesteps | 2002000  |
---------------------------------
Eval num_timesteps=2003000, episode_reward=-310.87 +/- 1.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -311     |
| time/              |          |
|    total_timesteps | 2003000  |
| train/             |          |
|    actor_loss      | -0.966   |
|    critic_loss     | 0.000355 |
|    ent_coef        | 0.000663 |
|    ent_coef_loss   | -17      |
|    learning_rate   | 0.003    |
|    n_updates       | 9780     |
---------------------------------
Eval num_timesteps=2004000, episode_reward=-309.17 +/- 1.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 2004000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -113     |
| time/              |          |
|    episodes        | 2004     |
|    fps             | 642      |
|    time_elapsed    | 3121     |
|    total_timesteps | 2004000  |
---------------------------------
Eval num_timesteps=2005000, episode_reward=-403.02 +/- 13.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -403     |
| time/              |          |
|    total_timesteps | 2005000  |
| train/             |          |
|    actor_loss      | -0.957   |
|    critic_loss     | 0.000498 |
|    ent_coef        | 0.000646 |
|    ent_coef_loss   | -17      |
|    learning_rate   | 0.003    |
|    n_updates       | 9790     |
---------------------------------
Eval num_timesteps=2006000, episode_reward=-404.47 +/- 15.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -404     |
| time/              |          |
|    total_timesteps | 2006000  |
---------------------------------
Eval num_timesteps=2007000, episode_reward=-410.60 +/- 2.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 2007000  |
---------------------------------
Eval num_timesteps=2008000, episode_reward=-476.34 +/- 2.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 2008000  |
| train/             |          |
|    actor_loss      | -0.959   |
|    critic_loss     | 0.00035  |
|    ent_coef        | 0.000631 |
|    ent_coef_loss   | -3.14    |
|    learning_rate   | 0.00299  |
|    n_updates       | 9800     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -156     |
| time/              |          |
|    episodes        | 2008     |
|    fps             | 642      |
|    time_elapsed    | 3127     |
|    total_timesteps | 2008000  |
---------------------------------
Eval num_timesteps=2009000, episode_reward=-475.54 +/- 2.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 2009000  |
---------------------------------
Eval num_timesteps=2010000, episode_reward=-446.70 +/- 2.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -447     |
| time/              |          |
|    total_timesteps | 2010000  |
| train/             |          |
|    actor_loss      | -0.962   |
|    critic_loss     | 0.000103 |
|    ent_coef        | 0.000623 |
|    ent_coef_loss   | -4.41    |
|    learning_rate   | 0.00299  |
|    n_updates       | 9810     |
---------------------------------
Eval num_timesteps=2011000, episode_reward=-446.02 +/- 1.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -446     |
| time/              |          |
|    total_timesteps | 2011000  |
---------------------------------
Eval num_timesteps=2012000, episode_reward=-358.39 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -358     |
| time/              |          |
|    total_timesteps | 2012000  |
| train/             |          |
|    actor_loss      | -0.956   |
|    critic_loss     | 9.69e-05 |
|    ent_coef        | 0.000616 |
|    ent_coef_loss   | -19.4    |
|    learning_rate   | 0.00299  |
|    n_updates       | 9820     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -199     |
| time/              |          |
|    episodes        | 2012     |
|    fps             | 642      |
|    time_elapsed    | 3133     |
|    total_timesteps | 2012000  |
---------------------------------
Eval num_timesteps=2013000, episode_reward=-356.79 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -357     |
| time/              |          |
|    total_timesteps | 2013000  |
---------------------------------
Eval num_timesteps=2014000, episode_reward=-258.68 +/- 1.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 2014000  |
| train/             |          |
|    actor_loss      | -0.954   |
|    critic_loss     | 0.000155 |
|    ent_coef        | 0.000602 |
|    ent_coef_loss   | -33.9    |
|    learning_rate   | 0.00299  |
|    n_updates       | 9830     |
---------------------------------
Eval num_timesteps=2015000, episode_reward=-259.63 +/- 1.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 2015000  |
---------------------------------
Eval num_timesteps=2016000, episode_reward=-367.33 +/- 14.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -367     |
| time/              |          |
|    total_timesteps | 2016000  |
| train/             |          |
|    actor_loss      | -0.948   |
|    critic_loss     | 0.000234 |
|    ent_coef        | 0.000578 |
|    ent_coef_loss   | -37.9    |
|    learning_rate   | 0.00298  |
|    n_updates       | 9840     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -241     |
| time/              |          |
|    episodes        | 2016     |
|    fps             | 642      |
|    time_elapsed    | 3139     |
|    total_timesteps | 2016000  |
---------------------------------
Eval num_timesteps=2017000, episode_reward=-375.51 +/- 11.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -376     |
| time/              |          |
|    total_timesteps | 2017000  |
---------------------------------
Eval num_timesteps=2018000, episode_reward=-225.25 +/- 1.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 2018000  |
| train/             |          |
|    actor_loss      | -0.939   |
|    critic_loss     | 0.0004   |
|    ent_coef        | 0.000549 |
|    ent_coef_loss   | -35.4    |
|    learning_rate   | 0.00298  |
|    n_updates       | 9850     |
---------------------------------
Eval num_timesteps=2019000, episode_reward=-227.13 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 2019000  |
---------------------------------
Eval num_timesteps=2020000, episode_reward=-402.10 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 2020000  |
| train/             |          |
|    actor_loss      | -0.941   |
|    critic_loss     | 0.000279 |
|    ent_coef        | 0.000522 |
|    ent_coef_loss   | -18.2    |
|    learning_rate   | 0.00298  |
|    n_updates       | 9860     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -270     |
| time/              |          |
|    episodes        | 2020     |
|    fps             | 642      |
|    time_elapsed    | 3146     |
|    total_timesteps | 2020000  |
---------------------------------
Eval num_timesteps=2021000, episode_reward=-402.37 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 2021000  |
---------------------------------
Eval num_timesteps=2022000, episode_reward=-287.55 +/- 2.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 2022000  |
| train/             |          |
|    actor_loss      | -0.95    |
|    critic_loss     | 8e-05    |
|    ent_coef        | 0.000504 |
|    ent_coef_loss   | -21.7    |
|    learning_rate   | 0.00298  |
|    n_updates       | 9870     |
---------------------------------
Eval num_timesteps=2023000, episode_reward=-286.90 +/- 2.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -287     |
| time/              |          |
|    total_timesteps | 2023000  |
---------------------------------
Eval num_timesteps=2024000, episode_reward=-111.29 +/- 245.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 2024000  |
| train/             |          |
|    actor_loss      | -0.954   |
|    critic_loss     | 0.000199 |
|    ent_coef        | 0.000488 |
|    ent_coef_loss   | -42.9    |
|    learning_rate   | 0.00298  |
|    n_updates       | 9880     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -276     |
| time/              |          |
|    episodes        | 2024     |
|    fps             | 642      |
|    time_elapsed    | 3152     |
|    total_timesteps | 2024000  |
---------------------------------
Eval num_timesteps=2025000, episode_reward=-379.99 +/- 275.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -380     |
| time/              |          |
|    total_timesteps | 2025000  |
---------------------------------
Eval num_timesteps=2026000, episode_reward=-106.11 +/- 244.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 2026000  |
| train/             |          |
|    actor_loss      | -0.956   |
|    critic_loss     | 0.000611 |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | -9.13    |
|    learning_rate   | 0.00297  |
|    n_updates       | 9890     |
---------------------------------
Eval num_timesteps=2027000, episode_reward=-298.87 +/- 208.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -299     |
| time/              |          |
|    total_timesteps | 2027000  |
---------------------------------
Eval num_timesteps=2028000, episode_reward=-620.93 +/- 65.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -621     |
| time/              |          |
|    total_timesteps | 2028000  |
| train/             |          |
|    actor_loss      | -0.987   |
|    critic_loss     | 0.00404  |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | 10.3     |
|    learning_rate   | 0.00297  |
|    n_updates       | 9900     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -267     |
| time/              |          |
|    episodes        | 2028     |
|    fps             | 642      |
|    time_elapsed    | 3158     |
|    total_timesteps | 2028000  |
---------------------------------
Eval num_timesteps=2029000, episode_reward=-632.56 +/- 27.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -633     |
| time/              |          |
|    total_timesteps | 2029000  |
---------------------------------
Eval num_timesteps=2030000, episode_reward=-244.55 +/- 3.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 2030000  |
| train/             |          |
|    actor_loss      | -0.977   |
|    critic_loss     | 0.00222  |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | 86.1     |
|    learning_rate   | 0.00297  |
|    n_updates       | 9910     |
---------------------------------
Eval num_timesteps=2031000, episode_reward=-239.69 +/- 2.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -240     |
| time/              |          |
|    total_timesteps | 2031000  |
---------------------------------
Eval num_timesteps=2032000, episode_reward=-442.93 +/- 1.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -443     |
| time/              |          |
|    total_timesteps | 2032000  |
| train/             |          |
|    actor_loss      | -0.963   |
|    critic_loss     | 0.000835 |
|    ent_coef        | 0.000487 |
|    ent_coef_loss   | 70.2     |
|    learning_rate   | 0.00297  |
|    n_updates       | 9920     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -314     |
| time/              |          |
|    episodes        | 2032     |
|    fps             | 642      |
|    time_elapsed    | 3164     |
|    total_timesteps | 2032000  |
---------------------------------
Eval num_timesteps=2033000, episode_reward=-441.86 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 2033000  |
---------------------------------
Eval num_timesteps=2034000, episode_reward=-517.10 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 2034000  |
| train/             |          |
|    actor_loss      | -0.975   |
|    critic_loss     | 0.000173 |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | 40       |
|    learning_rate   | 0.00297  |
|    n_updates       | 9930     |
---------------------------------
Eval num_timesteps=2035000, episode_reward=-517.41 +/- 0.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -517     |
| time/              |          |
|    total_timesteps | 2035000  |
---------------------------------
Eval num_timesteps=2036000, episode_reward=-519.30 +/- 0.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 2036000  |
| train/             |          |
|    actor_loss      | -0.976   |
|    critic_loss     | 4.39e-05 |
|    ent_coef        | 0.00056  |
|    ent_coef_loss   | 33.5     |
|    learning_rate   | 0.00296  |
|    n_updates       | 9940     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -329     |
| time/              |          |
|    episodes        | 2036     |
|    fps             | 642      |
|    time_elapsed    | 3170     |
|    total_timesteps | 2036000  |
---------------------------------
Eval num_timesteps=2037000, episode_reward=-518.41 +/- 1.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -518     |
| time/              |          |
|    total_timesteps | 2037000  |
---------------------------------
Eval num_timesteps=2038000, episode_reward=-515.79 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 2038000  |
| train/             |          |
|    actor_loss      | -0.968   |
|    critic_loss     | 5.13e-05 |
|    ent_coef        | 0.000588 |
|    ent_coef_loss   | 23.4     |
|    learning_rate   | 0.00296  |
|    n_updates       | 9950     |
---------------------------------
Eval num_timesteps=2039000, episode_reward=-515.71 +/- 1.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 2039000  |
---------------------------------
Eval num_timesteps=2040000, episode_reward=-487.75 +/- 0.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 2040000  |
| train/             |          |
|    actor_loss      | -0.968   |
|    critic_loss     | 4e-05    |
|    ent_coef        | 0.000609 |
|    ent_coef_loss   | 18.5     |
|    learning_rate   | 0.00296  |
|    n_updates       | 9960     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -337     |
| time/              |          |
|    episodes        | 2040     |
|    fps             | 642      |
|    time_elapsed    | 3177     |
|    total_timesteps | 2040000  |
---------------------------------
Eval num_timesteps=2041000, episode_reward=-487.58 +/- 0.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 2041000  |
---------------------------------
Eval num_timesteps=2042000, episode_reward=-423.87 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 2042000  |
| train/             |          |
|    actor_loss      | -0.966   |
|    critic_loss     | 5.07e-05 |
|    ent_coef        | 0.000626 |
|    ent_coef_loss   | 12.1     |
|    learning_rate   | 0.00296  |
|    n_updates       | 9970     |
---------------------------------
Eval num_timesteps=2043000, episode_reward=-424.20 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 2043000  |
---------------------------------
Eval num_timesteps=2044000, episode_reward=-359.82 +/- 1.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -360     |
| time/              |          |
|    total_timesteps | 2044000  |
| train/             |          |
|    actor_loss      | -0.965   |
|    critic_loss     | 7.36e-05 |
|    ent_coef        | 0.000637 |
|    ent_coef_loss   | 2.1      |
|    learning_rate   | 0.00296  |
|    n_updates       | 9980     |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -338     |
| time/              |          |
|    episodes        | 2044     |
|    fps             | 642      |
|    time_elapsed    | 3183     |
|    total_timesteps | 2044000  |
---------------------------------
Eval num_timesteps=2045000, episode_reward=-360.59 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -361     |
| time/              |          |
|    total_timesteps | 2045000  |
---------------------------------
Eval num_timesteps=2046000, episode_reward=-381.45 +/- 1.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -381     |
| time/              |          |
|    total_timesteps | 2046000  |
| train/             |          |
|    actor_loss      | -0.962   |
|    critic_loss     | 0.000109 |
|    ent_coef        | 0.000641 |
|    ent_coef_loss   | -0.784   |
|    learning_rate   | 0.00295  |
|    n_updates       | 9990     |
---------------------------------
Eval num_timesteps=2047000, episode_reward=-382.19 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 2047000  |
---------------------------------
Eval num_timesteps=2048000, episode_reward=-381.87 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 2048000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -334     |
| time/              |          |
|    episodes        | 2048     |
|    fps             | 642      |
|    time_elapsed    | 3189     |
|    total_timesteps | 2048000  |
---------------------------------
Eval num_timesteps=2049000, episode_reward=-464.84 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -465     |
| time/              |          |
|    total_timesteps | 2049000  |
| train/             |          |
|    actor_loss      | -0.943   |
|    critic_loss     | 8.25e-05 |
|    ent_coef        | 0.000643 |
|    ent_coef_loss   | 12.6     |
|    learning_rate   | 0.00295  |
|    n_updates       | 10000    |
---------------------------------
Eval num_timesteps=2050000, episode_reward=-465.22 +/- 1.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -465     |
| time/              |          |
|    total_timesteps | 2050000  |
---------------------------------
Eval num_timesteps=2051000, episode_reward=-441.74 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 2051000  |
| train/             |          |
|    actor_loss      | -0.932   |
|    critic_loss     | 3.46e-05 |
|    ent_coef        | 0.000651 |
|    ent_coef_loss   | 8.4      |
|    learning_rate   | 0.00295  |
|    n_updates       | 10010    |
---------------------------------
Eval num_timesteps=2052000, episode_reward=-442.25 +/- 1.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 2052000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -331     |
| time/              |          |
|    episodes        | 2052     |
|    fps             | 642      |
|    time_elapsed    | 3195     |
|    total_timesteps | 2052000  |
---------------------------------
Eval num_timesteps=2053000, episode_reward=-401.57 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 2053000  |
| train/             |          |
|    actor_loss      | -0.933   |
|    critic_loss     | 4.18e-05 |
|    ent_coef        | 0.000658 |
|    ent_coef_loss   | 0.74     |
|    learning_rate   | 0.00295  |
|    n_updates       | 10020    |
---------------------------------
Eval num_timesteps=2054000, episode_reward=-401.02 +/- 0.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 2054000  |
---------------------------------
Eval num_timesteps=2055000, episode_reward=-377.48 +/- 1.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 2055000  |
| train/             |          |
|    actor_loss      | -0.937   |
|    critic_loss     | 6.11e-05 |
|    ent_coef        | 0.000661 |
|    ent_coef_loss   | -6.68    |
|    learning_rate   | 0.00295  |
|    n_updates       | 10030    |
---------------------------------
Eval num_timesteps=2056000, episode_reward=-377.71 +/- 1.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 2056000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -329     |
| time/              |          |
|    episodes        | 2056     |
|    fps             | 642      |
|    time_elapsed    | 3201     |
|    total_timesteps | 2056000  |
---------------------------------
Eval num_timesteps=2057000, episode_reward=-339.27 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 2057000  |
| train/             |          |
|    actor_loss      | -0.937   |
|    critic_loss     | 7.37e-05 |
|    ent_coef        | 0.000658 |
|    ent_coef_loss   | -7.83    |
|    learning_rate   | 0.00294  |
|    n_updates       | 10040    |
---------------------------------
Eval num_timesteps=2058000, episode_reward=-339.69 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 2058000  |
---------------------------------
Eval num_timesteps=2059000, episode_reward=-327.52 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -328     |
| time/              |          |
|    total_timesteps | 2059000  |
| train/             |          |
|    actor_loss      | -0.941   |
|    critic_loss     | 9.18e-05 |
|    ent_coef        | 0.000653 |
|    ent_coef_loss   | -9.67    |
|    learning_rate   | 0.00294  |
|    n_updates       | 10050    |
---------------------------------
Eval num_timesteps=2060000, episode_reward=-327.35 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -327     |
| time/              |          |
|    total_timesteps | 2060000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -324     |
| time/              |          |
|    episodes        | 2060     |
|    fps             | 642      |
|    time_elapsed    | 3208     |
|    total_timesteps | 2060000  |
---------------------------------
Eval num_timesteps=2061000, episode_reward=-338.82 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 2061000  |
| train/             |          |
|    actor_loss      | -0.942   |
|    critic_loss     | 9.29e-05 |
|    ent_coef        | 0.000646 |
|    ent_coef_loss   | -8.8     |
|    learning_rate   | 0.00294  |
|    n_updates       | 10060    |
---------------------------------
Eval num_timesteps=2062000, episode_reward=-338.72 +/- 0.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 2062000  |
---------------------------------
Eval num_timesteps=2063000, episode_reward=-347.72 +/- 0.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -348     |
| time/              |          |
|    total_timesteps | 2063000  |
| train/             |          |
|    actor_loss      | -0.941   |
|    critic_loss     | 8.84e-05 |
|    ent_coef        | 0.000638 |
|    ent_coef_loss   | -11.2    |
|    learning_rate   | 0.00294  |
|    n_updates       | 10070    |
---------------------------------
Eval num_timesteps=2064000, episode_reward=-347.16 +/- 1.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 2064000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -318     |
| time/              |          |
|    episodes        | 2064     |
|    fps             | 642      |
|    time_elapsed    | 3214     |
|    total_timesteps | 2064000  |
---------------------------------
Eval num_timesteps=2065000, episode_reward=-242.10 +/- 0.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 2065000  |
| train/             |          |
|    actor_loss      | -0.939   |
|    critic_loss     | 9.58e-05 |
|    ent_coef        | 0.00063  |
|    ent_coef_loss   | -13.2    |
|    learning_rate   | 0.00294  |
|    n_updates       | 10080    |
---------------------------------
Eval num_timesteps=2066000, episode_reward=-242.03 +/- 0.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 2066000  |
---------------------------------
Eval num_timesteps=2067000, episode_reward=-236.37 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 2067000  |
| train/             |          |
|    actor_loss      | -0.944   |
|    critic_loss     | 0.000103 |
|    ent_coef        | 0.00062  |
|    ent_coef_loss   | -11.7    |
|    learning_rate   | 0.00293  |
|    n_updates       | 10090    |
---------------------------------
Eval num_timesteps=2068000, episode_reward=-236.26 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 2068000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -313     |
| time/              |          |
|    episodes        | 2068     |
|    fps             | 642      |
|    time_elapsed    | 3220     |
|    total_timesteps | 2068000  |
---------------------------------
Eval num_timesteps=2069000, episode_reward=-227.62 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 2069000  |
| train/             |          |
|    actor_loss      | -0.947   |
|    critic_loss     | 9.11e-05 |
|    ent_coef        | 0.000611 |
|    ent_coef_loss   | -12.1    |
|    learning_rate   | 0.00293  |
|    n_updates       | 10100    |
---------------------------------
Eval num_timesteps=2070000, episode_reward=-230.07 +/- 4.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -230     |
| time/              |          |
|    total_timesteps | 2070000  |
---------------------------------
Eval num_timesteps=2071000, episode_reward=-238.45 +/- 1.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 2071000  |
| train/             |          |
|    actor_loss      | -0.943   |
|    critic_loss     | 9.78e-05 |
|    ent_coef        | 0.000601 |
|    ent_coef_loss   | -14.8    |
|    learning_rate   | 0.00293  |
|    n_updates       | 10110    |
---------------------------------
Eval num_timesteps=2072000, episode_reward=-237.54 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 2072000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -307     |
| time/              |          |
|    episodes        | 2072     |
|    fps             | 642      |
|    time_elapsed    | 3226     |
|    total_timesteps | 2072000  |
---------------------------------
Eval num_timesteps=2073000, episode_reward=-228.92 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 2073000  |
| train/             |          |
|    actor_loss      | -0.946   |
|    critic_loss     | 0.000102 |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | -13.1    |
|    learning_rate   | 0.00293  |
|    n_updates       | 10120    |
---------------------------------
Eval num_timesteps=2074000, episode_reward=-228.20 +/- 1.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 2074000  |
---------------------------------
Eval num_timesteps=2075000, episode_reward=-259.67 +/- 0.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 2075000  |
| train/             |          |
|    actor_loss      | -0.948   |
|    critic_loss     | 0.000116 |
|    ent_coef        | 0.00058  |
|    ent_coef_loss   | -18.7    |
|    learning_rate   | 0.00293  |
|    n_updates       | 10130    |
---------------------------------
Eval num_timesteps=2076000, episode_reward=-259.11 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 2076000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -299     |
| time/              |          |
|    episodes        | 2076     |
|    fps             | 642      |
|    time_elapsed    | 3232     |
|    total_timesteps | 2076000  |
---------------------------------
Eval num_timesteps=2077000, episode_reward=-258.43 +/- 23.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 2077000  |
| train/             |          |
|    actor_loss      | -0.948   |
|    critic_loss     | 0.000187 |
|    ent_coef        | 0.000567 |
|    ent_coef_loss   | -22.9    |
|    learning_rate   | 0.00292  |
|    n_updates       | 10140    |
---------------------------------
Eval num_timesteps=2078000, episode_reward=-254.81 +/- 24.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 2078000  |
---------------------------------
Eval num_timesteps=2079000, episode_reward=-262.36 +/- 2.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 2079000  |
| train/             |          |
|    actor_loss      | -0.901   |
|    critic_loss     | 0.000621 |
|    ent_coef        | 0.000553 |
|    ent_coef_loss   | -1       |
|    learning_rate   | 0.00292  |
|    n_updates       | 10150    |
---------------------------------
Eval num_timesteps=2080000, episode_reward=-262.52 +/- 3.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 2080000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -298     |
| time/              |          |
|    episodes        | 2080     |
|    fps             | 642      |
|    time_elapsed    | 3239     |
|    total_timesteps | 2080000  |
---------------------------------
Eval num_timesteps=2081000, episode_reward=-267.16 +/- 0.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 2081000  |
| train/             |          |
|    actor_loss      | -0.926   |
|    critic_loss     | 0.000441 |
|    ent_coef        | 0.000546 |
|    ent_coef_loss   | -0.577   |
|    learning_rate   | 0.00292  |
|    n_updates       | 10160    |
---------------------------------
Eval num_timesteps=2082000, episode_reward=-267.97 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -268     |
| time/              |          |
|    total_timesteps | 2082000  |
---------------------------------
Eval num_timesteps=2083000, episode_reward=-335.29 +/- 0.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 2083000  |
| train/             |          |
|    actor_loss      | -0.936   |
|    critic_loss     | 0.000158 |
|    ent_coef        | 0.000544 |
|    ent_coef_loss   | 0.581    |
|    learning_rate   | 0.00292  |
|    n_updates       | 10170    |
---------------------------------
Eval num_timesteps=2084000, episode_reward=-334.77 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 2084000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -302     |
| time/              |          |
|    episodes        | 2084     |
|    fps             | 642      |
|    time_elapsed    | 3245     |
|    total_timesteps | 2084000  |
---------------------------------
Eval num_timesteps=2085000, episode_reward=-378.41 +/- 1.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 2085000  |
| train/             |          |
|    actor_loss      | -0.929   |
|    critic_loss     | 9.16e-05 |
|    ent_coef        | 0.000543 |
|    ent_coef_loss   | -7.92    |
|    learning_rate   | 0.00292  |
|    n_updates       | 10180    |
---------------------------------
Eval num_timesteps=2086000, episode_reward=-378.40 +/- 1.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 2086000  |
---------------------------------
Eval num_timesteps=2087000, episode_reward=-292.81 +/- 0.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -293     |
| time/              |          |
|    total_timesteps | 2087000  |
| train/             |          |
|    actor_loss      | -0.902   |
|    critic_loss     | 0.000152 |
|    ent_coef        | 0.00054  |
|    ent_coef_loss   | -23.6    |
|    learning_rate   | 0.00291  |
|    n_updates       | 10190    |
---------------------------------
Eval num_timesteps=2088000, episode_reward=-291.77 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -292     |
| time/              |          |
|    total_timesteps | 2088000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -308     |
| time/              |          |
|    episodes        | 2088     |
|    fps             | 642      |
|    time_elapsed    | 3251     |
|    total_timesteps | 2088000  |
---------------------------------
Eval num_timesteps=2089000, episode_reward=-363.86 +/- 1.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 2089000  |
| train/             |          |
|    actor_loss      | -0.919   |
|    critic_loss     | 0.000124 |
|    ent_coef        | 0.000529 |
|    ent_coef_loss   | -12      |
|    learning_rate   | 0.00291  |
|    n_updates       | 10200    |
---------------------------------
Eval num_timesteps=2090000, episode_reward=-363.28 +/- 1.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -363     |
| time/              |          |
|    total_timesteps | 2090000  |
---------------------------------
Eval num_timesteps=2091000, episode_reward=-363.09 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -363     |
| time/              |          |
|    total_timesteps | 2091000  |
---------------------------------
Eval num_timesteps=2092000, episode_reward=-215.29 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 2092000  |
| train/             |          |
|    actor_loss      | -0.916   |
|    critic_loss     | 0.00014  |
|    ent_coef        | 0.000519 |
|    ent_coef_loss   | -29.4    |
|    learning_rate   | 0.00291  |
|    n_updates       | 10210    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -312     |
| time/              |          |
|    episodes        | 2092     |
|    fps             | 642      |
|    time_elapsed    | 3257     |
|    total_timesteps | 2092000  |
---------------------------------
Eval num_timesteps=2093000, episode_reward=-214.45 +/- 0.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -214     |
| time/              |          |
|    total_timesteps | 2093000  |
---------------------------------
Eval num_timesteps=2094000, episode_reward=-314.38 +/- 3.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -314     |
| time/              |          |
|    total_timesteps | 2094000  |
| train/             |          |
|    actor_loss      | -0.902   |
|    critic_loss     | 0.000398 |
|    ent_coef        | 0.000504 |
|    ent_coef_loss   | -13.3    |
|    learning_rate   | 0.00291  |
|    n_updates       | 10220    |
---------------------------------
Eval num_timesteps=2095000, episode_reward=-315.76 +/- 3.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 2095000  |
---------------------------------
Eval num_timesteps=2096000, episode_reward=-416.40 +/- 0.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -416     |
| time/              |          |
|    total_timesteps | 2096000  |
| train/             |          |
|    actor_loss      | -0.922   |
|    critic_loss     | 0.0007   |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | 13.9     |
|    learning_rate   | 0.0029   |
|    n_updates       | 10230    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -316     |
| time/              |          |
|    episodes        | 2096     |
|    fps             | 642      |
|    time_elapsed    | 3264     |
|    total_timesteps | 2096000  |
---------------------------------
Eval num_timesteps=2097000, episode_reward=-415.28 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -415     |
| time/              |          |
|    total_timesteps | 2097000  |
---------------------------------
Eval num_timesteps=2098000, episode_reward=-353.46 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -353     |
| time/              |          |
|    total_timesteps | 2098000  |
| train/             |          |
|    actor_loss      | -0.9     |
|    critic_loss     | 0.000685 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | -2.25    |
|    learning_rate   | 0.0029   |
|    n_updates       | 10240    |
---------------------------------
Eval num_timesteps=2099000, episode_reward=-353.31 +/- 1.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -353     |
| time/              |          |
|    total_timesteps | 2099000  |
---------------------------------
Eval num_timesteps=2100000, episode_reward=-341.04 +/- 0.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -341     |
| time/              |          |
|    total_timesteps | 2100000  |
| train/             |          |
|    actor_loss      | -0.903   |
|    critic_loss     | 0.000374 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | 6.06     |
|    learning_rate   | 0.0029   |
|    n_updates       | 10250    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -329     |
| time/              |          |
|    episodes        | 2100     |
|    fps             | 642      |
|    time_elapsed    | 3270     |
|    total_timesteps | 2100000  |
---------------------------------
Eval num_timesteps=2101000, episode_reward=-340.02 +/- 1.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 2101000  |
---------------------------------
Eval num_timesteps=2102000, episode_reward=-344.43 +/- 0.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 2102000  |
| train/             |          |
|    actor_loss      | -0.906   |
|    critic_loss     | 0.000839 |
|    ent_coef        | 0.000498 |
|    ent_coef_loss   | -6.16    |
|    learning_rate   | 0.0029   |
|    n_updates       | 10260    |
---------------------------------
Eval num_timesteps=2103000, episode_reward=-344.28 +/- 1.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 2103000  |
---------------------------------
Eval num_timesteps=2104000, episode_reward=-262.88 +/- 1.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -263     |
| time/              |          |
|    total_timesteps | 2104000  |
| train/             |          |
|    actor_loss      | -0.901   |
|    critic_loss     | 0.000276 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | -14.7    |
|    learning_rate   | 0.0029   |
|    n_updates       | 10270    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -335     |
| time/              |          |
|    episodes        | 2104     |
|    fps             | 642      |
|    time_elapsed    | 3276     |
|    total_timesteps | 2104000  |
---------------------------------
Eval num_timesteps=2105000, episode_reward=-256.38 +/- 14.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 2105000  |
---------------------------------
Eval num_timesteps=2106000, episode_reward=-289.11 +/- 4.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -289     |
| time/              |          |
|    total_timesteps | 2106000  |
| train/             |          |
|    actor_loss      | -0.911   |
|    critic_loss     | 0.000751 |
|    ent_coef        | 0.000489 |
|    ent_coef_loss   | -7.76    |
|    learning_rate   | 0.00289  |
|    n_updates       | 10280    |
---------------------------------
Eval num_timesteps=2107000, episode_reward=-288.56 +/- 6.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -289     |
| time/              |          |
|    total_timesteps | 2107000  |
---------------------------------
Eval num_timesteps=2108000, episode_reward=-248.56 +/- 60.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -249     |
| time/              |          |
|    total_timesteps | 2108000  |
| train/             |          |
|    actor_loss      | -0.93    |
|    critic_loss     | 0.000347 |
|    ent_coef        | 0.000485 |
|    ent_coef_loss   | -4.03    |
|    learning_rate   | 0.00289  |
|    n_updates       | 10290    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -327     |
| time/              |          |
|    episodes        | 2108     |
|    fps             | 642      |
|    time_elapsed    | 3282     |
|    total_timesteps | 2108000  |
---------------------------------
Eval num_timesteps=2109000, episode_reward=-283.71 +/- 39.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 2109000  |
---------------------------------
Eval num_timesteps=2110000, episode_reward=-330.43 +/- 7.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 2110000  |
| train/             |          |
|    actor_loss      | -0.957   |
|    critic_loss     | 0.000357 |
|    ent_coef        | 0.00048  |
|    ent_coef_loss   | -12.6    |
|    learning_rate   | 0.00289  |
|    n_updates       | 10300    |
---------------------------------
Eval num_timesteps=2111000, episode_reward=-320.00 +/- 20.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -320     |
| time/              |          |
|    total_timesteps | 2111000  |
---------------------------------
Eval num_timesteps=2112000, episode_reward=-273.82 +/- 55.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -274     |
| time/              |          |
|    total_timesteps | 2112000  |
| train/             |          |
|    actor_loss      | -0.949   |
|    critic_loss     | 0.000321 |
|    ent_coef        | 0.000474 |
|    ent_coef_loss   | -3.79    |
|    learning_rate   | 0.00289  |
|    n_updates       | 10310    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -317     |
| time/              |          |
|    episodes        | 2112     |
|    fps             | 642      |
|    time_elapsed    | 3289     |
|    total_timesteps | 2112000  |
---------------------------------
Eval num_timesteps=2113000, episode_reward=-256.45 +/- 68.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 2113000  |
---------------------------------
Eval num_timesteps=2114000, episode_reward=-225.57 +/- 14.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 2114000  |
| train/             |          |
|    actor_loss      | -0.933   |
|    critic_loss     | 0.000174 |
|    ent_coef        | 0.000469 |
|    ent_coef_loss   | -24.8    |
|    learning_rate   | 0.00289  |
|    n_updates       | 10320    |
---------------------------------
Eval num_timesteps=2115000, episode_reward=-214.69 +/- 12.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 2115000  |
---------------------------------
Eval num_timesteps=2116000, episode_reward=-16.30 +/- 323.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -16.3    |
| time/              |          |
|    total_timesteps | 2116000  |
| train/             |          |
|    actor_loss      | -0.934   |
|    critic_loss     | 0.000265 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | -16.8    |
|    learning_rate   | 0.00288  |
|    n_updates       | 10330    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -312     |
| time/              |          |
|    episodes        | 2116     |
|    fps             | 642      |
|    time_elapsed    | 3295     |
|    total_timesteps | 2116000  |
---------------------------------
Eval num_timesteps=2117000, episode_reward=-142.45 +/- 329.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 2117000  |
---------------------------------
Eval num_timesteps=2118000, episode_reward=-204.54 +/- 364.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 2118000  |
| train/             |          |
|    actor_loss      | -0.925   |
|    critic_loss     | 0.000317 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | -10.8    |
|    learning_rate   | 0.00288  |
|    n_updates       | 10340    |
---------------------------------
Eval num_timesteps=2119000, episode_reward=-215.40 +/- 343.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 2119000  |
---------------------------------
Eval num_timesteps=2120000, episode_reward=-215.42 +/- 386.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -215     |
| time/              |          |
|    total_timesteps | 2120000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.00055  |
|    ent_coef        | 0.000444 |
|    ent_coef_loss   | 10.4     |
|    learning_rate   | 0.00288  |
|    n_updates       | 10350    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -290     |
| time/              |          |
|    episodes        | 2120     |
|    fps             | 642      |
|    time_elapsed    | 3301     |
|    total_timesteps | 2120000  |
---------------------------------
Eval num_timesteps=2121000, episode_reward=-431.92 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -432     |
| time/              |          |
|    total_timesteps | 2121000  |
---------------------------------
Eval num_timesteps=2122000, episode_reward=-383.36 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 2122000  |
| train/             |          |
|    actor_loss      | -0.855   |
|    critic_loss     | 0.000266 |
|    ent_coef        | 0.000444 |
|    ent_coef_loss   | 3.07     |
|    learning_rate   | 0.00288  |
|    n_updates       | 10360    |
---------------------------------
Eval num_timesteps=2123000, episode_reward=-195.42 +/- 378.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 2123000  |
---------------------------------
Eval num_timesteps=2124000, episode_reward=-503.22 +/- 1.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 2124000  |
| train/             |          |
|    actor_loss      | -0.816   |
|    critic_loss     | 0.000104 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 6.67     |
|    learning_rate   | 0.00288  |
|    n_updates       | 10370    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -299     |
| time/              |          |
|    episodes        | 2124     |
|    fps             | 642      |
|    time_elapsed    | 3307     |
|    total_timesteps | 2124000  |
---------------------------------
Eval num_timesteps=2125000, episode_reward=-503.10 +/- 0.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 2125000  |
---------------------------------
Eval num_timesteps=2126000, episode_reward=-524.72 +/- 1.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -525     |
| time/              |          |
|    total_timesteps | 2126000  |
| train/             |          |
|    actor_loss      | -0.821   |
|    critic_loss     | 5.72e-05 |
|    ent_coef        | 0.000449 |
|    ent_coef_loss   | 20.2     |
|    learning_rate   | 0.00287  |
|    n_updates       | 10380    |
---------------------------------
Eval num_timesteps=2127000, episode_reward=-108.69 +/- 509.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 2127000  |
---------------------------------
Eval num_timesteps=2128000, episode_reward=-441.36 +/- 117.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -441     |
| time/              |          |
|    total_timesteps | 2128000  |
| train/             |          |
|    actor_loss      | -0.971   |
|    critic_loss     | 0.000544 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | -6.83    |
|    learning_rate   | 0.00287  |
|    n_updates       | 10390    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -301     |
| time/              |          |
|    episodes        | 2128     |
|    fps             | 642      |
|    time_elapsed    | 3313     |
|    total_timesteps | 2128000  |
---------------------------------
Eval num_timesteps=2129000, episode_reward=-487.76 +/- 97.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 2129000  |
---------------------------------
Eval num_timesteps=2130000, episode_reward=-333.28 +/- 83.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 2130000  |
| train/             |          |
|    actor_loss      | -0.998   |
|    critic_loss     | 0.000463 |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | -11.3    |
|    learning_rate   | 0.00287  |
|    n_updates       | 10400    |
---------------------------------
Eval num_timesteps=2131000, episode_reward=-373.11 +/- 103.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 2131000  |
---------------------------------
Eval num_timesteps=2132000, episode_reward=-401.00 +/- 69.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 2132000  |
| train/             |          |
|    actor_loss      | -0.826   |
|    critic_loss     | 0.000105 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | 15.5     |
|    learning_rate   | 0.00287  |
|    n_updates       | 10410    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -278     |
| time/              |          |
|    episodes        | 2132     |
|    fps             | 642      |
|    time_elapsed    | 3320     |
|    total_timesteps | 2132000  |
---------------------------------
Eval num_timesteps=2133000, episode_reward=-312.19 +/- 4.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 2133000  |
---------------------------------
Eval num_timesteps=2134000, episode_reward=-346.83 +/- 55.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 2134000  |
---------------------------------
Eval num_timesteps=2135000, episode_reward=42.67 +/- 512.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 42.7     |
| time/              |          |
|    total_timesteps | 2135000  |
| train/             |          |
|    actor_loss      | -0.812   |
|    critic_loss     | 0.000114 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | 9.41     |
|    learning_rate   | 0.00287  |
|    n_updates       | 10420    |
---------------------------------
Eval num_timesteps=2136000, episode_reward=-166.41 +/- 425.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 2136000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -267     |
| time/              |          |
|    episodes        | 2136     |
|    fps             | 642      |
|    time_elapsed    | 3326     |
|    total_timesteps | 2136000  |
---------------------------------
Eval num_timesteps=2137000, episode_reward=-357.60 +/- 64.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -358     |
| time/              |          |
|    total_timesteps | 2137000  |
| train/             |          |
|    actor_loss      | -0.919   |
|    critic_loss     | 0.000347 |
|    ent_coef        | 0.000464 |
|    ent_coef_loss   | -2.69    |
|    learning_rate   | 0.00286  |
|    n_updates       | 10430    |
---------------------------------
Eval num_timesteps=2138000, episode_reward=-373.87 +/- 54.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 2138000  |
---------------------------------
Eval num_timesteps=2139000, episode_reward=686.87 +/- 39.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 687      |
| time/              |          |
|    total_timesteps | 2139000  |
| train/             |          |
|    actor_loss      | -0.817   |
|    critic_loss     | 5.62e-05 |
|    ent_coef        | 0.000465 |
|    ent_coef_loss   | 7.53     |
|    learning_rate   | 0.00286  |
|    n_updates       | 10440    |
---------------------------------
Eval num_timesteps=2140000, episode_reward=744.16 +/- 36.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 2140000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -254     |
| time/              |          |
|    episodes        | 2140     |
|    fps             | 642      |
|    time_elapsed    | 3332     |
|    total_timesteps | 2140000  |
---------------------------------
Eval num_timesteps=2141000, episode_reward=801.13 +/- 24.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 2141000  |
| train/             |          |
|    actor_loss      | -0.915   |
|    critic_loss     | 0.000336 |
|    ent_coef        | 0.000468 |
|    ent_coef_loss   | 3.79     |
|    learning_rate   | 0.00286  |
|    n_updates       | 10450    |
---------------------------------
Eval num_timesteps=2142000, episode_reward=793.20 +/- 32.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 793      |
| time/              |          |
|    total_timesteps | 2142000  |
---------------------------------
Eval num_timesteps=2143000, episode_reward=560.06 +/- 504.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 560      |
| time/              |          |
|    total_timesteps | 2143000  |
| train/             |          |
|    actor_loss      | -0.933   |
|    critic_loss     | 0.000254 |
|    ent_coef        | 0.00047  |
|    ent_coef_loss   | -0.39    |
|    learning_rate   | 0.00286  |
|    n_updates       | 10460    |
---------------------------------
Eval num_timesteps=2144000, episode_reward=790.85 +/- 31.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 2144000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -222     |
| time/              |          |
|    episodes        | 2144     |
|    fps             | 642      |
|    time_elapsed    | 3338     |
|    total_timesteps | 2144000  |
---------------------------------
Eval num_timesteps=2145000, episode_reward=569.67 +/- 497.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 2145000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.00036  |
|    ent_coef        | 0.00047  |
|    ent_coef_loss   | -11.8    |
|    learning_rate   | 0.00286  |
|    n_updates       | 10470    |
---------------------------------
Eval num_timesteps=2146000, episode_reward=595.06 +/- 512.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 595      |
| time/              |          |
|    total_timesteps | 2146000  |
---------------------------------
Eval num_timesteps=2147000, episode_reward=450.74 +/- 450.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 451      |
| time/              |          |
|    total_timesteps | 2147000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.000388 |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | -6.88    |
|    learning_rate   | 0.00285  |
|    n_updates       | 10480    |
---------------------------------
Eval num_timesteps=2148000, episode_reward=388.37 +/- 420.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 388      |
| time/              |          |
|    total_timesteps | 2148000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -198     |
| time/              |          |
|    episodes        | 2148     |
|    fps             | 642      |
|    time_elapsed    | 3344     |
|    total_timesteps | 2148000  |
---------------------------------
Eval num_timesteps=2149000, episode_reward=131.09 +/- 465.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 131      |
| time/              |          |
|    total_timesteps | 2149000  |
| train/             |          |
|    actor_loss      | -0.91    |
|    critic_loss     | 0.000234 |
|    ent_coef        | 0.000462 |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.00285  |
|    n_updates       | 10490    |
---------------------------------
Eval num_timesteps=2150000, episode_reward=-47.72 +/- 480.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -47.7    |
| time/              |          |
|    total_timesteps | 2150000  |
---------------------------------
Eval num_timesteps=2151000, episode_reward=-237.26 +/- 363.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -237     |
| time/              |          |
|    total_timesteps | 2151000  |
| train/             |          |
|    actor_loss      | -0.952   |
|    critic_loss     | 0.00029  |
|    ent_coef        | 0.000461 |
|    ent_coef_loss   | -1.45    |
|    learning_rate   | 0.00285  |
|    n_updates       | 10500    |
---------------------------------
Eval num_timesteps=2152000, episode_reward=-233.95 +/- 369.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 2152000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -170     |
| time/              |          |
|    episodes        | 2152     |
|    fps             | 642      |
|    time_elapsed    | 3350     |
|    total_timesteps | 2152000  |
---------------------------------
Eval num_timesteps=2153000, episode_reward=-184.07 +/- 486.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -184     |
| time/              |          |
|    total_timesteps | 2153000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000438 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | -6.57    |
|    learning_rate   | 0.00285  |
|    n_updates       | 10510    |
---------------------------------
Eval num_timesteps=2154000, episode_reward=-193.43 +/- 466.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 2154000  |
---------------------------------
Eval num_timesteps=2155000, episode_reward=693.27 +/- 28.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 693      |
| time/              |          |
|    total_timesteps | 2155000  |
| train/             |          |
|    actor_loss      | -0.861   |
|    critic_loss     | 0.000155 |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | 0.579    |
|    learning_rate   | 0.00285  |
|    n_updates       | 10520    |
---------------------------------
Eval num_timesteps=2156000, episode_reward=43.57 +/- 558.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 43.6     |
| time/              |          |
|    total_timesteps | 2156000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -155     |
| time/              |          |
|    episodes        | 2156     |
|    fps             | 642      |
|    time_elapsed    | 3356     |
|    total_timesteps | 2156000  |
---------------------------------
Eval num_timesteps=2157000, episode_reward=588.81 +/- 47.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 2157000  |
| train/             |          |
|    actor_loss      | -0.909   |
|    critic_loss     | 0.000246 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | -6.51    |
|    learning_rate   | 0.00284  |
|    n_updates       | 10530    |
---------------------------------
Eval num_timesteps=2158000, episode_reward=639.65 +/- 20.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 2158000  |
---------------------------------
Eval num_timesteps=2159000, episode_reward=596.21 +/- 46.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 596      |
| time/              |          |
|    total_timesteps | 2159000  |
| train/             |          |
|    actor_loss      | -0.96    |
|    critic_loss     | 0.000314 |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | -8.33    |
|    learning_rate   | 0.00284  |
|    n_updates       | 10540    |
---------------------------------
Eval num_timesteps=2160000, episode_reward=551.74 +/- 41.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 552      |
| time/              |          |
|    total_timesteps | 2160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -134     |
| time/              |          |
|    episodes        | 2160     |
|    fps             | 642      |
|    time_elapsed    | 3362     |
|    total_timesteps | 2160000  |
---------------------------------
Eval num_timesteps=2161000, episode_reward=161.10 +/- 453.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 161      |
| time/              |          |
|    total_timesteps | 2161000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000498 |
|    ent_coef        | 0.000448 |
|    ent_coef_loss   | -8.98    |
|    learning_rate   | 0.00284  |
|    n_updates       | 10550    |
---------------------------------
Eval num_timesteps=2162000, episode_reward=607.80 +/- 42.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 2162000  |
---------------------------------
Eval num_timesteps=2163000, episode_reward=648.19 +/- 47.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 648      |
| time/              |          |
|    total_timesteps | 2163000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.000424 |
|    ent_coef        | 0.000444 |
|    ent_coef_loss   | -7.38    |
|    learning_rate   | 0.00284  |
|    n_updates       | 10560    |
---------------------------------
Eval num_timesteps=2164000, episode_reward=639.87 +/- 38.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 640      |
| time/              |          |
|    total_timesteps | 2164000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -101     |
| time/              |          |
|    episodes        | 2164     |
|    fps             | 642      |
|    time_elapsed    | 3369     |
|    total_timesteps | 2164000  |
---------------------------------
Eval num_timesteps=2165000, episode_reward=723.36 +/- 38.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 723      |
| time/              |          |
|    total_timesteps | 2165000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.00036  |
|    ent_coef        | 0.000439 |
|    ent_coef_loss   | -10.6    |
|    learning_rate   | 0.00284  |
|    n_updates       | 10570    |
---------------------------------
Eval num_timesteps=2166000, episode_reward=696.40 +/- 59.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 696      |
| time/              |          |
|    total_timesteps | 2166000  |
---------------------------------
Eval num_timesteps=2167000, episode_reward=522.76 +/- 16.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 523      |
| time/              |          |
|    total_timesteps | 2167000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000398 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -17.1    |
|    learning_rate   | 0.00283  |
|    n_updates       | 10580    |
---------------------------------
Eval num_timesteps=2168000, episode_reward=539.04 +/- 54.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 539      |
| time/              |          |
|    total_timesteps | 2168000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -70.7    |
| time/              |          |
|    episodes        | 2168     |
|    fps             | 642      |
|    time_elapsed    | 3375     |
|    total_timesteps | 2168000  |
---------------------------------
Eval num_timesteps=2169000, episode_reward=342.80 +/- 33.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 343      |
| time/              |          |
|    total_timesteps | 2169000  |
| train/             |          |
|    actor_loss      | -0.915   |
|    critic_loss     | 0.000282 |
|    ent_coef        | 0.000425 |
|    ent_coef_loss   | -7.96    |
|    learning_rate   | 0.00283  |
|    n_updates       | 10590    |
---------------------------------
Eval num_timesteps=2170000, episode_reward=347.84 +/- 27.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 2170000  |
---------------------------------
Eval num_timesteps=2171000, episode_reward=363.17 +/- 31.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 363      |
| time/              |          |
|    total_timesteps | 2171000  |
| train/             |          |
|    actor_loss      | -0.879   |
|    critic_loss     | 0.000275 |
|    ent_coef        | 0.000419 |
|    ent_coef_loss   | -5.6     |
|    learning_rate   | 0.00283  |
|    n_updates       | 10600    |
---------------------------------
Eval num_timesteps=2172000, episode_reward=340.57 +/- 32.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 341      |
| time/              |          |
|    total_timesteps | 2172000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -62.5    |
| time/              |          |
|    episodes        | 2172     |
|    fps             | 642      |
|    time_elapsed    | 3381     |
|    total_timesteps | 2172000  |
---------------------------------
Eval num_timesteps=2173000, episode_reward=476.45 +/- 22.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 476      |
| time/              |          |
|    total_timesteps | 2173000  |
| train/             |          |
|    actor_loss      | -0.882   |
|    critic_loss     | 0.000254 |
|    ent_coef        | 0.000415 |
|    ent_coef_loss   | -7.34    |
|    learning_rate   | 0.00283  |
|    n_updates       | 10610    |
---------------------------------
Eval num_timesteps=2174000, episode_reward=433.15 +/- 36.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 433      |
| time/              |          |
|    total_timesteps | 2174000  |
---------------------------------
Eval num_timesteps=2175000, episode_reward=-30.39 +/- 449.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -30.4    |
| time/              |          |
|    total_timesteps | 2175000  |
| train/             |          |
|    actor_loss      | -0.885   |
|    critic_loss     | 0.000197 |
|    ent_coef        | 0.00041  |
|    ent_coef_loss   | -6.4     |
|    learning_rate   | 0.00283  |
|    n_updates       | 10620    |
---------------------------------
Eval num_timesteps=2176000, episode_reward=-43.25 +/- 432.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -43.3    |
| time/              |          |
|    total_timesteps | 2176000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -65.1    |
| time/              |          |
|    episodes        | 2176     |
|    fps             | 642      |
|    time_elapsed    | 3387     |
|    total_timesteps | 2176000  |
---------------------------------
Eval num_timesteps=2177000, episode_reward=-26.79 +/- 446.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -26.8    |
| time/              |          |
|    total_timesteps | 2177000  |
---------------------------------
Eval num_timesteps=2178000, episode_reward=429.04 +/- 150.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 429      |
| time/              |          |
|    total_timesteps | 2178000  |
| train/             |          |
|    actor_loss      | -0.786   |
|    critic_loss     | 5.51e-05 |
|    ent_coef        | 0.000407 |
|    ent_coef_loss   | 5.81     |
|    learning_rate   | 0.00282  |
|    n_updates       | 10630    |
---------------------------------
Eval num_timesteps=2179000, episode_reward=309.66 +/- 386.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 310      |
| time/              |          |
|    total_timesteps | 2179000  |
---------------------------------
Eval num_timesteps=2180000, episode_reward=-350.11 +/- 76.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 2180000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000373 |
|    ent_coef        | 0.000407 |
|    ent_coef_loss   | -9.35    |
|    learning_rate   | 0.00282  |
|    n_updates       | 10640    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -45.6    |
| time/              |          |
|    episodes        | 2180     |
|    fps             | 642      |
|    time_elapsed    | 3393     |
|    total_timesteps | 2180000  |
---------------------------------
Eval num_timesteps=2181000, episode_reward=-309.02 +/- 2.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -309     |
| time/              |          |
|    total_timesteps | 2181000  |
---------------------------------
Eval num_timesteps=2182000, episode_reward=-376.59 +/- 1.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 2182000  |
| train/             |          |
|    actor_loss      | -0.89    |
|    critic_loss     | 0.000236 |
|    ent_coef        | 0.000405 |
|    ent_coef_loss   | 4.53     |
|    learning_rate   | 0.00282  |
|    n_updates       | 10650    |
---------------------------------
Eval num_timesteps=2183000, episode_reward=-377.32 +/- 0.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 2183000  |
---------------------------------
Eval num_timesteps=2184000, episode_reward=-384.01 +/- 2.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -384     |
| time/              |          |
|    total_timesteps | 2184000  |
| train/             |          |
|    actor_loss      | -0.891   |
|    critic_loss     | 0.00022  |
|    ent_coef        | 0.000405 |
|    ent_coef_loss   | -1.08    |
|    learning_rate   | 0.00282  |
|    n_updates       | 10660    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -33.9    |
| time/              |          |
|    episodes        | 2184     |
|    fps             | 642      |
|    time_elapsed    | 3399     |
|    total_timesteps | 2184000  |
---------------------------------
Eval num_timesteps=2185000, episode_reward=-383.73 +/- 2.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -384     |
| time/              |          |
|    total_timesteps | 2185000  |
---------------------------------
Eval num_timesteps=2186000, episode_reward=-487.67 +/- 2.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 2186000  |
| train/             |          |
|    actor_loss      | -0.921   |
|    critic_loss     | 0.000219 |
|    ent_coef        | 0.000406 |
|    ent_coef_loss   | 7.26     |
|    learning_rate   | 0.00281  |
|    n_updates       | 10670    |
---------------------------------
Eval num_timesteps=2187000, episode_reward=-487.78 +/- 2.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -488     |
| time/              |          |
|    total_timesteps | 2187000  |
---------------------------------
Eval num_timesteps=2188000, episode_reward=-502.00 +/- 2.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -502     |
| time/              |          |
|    total_timesteps | 2188000  |
| train/             |          |
|    actor_loss      | -0.859   |
|    critic_loss     | 2.31e-05 |
|    ent_coef        | 0.000409 |
|    ent_coef_loss   | 29       |
|    learning_rate   | 0.00281  |
|    n_updates       | 10680    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -36.3    |
| time/              |          |
|    episodes        | 2188     |
|    fps             | 642      |
|    time_elapsed    | 3406     |
|    total_timesteps | 2188000  |
---------------------------------
Eval num_timesteps=2189000, episode_reward=-502.54 +/- 1.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 2189000  |
---------------------------------
Eval num_timesteps=2190000, episode_reward=-487.40 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -487     |
| time/              |          |
|    total_timesteps | 2190000  |
| train/             |          |
|    actor_loss      | -0.857   |
|    critic_loss     | 4.12e-05 |
|    ent_coef        | 0.00042  |
|    ent_coef_loss   | 12.1     |
|    learning_rate   | 0.00281  |
|    n_updates       | 10690    |
---------------------------------
Eval num_timesteps=2191000, episode_reward=-485.74 +/- 1.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 2191000  |
---------------------------------
Eval num_timesteps=2192000, episode_reward=-470.57 +/- 4.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -471     |
| time/              |          |
|    total_timesteps | 2192000  |
| train/             |          |
|    actor_loss      | -0.859   |
|    critic_loss     | 3.17e-05 |
|    ent_coef        | 0.000429 |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 0.00281  |
|    n_updates       | 10700    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -36.7    |
| time/              |          |
|    episodes        | 2192     |
|    fps             | 642      |
|    time_elapsed    | 3412     |
|    total_timesteps | 2192000  |
---------------------------------
Eval num_timesteps=2193000, episode_reward=-470.38 +/- 6.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -470     |
| time/              |          |
|    total_timesteps | 2193000  |
---------------------------------
Eval num_timesteps=2194000, episode_reward=191.05 +/- 498.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 2194000  |
| train/             |          |
|    actor_loss      | -0.882   |
|    critic_loss     | 0.000285 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -2.95    |
|    learning_rate   | 0.00281  |
|    n_updates       | 10710    |
---------------------------------
Eval num_timesteps=2195000, episode_reward=-27.17 +/- 477.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -27.2    |
| time/              |          |
|    total_timesteps | 2195000  |
---------------------------------
Eval num_timesteps=2196000, episode_reward=169.59 +/- 413.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 170      |
| time/              |          |
|    total_timesteps | 2196000  |
| train/             |          |
|    actor_loss      | -0.995   |
|    critic_loss     | 0.000406 |
|    ent_coef        | 0.000431 |
|    ent_coef_loss   | -15.7    |
|    learning_rate   | 0.0028   |
|    n_updates       | 10720    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -15.5    |
| time/              |          |
|    episodes        | 2196     |
|    fps             | 642      |
|    time_elapsed    | 3418     |
|    total_timesteps | 2196000  |
---------------------------------
Eval num_timesteps=2197000, episode_reward=380.08 +/- 359.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 380      |
| time/              |          |
|    total_timesteps | 2197000  |
---------------------------------
Eval num_timesteps=2198000, episode_reward=578.88 +/- 23.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 579      |
| time/              |          |
|    total_timesteps | 2198000  |
| train/             |          |
|    actor_loss      | -0.997   |
|    critic_loss     | 0.000514 |
|    ent_coef        | 0.000426 |
|    ent_coef_loss   | -7.98    |
|    learning_rate   | 0.0028   |
|    n_updates       | 10730    |
---------------------------------
Eval num_timesteps=2199000, episode_reward=190.90 +/- 421.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 191      |
| time/              |          |
|    total_timesteps | 2199000  |
---------------------------------
Eval num_timesteps=2200000, episode_reward=534.63 +/- 16.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 535      |
| time/              |          |
|    total_timesteps | 2200000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.000444 |
|    ent_coef        | 0.00042  |
|    ent_coef_loss   | -10.6    |
|    learning_rate   | 0.0028   |
|    n_updates       | 10740    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 16.2     |
| time/              |          |
|    episodes        | 2200     |
|    fps             | 642      |
|    time_elapsed    | 3424     |
|    total_timesteps | 2200000  |
---------------------------------
Eval num_timesteps=2201000, episode_reward=528.21 +/- 44.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 528      |
| time/              |          |
|    total_timesteps | 2201000  |
---------------------------------
Eval num_timesteps=2202000, episode_reward=256.42 +/- 292.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 256      |
| time/              |          |
|    total_timesteps | 2202000  |
| train/             |          |
|    actor_loss      | -0.996   |
|    critic_loss     | 0.000476 |
|    ent_coef        | 0.000414 |
|    ent_coef_loss   | -8.49    |
|    learning_rate   | 0.0028   |
|    n_updates       | 10750    |
---------------------------------
Eval num_timesteps=2203000, episode_reward=274.86 +/- 298.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 2203000  |
---------------------------------
Eval num_timesteps=2204000, episode_reward=587.98 +/- 24.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 588      |
| time/              |          |
|    total_timesteps | 2204000  |
| train/             |          |
|    actor_loss      | -0.98    |
|    critic_loss     | 0.000449 |
|    ent_coef        | 0.000409 |
|    ent_coef_loss   | -2.58    |
|    learning_rate   | 0.0028   |
|    n_updates       | 10760    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 44       |
| time/              |          |
|    episodes        | 2204     |
|    fps             | 642      |
|    time_elapsed    | 3430     |
|    total_timesteps | 2204000  |
---------------------------------
Eval num_timesteps=2205000, episode_reward=210.47 +/- 442.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 210      |
| time/              |          |
|    total_timesteps | 2205000  |
---------------------------------
Eval num_timesteps=2206000, episode_reward=-158.74 +/- 393.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 2206000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.000454 |
|    ent_coef        | 0.000406 |
|    ent_coef_loss   | -8.2     |
|    learning_rate   | 0.00279  |
|    n_updates       | 10770    |
---------------------------------
Eval num_timesteps=2207000, episode_reward=11.15 +/- 444.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 11.1     |
| time/              |          |
|    total_timesteps | 2207000  |
---------------------------------
Eval num_timesteps=2208000, episode_reward=659.49 +/- 39.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 659      |
| time/              |          |
|    total_timesteps | 2208000  |
| train/             |          |
|    actor_loss      | -0.995   |
|    critic_loss     | 0.000439 |
|    ent_coef        | 0.000402 |
|    ent_coef_loss   | -8.62    |
|    learning_rate   | 0.00279  |
|    n_updates       | 10780    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 72.5     |
| time/              |          |
|    episodes        | 2208     |
|    fps             | 642      |
|    time_elapsed    | 3437     |
|    total_timesteps | 2208000  |
---------------------------------
Eval num_timesteps=2209000, episode_reward=683.35 +/- 42.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 683      |
| time/              |          |
|    total_timesteps | 2209000  |
---------------------------------
Eval num_timesteps=2210000, episode_reward=589.92 +/- 39.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 590      |
| time/              |          |
|    total_timesteps | 2210000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000432 |
|    ent_coef        | 0.000397 |
|    ent_coef_loss   | -10      |
|    learning_rate   | 0.00279  |
|    n_updates       | 10790    |
---------------------------------
Eval num_timesteps=2211000, episode_reward=605.89 +/- 19.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 606      |
| time/              |          |
|    total_timesteps | 2211000  |
---------------------------------
Eval num_timesteps=2212000, episode_reward=509.23 +/- 68.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 2212000  |
| train/             |          |
|    actor_loss      | -0.891   |
|    critic_loss     | 0.000341 |
|    ent_coef        | 0.000392 |
|    ent_coef_loss   | -11.9    |
|    learning_rate   | 0.00279  |
|    n_updates       | 10800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 90.4     |
| time/              |          |
|    episodes        | 2212     |
|    fps             | 642      |
|    time_elapsed    | 3443     |
|    total_timesteps | 2212000  |
---------------------------------
Eval num_timesteps=2213000, episode_reward=487.10 +/- 75.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 487      |
| time/              |          |
|    total_timesteps | 2213000  |
---------------------------------
Eval num_timesteps=2214000, episode_reward=-346.12 +/- 2.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -346     |
| time/              |          |
|    total_timesteps | 2214000  |
| train/             |          |
|    actor_loss      | -0.966   |
|    critic_loss     | 0.000375 |
|    ent_coef        | 0.000386 |
|    ent_coef_loss   | -15.5    |
|    learning_rate   | 0.00279  |
|    n_updates       | 10810    |
---------------------------------
Eval num_timesteps=2215000, episode_reward=-346.52 +/- 2.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 2215000  |
---------------------------------
Eval num_timesteps=2216000, episode_reward=-367.89 +/- 4.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 2216000  |
| train/             |          |
|    actor_loss      | -0.999   |
|    critic_loss     | 0.000339 |
|    ent_coef        | 0.000378 |
|    ent_coef_loss   | -14.6    |
|    learning_rate   | 0.00278  |
|    n_updates       | 10820    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 117      |
| time/              |          |
|    episodes        | 2216     |
|    fps             | 642      |
|    time_elapsed    | 3449     |
|    total_timesteps | 2216000  |
---------------------------------
Eval num_timesteps=2217000, episode_reward=-367.87 +/- 2.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 2217000  |
---------------------------------
Eval num_timesteps=2218000, episode_reward=-340.58 +/- 3.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -341     |
| time/              |          |
|    total_timesteps | 2218000  |
| train/             |          |
|    actor_loss      | -0.985   |
|    critic_loss     | 0.000329 |
|    ent_coef        | 0.000371 |
|    ent_coef_loss   | -9.74    |
|    learning_rate   | 0.00278  |
|    n_updates       | 10830    |
---------------------------------
Eval num_timesteps=2219000, episode_reward=-340.19 +/- 2.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 2219000  |
---------------------------------
Eval num_timesteps=2220000, episode_reward=-340.24 +/- 1.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 2220000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 130      |
| time/              |          |
|    episodes        | 2220     |
|    fps             | 642      |
|    time_elapsed    | 3455     |
|    total_timesteps | 2220000  |
---------------------------------
Eval num_timesteps=2221000, episode_reward=75.13 +/- 395.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 75.1     |
| time/              |          |
|    total_timesteps | 2221000  |
| train/             |          |
|    actor_loss      | -0.991   |
|    critic_loss     | 0.000384 |
|    ent_coef        | 0.000365 |
|    ent_coef_loss   | -6.79    |
|    learning_rate   | 0.00278  |
|    n_updates       | 10840    |
---------------------------------
Eval num_timesteps=2222000, episode_reward=-267.06 +/- 53.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 2222000  |
---------------------------------
Eval num_timesteps=2223000, episode_reward=-377.47 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 2223000  |
| train/             |          |
|    actor_loss      | -0.994   |
|    critic_loss     | 0.000346 |
|    ent_coef        | 0.00036  |
|    ent_coef_loss   | -7.21    |
|    learning_rate   | 0.00278  |
|    n_updates       | 10850    |
---------------------------------
Eval num_timesteps=2224000, episode_reward=-373.39 +/- 2.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 2224000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 162      |
| time/              |          |
|    episodes        | 2224     |
|    fps             | 642      |
|    time_elapsed    | 3461     |
|    total_timesteps | 2224000  |
---------------------------------
Eval num_timesteps=2225000, episode_reward=821.80 +/- 30.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 2225000  |
| train/             |          |
|    actor_loss      | -0.916   |
|    critic_loss     | 0.000226 |
|    ent_coef        | 0.000356 |
|    ent_coef_loss   | -10.7    |
|    learning_rate   | 0.00278  |
|    n_updates       | 10860    |
---------------------------------
Eval num_timesteps=2226000, episode_reward=768.79 +/- 18.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 2226000  |
---------------------------------
Eval num_timesteps=2227000, episode_reward=127.60 +/- 522.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 128      |
| time/              |          |
|    total_timesteps | 2227000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000439 |
|    ent_coef        | 0.000352 |
|    ent_coef_loss   | -2.59    |
|    learning_rate   | 0.00277  |
|    n_updates       | 10870    |
---------------------------------
Eval num_timesteps=2228000, episode_reward=570.70 +/- 437.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 571      |
| time/              |          |
|    total_timesteps | 2228000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 191      |
| time/              |          |
|    episodes        | 2228     |
|    fps             | 642      |
|    time_elapsed    | 3467     |
|    total_timesteps | 2228000  |
---------------------------------
Eval num_timesteps=2229000, episode_reward=835.32 +/- 54.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 2229000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000401 |
|    ent_coef        | 0.000349 |
|    ent_coef_loss   | -7.4     |
|    learning_rate   | 0.00277  |
|    n_updates       | 10880    |
---------------------------------
Eval num_timesteps=2230000, episode_reward=843.63 +/- 20.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 844      |
| time/              |          |
|    total_timesteps | 2230000  |
---------------------------------
Eval num_timesteps=2231000, episode_reward=792.50 +/- 16.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 792      |
| time/              |          |
|    total_timesteps | 2231000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000405 |
|    ent_coef        | 0.000345 |
|    ent_coef_loss   | -8.3     |
|    learning_rate   | 0.00277  |
|    n_updates       | 10890    |
---------------------------------
Eval num_timesteps=2232000, episode_reward=749.20 +/- 57.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 749      |
| time/              |          |
|    total_timesteps | 2232000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 223      |
| time/              |          |
|    episodes        | 2232     |
|    fps             | 642      |
|    time_elapsed    | 3473     |
|    total_timesteps | 2232000  |
---------------------------------
Eval num_timesteps=2233000, episode_reward=532.52 +/- 57.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 533      |
| time/              |          |
|    total_timesteps | 2233000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.000465 |
|    ent_coef        | 0.000342 |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.00277  |
|    n_updates       | 10900    |
---------------------------------
Eval num_timesteps=2234000, episode_reward=561.06 +/- 38.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 561      |
| time/              |          |
|    total_timesteps | 2234000  |
---------------------------------
Eval num_timesteps=2235000, episode_reward=593.18 +/- 33.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 2235000  |
| train/             |          |
|    actor_loss      | -0.985   |
|    critic_loss     | 0.000491 |
|    ent_coef        | 0.00034  |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.00277  |
|    n_updates       | 10910    |
---------------------------------
Eval num_timesteps=2236000, episode_reward=590.56 +/- 38.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 591      |
| time/              |          |
|    total_timesteps | 2236000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 253      |
| time/              |          |
|    episodes        | 2236     |
|    fps             | 642      |
|    time_elapsed    | 3480     |
|    total_timesteps | 2236000  |
---------------------------------
Eval num_timesteps=2237000, episode_reward=727.05 +/- 34.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 727      |
| time/              |          |
|    total_timesteps | 2237000  |
| train/             |          |
|    actor_loss      | -0.998   |
|    critic_loss     | 0.000448 |
|    ent_coef        | 0.00034  |
|    ent_coef_loss   | 0.892    |
|    learning_rate   | 0.00276  |
|    n_updates       | 10920    |
---------------------------------
Eval num_timesteps=2238000, episode_reward=778.85 +/- 53.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 2238000  |
---------------------------------
Eval num_timesteps=2239000, episode_reward=853.20 +/- 29.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 2239000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000422 |
|    ent_coef        | 0.00034  |
|    ent_coef_loss   | -4.05    |
|    learning_rate   | 0.00276  |
|    n_updates       | 10930    |
---------------------------------
Eval num_timesteps=2240000, episode_reward=876.01 +/- 67.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 2240000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 287      |
| time/              |          |
|    episodes        | 2240     |
|    fps             | 642      |
|    time_elapsed    | 3486     |
|    total_timesteps | 2240000  |
---------------------------------
Eval num_timesteps=2241000, episode_reward=510.32 +/- 42.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 510      |
| time/              |          |
|    total_timesteps | 2241000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000432 |
|    ent_coef        | 0.000338 |
|    ent_coef_loss   | -5.8     |
|    learning_rate   | 0.00276  |
|    n_updates       | 10940    |
---------------------------------
Eval num_timesteps=2242000, episode_reward=566.19 +/- 42.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 566      |
| time/              |          |
|    total_timesteps | 2242000  |
---------------------------------
Eval num_timesteps=2243000, episode_reward=821.66 +/- 33.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 822      |
| time/              |          |
|    total_timesteps | 2243000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.000402 |
|    ent_coef        | 0.000336 |
|    ent_coef_loss   | -7.12    |
|    learning_rate   | 0.00276  |
|    n_updates       | 10950    |
---------------------------------
Eval num_timesteps=2244000, episode_reward=779.34 +/- 26.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 2244000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 299      |
| time/              |          |
|    episodes        | 2244     |
|    fps             | 642      |
|    time_elapsed    | 3492     |
|    total_timesteps | 2244000  |
---------------------------------
Eval num_timesteps=2245000, episode_reward=759.28 +/- 51.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 2245000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.000365 |
|    ent_coef        | 0.000333 |
|    ent_coef_loss   | -6.46    |
|    learning_rate   | 0.00276  |
|    n_updates       | 10960    |
---------------------------------
Eval num_timesteps=2246000, episode_reward=725.43 +/- 54.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 725      |
| time/              |          |
|    total_timesteps | 2246000  |
---------------------------------
Eval num_timesteps=2247000, episode_reward=779.72 +/- 34.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 780      |
| time/              |          |
|    total_timesteps | 2247000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000407 |
|    ent_coef        | 0.00033  |
|    ent_coef_loss   | -3.15    |
|    learning_rate   | 0.00275  |
|    n_updates       | 10970    |
---------------------------------
Eval num_timesteps=2248000, episode_reward=759.02 +/- 34.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 2248000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 317      |
| time/              |          |
|    episodes        | 2248     |
|    fps             | 642      |
|    time_elapsed    | 3498     |
|    total_timesteps | 2248000  |
---------------------------------
Eval num_timesteps=2249000, episode_reward=-211.96 +/- 2.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 2249000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.000502 |
|    ent_coef        | 0.000328 |
|    ent_coef_loss   | -0.788   |
|    learning_rate   | 0.00275  |
|    n_updates       | 10980    |
---------------------------------
Eval num_timesteps=2250000, episode_reward=-211.45 +/- 1.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 2250000  |
---------------------------------
Eval num_timesteps=2251000, episode_reward=694.46 +/- 26.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 694      |
| time/              |          |
|    total_timesteps | 2251000  |
| train/             |          |
|    actor_loss      | -1.01    |
|    critic_loss     | 0.000382 |
|    ent_coef        | 0.000326 |
|    ent_coef_loss   | -8.77    |
|    learning_rate   | 0.00275  |
|    n_updates       | 10990    |
---------------------------------
Eval num_timesteps=2252000, episode_reward=704.41 +/- 25.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 704      |
| time/              |          |
|    total_timesteps | 2252000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 333      |
| time/              |          |
|    episodes        | 2252     |
|    fps             | 642      |
|    time_elapsed    | 3504     |
|    total_timesteps | 2252000  |
---------------------------------
Eval num_timesteps=2253000, episode_reward=903.48 +/- 37.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 2253000  |
| train/             |          |
|    actor_loss      | -1       |
|    critic_loss     | 0.000474 |
|    ent_coef        | 0.000323 |
|    ent_coef_loss   | -5.12    |
|    learning_rate   | 0.00275  |
|    n_updates       | 11000    |
---------------------------------
Eval num_timesteps=2254000, episode_reward=902.26 +/- 41.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 902      |
| time/              |          |
|    total_timesteps | 2254000  |
---------------------------------
Eval num_timesteps=2255000, episode_reward=862.45 +/- 46.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 2255000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000422 |
|    ent_coef        | 0.000321 |
|    ent_coef_loss   | -3.34    |
|    learning_rate   | 0.00275  |
|    n_updates       | 11010    |
---------------------------------
Eval num_timesteps=2256000, episode_reward=841.26 +/- 24.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 2256000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 365      |
| time/              |          |
|    episodes        | 2256     |
|    fps             | 642      |
|    time_elapsed    | 3510     |
|    total_timesteps | 2256000  |
---------------------------------
Eval num_timesteps=2257000, episode_reward=769.32 +/- 36.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 2257000  |
| train/             |          |
|    actor_loss      | -1.04    |
|    critic_loss     | 0.000403 |
|    ent_coef        | 0.000319 |
|    ent_coef_loss   | -4.34    |
|    learning_rate   | 0.00274  |
|    n_updates       | 11020    |
---------------------------------
Eval num_timesteps=2258000, episode_reward=807.98 +/- 32.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 808      |
| time/              |          |
|    total_timesteps | 2258000  |
---------------------------------
Eval num_timesteps=2259000, episode_reward=905.58 +/- 48.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 2259000  |
| train/             |          |
|    actor_loss      | -1.02    |
|    critic_loss     | 0.000421 |
|    ent_coef        | 0.000317 |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.00274  |
|    n_updates       | 11030    |
---------------------------------
Eval num_timesteps=2260000, episode_reward=914.93 +/- 56.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 2260000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 389      |
| time/              |          |
|    episodes        | 2260     |
|    fps             | 642      |
|    time_elapsed    | 3516     |
|    total_timesteps | 2260000  |
---------------------------------
Eval num_timesteps=2261000, episode_reward=-208.84 +/- 2.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -209     |
| time/              |          |
|    total_timesteps | 2261000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000465 |
|    ent_coef        | 0.000315 |
|    ent_coef_loss   | -1.18    |
|    learning_rate   | 0.00274  |
|    n_updates       | 11040    |
---------------------------------
Eval num_timesteps=2262000, episode_reward=-206.55 +/- 1.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 2262000  |
---------------------------------
Eval num_timesteps=2263000, episode_reward=-207.45 +/- 2.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -207     |
| time/              |          |
|    total_timesteps | 2263000  |
---------------------------------
Eval num_timesteps=2264000, episode_reward=-252.86 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 2264000  |
| train/             |          |
|    actor_loss      | -0.92    |
|    critic_loss     | 0.000309 |
|    ent_coef        | 0.000314 |
|    ent_coef_loss   | -9.04    |
|    learning_rate   | 0.00274  |
|    n_updates       | 11050    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 381      |
| time/              |          |
|    episodes        | 2264     |
|    fps             | 642      |
|    time_elapsed    | 3523     |
|    total_timesteps | 2264000  |
---------------------------------
Eval num_timesteps=2265000, episode_reward=-253.96 +/- 2.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 2265000  |
---------------------------------
Eval num_timesteps=2266000, episode_reward=918.84 +/- 46.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 2266000  |
| train/             |          |
|    actor_loss      | -0.812   |
|    critic_loss     | 7.99e-05 |
|    ent_coef        | 0.00031  |
|    ent_coef_loss   | -13.4    |
|    learning_rate   | 0.00273  |
|    n_updates       | 11060    |
---------------------------------
Eval num_timesteps=2267000, episode_reward=878.07 +/- 32.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 878      |
| time/              |          |
|    total_timesteps | 2267000  |
---------------------------------
Eval num_timesteps=2268000, episode_reward=-238.18 +/- 1.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 2268000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000796 |
|    ent_coef        | 0.000307 |
|    ent_coef_loss   | 5.3      |
|    learning_rate   | 0.00273  |
|    n_updates       | 11070    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 385      |
| time/              |          |
|    episodes        | 2268     |
|    fps             | 642      |
|    time_elapsed    | 3529     |
|    total_timesteps | 2268000  |
---------------------------------
Eval num_timesteps=2269000, episode_reward=-238.42 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 2269000  |
---------------------------------
Eval num_timesteps=2270000, episode_reward=-156.57 +/- 1.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 2270000  |
| train/             |          |
|    actor_loss      | -1.03    |
|    critic_loss     | 0.000556 |
|    ent_coef        | 0.000306 |
|    ent_coef_loss   | 2.69     |
|    learning_rate   | 0.00273  |
|    n_updates       | 11080    |
---------------------------------
Eval num_timesteps=2271000, episode_reward=-156.51 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 2271000  |
---------------------------------
Eval num_timesteps=2272000, episode_reward=-210.95 +/- 1.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 2272000  |
| train/             |          |
|    actor_loss      | -0.942   |
|    critic_loss     | 0.000605 |
|    ent_coef        | 0.000306 |
|    ent_coef_loss   | -13.1    |
|    learning_rate   | 0.00273  |
|    n_updates       | 11090    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 403      |
| time/              |          |
|    episodes        | 2272     |
|    fps             | 642      |
|    time_elapsed    | 3535     |
|    total_timesteps | 2272000  |
---------------------------------
Eval num_timesteps=2273000, episode_reward=-209.96 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 2273000  |
---------------------------------
Eval num_timesteps=2274000, episode_reward=-307.64 +/- 0.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 2274000  |
| train/             |          |
|    actor_loss      | -0.853   |
|    critic_loss     | 0.000125 |
|    ent_coef        | 0.000302 |
|    ent_coef_loss   | -14.5    |
|    learning_rate   | 0.00273  |
|    n_updates       | 11100    |
---------------------------------
Eval num_timesteps=2275000, episode_reward=-308.11 +/- 0.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 2275000  |
---------------------------------
Eval num_timesteps=2276000, episode_reward=460.68 +/- 8.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 461      |
| time/              |          |
|    total_timesteps | 2276000  |
| train/             |          |
|    actor_loss      | -0.855   |
|    critic_loss     | 0.000146 |
|    ent_coef        | 0.000297 |
|    ent_coef_loss   | -26.6    |
|    learning_rate   | 0.00272  |
|    n_updates       | 11110    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 403      |
| time/              |          |
|    episodes        | 2276     |
|    fps             | 642      |
|    time_elapsed    | 3541     |
|    total_timesteps | 2276000  |
---------------------------------
Eval num_timesteps=2277000, episode_reward=468.21 +/- 14.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 468      |
| time/              |          |
|    total_timesteps | 2277000  |
---------------------------------
Eval num_timesteps=2278000, episode_reward=-390.49 +/- 1.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -390     |
| time/              |          |
|    total_timesteps | 2278000  |
| train/             |          |
|    actor_loss      | -0.899   |
|    critic_loss     | 0.000771 |
|    ent_coef        | 0.000288 |
|    ent_coef_loss   | -2.05    |
|    learning_rate   | 0.00272  |
|    n_updates       | 11120    |
---------------------------------
Eval num_timesteps=2279000, episode_reward=-390.57 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -391     |
| time/              |          |
|    total_timesteps | 2279000  |
---------------------------------
Eval num_timesteps=2280000, episode_reward=-485.28 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -485     |
| time/              |          |
|    total_timesteps | 2280000  |
| train/             |          |
|    actor_loss      | -0.846   |
|    critic_loss     | 0.000219 |
|    ent_coef        | 0.000287 |
|    ent_coef_loss   | 69.4     |
|    learning_rate   | 0.00272  |
|    n_updates       | 11130    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 388      |
| time/              |          |
|    episodes        | 2280     |
|    fps             | 642      |
|    time_elapsed    | 3548     |
|    total_timesteps | 2280000  |
---------------------------------
Eval num_timesteps=2281000, episode_reward=-486.00 +/- 0.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 2281000  |
---------------------------------
Eval num_timesteps=2282000, episode_reward=-489.19 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -489     |
| time/              |          |
|    total_timesteps | 2282000  |
| train/             |          |
|    actor_loss      | -0.831   |
|    critic_loss     | 0.000194 |
|    ent_coef        | 0.000303 |
|    ent_coef_loss   | 40.4     |
|    learning_rate   | 0.00272  |
|    n_updates       | 11140    |
---------------------------------
Eval num_timesteps=2283000, episode_reward=-488.65 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -489     |
| time/              |          |
|    total_timesteps | 2283000  |
---------------------------------
Eval num_timesteps=2284000, episode_reward=-487.06 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -487     |
| time/              |          |
|    total_timesteps | 2284000  |
| train/             |          |
|    actor_loss      | -0.841   |
|    critic_loss     | 0.000135 |
|    ent_coef        | 0.00032  |
|    ent_coef_loss   | 13.6     |
|    learning_rate   | 0.00272  |
|    n_updates       | 11150    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 372      |
| time/              |          |
|    episodes        | 2284     |
|    fps             | 642      |
|    time_elapsed    | 3554     |
|    total_timesteps | 2284000  |
---------------------------------
Eval num_timesteps=2285000, episode_reward=-486.21 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -486     |
| time/              |          |
|    total_timesteps | 2285000  |
---------------------------------
Eval num_timesteps=2286000, episode_reward=-386.18 +/- 0.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 2286000  |
| train/             |          |
|    actor_loss      | -0.839   |
|    critic_loss     | 8.5e-05  |
|    ent_coef        | 0.00033  |
|    ent_coef_loss   | 11.3     |
|    learning_rate   | 0.00271  |
|    n_updates       | 11160    |
---------------------------------
Eval num_timesteps=2287000, episode_reward=-386.49 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -386     |
| time/              |          |
|    total_timesteps | 2287000  |
---------------------------------
Eval num_timesteps=2288000, episode_reward=-379.78 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -380     |
| time/              |          |
|    total_timesteps | 2288000  |
| train/             |          |
|    actor_loss      | -0.841   |
|    critic_loss     | 0.000121 |
|    ent_coef        | 0.000338 |
|    ent_coef_loss   | 14.1     |
|    learning_rate   | 0.00271  |
|    n_updates       | 11170    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 374      |
| time/              |          |
|    episodes        | 2288     |
|    fps             | 642      |
|    time_elapsed    | 3560     |
|    total_timesteps | 2288000  |
---------------------------------
Eval num_timesteps=2289000, episode_reward=-379.74 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -380     |
| time/              |          |
|    total_timesteps | 2289000  |
---------------------------------
Eval num_timesteps=2290000, episode_reward=-364.82 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -365     |
| time/              |          |
|    total_timesteps | 2290000  |
| train/             |          |
|    actor_loss      | -0.851   |
|    critic_loss     | 0.000108 |
|    ent_coef        | 0.000345 |
|    ent_coef_loss   | 17       |
|    learning_rate   | 0.00271  |
|    n_updates       | 11180    |
---------------------------------
Eval num_timesteps=2291000, episode_reward=-365.00 +/- 0.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -365     |
| time/              |          |
|    total_timesteps | 2291000  |
---------------------------------
Eval num_timesteps=2292000, episode_reward=-375.18 +/- 0.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -375     |
| time/              |          |
|    total_timesteps | 2292000  |
| train/             |          |
|    actor_loss      | -0.845   |
|    critic_loss     | 0.000115 |
|    ent_coef        | 0.000354 |
|    ent_coef_loss   | 35.2     |
|    learning_rate   | 0.00271  |
|    n_updates       | 11190    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 374      |
| time/              |          |
|    episodes        | 2292     |
|    fps             | 642      |
|    time_elapsed    | 3566     |
|    total_timesteps | 2292000  |
---------------------------------
Eval num_timesteps=2293000, episode_reward=-375.31 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -375     |
| time/              |          |
|    total_timesteps | 2293000  |
---------------------------------
Eval num_timesteps=2294000, episode_reward=-335.44 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 2294000  |
| train/             |          |
|    actor_loss      | -0.842   |
|    critic_loss     | 5.54e-05 |
|    ent_coef        | 0.000367 |
|    ent_coef_loss   | 19.6     |
|    learning_rate   | 0.00271  |
|    n_updates       | 11200    |
---------------------------------
Eval num_timesteps=2295000, episode_reward=-334.87 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 2295000  |
---------------------------------
Eval num_timesteps=2296000, episode_reward=-337.66 +/- 7.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -338     |
| time/              |          |
|    total_timesteps | 2296000  |
| train/             |          |
|    actor_loss      | -0.842   |
|    critic_loss     | 5.75e-05 |
|    ent_coef        | 0.000379 |
|    ent_coef_loss   | 21.8     |
|    learning_rate   | 0.0027   |
|    n_updates       | 11210    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 351      |
| time/              |          |
|    episodes        | 2296     |
|    fps             | 642      |
|    time_elapsed    | 3573     |
|    total_timesteps | 2296000  |
---------------------------------
Eval num_timesteps=2297000, episode_reward=-332.73 +/- 7.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -333     |
| time/              |          |
|    total_timesteps | 2297000  |
---------------------------------
Eval num_timesteps=2298000, episode_reward=-258.66 +/- 1.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 2298000  |
| train/             |          |
|    actor_loss      | -0.849   |
|    critic_loss     | 4.69e-05 |
|    ent_coef        | 0.000392 |
|    ent_coef_loss   | 18.8     |
|    learning_rate   | 0.0027   |
|    n_updates       | 11220    |
---------------------------------
Eval num_timesteps=2299000, episode_reward=-258.12 +/- 2.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 2299000  |
---------------------------------
Eval num_timesteps=2300000, episode_reward=-229.38 +/- 4.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 2300000  |
| train/             |          |
|    actor_loss      | -0.844   |
|    critic_loss     | 6.69e-05 |
|    ent_coef        | 0.000403 |
|    ent_coef_loss   | 20.1     |
|    learning_rate   | 0.0027   |
|    n_updates       | 11230    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 323      |
| time/              |          |
|    episodes        | 2300     |
|    fps             | 642      |
|    time_elapsed    | 3579     |
|    total_timesteps | 2300000  |
---------------------------------
Eval num_timesteps=2301000, episode_reward=-231.12 +/- 7.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 2301000  |
---------------------------------
Eval num_timesteps=2302000, episode_reward=-266.83 +/- 1.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 2302000  |
| train/             |          |
|    actor_loss      | -0.85    |
|    critic_loss     | 6.26e-05 |
|    ent_coef        | 0.000414 |
|    ent_coef_loss   | 14.9     |
|    learning_rate   | 0.0027   |
|    n_updates       | 11240    |
---------------------------------
Eval num_timesteps=2303000, episode_reward=-267.42 +/- 2.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 2303000  |
---------------------------------
Eval num_timesteps=2304000, episode_reward=-268.53 +/- 2.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -269     |
| time/              |          |
|    total_timesteps | 2304000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 296      |
| time/              |          |
|    episodes        | 2304     |
|    fps             | 642      |
|    time_elapsed    | 3585     |
|    total_timesteps | 2304000  |
---------------------------------
Eval num_timesteps=2305000, episode_reward=-380.62 +/- 30.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -381     |
| time/              |          |
|    total_timesteps | 2305000  |
| train/             |          |
|    actor_loss      | -0.842   |
|    critic_loss     | 0.000102 |
|    ent_coef        | 0.000425 |
|    ent_coef_loss   | 22.4     |
|    learning_rate   | 0.0027   |
|    n_updates       | 11250    |
---------------------------------
Eval num_timesteps=2306000, episode_reward=-393.77 +/- 25.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -394     |
| time/              |          |
|    total_timesteps | 2306000  |
---------------------------------
Eval num_timesteps=2307000, episode_reward=-368.80 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 2307000  |
| train/             |          |
|    actor_loss      | -0.854   |
|    critic_loss     | 6.65e-05 |
|    ent_coef        | 0.000437 |
|    ent_coef_loss   | 23.1     |
|    learning_rate   | 0.00269  |
|    n_updates       | 11260    |
---------------------------------
Eval num_timesteps=2308000, episode_reward=-367.58 +/- 1.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 2308000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 262      |
| time/              |          |
|    episodes        | 2308     |
|    fps             | 642      |
|    time_elapsed    | 3591     |
|    total_timesteps | 2308000  |
---------------------------------
Eval num_timesteps=2309000, episode_reward=-402.27 +/- 0.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 2309000  |
| train/             |          |
|    actor_loss      | -0.852   |
|    critic_loss     | 6.31e-05 |
|    ent_coef        | 0.000451 |
|    ent_coef_loss   | 22.8     |
|    learning_rate   | 0.00269  |
|    n_updates       | 11270    |
---------------------------------
Eval num_timesteps=2310000, episode_reward=-402.05 +/- 1.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -402     |
| time/              |          |
|    total_timesteps | 2310000  |
---------------------------------
Eval num_timesteps=2311000, episode_reward=-406.89 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 2311000  |
| train/             |          |
|    actor_loss      | -0.849   |
|    critic_loss     | 5.08e-05 |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | 19.9     |
|    learning_rate   | 0.00269  |
|    n_updates       | 11280    |
---------------------------------
Eval num_timesteps=2312000, episode_reward=-406.91 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -407     |
| time/              |          |
|    total_timesteps | 2312000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 236      |
| time/              |          |
|    episodes        | 2312     |
|    fps             | 642      |
|    time_elapsed    | 3597     |
|    total_timesteps | 2312000  |
---------------------------------
Eval num_timesteps=2313000, episode_reward=-335.90 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 2313000  |
| train/             |          |
|    actor_loss      | -0.848   |
|    critic_loss     | 4.33e-05 |
|    ent_coef        | 0.00048  |
|    ent_coef_loss   | 12.5     |
|    learning_rate   | 0.00269  |
|    n_updates       | 11290    |
---------------------------------
Eval num_timesteps=2314000, episode_reward=-336.39 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 2314000  |
---------------------------------
Eval num_timesteps=2315000, episode_reward=-339.74 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 2315000  |
| train/             |          |
|    actor_loss      | -0.84    |
|    critic_loss     | 6.07e-05 |
|    ent_coef        | 0.00049  |
|    ent_coef_loss   | 6.32     |
|    learning_rate   | 0.00269  |
|    n_updates       | 11300    |
---------------------------------
Eval num_timesteps=2316000, episode_reward=-338.70 +/- 1.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 2316000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 207      |
| time/              |          |
|    episodes        | 2316     |
|    fps             | 642      |
|    time_elapsed    | 3604     |
|    total_timesteps | 2316000  |
---------------------------------
Eval num_timesteps=2317000, episode_reward=-266.19 +/- 14.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 2317000  |
| train/             |          |
|    actor_loss      | -0.846   |
|    critic_loss     | 6.37e-05 |
|    ent_coef        | 0.000497 |
|    ent_coef_loss   | 6.64     |
|    learning_rate   | 0.00268  |
|    n_updates       | 11310    |
---------------------------------
Eval num_timesteps=2318000, episode_reward=-257.43 +/- 6.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -257     |
| time/              |          |
|    total_timesteps | 2318000  |
---------------------------------
Eval num_timesteps=2319000, episode_reward=-240.92 +/- 5.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -241     |
| time/              |          |
|    total_timesteps | 2319000  |
| train/             |          |
|    actor_loss      | -0.842   |
|    critic_loss     | 9e-05    |
|    ent_coef        | 0.000502 |
|    ent_coef_loss   | 12.4     |
|    learning_rate   | 0.00268  |
|    n_updates       | 11320    |
---------------------------------
Eval num_timesteps=2320000, episode_reward=-243.74 +/- 4.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 2320000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 177      |
| time/              |          |
|    episodes        | 2320     |
|    fps             | 642      |
|    time_elapsed    | 3610     |
|    total_timesteps | 2320000  |
---------------------------------
Eval num_timesteps=2321000, episode_reward=-318.10 +/- 20.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -318     |
| time/              |          |
|    total_timesteps | 2321000  |
| train/             |          |
|    actor_loss      | -0.836   |
|    critic_loss     | 0.000114 |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | 16.5     |
|    learning_rate   | 0.00268  |
|    n_updates       | 11330    |
---------------------------------
Eval num_timesteps=2322000, episode_reward=-335.07 +/- 9.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 2322000  |
---------------------------------
Eval num_timesteps=2323000, episode_reward=-231.33 +/- 2.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 2323000  |
| train/             |          |
|    actor_loss      | -0.841   |
|    critic_loss     | 8.27e-05 |
|    ent_coef        | 0.000521 |
|    ent_coef_loss   | 10.8     |
|    learning_rate   | 0.00268  |
|    n_updates       | 11340    |
---------------------------------
Eval num_timesteps=2324000, episode_reward=-231.06 +/- 8.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 2324000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 154      |
| time/              |          |
|    episodes        | 2324     |
|    fps             | 642      |
|    time_elapsed    | 3616     |
|    total_timesteps | 2324000  |
---------------------------------
Eval num_timesteps=2325000, episode_reward=-203.29 +/- 6.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 2325000  |
| train/             |          |
|    actor_loss      | -0.832   |
|    critic_loss     | 0.000129 |
|    ent_coef        | 0.000531 |
|    ent_coef_loss   | 16       |
|    learning_rate   | 0.00268  |
|    n_updates       | 11350    |
---------------------------------
Eval num_timesteps=2326000, episode_reward=-205.54 +/- 11.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -206     |
| time/              |          |
|    total_timesteps | 2326000  |
---------------------------------
Eval num_timesteps=2327000, episode_reward=-220.64 +/- 8.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -221     |
| time/              |          |
|    total_timesteps | 2327000  |
| train/             |          |
|    actor_loss      | -0.832   |
|    critic_loss     | 0.000145 |
|    ent_coef        | 0.000542 |
|    ent_coef_loss   | 11.4     |
|    learning_rate   | 0.00267  |
|    n_updates       | 11360    |
---------------------------------
Eval num_timesteps=2328000, episode_reward=-217.08 +/- 6.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -217     |
| time/              |          |
|    total_timesteps | 2328000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 121      |
| time/              |          |
|    episodes        | 2328     |
|    fps             | 642      |
|    time_elapsed    | 3622     |
|    total_timesteps | 2328000  |
---------------------------------
Eval num_timesteps=2329000, episode_reward=-205.18 +/- 8.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -205     |
| time/              |          |
|    total_timesteps | 2329000  |
| train/             |          |
|    actor_loss      | -0.83    |
|    critic_loss     | 0.000151 |
|    ent_coef        | 0.000553 |
|    ent_coef_loss   | 9.82     |
|    learning_rate   | 0.00267  |
|    n_updates       | 11370    |
---------------------------------
Eval num_timesteps=2330000, episode_reward=-190.92 +/- 10.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 2330000  |
---------------------------------
Eval num_timesteps=2331000, episode_reward=-235.38 +/- 54.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 2331000  |
| train/             |          |
|    actor_loss      | -0.829   |
|    critic_loss     | 0.000175 |
|    ent_coef        | 0.000562 |
|    ent_coef_loss   | 8.34     |
|    learning_rate   | 0.00267  |
|    n_updates       | 11380    |
---------------------------------
Eval num_timesteps=2332000, episode_reward=-183.13 +/- 13.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -183     |
| time/              |          |
|    total_timesteps | 2332000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 83.9     |
| time/              |          |
|    episodes        | 2332     |
|    fps             | 642      |
|    time_elapsed    | 3629     |
|    total_timesteps | 2332000  |
---------------------------------
Eval num_timesteps=2333000, episode_reward=-196.74 +/- 2.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 2333000  |
| train/             |          |
|    actor_loss      | -0.817   |
|    critic_loss     | 0.000283 |
|    ent_coef        | 0.000569 |
|    ent_coef_loss   | 10.4     |
|    learning_rate   | 0.00267  |
|    n_updates       | 11390    |
---------------------------------
Eval num_timesteps=2334000, episode_reward=-198.97 +/- 3.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -199     |
| time/              |          |
|    total_timesteps | 2334000  |
---------------------------------
Eval num_timesteps=2335000, episode_reward=-153.67 +/- 56.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 2335000  |
| train/             |          |
|    actor_loss      | -0.835   |
|    critic_loss     | 0.000127 |
|    ent_coef        | 0.000576 |
|    ent_coef_loss   | -5.57    |
|    learning_rate   | 0.00267  |
|    n_updates       | 11400    |
---------------------------------
Eval num_timesteps=2336000, episode_reward=-143.68 +/- 55.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 2336000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 58.3     |
| time/              |          |
|    episodes        | 2336     |
|    fps             | 642      |
|    time_elapsed    | 3635     |
|    total_timesteps | 2336000  |
---------------------------------
Eval num_timesteps=2337000, episode_reward=4.18 +/- 12.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 4.18     |
| time/              |          |
|    total_timesteps | 2337000  |
| train/             |          |
|    actor_loss      | -0.82    |
|    critic_loss     | 0.000492 |
|    ent_coef        | 0.000577 |
|    ent_coef_loss   | -6.2     |
|    learning_rate   | 0.00266  |
|    n_updates       | 11410    |
---------------------------------
Eval num_timesteps=2338000, episode_reward=-0.78 +/- 11.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.779   |
| time/              |          |
|    total_timesteps | 2338000  |
---------------------------------
Eval num_timesteps=2339000, episode_reward=-104.02 +/- 15.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 2339000  |
| train/             |          |
|    actor_loss      | -0.807   |
|    critic_loss     | 0.000656 |
|    ent_coef        | 0.000571 |
|    ent_coef_loss   | -21.3    |
|    learning_rate   | 0.00266  |
|    n_updates       | 11420    |
---------------------------------
Eval num_timesteps=2340000, episode_reward=-115.30 +/- 7.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -115     |
| time/              |          |
|    total_timesteps | 2340000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 28.7     |
| time/              |          |
|    episodes        | 2340     |
|    fps             | 642      |
|    time_elapsed    | 3641     |
|    total_timesteps | 2340000  |
---------------------------------
Eval num_timesteps=2341000, episode_reward=-350.71 +/- 1.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -351     |
| time/              |          |
|    total_timesteps | 2341000  |
| train/             |          |
|    actor_loss      | -0.785   |
|    critic_loss     | 0.000551 |
|    ent_coef        | 0.000558 |
|    ent_coef_loss   | -11.9    |
|    learning_rate   | 0.00266  |
|    n_updates       | 11430    |
---------------------------------
Eval num_timesteps=2342000, episode_reward=-350.96 +/- 1.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -351     |
| time/              |          |
|    total_timesteps | 2342000  |
---------------------------------
Eval num_timesteps=2343000, episode_reward=-168.18 +/- 2.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 2343000  |
| train/             |          |
|    actor_loss      | -0.775   |
|    critic_loss     | 0.000174 |
|    ent_coef        | 0.000548 |
|    ent_coef_loss   | -12.2    |
|    learning_rate   | 0.00266  |
|    n_updates       | 11440    |
---------------------------------
Eval num_timesteps=2344000, episode_reward=-167.95 +/- 2.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 2344000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -2.8     |
| time/              |          |
|    episodes        | 2344     |
|    fps             | 642      |
|    time_elapsed    | 3647     |
|    total_timesteps | 2344000  |
---------------------------------
Eval num_timesteps=2345000, episode_reward=64.69 +/- 285.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 64.7     |
| time/              |          |
|    total_timesteps | 2345000  |
| train/             |          |
|    actor_loss      | -0.831   |
|    critic_loss     | 0.000958 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | -12.1    |
|    learning_rate   | 0.00266  |
|    n_updates       | 11450    |
---------------------------------
Eval num_timesteps=2346000, episode_reward=-59.75 +/- 41.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -59.7    |
| time/              |          |
|    total_timesteps | 2346000  |
---------------------------------
Eval num_timesteps=2347000, episode_reward=-87.12 +/- 29.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -87.1    |
| time/              |          |
|    total_timesteps | 2347000  |
---------------------------------
Eval num_timesteps=2348000, episode_reward=892.85 +/- 54.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 2348000  |
| train/             |          |
|    actor_loss      | -0.89    |
|    critic_loss     | 0.00153  |
|    ent_coef        | 0.000528 |
|    ent_coef_loss   | -8.72    |
|    learning_rate   | 0.00265  |
|    n_updates       | 11460    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -22.7    |
| time/              |          |
|    episodes        | 2348     |
|    fps             | 642      |
|    time_elapsed    | 3653     |
|    total_timesteps | 2348000  |
---------------------------------
Eval num_timesteps=2349000, episode_reward=895.32 +/- 29.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 895      |
| time/              |          |
|    total_timesteps | 2349000  |
---------------------------------
Eval num_timesteps=2350000, episode_reward=174.19 +/- 163.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 174      |
| time/              |          |
|    total_timesteps | 2350000  |
| train/             |          |
|    actor_loss      | -0.982   |
|    critic_loss     | 0.000791 |
|    ent_coef        | 0.000521 |
|    ent_coef_loss   | -2.22    |
|    learning_rate   | 0.00265  |
|    n_updates       | 11470    |
---------------------------------
Eval num_timesteps=2351000, episode_reward=-72.07 +/- 33.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -72.1    |
| time/              |          |
|    total_timesteps | 2351000  |
---------------------------------
Eval num_timesteps=2352000, episode_reward=-125.38 +/- 4.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 2352000  |
| train/             |          |
|    actor_loss      | -0.864   |
|    critic_loss     | 0.000843 |
|    ent_coef        | 0.000514 |
|    ent_coef_loss   | -25.3    |
|    learning_rate   | 0.00265  |
|    n_updates       | 11480    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -40.8    |
| time/              |          |
|    episodes        | 2352     |
|    fps             | 642      |
|    time_elapsed    | 3660     |
|    total_timesteps | 2352000  |
---------------------------------
Eval num_timesteps=2353000, episode_reward=-126.33 +/- 4.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 2353000  |
---------------------------------
Eval num_timesteps=2354000, episode_reward=-236.20 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -236     |
| time/              |          |
|    total_timesteps | 2354000  |
| train/             |          |
|    actor_loss      | -0.771   |
|    critic_loss     | 0.000297 |
|    ent_coef        | 0.000501 |
|    ent_coef_loss   | -20.4    |
|    learning_rate   | 0.00265  |
|    n_updates       | 11490    |
---------------------------------
Eval num_timesteps=2355000, episode_reward=-234.77 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 2355000  |
---------------------------------
Eval num_timesteps=2356000, episode_reward=-305.92 +/- 47.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 2356000  |
| train/             |          |
|    actor_loss      | -0.739   |
|    critic_loss     | 0.000352 |
|    ent_coef        | 0.000487 |
|    ent_coef_loss   | -15.3    |
|    learning_rate   | 0.00264  |
|    n_updates       | 11500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -81.3    |
| time/              |          |
|    episodes        | 2356     |
|    fps             | 642      |
|    time_elapsed    | 3666     |
|    total_timesteps | 2356000  |
---------------------------------
Eval num_timesteps=2357000, episode_reward=-211.72 +/- 170.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 2357000  |
---------------------------------
Eval num_timesteps=2358000, episode_reward=588.88 +/- 104.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 2358000  |
| train/             |          |
|    actor_loss      | -0.75    |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000474 |
|    ent_coef_loss   | -18.7    |
|    learning_rate   | 0.00264  |
|    n_updates       | 11510    |
---------------------------------
Eval num_timesteps=2359000, episode_reward=648.88 +/- 73.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 2359000  |
---------------------------------
Eval num_timesteps=2360000, episode_reward=775.48 +/- 31.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 775      |
| time/              |          |
|    total_timesteps | 2360000  |
| train/             |          |
|    actor_loss      | -0.953   |
|    critic_loss     | 0.000691 |
|    ent_coef        | 0.000462 |
|    ent_coef_loss   | -12.6    |
|    learning_rate   | 0.00264  |
|    n_updates       | 11520    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -101     |
| time/              |          |
|    episodes        | 2360     |
|    fps             | 642      |
|    time_elapsed    | 3672     |
|    total_timesteps | 2360000  |
---------------------------------
Eval num_timesteps=2361000, episode_reward=340.18 +/- 550.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 340      |
| time/              |          |
|    total_timesteps | 2361000  |
---------------------------------
Eval num_timesteps=2362000, episode_reward=818.09 +/- 14.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 818      |
| time/              |          |
|    total_timesteps | 2362000  |
| train/             |          |
|    actor_loss      | -0.991   |
|    critic_loss     | 0.000629 |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | -0.00741 |
|    learning_rate   | 0.00264  |
|    n_updates       | 11530    |
---------------------------------
Eval num_timesteps=2363000, episode_reward=809.79 +/- 56.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 810      |
| time/              |          |
|    total_timesteps | 2363000  |
---------------------------------
Eval num_timesteps=2364000, episode_reward=-162.15 +/- 84.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -162     |
| time/              |          |
|    total_timesteps | 2364000  |
| train/             |          |
|    actor_loss      | -0.994   |
|    critic_loss     | 0.000556 |
|    ent_coef        | 0.000451 |
|    ent_coef_loss   | 2.93     |
|    learning_rate   | 0.00264  |
|    n_updates       | 11540    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -87.5    |
| time/              |          |
|    episodes        | 2364     |
|    fps             | 642      |
|    time_elapsed    | 3678     |
|    total_timesteps | 2364000  |
---------------------------------
Eval num_timesteps=2365000, episode_reward=-219.43 +/- 37.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 2365000  |
---------------------------------
Eval num_timesteps=2366000, episode_reward=-146.90 +/- 10.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 2366000  |
| train/             |          |
|    actor_loss      | -0.826   |
|    critic_loss     | 0.000981 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | -8.43    |
|    learning_rate   | 0.00263  |
|    n_updates       | 11550    |
---------------------------------
Eval num_timesteps=2367000, episode_reward=-157.09 +/- 7.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 2367000  |
---------------------------------
Eval num_timesteps=2368000, episode_reward=234.68 +/- 18.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 235      |
| time/              |          |
|    total_timesteps | 2368000  |
| train/             |          |
|    actor_loss      | -0.826   |
|    critic_loss     | 0.000626 |
|    ent_coef        | 0.000447 |
|    ent_coef_loss   | -6.64    |
|    learning_rate   | 0.00263  |
|    n_updates       | 11560    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -107     |
| time/              |          |
|    episodes        | 2368     |
|    fps             | 642      |
|    time_elapsed    | 3684     |
|    total_timesteps | 2368000  |
---------------------------------
Eval num_timesteps=2369000, episode_reward=244.20 +/- 25.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 244      |
| time/              |          |
|    total_timesteps | 2369000  |
---------------------------------
Eval num_timesteps=2370000, episode_reward=-237.76 +/- 11.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 2370000  |
| train/             |          |
|    actor_loss      | -0.78    |
|    critic_loss     | 0.000613 |
|    ent_coef        | 0.000442 |
|    ent_coef_loss   | -17.5    |
|    learning_rate   | 0.00263  |
|    n_updates       | 11570    |
---------------------------------
Eval num_timesteps=2371000, episode_reward=-245.08 +/- 13.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -245     |
| time/              |          |
|    total_timesteps | 2371000  |
---------------------------------
Eval num_timesteps=2372000, episode_reward=-81.95 +/- 62.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -82      |
| time/              |          |
|    total_timesteps | 2372000  |
| train/             |          |
|    actor_loss      | -0.753   |
|    critic_loss     | 0.000193 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | -15.8    |
|    learning_rate   | 0.00263  |
|    n_updates       | 11580    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -128     |
| time/              |          |
|    episodes        | 2372     |
|    fps             | 642      |
|    time_elapsed    | 3691     |
|    total_timesteps | 2372000  |
---------------------------------
Eval num_timesteps=2373000, episode_reward=-97.13 +/- 72.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -97.1    |
| time/              |          |
|    total_timesteps | 2373000  |
---------------------------------
Eval num_timesteps=2374000, episode_reward=-294.81 +/- 2.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 2374000  |
| train/             |          |
|    actor_loss      | -0.763   |
|    critic_loss     | 0.000204 |
|    ent_coef        | 0.000425 |
|    ent_coef_loss   | -8.83    |
|    learning_rate   | 0.00263  |
|    n_updates       | 11590    |
---------------------------------
Eval num_timesteps=2375000, episode_reward=-295.03 +/- 2.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -295     |
| time/              |          |
|    total_timesteps | 2375000  |
---------------------------------
Eval num_timesteps=2376000, episode_reward=-253.17 +/- 23.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 2376000  |
| train/             |          |
|    actor_loss      | -0.753   |
|    critic_loss     | 0.000117 |
|    ent_coef        | 0.000418 |
|    ent_coef_loss   | -16.2    |
|    learning_rate   | 0.00262  |
|    n_updates       | 11600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -126     |
| time/              |          |
|    episodes        | 2376     |
|    fps             | 642      |
|    time_elapsed    | 3697     |
|    total_timesteps | 2376000  |
---------------------------------
Eval num_timesteps=2377000, episode_reward=-239.76 +/- 30.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -240     |
| time/              |          |
|    total_timesteps | 2377000  |
---------------------------------
Eval num_timesteps=2378000, episode_reward=-261.40 +/- 1.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -261     |
| time/              |          |
|    total_timesteps | 2378000  |
| train/             |          |
|    actor_loss      | -0.705   |
|    critic_loss     | 0.000301 |
|    ent_coef        | 0.00041  |
|    ent_coef_loss   | 2.41     |
|    learning_rate   | 0.00262  |
|    n_updates       | 11610    |
---------------------------------
Eval num_timesteps=2379000, episode_reward=-261.62 +/- 1.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -262     |
| time/              |          |
|    total_timesteps | 2379000  |
---------------------------------
Eval num_timesteps=2380000, episode_reward=-272.38 +/- 7.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -272     |
| time/              |          |
|    total_timesteps | 2380000  |
| train/             |          |
|    actor_loss      | -0.749   |
|    critic_loss     | 0.000265 |
|    ent_coef        | 0.000406 |
|    ent_coef_loss   | -27.5    |
|    learning_rate   | 0.00262  |
|    n_updates       | 11620    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -134     |
| time/              |          |
|    episodes        | 2380     |
|    fps             | 642      |
|    time_elapsed    | 3703     |
|    total_timesteps | 2380000  |
---------------------------------
Eval num_timesteps=2381000, episode_reward=-263.73 +/- 5.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 2381000  |
---------------------------------
Eval num_timesteps=2382000, episode_reward=-386.96 +/- 2.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -387     |
| time/              |          |
|    total_timesteps | 2382000  |
| train/             |          |
|    actor_loss      | -0.718   |
|    critic_loss     | 0.000599 |
|    ent_coef        | 0.000396 |
|    ent_coef_loss   | -12.4    |
|    learning_rate   | 0.00262  |
|    n_updates       | 11630    |
---------------------------------
Eval num_timesteps=2383000, episode_reward=-385.46 +/- 1.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -385     |
| time/              |          |
|    total_timesteps | 2383000  |
---------------------------------
Eval num_timesteps=2384000, episode_reward=-453.61 +/- 2.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -454     |
| time/              |          |
|    total_timesteps | 2384000  |
| train/             |          |
|    actor_loss      | -0.705   |
|    critic_loss     | 0.000289 |
|    ent_coef        | 0.00039  |
|    ent_coef_loss   | 29.4     |
|    learning_rate   | 0.00262  |
|    n_updates       | 11640    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -129     |
| time/              |          |
|    episodes        | 2384     |
|    fps             | 642      |
|    time_elapsed    | 3709     |
|    total_timesteps | 2384000  |
---------------------------------
Eval num_timesteps=2385000, episode_reward=-452.51 +/- 1.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -453     |
| time/              |          |
|    total_timesteps | 2385000  |
---------------------------------
Eval num_timesteps=2386000, episode_reward=-442.72 +/- 2.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -443     |
| time/              |          |
|    total_timesteps | 2386000  |
| train/             |          |
|    actor_loss      | -0.809   |
|    critic_loss     | 0.000425 |
|    ent_coef        | 0.000397 |
|    ent_coef_loss   | 19.3     |
|    learning_rate   | 0.00261  |
|    n_updates       | 11650    |
---------------------------------
Eval num_timesteps=2387000, episode_reward=-442.29 +/- 2.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 2387000  |
---------------------------------
Eval num_timesteps=2388000, episode_reward=-373.56 +/- 0.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 2388000  |
| train/             |          |
|    actor_loss      | -0.688   |
|    critic_loss     | 0.000101 |
|    ent_coef        | 0.000406 |
|    ent_coef_loss   | 0.0567   |
|    learning_rate   | 0.00261  |
|    n_updates       | 11660    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -122     |
| time/              |          |
|    episodes        | 2388     |
|    fps             | 642      |
|    time_elapsed    | 3715     |
|    total_timesteps | 2388000  |
---------------------------------
Eval num_timesteps=2389000, episode_reward=-371.35 +/- 1.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 2389000  |
---------------------------------
Eval num_timesteps=2390000, episode_reward=-373.24 +/- 1.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 2390000  |
---------------------------------
Eval num_timesteps=2391000, episode_reward=-413.27 +/- 2.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -413     |
| time/              |          |
|    total_timesteps | 2391000  |
| train/             |          |
|    actor_loss      | -0.696   |
|    critic_loss     | 0.000104 |
|    ent_coef        | 0.000409 |
|    ent_coef_loss   | -12.6    |
|    learning_rate   | 0.00261  |
|    n_updates       | 11670    |
---------------------------------
Eval num_timesteps=2392000, episode_reward=-228.35 +/- 365.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 2392000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -125     |
| time/              |          |
|    episodes        | 2392     |
|    fps             | 642      |
|    time_elapsed    | 3722     |
|    total_timesteps | 2392000  |
---------------------------------
Eval num_timesteps=2393000, episode_reward=-364.15 +/- 2.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 2393000  |
| train/             |          |
|    actor_loss      | -0.683   |
|    critic_loss     | 0.000118 |
|    ent_coef        | 0.000406 |
|    ent_coef_loss   | -5.76    |
|    learning_rate   | 0.00261  |
|    n_updates       | 11680    |
---------------------------------
Eval num_timesteps=2394000, episode_reward=-363.81 +/- 3.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -364     |
| time/              |          |
|    total_timesteps | 2394000  |
---------------------------------
Eval num_timesteps=2395000, episode_reward=-474.83 +/- 1.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -475     |
| time/              |          |
|    total_timesteps | 2395000  |
| train/             |          |
|    actor_loss      | -0.809   |
|    critic_loss     | 0.000458 |
|    ent_coef        | 0.000402 |
|    ent_coef_loss   | -4.71    |
|    learning_rate   | 0.00261  |
|    n_updates       | 11690    |
---------------------------------
Eval num_timesteps=2396000, episode_reward=-476.41 +/- 3.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -476     |
| time/              |          |
|    total_timesteps | 2396000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -110     |
| time/              |          |
|    episodes        | 2396     |
|    fps             | 642      |
|    time_elapsed    | 3728     |
|    total_timesteps | 2396000  |
---------------------------------
Eval num_timesteps=2397000, episode_reward=803.97 +/- 39.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 804      |
| time/              |          |
|    total_timesteps | 2397000  |
| train/             |          |
|    actor_loss      | -0.792   |
|    critic_loss     | 0.000283 |
|    ent_coef        | 0.000399 |
|    ent_coef_loss   | 7.64     |
|    learning_rate   | 0.0026   |
|    n_updates       | 11700    |
---------------------------------
Eval num_timesteps=2398000, episode_reward=806.56 +/- 32.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 807      |
| time/              |          |
|    total_timesteps | 2398000  |
---------------------------------
Eval num_timesteps=2399000, episode_reward=491.49 +/- 61.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 491      |
| time/              |          |
|    total_timesteps | 2399000  |
| train/             |          |
|    actor_loss      | -0.834   |
|    critic_loss     | 0.00035  |
|    ent_coef        | 0.000402 |
|    ent_coef_loss   | 12       |
|    learning_rate   | 0.0026   |
|    n_updates       | 11710    |
---------------------------------
Eval num_timesteps=2400000, episode_reward=504.34 +/- 41.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 504      |
| time/              |          |
|    total_timesteps | 2400000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -87.8    |
| time/              |          |
|    episodes        | 2400     |
|    fps             | 642      |
|    time_elapsed    | 3734     |
|    total_timesteps | 2400000  |
---------------------------------
Eval num_timesteps=2401000, episode_reward=669.68 +/- 71.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 670      |
| time/              |          |
|    total_timesteps | 2401000  |
| train/             |          |
|    actor_loss      | -0.934   |
|    critic_loss     | 0.000583 |
|    ent_coef        | 0.000406 |
|    ent_coef_loss   | -3.12    |
|    learning_rate   | 0.0026   |
|    n_updates       | 11720    |
---------------------------------
Eval num_timesteps=2402000, episode_reward=742.84 +/- 32.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 743      |
| time/              |          |
|    total_timesteps | 2402000  |
---------------------------------
Eval num_timesteps=2403000, episode_reward=550.81 +/- 51.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 551      |
| time/              |          |
|    total_timesteps | 2403000  |
| train/             |          |
|    actor_loss      | -0.922   |
|    critic_loss     | 0.000462 |
|    ent_coef        | 0.000407 |
|    ent_coef_loss   | -1.01    |
|    learning_rate   | 0.0026   |
|    n_updates       | 11730    |
---------------------------------
Eval num_timesteps=2404000, episode_reward=546.39 +/- 27.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 546      |
| time/              |          |
|    total_timesteps | 2404000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -63.5    |
| time/              |          |
|    episodes        | 2404     |
|    fps             | 642      |
|    time_elapsed    | 3740     |
|    total_timesteps | 2404000  |
---------------------------------
Eval num_timesteps=2405000, episode_reward=701.25 +/- 35.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 2405000  |
| train/             |          |
|    actor_loss      | -0.835   |
|    critic_loss     | 0.000379 |
|    ent_coef        | 0.000407 |
|    ent_coef_loss   | -0.301   |
|    learning_rate   | 0.0026   |
|    n_updates       | 11740    |
---------------------------------
Eval num_timesteps=2406000, episode_reward=725.60 +/- 59.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 2406000  |
---------------------------------
Eval num_timesteps=2407000, episode_reward=527.08 +/- 458.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 527      |
| time/              |          |
|    total_timesteps | 2407000  |
| train/             |          |
|    actor_loss      | -0.903   |
|    critic_loss     | 0.000437 |
|    ent_coef        | 0.000407 |
|    ent_coef_loss   | -1.84    |
|    learning_rate   | 0.00259  |
|    n_updates       | 11750    |
---------------------------------
Eval num_timesteps=2408000, episode_reward=763.64 +/- 49.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 2408000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -34.5    |
| time/              |          |
|    episodes        | 2408     |
|    fps             | 642      |
|    time_elapsed    | 3746     |
|    total_timesteps | 2408000  |
---------------------------------
Eval num_timesteps=2409000, episode_reward=769.65 +/- 74.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 770      |
| time/              |          |
|    total_timesteps | 2409000  |
| train/             |          |
|    actor_loss      | -0.874   |
|    critic_loss     | 0.000371 |
|    ent_coef        | 0.000406 |
|    ent_coef_loss   | 2.27     |
|    learning_rate   | 0.00259  |
|    n_updates       | 11760    |
---------------------------------
Eval num_timesteps=2410000, episode_reward=745.35 +/- 49.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 745      |
| time/              |          |
|    total_timesteps | 2410000  |
---------------------------------
Eval num_timesteps=2411000, episode_reward=520.21 +/- 63.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 520      |
| time/              |          |
|    total_timesteps | 2411000  |
| train/             |          |
|    actor_loss      | -0.962   |
|    critic_loss     | 0.000437 |
|    ent_coef        | 0.000406 |
|    ent_coef_loss   | -6.55    |
|    learning_rate   | 0.00259  |
|    n_updates       | 11770    |
---------------------------------
Eval num_timesteps=2412000, episode_reward=570.30 +/- 49.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 570      |
| time/              |          |
|    total_timesteps | 2412000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 4.12     |
| time/              |          |
|    episodes        | 2412     |
|    fps             | 642      |
|    time_elapsed    | 3752     |
|    total_timesteps | 2412000  |
---------------------------------
Eval num_timesteps=2413000, episode_reward=335.82 +/- 57.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 336      |
| time/              |          |
|    total_timesteps | 2413000  |
| train/             |          |
|    actor_loss      | -0.934   |
|    critic_loss     | 0.000459 |
|    ent_coef        | 0.000405 |
|    ent_coef_loss   | 0.374    |
|    learning_rate   | 0.00259  |
|    n_updates       | 11780    |
---------------------------------
Eval num_timesteps=2414000, episode_reward=371.80 +/- 103.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 372      |
| time/              |          |
|    total_timesteps | 2414000  |
---------------------------------
Eval num_timesteps=2415000, episode_reward=404.82 +/- 375.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 405      |
| time/              |          |
|    total_timesteps | 2415000  |
| train/             |          |
|    actor_loss      | -0.904   |
|    critic_loss     | 0.00054  |
|    ent_coef        | 0.000404 |
|    ent_coef_loss   | -0.71    |
|    learning_rate   | 0.00259  |
|    n_updates       | 11790    |
---------------------------------
Eval num_timesteps=2416000, episode_reward=614.51 +/- 76.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 615      |
| time/              |          |
|    total_timesteps | 2416000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 30.8     |
| time/              |          |
|    episodes        | 2416     |
|    fps             | 642      |
|    time_elapsed    | 3758     |
|    total_timesteps | 2416000  |
---------------------------------
Eval num_timesteps=2417000, episode_reward=471.34 +/- 416.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 471      |
| time/              |          |
|    total_timesteps | 2417000  |
| train/             |          |
|    actor_loss      | -0.939   |
|    critic_loss     | 0.000446 |
|    ent_coef        | 0.000403 |
|    ent_coef_loss   | -0.509   |
|    learning_rate   | 0.00258  |
|    n_updates       | 11800    |
---------------------------------
Eval num_timesteps=2418000, episode_reward=51.23 +/- 498.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 51.2     |
| time/              |          |
|    total_timesteps | 2418000  |
---------------------------------
Eval num_timesteps=2419000, episode_reward=269.46 +/- 506.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 269      |
| time/              |          |
|    total_timesteps | 2419000  |
| train/             |          |
|    actor_loss      | -0.951   |
|    critic_loss     | 0.000464 |
|    ent_coef        | 0.000403 |
|    ent_coef_loss   | -5.8     |
|    learning_rate   | 0.00258  |
|    n_updates       | 11810    |
---------------------------------
Eval num_timesteps=2420000, episode_reward=-172.15 +/- 359.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -172     |
| time/              |          |
|    total_timesteps | 2420000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 62.9     |
| time/              |          |
|    episodes        | 2420     |
|    fps             | 642      |
|    time_elapsed    | 3765     |
|    total_timesteps | 2420000  |
---------------------------------
Eval num_timesteps=2421000, episode_reward=608.63 +/- 462.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 609      |
| time/              |          |
|    total_timesteps | 2421000  |
| train/             |          |
|    actor_loss      | -0.852   |
|    critic_loss     | 0.000429 |
|    ent_coef        | 0.0004   |
|    ent_coef_loss   | -6.64    |
|    learning_rate   | 0.00258  |
|    n_updates       | 11820    |
---------------------------------
Eval num_timesteps=2422000, episode_reward=426.77 +/- 604.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 427      |
| time/              |          |
|    total_timesteps | 2422000  |
---------------------------------
Eval num_timesteps=2423000, episode_reward=825.47 +/- 54.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 825      |
| time/              |          |
|    total_timesteps | 2423000  |
| train/             |          |
|    actor_loss      | -0.77    |
|    critic_loss     | 0.000252 |
|    ent_coef        | 0.000397 |
|    ent_coef_loss   | -11.7    |
|    learning_rate   | 0.00258  |
|    n_updates       | 11830    |
---------------------------------
Eval num_timesteps=2424000, episode_reward=828.64 +/- 47.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 2424000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 79.1     |
| time/              |          |
|    episodes        | 2424     |
|    fps             | 642      |
|    time_elapsed    | 3771     |
|    total_timesteps | 2424000  |
---------------------------------
Eval num_timesteps=2425000, episode_reward=780.76 +/- 47.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 781      |
| time/              |          |
|    total_timesteps | 2425000  |
| train/             |          |
|    actor_loss      | -0.969   |
|    critic_loss     | 0.000511 |
|    ent_coef        | 0.000391 |
|    ent_coef_loss   | -5.36    |
|    learning_rate   | 0.00258  |
|    n_updates       | 11840    |
---------------------------------
Eval num_timesteps=2426000, episode_reward=752.01 +/- 70.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 752      |
| time/              |          |
|    total_timesteps | 2426000  |
---------------------------------
Eval num_timesteps=2427000, episode_reward=874.99 +/- 21.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 2427000  |
| train/             |          |
|    actor_loss      | -0.96    |
|    critic_loss     | 0.000444 |
|    ent_coef        | 0.000387 |
|    ent_coef_loss   | -4.4     |
|    learning_rate   | 0.00257  |
|    n_updates       | 11850    |
---------------------------------
Eval num_timesteps=2428000, episode_reward=891.22 +/- 40.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 2428000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 105      |
| time/              |          |
|    episodes        | 2428     |
|    fps             | 642      |
|    time_elapsed    | 3777     |
|    total_timesteps | 2428000  |
---------------------------------
Eval num_timesteps=2429000, episode_reward=896.32 +/- 41.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 2429000  |
| train/             |          |
|    actor_loss      | -0.802   |
|    critic_loss     | 0.000281 |
|    ent_coef        | 0.000384 |
|    ent_coef_loss   | -3.08    |
|    learning_rate   | 0.00257  |
|    n_updates       | 11860    |
---------------------------------
Eval num_timesteps=2430000, episode_reward=910.31 +/- 55.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 910      |
| time/              |          |
|    total_timesteps | 2430000  |
---------------------------------
Eval num_timesteps=2431000, episode_reward=593.05 +/- 47.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 593      |
| time/              |          |
|    total_timesteps | 2431000  |
| train/             |          |
|    actor_loss      | -0.814   |
|    critic_loss     | 0.000309 |
|    ent_coef        | 0.000382 |
|    ent_coef_loss   | -11.7    |
|    learning_rate   | 0.00257  |
|    n_updates       | 11870    |
---------------------------------
Eval num_timesteps=2432000, episode_reward=623.37 +/- 54.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 623      |
| time/              |          |
|    total_timesteps | 2432000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 128      |
| time/              |          |
|    episodes        | 2432     |
|    fps             | 642      |
|    time_elapsed    | 3783     |
|    total_timesteps | 2432000  |
---------------------------------
Eval num_timesteps=2433000, episode_reward=649.39 +/- 55.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 2433000  |
---------------------------------
Eval num_timesteps=2434000, episode_reward=819.02 +/- 59.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 819      |
| time/              |          |
|    total_timesteps | 2434000  |
| train/             |          |
|    actor_loss      | -0.921   |
|    critic_loss     | 0.000528 |
|    ent_coef        | 0.000377 |
|    ent_coef_loss   | -7.85    |
|    learning_rate   | 0.00257  |
|    n_updates       | 11880    |
---------------------------------
Eval num_timesteps=2435000, episode_reward=854.26 +/- 43.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 854      |
| time/              |          |
|    total_timesteps | 2435000  |
---------------------------------
Eval num_timesteps=2436000, episode_reward=922.56 +/- 36.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 923      |
| time/              |          |
|    total_timesteps | 2436000  |
| train/             |          |
|    actor_loss      | -0.979   |
|    critic_loss     | 0.000488 |
|    ent_coef        | 0.000373 |
|    ent_coef_loss   | 3.57     |
|    learning_rate   | 0.00256  |
|    n_updates       | 11890    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 161      |
| time/              |          |
|    episodes        | 2436     |
|    fps             | 642      |
|    time_elapsed    | 3789     |
|    total_timesteps | 2436000  |
---------------------------------
Eval num_timesteps=2437000, episode_reward=916.59 +/- 56.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 2437000  |
---------------------------------
Eval num_timesteps=2438000, episode_reward=576.87 +/- 54.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 577      |
| time/              |          |
|    total_timesteps | 2438000  |
| train/             |          |
|    actor_loss      | -0.973   |
|    critic_loss     | 0.000426 |
|    ent_coef        | 0.000372 |
|    ent_coef_loss   | 0.203    |
|    learning_rate   | 0.00256  |
|    n_updates       | 11900    |
---------------------------------
Eval num_timesteps=2439000, episode_reward=513.05 +/- 51.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 513      |
| time/              |          |
|    total_timesteps | 2439000  |
---------------------------------
Eval num_timesteps=2440000, episode_reward=642.44 +/- 56.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 2440000  |
| train/             |          |
|    actor_loss      | -0.938   |
|    critic_loss     | 0.00056  |
|    ent_coef        | 0.000372 |
|    ent_coef_loss   | 3.44     |
|    learning_rate   | 0.00256  |
|    n_updates       | 11910    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 188      |
| time/              |          |
|    episodes        | 2440     |
|    fps             | 642      |
|    time_elapsed    | 3795     |
|    total_timesteps | 2440000  |
---------------------------------
Eval num_timesteps=2441000, episode_reward=656.44 +/- 43.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 2441000  |
---------------------------------
Eval num_timesteps=2442000, episode_reward=1022.64 +/- 39.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2442000  |
| train/             |          |
|    actor_loss      | -0.937   |
|    critic_loss     | 0.000537 |
|    ent_coef        | 0.000373 |
|    ent_coef_loss   | -1.81    |
|    learning_rate   | 0.00256  |
|    n_updates       | 11920    |
---------------------------------
Eval num_timesteps=2443000, episode_reward=1054.41 +/- 22.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2443000  |
---------------------------------
Eval num_timesteps=2444000, episode_reward=972.62 +/- 35.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 973      |
| time/              |          |
|    total_timesteps | 2444000  |
| train/             |          |
|    actor_loss      | -0.99    |
|    critic_loss     | 0.000463 |
|    ent_coef        | 0.000373 |
|    ent_coef_loss   | -6.72    |
|    learning_rate   | 0.00256  |
|    n_updates       | 11930    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 224      |
| time/              |          |
|    episodes        | 2444     |
|    fps             | 642      |
|    time_elapsed    | 3802     |
|    total_timesteps | 2444000  |
---------------------------------
Eval num_timesteps=2445000, episode_reward=962.43 +/- 53.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 2445000  |
---------------------------------
Eval num_timesteps=2446000, episode_reward=733.42 +/- 50.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 2446000  |
| train/             |          |
|    actor_loss      | -0.983   |
|    critic_loss     | 0.000494 |
|    ent_coef        | 0.000371 |
|    ent_coef_loss   | 1.82     |
|    learning_rate   | 0.00255  |
|    n_updates       | 11940    |
---------------------------------
Eval num_timesteps=2447000, episode_reward=672.10 +/- 66.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 2447000  |
---------------------------------
Eval num_timesteps=2448000, episode_reward=790.50 +/- 50.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 2448000  |
| train/             |          |
|    actor_loss      | -0.949   |
|    critic_loss     | 0.000537 |
|    ent_coef        | 0.00037  |
|    ent_coef_loss   | 3.84     |
|    learning_rate   | 0.00255  |
|    n_updates       | 11950    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 245      |
| time/              |          |
|    episodes        | 2448     |
|    fps             | 642      |
|    time_elapsed    | 3808     |
|    total_timesteps | 2448000  |
---------------------------------
Eval num_timesteps=2449000, episode_reward=762.82 +/- 35.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 763      |
| time/              |          |
|    total_timesteps | 2449000  |
---------------------------------
Eval num_timesteps=2450000, episode_reward=1029.65 +/- 57.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2450000  |
| train/             |          |
|    actor_loss      | -0.974   |
|    critic_loss     | 0.000503 |
|    ent_coef        | 0.000372 |
|    ent_coef_loss   | 9.13     |
|    learning_rate   | 0.00255  |
|    n_updates       | 11960    |
---------------------------------
Eval num_timesteps=2451000, episode_reward=1046.57 +/- 12.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2451000  |
---------------------------------
Eval num_timesteps=2452000, episode_reward=831.60 +/- 39.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 832      |
| time/              |          |
|    total_timesteps | 2452000  |
| train/             |          |
|    actor_loss      | -0.999   |
|    critic_loss     | 0.000545 |
|    ent_coef        | 0.000375 |
|    ent_coef_loss   | -2.6     |
|    learning_rate   | 0.00255  |
|    n_updates       | 11970    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 271      |
| time/              |          |
|    episodes        | 2452     |
|    fps             | 642      |
|    time_elapsed    | 3814     |
|    total_timesteps | 2452000  |
---------------------------------
Eval num_timesteps=2453000, episode_reward=808.64 +/- 24.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 809      |
| time/              |          |
|    total_timesteps | 2453000  |
---------------------------------
Eval num_timesteps=2454000, episode_reward=750.19 +/- 35.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 750      |
| time/              |          |
|    total_timesteps | 2454000  |
| train/             |          |
|    actor_loss      | -0.968   |
|    critic_loss     | 0.000539 |
|    ent_coef        | 0.000376 |
|    ent_coef_loss   | -1.52    |
|    learning_rate   | 0.00255  |
|    n_updates       | 11980    |
---------------------------------
Eval num_timesteps=2455000, episode_reward=734.81 +/- 21.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 735      |
| time/              |          |
|    total_timesteps | 2455000  |
---------------------------------
Eval num_timesteps=2456000, episode_reward=741.99 +/- 16.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 742      |
| time/              |          |
|    total_timesteps | 2456000  |
| train/             |          |
|    actor_loss      | -0.953   |
|    critic_loss     | 0.00052  |
|    ent_coef        | 0.000375 |
|    ent_coef_loss   | 0.765    |
|    learning_rate   | 0.00254  |
|    n_updates       | 11990    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 308      |
| time/              |          |
|    episodes        | 2456     |
|    fps             | 642      |
|    time_elapsed    | 3820     |
|    total_timesteps | 2456000  |
---------------------------------
Eval num_timesteps=2457000, episode_reward=759.15 +/- 12.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 759      |
| time/              |          |
|    total_timesteps | 2457000  |
---------------------------------
Eval num_timesteps=2458000, episode_reward=925.78 +/- 29.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 926      |
| time/              |          |
|    total_timesteps | 2458000  |
| train/             |          |
|    actor_loss      | -0.959   |
|    critic_loss     | 0.000512 |
|    ent_coef        | 0.000376 |
|    ent_coef_loss   | 3.24     |
|    learning_rate   | 0.00254  |
|    n_updates       | 12000    |
---------------------------------
Eval num_timesteps=2459000, episode_reward=935.39 +/- 39.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 935      |
| time/              |          |
|    total_timesteps | 2459000  |
---------------------------------
Eval num_timesteps=2460000, episode_reward=882.16 +/- 29.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 882      |
| time/              |          |
|    total_timesteps | 2460000  |
| train/             |          |
|    actor_loss      | -0.992   |
|    critic_loss     | 0.000542 |
|    ent_coef        | 0.000377 |
|    ent_coef_loss   | 5.59     |
|    learning_rate   | 0.00254  |
|    n_updates       | 12010    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 328      |
| time/              |          |
|    episodes        | 2460     |
|    fps             | 642      |
|    time_elapsed    | 3826     |
|    total_timesteps | 2460000  |
---------------------------------
Eval num_timesteps=2461000, episode_reward=838.14 +/- 48.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 2461000  |
---------------------------------
Eval num_timesteps=2462000, episode_reward=612.75 +/- 46.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 613      |
| time/              |          |
|    total_timesteps | 2462000  |
| train/             |          |
|    actor_loss      | -0.986   |
|    critic_loss     | 0.000466 |
|    ent_coef        | 0.00038  |
|    ent_coef_loss   | 8.61     |
|    learning_rate   | 0.00254  |
|    n_updates       | 12020    |
---------------------------------
Eval num_timesteps=2463000, episode_reward=588.36 +/- 49.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 588      |
| time/              |          |
|    total_timesteps | 2463000  |
---------------------------------
Eval num_timesteps=2464000, episode_reward=672.11 +/- 22.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 672      |
| time/              |          |
|    total_timesteps | 2464000  |
| train/             |          |
|    actor_loss      | -0.933   |
|    critic_loss     | 0.000632 |
|    ent_coef        | 0.000384 |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.00254  |
|    n_updates       | 12030    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 330      |
| time/              |          |
|    episodes        | 2464     |
|    fps             | 642      |
|    time_elapsed    | 3832     |
|    total_timesteps | 2464000  |
---------------------------------
Eval num_timesteps=2465000, episode_reward=684.15 +/- 14.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 684      |
| time/              |          |
|    total_timesteps | 2465000  |
---------------------------------
Eval num_timesteps=2466000, episode_reward=768.97 +/- 41.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 769      |
| time/              |          |
|    total_timesteps | 2466000  |
| train/             |          |
|    actor_loss      | -0.944   |
|    critic_loss     | 0.000588 |
|    ent_coef        | 0.000385 |
|    ent_coef_loss   | -1.09    |
|    learning_rate   | 0.00253  |
|    n_updates       | 12040    |
---------------------------------
Eval num_timesteps=2467000, episode_reward=778.56 +/- 17.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 779      |
| time/              |          |
|    total_timesteps | 2467000  |
---------------------------------
Eval num_timesteps=2468000, episode_reward=836.52 +/- 28.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 2468000  |
| train/             |          |
|    actor_loss      | -0.956   |
|    critic_loss     | 0.000595 |
|    ent_coef        | 0.000385 |
|    ent_coef_loss   | 0.931    |
|    learning_rate   | 0.00253  |
|    n_updates       | 12050    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 353      |
| time/              |          |
|    episodes        | 2468     |
|    fps             | 642      |
|    time_elapsed    | 3838     |
|    total_timesteps | 2468000  |
---------------------------------
Eval num_timesteps=2469000, episode_reward=835.42 +/- 53.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 835      |
| time/              |          |
|    total_timesteps | 2469000  |
---------------------------------
Eval num_timesteps=2470000, episode_reward=726.16 +/- 53.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 726      |
| time/              |          |
|    total_timesteps | 2470000  |
| train/             |          |
|    actor_loss      | -0.976   |
|    critic_loss     | 0.000488 |
|    ent_coef        | 0.000386 |
|    ent_coef_loss   | 5.47     |
|    learning_rate   | 0.00253  |
|    n_updates       | 12060    |
---------------------------------
Eval num_timesteps=2471000, episode_reward=755.60 +/- 44.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 756      |
| time/              |          |
|    total_timesteps | 2471000  |
---------------------------------
Eval num_timesteps=2472000, episode_reward=431.01 +/- 564.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 2472000  |
| train/             |          |
|    actor_loss      | -0.955   |
|    critic_loss     | 0.000499 |
|    ent_coef        | 0.000388 |
|    ent_coef_loss   | 2.79     |
|    learning_rate   | 0.00253  |
|    n_updates       | 12070    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 388      |
| time/              |          |
|    episodes        | 2472     |
|    fps             | 642      |
|    time_elapsed    | 3844     |
|    total_timesteps | 2472000  |
---------------------------------
Eval num_timesteps=2473000, episode_reward=905.97 +/- 37.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 906      |
| time/              |          |
|    total_timesteps | 2473000  |
---------------------------------
Eval num_timesteps=2474000, episode_reward=477.33 +/- 612.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 477      |
| time/              |          |
|    total_timesteps | 2474000  |
| train/             |          |
|    actor_loss      | -0.976   |
|    critic_loss     | 0.000544 |
|    ent_coef        | 0.00039  |
|    ent_coef_loss   | 5.8      |
|    learning_rate   | 0.00253  |
|    n_updates       | 12080    |
---------------------------------
Eval num_timesteps=2475000, episode_reward=965.78 +/- 33.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 2475000  |
---------------------------------
Eval num_timesteps=2476000, episode_reward=732.78 +/- 500.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 2476000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 431      |
| time/              |          |
|    episodes        | 2476     |
|    fps             | 642      |
|    time_elapsed    | 3850     |
|    total_timesteps | 2476000  |
---------------------------------
Eval num_timesteps=2477000, episode_reward=903.27 +/- 64.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 2477000  |
| train/             |          |
|    actor_loss      | -0.988   |
|    critic_loss     | 0.000452 |
|    ent_coef        | 0.000393 |
|    ent_coef_loss   | 8.94     |
|    learning_rate   | 0.00252  |
|    n_updates       | 12090    |
---------------------------------
Eval num_timesteps=2478000, episode_reward=421.23 +/- 573.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 2478000  |
---------------------------------
Eval num_timesteps=2479000, episode_reward=945.57 +/- 37.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 2479000  |
| train/             |          |
|    actor_loss      | -0.976   |
|    critic_loss     | 0.000507 |
|    ent_coef        | 0.000398 |
|    ent_coef_loss   | 6.21     |
|    learning_rate   | 0.00252  |
|    n_updates       | 12100    |
---------------------------------
Eval num_timesteps=2480000, episode_reward=929.24 +/- 35.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 2480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 478      |
| time/              |          |
|    episodes        | 2480     |
|    fps             | 642      |
|    time_elapsed    | 3857     |
|    total_timesteps | 2480000  |
---------------------------------
Eval num_timesteps=2481000, episode_reward=959.95 +/- 21.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 960      |
| time/              |          |
|    total_timesteps | 2481000  |
| train/             |          |
|    actor_loss      | -0.978   |
|    critic_loss     | 0.000502 |
|    ent_coef        | 0.000402 |
|    ent_coef_loss   | 4.25     |
|    learning_rate   | 0.00252  |
|    n_updates       | 12110    |
---------------------------------
Eval num_timesteps=2482000, episode_reward=906.76 +/- 61.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 2482000  |
---------------------------------
Eval num_timesteps=2483000, episode_reward=842.54 +/- 40.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 843      |
| time/              |          |
|    total_timesteps | 2483000  |
| train/             |          |
|    actor_loss      | -0.979   |
|    critic_loss     | 0.000563 |
|    ent_coef        | 0.000405 |
|    ent_coef_loss   | 1.09     |
|    learning_rate   | 0.00252  |
|    n_updates       | 12120    |
---------------------------------
Eval num_timesteps=2484000, episode_reward=837.89 +/- 39.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 2484000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 526      |
| time/              |          |
|    episodes        | 2484     |
|    fps             | 642      |
|    time_elapsed    | 3863     |
|    total_timesteps | 2484000  |
---------------------------------
Eval num_timesteps=2485000, episode_reward=1038.76 +/- 25.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2485000  |
| train/             |          |
|    actor_loss      | -0.97    |
|    critic_loss     | 0.000552 |
|    ent_coef        | 0.000407 |
|    ent_coef_loss   | 8.68     |
|    learning_rate   | 0.00252  |
|    n_updates       | 12130    |
---------------------------------
Eval num_timesteps=2486000, episode_reward=1047.58 +/- 67.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2486000  |
---------------------------------
Eval num_timesteps=2487000, episode_reward=961.82 +/- 46.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 962      |
| time/              |          |
|    total_timesteps | 2487000  |
| train/             |          |
|    actor_loss      | -0.992   |
|    critic_loss     | 0.000475 |
|    ent_coef        | 0.000411 |
|    ent_coef_loss   | 4.89     |
|    learning_rate   | 0.00251  |
|    n_updates       | 12140    |
---------------------------------
Eval num_timesteps=2488000, episode_reward=970.06 +/- 20.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 2488000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 569      |
| time/              |          |
|    episodes        | 2488     |
|    fps             | 642      |
|    time_elapsed    | 3869     |
|    total_timesteps | 2488000  |
---------------------------------
Eval num_timesteps=2489000, episode_reward=897.92 +/- 37.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 2489000  |
| train/             |          |
|    actor_loss      | -0.963   |
|    critic_loss     | 0.000544 |
|    ent_coef        | 0.000415 |
|    ent_coef_loss   | 0.132    |
|    learning_rate   | 0.00251  |
|    n_updates       | 12150    |
---------------------------------
Eval num_timesteps=2490000, episode_reward=873.87 +/- 32.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 2490000  |
---------------------------------
Eval num_timesteps=2491000, episode_reward=989.30 +/- 39.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 2491000  |
| train/             |          |
|    actor_loss      | -0.968   |
|    critic_loss     | 0.000521 |
|    ent_coef        | 0.000417 |
|    ent_coef_loss   | 5.14     |
|    learning_rate   | 0.00251  |
|    n_updates       | 12160    |
---------------------------------
Eval num_timesteps=2492000, episode_reward=980.30 +/- 53.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 2492000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 620      |
| time/              |          |
|    episodes        | 2492     |
|    fps             | 643      |
|    time_elapsed    | 3875     |
|    total_timesteps | 2492000  |
---------------------------------
Eval num_timesteps=2493000, episode_reward=1028.11 +/- 13.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2493000  |
| train/             |          |
|    actor_loss      | -0.985   |
|    critic_loss     | 0.000482 |
|    ent_coef        | 0.00042  |
|    ent_coef_loss   | 8.87     |
|    learning_rate   | 0.00251  |
|    n_updates       | 12170    |
---------------------------------
Eval num_timesteps=2494000, episode_reward=1024.08 +/- 29.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2494000  |
---------------------------------
Eval num_timesteps=2495000, episode_reward=980.07 +/- 72.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 2495000  |
| train/             |          |
|    actor_loss      | -0.994   |
|    critic_loss     | 0.000507 |
|    ent_coef        | 0.000425 |
|    ent_coef_loss   | 4.83     |
|    learning_rate   | 0.00251  |
|    n_updates       | 12180    |
---------------------------------
Eval num_timesteps=2496000, episode_reward=944.02 +/- 41.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 944      |
| time/              |          |
|    total_timesteps | 2496000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 655      |
| time/              |          |
|    episodes        | 2496     |
|    fps             | 643      |
|    time_elapsed    | 3881     |
|    total_timesteps | 2496000  |
---------------------------------
Eval num_timesteps=2497000, episode_reward=881.20 +/- 40.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 2497000  |
| train/             |          |
|    actor_loss      | -0.977   |
|    critic_loss     | 0.000585 |
|    ent_coef        | 0.000429 |
|    ent_coef_loss   | 3.1      |
|    learning_rate   | 0.0025   |
|    n_updates       | 12190    |
---------------------------------
Eval num_timesteps=2498000, episode_reward=896.24 +/- 19.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 896      |
| time/              |          |
|    total_timesteps | 2498000  |
---------------------------------
Eval num_timesteps=2499000, episode_reward=948.58 +/- 22.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 2499000  |
| train/             |          |
|    actor_loss      | -0.969   |
|    critic_loss     | 0.000578 |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | 2.64     |
|    learning_rate   | 0.0025   |
|    n_updates       | 12200    |
---------------------------------
Eval num_timesteps=2500000, episode_reward=971.49 +/- 29.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 971      |
| time/              |          |
|    total_timesteps | 2500000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 678      |
| time/              |          |
|    episodes        | 2500     |
|    fps             | 643      |
|    time_elapsed    | 3887     |
|    total_timesteps | 2500000  |
---------------------------------
Eval num_timesteps=2501000, episode_reward=840.23 +/- 5.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 2501000  |
| train/             |          |
|    actor_loss      | -0.98    |
|    critic_loss     | 0.000569 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | 0.806    |
|    learning_rate   | 0.0025   |
|    n_updates       | 12210    |
---------------------------------
Eval num_timesteps=2502000, episode_reward=837.29 +/- 24.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 837      |
| time/              |          |
|    total_timesteps | 2502000  |
---------------------------------
Eval num_timesteps=2503000, episode_reward=928.58 +/- 39.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 2503000  |
| train/             |          |
|    actor_loss      | -0.952   |
|    critic_loss     | 0.000573 |
|    ent_coef        | 0.000435 |
|    ent_coef_loss   | 0.37     |
|    learning_rate   | 0.0025   |
|    n_updates       | 12220    |
---------------------------------
Eval num_timesteps=2504000, episode_reward=904.23 +/- 22.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 904      |
| time/              |          |
|    total_timesteps | 2504000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 696      |
| time/              |          |
|    episodes        | 2504     |
|    fps             | 643      |
|    time_elapsed    | 3893     |
|    total_timesteps | 2504000  |
---------------------------------
Eval num_timesteps=2505000, episode_reward=1044.60 +/- 30.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2505000  |
| train/             |          |
|    actor_loss      | -0.968   |
|    critic_loss     | 0.000579 |
|    ent_coef        | 0.000436 |
|    ent_coef_loss   | -2.38    |
|    learning_rate   | 0.0025   |
|    n_updates       | 12230    |
---------------------------------
Eval num_timesteps=2506000, episode_reward=1069.43 +/- 14.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2506000  |
---------------------------------
Eval num_timesteps=2507000, episode_reward=840.02 +/- 26.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 840      |
| time/              |          |
|    total_timesteps | 2507000  |
| train/             |          |
|    actor_loss      | -0.993   |
|    critic_loss     | 0.000507 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | -3.7     |
|    learning_rate   | 0.00249  |
|    n_updates       | 12240    |
---------------------------------
Eval num_timesteps=2508000, episode_reward=852.67 +/- 11.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 2508000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 717      |
| time/              |          |
|    episodes        | 2508     |
|    fps             | 643      |
|    time_elapsed    | 3900     |
|    total_timesteps | 2508000  |
---------------------------------
Eval num_timesteps=2509000, episode_reward=805.22 +/- 40.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 805      |
| time/              |          |
|    total_timesteps | 2509000  |
| train/             |          |
|    actor_loss      | -0.963   |
|    critic_loss     | 0.0006   |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | -1.69    |
|    learning_rate   | 0.00249  |
|    n_updates       | 12250    |
---------------------------------
Eval num_timesteps=2510000, episode_reward=789.92 +/- 36.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 790      |
| time/              |          |
|    total_timesteps | 2510000  |
---------------------------------
Eval num_timesteps=2511000, episode_reward=964.75 +/- 35.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 2511000  |
| train/             |          |
|    actor_loss      | -0.953   |
|    critic_loss     | 0.000563 |
|    ent_coef        | 0.000431 |
|    ent_coef_loss   | -1.17    |
|    learning_rate   | 0.00249  |
|    n_updates       | 12260    |
---------------------------------
Eval num_timesteps=2512000, episode_reward=988.58 +/- 32.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 989      |
| time/              |          |
|    total_timesteps | 2512000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 723      |
| time/              |          |
|    episodes        | 2512     |
|    fps             | 643      |
|    time_elapsed    | 3906     |
|    total_timesteps | 2512000  |
---------------------------------
Eval num_timesteps=2513000, episode_reward=833.87 +/- 43.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 834      |
| time/              |          |
|    total_timesteps | 2513000  |
| train/             |          |
|    actor_loss      | -0.97    |
|    critic_loss     | 0.000585 |
|    ent_coef        | 0.000429 |
|    ent_coef_loss   | -5.15    |
|    learning_rate   | 0.00249  |
|    n_updates       | 12270    |
---------------------------------
Eval num_timesteps=2514000, episode_reward=874.04 +/- 46.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 2514000  |
---------------------------------
Eval num_timesteps=2515000, episode_reward=1001.56 +/- 41.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 2515000  |
| train/             |          |
|    actor_loss      | -0.962   |
|    critic_loss     | 0.000543 |
|    ent_coef        | 0.000427 |
|    ent_coef_loss   | 4.69     |
|    learning_rate   | 0.00249  |
|    n_updates       | 12280    |
---------------------------------
Eval num_timesteps=2516000, episode_reward=991.35 +/- 43.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 2516000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 743      |
| time/              |          |
|    episodes        | 2516     |
|    fps             | 643      |
|    time_elapsed    | 3912     |
|    total_timesteps | 2516000  |
---------------------------------
Eval num_timesteps=2517000, episode_reward=1105.19 +/- 40.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2517000  |
| train/             |          |
|    actor_loss      | -0.978   |
|    critic_loss     | 0.000553 |
|    ent_coef        | 0.000429 |
|    ent_coef_loss   | 2.48     |
|    learning_rate   | 0.00248  |
|    n_updates       | 12290    |
---------------------------------
Eval num_timesteps=2518000, episode_reward=1079.07 +/- 30.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2518000  |
---------------------------------
Eval num_timesteps=2519000, episode_reward=1087.62 +/- 17.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2519000  |
---------------------------------
Eval num_timesteps=2520000, episode_reward=974.89 +/- 90.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 975      |
| time/              |          |
|    total_timesteps | 2520000  |
| train/             |          |
|    actor_loss      | -0.999   |
|    critic_loss     | 0.000557 |
|    ent_coef        | 0.00043  |
|    ent_coef_loss   | -0.445   |
|    learning_rate   | 0.00248  |
|    n_updates       | 12300    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 758      |
| time/              |          |
|    episodes        | 2520     |
|    fps             | 643      |
|    time_elapsed    | 3918     |
|    total_timesteps | 2520000  |
---------------------------------
Eval num_timesteps=2521000, episode_reward=989.92 +/- 39.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 2521000  |
---------------------------------
Eval num_timesteps=2522000, episode_reward=862.02 +/- 26.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 862      |
| time/              |          |
|    total_timesteps | 2522000  |
| train/             |          |
|    actor_loss      | -0.966   |
|    critic_loss     | 0.000562 |
|    ent_coef        | 0.00043  |
|    ent_coef_loss   | 0.0856   |
|    learning_rate   | 0.00248  |
|    n_updates       | 12310    |
---------------------------------
Eval num_timesteps=2523000, episode_reward=891.75 +/- 23.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 2523000  |
---------------------------------
Eval num_timesteps=2524000, episode_reward=849.72 +/- 20.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 850      |
| time/              |          |
|    total_timesteps | 2524000  |
| train/             |          |
|    actor_loss      | -0.956   |
|    critic_loss     | 0.000592 |
|    ent_coef        | 0.000431 |
|    ent_coef_loss   | 8.28     |
|    learning_rate   | 0.00248  |
|    n_updates       | 12320    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 785      |
| time/              |          |
|    episodes        | 2524     |
|    fps             | 643      |
|    time_elapsed    | 3924     |
|    total_timesteps | 2524000  |
---------------------------------
Eval num_timesteps=2525000, episode_reward=876.03 +/- 25.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 876      |
| time/              |          |
|    total_timesteps | 2525000  |
---------------------------------
Eval num_timesteps=2526000, episode_reward=791.05 +/- 14.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 791      |
| time/              |          |
|    total_timesteps | 2526000  |
| train/             |          |
|    actor_loss      | -0.958   |
|    critic_loss     | 0.00061  |
|    ent_coef        | 0.000435 |
|    ent_coef_loss   | 3.3      |
|    learning_rate   | 0.00247  |
|    n_updates       | 12330    |
---------------------------------
Eval num_timesteps=2527000, episode_reward=800.54 +/- 10.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 801      |
| time/              |          |
|    total_timesteps | 2527000  |
---------------------------------
Eval num_timesteps=2528000, episode_reward=766.09 +/- 21.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 766      |
| time/              |          |
|    total_timesteps | 2528000  |
| train/             |          |
|    actor_loss      | -0.949   |
|    critic_loss     | 0.000611 |
|    ent_coef        | 0.000438 |
|    ent_coef_loss   | 0.711    |
|    learning_rate   | 0.00247  |
|    n_updates       | 12340    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 798      |
| time/              |          |
|    episodes        | 2528     |
|    fps             | 643      |
|    time_elapsed    | 3930     |
|    total_timesteps | 2528000  |
---------------------------------
Eval num_timesteps=2529000, episode_reward=766.94 +/- 33.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 2529000  |
---------------------------------
Eval num_timesteps=2530000, episode_reward=891.42 +/- 37.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 2530000  |
| train/             |          |
|    actor_loss      | -0.956   |
|    critic_loss     | 0.000633 |
|    ent_coef        | 0.00044  |
|    ent_coef_loss   | 1.23     |
|    learning_rate   | 0.00247  |
|    n_updates       | 12350    |
---------------------------------
Eval num_timesteps=2531000, episode_reward=942.01 +/- 20.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 2531000  |
---------------------------------
Eval num_timesteps=2532000, episode_reward=1086.33 +/- 17.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2532000  |
| train/             |          |
|    actor_loss      | -0.97    |
|    critic_loss     | 0.000545 |
|    ent_coef        | 0.000441 |
|    ent_coef_loss   | 0.596    |
|    learning_rate   | 0.00247  |
|    n_updates       | 12360    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 818      |
| time/              |          |
|    episodes        | 2532     |
|    fps             | 643      |
|    time_elapsed    | 3936     |
|    total_timesteps | 2532000  |
---------------------------------
Eval num_timesteps=2533000, episode_reward=1048.86 +/- 23.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2533000  |
---------------------------------
Eval num_timesteps=2534000, episode_reward=1016.76 +/- 23.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2534000  |
| train/             |          |
|    actor_loss      | -0.99    |
|    critic_loss     | 0.000551 |
|    ent_coef        | 0.000442 |
|    ent_coef_loss   | 2.27     |
|    learning_rate   | 0.00247  |
|    n_updates       | 12370    |
---------------------------------
Eval num_timesteps=2535000, episode_reward=1034.02 +/- 41.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2535000  |
---------------------------------
Eval num_timesteps=2536000, episode_reward=1046.28 +/- 11.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2536000  |
| train/             |          |
|    actor_loss      | -0.974   |
|    critic_loss     | 0.000617 |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | 1.98     |
|    learning_rate   | 0.00246  |
|    n_updates       | 12380    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 828      |
| time/              |          |
|    episodes        | 2536     |
|    fps             | 643      |
|    time_elapsed    | 3942     |
|    total_timesteps | 2536000  |
---------------------------------
Eval num_timesteps=2537000, episode_reward=1016.78 +/- 28.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2537000  |
---------------------------------
Eval num_timesteps=2538000, episode_reward=934.16 +/- 9.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 2538000  |
| train/             |          |
|    actor_loss      | -0.977   |
|    critic_loss     | 0.000637 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 5.39     |
|    learning_rate   | 0.00246  |
|    n_updates       | 12390    |
---------------------------------
Eval num_timesteps=2539000, episode_reward=941.87 +/- 50.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 2539000  |
---------------------------------
Eval num_timesteps=2540000, episode_reward=717.27 +/- 14.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 717      |
| time/              |          |
|    total_timesteps | 2540000  |
| train/             |          |
|    actor_loss      | -0.96    |
|    critic_loss     | 0.000723 |
|    ent_coef        | 0.000447 |
|    ent_coef_loss   | -1.66    |
|    learning_rate   | 0.00246  |
|    n_updates       | 12400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 837      |
| time/              |          |
|    episodes        | 2540     |
|    fps             | 643      |
|    time_elapsed    | 3949     |
|    total_timesteps | 2540000  |
---------------------------------
Eval num_timesteps=2541000, episode_reward=701.26 +/- 32.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 701      |
| time/              |          |
|    total_timesteps | 2541000  |
---------------------------------
Eval num_timesteps=2542000, episode_reward=880.53 +/- 27.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 881      |
| time/              |          |
|    total_timesteps | 2542000  |
| train/             |          |
|    actor_loss      | -0.925   |
|    critic_loss     | 0.00078  |
|    ent_coef        | 0.000448 |
|    ent_coef_loss   | 2.7      |
|    learning_rate   | 0.00246  |
|    n_updates       | 12410    |
---------------------------------
Eval num_timesteps=2543000, episode_reward=873.90 +/- 27.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 874      |
| time/              |          |
|    total_timesteps | 2543000  |
---------------------------------
Eval num_timesteps=2544000, episode_reward=879.66 +/- 48.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 880      |
| time/              |          |
|    total_timesteps | 2544000  |
| train/             |          |
|    actor_loss      | -0.962   |
|    critic_loss     | 0.000675 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | 3.1      |
|    learning_rate   | 0.00246  |
|    n_updates       | 12420    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 838      |
| time/              |          |
|    episodes        | 2544     |
|    fps             | 643      |
|    time_elapsed    | 3955     |
|    total_timesteps | 2544000  |
---------------------------------
Eval num_timesteps=2545000, episode_reward=898.99 +/- 35.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 899      |
| time/              |          |
|    total_timesteps | 2545000  |
---------------------------------
Eval num_timesteps=2546000, episode_reward=875.02 +/- 40.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 2546000  |
| train/             |          |
|    actor_loss      | -0.946   |
|    critic_loss     | 0.000712 |
|    ent_coef        | 0.000452 |
|    ent_coef_loss   | -0.085   |
|    learning_rate   | 0.00245  |
|    n_updates       | 12430    |
---------------------------------
Eval num_timesteps=2547000, episode_reward=885.33 +/- 26.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 885      |
| time/              |          |
|    total_timesteps | 2547000  |
---------------------------------
Eval num_timesteps=2548000, episode_reward=597.34 +/- 30.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 2548000  |
| train/             |          |
|    actor_loss      | -0.947   |
|    critic_loss     | 0.000722 |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | 3.89     |
|    learning_rate   | 0.00245  |
|    n_updates       | 12440    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 840      |
| time/              |          |
|    episodes        | 2548     |
|    fps             | 643      |
|    time_elapsed    | 3961     |
|    total_timesteps | 2548000  |
---------------------------------
Eval num_timesteps=2549000, episode_reward=608.32 +/- 42.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 608      |
| time/              |          |
|    total_timesteps | 2549000  |
---------------------------------
Eval num_timesteps=2550000, episode_reward=756.98 +/- 31.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 757      |
| time/              |          |
|    total_timesteps | 2550000  |
| train/             |          |
|    actor_loss      | -0.905   |
|    critic_loss     | 0.000779 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | 0.629    |
|    learning_rate   | 0.00245  |
|    n_updates       | 12450    |
---------------------------------
Eval num_timesteps=2551000, episode_reward=796.54 +/- 26.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 797      |
| time/              |          |
|    total_timesteps | 2551000  |
---------------------------------
Eval num_timesteps=2552000, episode_reward=1011.90 +/- 18.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2552000  |
| train/             |          |
|    actor_loss      | -0.941   |
|    critic_loss     | 0.000697 |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | 3.48     |
|    learning_rate   | 0.00245  |
|    n_updates       | 12460    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 834      |
| time/              |          |
|    episodes        | 2552     |
|    fps             | 643      |
|    time_elapsed    | 3967     |
|    total_timesteps | 2552000  |
---------------------------------
Eval num_timesteps=2553000, episode_reward=989.89 +/- 48.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 2553000  |
---------------------------------
Eval num_timesteps=2554000, episode_reward=1013.72 +/- 63.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2554000  |
| train/             |          |
|    actor_loss      | -0.968   |
|    critic_loss     | 0.000653 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | -2.65    |
|    learning_rate   | 0.00245  |
|    n_updates       | 12470    |
---------------------------------
Eval num_timesteps=2555000, episode_reward=1043.66 +/- 15.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2555000  |
---------------------------------
Eval num_timesteps=2556000, episode_reward=1032.24 +/- 35.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2556000  |
| train/             |          |
|    actor_loss      | -0.972   |
|    critic_loss     | 0.000687 |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | -4.1     |
|    learning_rate   | 0.00244  |
|    n_updates       | 12480    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 843      |
| time/              |          |
|    episodes        | 2556     |
|    fps             | 643      |
|    time_elapsed    | 3973     |
|    total_timesteps | 2556000  |
---------------------------------
Eval num_timesteps=2557000, episode_reward=985.50 +/- 21.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 2557000  |
---------------------------------
Eval num_timesteps=2558000, episode_reward=979.22 +/- 53.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 979      |
| time/              |          |
|    total_timesteps | 2558000  |
| train/             |          |
|    actor_loss      | -0.974   |
|    critic_loss     | 0.000641 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | 1.25     |
|    learning_rate   | 0.00244  |
|    n_updates       | 12490    |
---------------------------------
Eval num_timesteps=2559000, episode_reward=984.21 +/- 54.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 984      |
| time/              |          |
|    total_timesteps | 2559000  |
---------------------------------
Eval num_timesteps=2560000, episode_reward=948.13 +/- 40.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 2560000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 847      |
| time/              |          |
|    episodes        | 2560     |
|    fps             | 643      |
|    time_elapsed    | 3979     |
|    total_timesteps | 2560000  |
---------------------------------
Eval num_timesteps=2561000, episode_reward=954.94 +/- 52.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 955      |
| time/              |          |
|    total_timesteps | 2561000  |
| train/             |          |
|    actor_loss      | -0.942   |
|    critic_loss     | 0.000709 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | -0.963   |
|    learning_rate   | 0.00244  |
|    n_updates       | 12500    |
---------------------------------
Eval num_timesteps=2562000, episode_reward=919.13 +/- 39.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 919      |
| time/              |          |
|    total_timesteps | 2562000  |
---------------------------------
Eval num_timesteps=2563000, episode_reward=465.78 +/- 708.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 466      |
| time/              |          |
|    total_timesteps | 2563000  |
| train/             |          |
|    actor_loss      | -0.949   |
|    critic_loss     | 0.000702 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | 1.81     |
|    learning_rate   | 0.00244  |
|    n_updates       | 12510    |
---------------------------------
Eval num_timesteps=2564000, episode_reward=-117.77 +/- 567.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 2564000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 844      |
| time/              |          |
|    episodes        | 2564     |
|    fps             | 643      |
|    time_elapsed    | 3985     |
|    total_timesteps | 2564000  |
---------------------------------
Eval num_timesteps=2565000, episode_reward=677.79 +/- 463.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 678      |
| time/              |          |
|    total_timesteps | 2565000  |
| train/             |          |
|    actor_loss      | -0.83    |
|    critic_loss     | 0.000772 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | -1.93    |
|    learning_rate   | 0.00244  |
|    n_updates       | 12520    |
---------------------------------
Eval num_timesteps=2566000, episode_reward=943.31 +/- 49.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 943      |
| time/              |          |
|    total_timesteps | 2566000  |
---------------------------------
Eval num_timesteps=2567000, episode_reward=919.78 +/- 17.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 920      |
| time/              |          |
|    total_timesteps | 2567000  |
| train/             |          |
|    actor_loss      | -0.935   |
|    critic_loss     | 0.000755 |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | 8.28     |
|    learning_rate   | 0.00243  |
|    n_updates       | 12530    |
---------------------------------
Eval num_timesteps=2568000, episode_reward=944.69 +/- 30.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 945      |
| time/              |          |
|    total_timesteps | 2568000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 851      |
| time/              |          |
|    episodes        | 2568     |
|    fps             | 643      |
|    time_elapsed    | 3992     |
|    total_timesteps | 2568000  |
---------------------------------
Eval num_timesteps=2569000, episode_reward=261.74 +/- 646.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 262      |
| time/              |          |
|    total_timesteps | 2569000  |
| train/             |          |
|    actor_loss      | -0.954   |
|    critic_loss     | 0.000723 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | 2.43     |
|    learning_rate   | 0.00243  |
|    n_updates       | 12540    |
---------------------------------
Eval num_timesteps=2570000, episode_reward=796.28 +/- 530.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 796      |
| time/              |          |
|    total_timesteps | 2570000  |
---------------------------------
Eval num_timesteps=2571000, episode_reward=764.11 +/- 521.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 764      |
| time/              |          |
|    total_timesteps | 2571000  |
| train/             |          |
|    actor_loss      | -0.986   |
|    critic_loss     | 0.000696 |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | 0.285    |
|    learning_rate   | 0.00243  |
|    n_updates       | 12550    |
---------------------------------
Eval num_timesteps=2572000, episode_reward=509.28 +/- 644.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 509      |
| time/              |          |
|    total_timesteps | 2572000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 861      |
| time/              |          |
|    episodes        | 2572     |
|    fps             | 643      |
|    time_elapsed    | 3998     |
|    total_timesteps | 2572000  |
---------------------------------
Eval num_timesteps=2573000, episode_reward=815.03 +/- 32.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 815      |
| time/              |          |
|    total_timesteps | 2573000  |
| train/             |          |
|    actor_loss      | -0.943   |
|    critic_loss     | 0.00077  |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | -1.11    |
|    learning_rate   | 0.00243  |
|    n_updates       | 12560    |
---------------------------------
Eval num_timesteps=2574000, episode_reward=575.83 +/- 421.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 2574000  |
---------------------------------
Eval num_timesteps=2575000, episode_reward=820.16 +/- 33.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 820      |
| time/              |          |
|    total_timesteps | 2575000  |
| train/             |          |
|    actor_loss      | -0.925   |
|    critic_loss     | 0.000813 |
|    ent_coef        | 0.000464 |
|    ent_coef_loss   | 5.66     |
|    learning_rate   | 0.00243  |
|    n_updates       | 12570    |
---------------------------------
Eval num_timesteps=2576000, episode_reward=838.28 +/- 48.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 2576000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 856      |
| time/              |          |
|    episodes        | 2576     |
|    fps             | 643      |
|    time_elapsed    | 4004     |
|    total_timesteps | 2576000  |
---------------------------------
Eval num_timesteps=2577000, episode_reward=1033.33 +/- 65.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2577000  |
| train/             |          |
|    actor_loss      | -0.93    |
|    critic_loss     | 0.00077  |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | 2.54     |
|    learning_rate   | 0.00242  |
|    n_updates       | 12580    |
---------------------------------
Eval num_timesteps=2578000, episode_reward=1010.93 +/- 72.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2578000  |
---------------------------------
Eval num_timesteps=2579000, episode_reward=1010.53 +/- 26.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2579000  |
| train/             |          |
|    actor_loss      | -0.975   |
|    critic_loss     | 0.00064  |
|    ent_coef        | 0.00047  |
|    ent_coef_loss   | 9.35     |
|    learning_rate   | 0.00242  |
|    n_updates       | 12590    |
---------------------------------
Eval num_timesteps=2580000, episode_reward=997.43 +/- 34.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 997      |
| time/              |          |
|    total_timesteps | 2580000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 860      |
| time/              |          |
|    episodes        | 2580     |
|    fps             | 643      |
|    time_elapsed    | 4010     |
|    total_timesteps | 2580000  |
---------------------------------
Eval num_timesteps=2581000, episode_reward=965.89 +/- 42.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 966      |
| time/              |          |
|    total_timesteps | 2581000  |
| train/             |          |
|    actor_loss      | -0.965   |
|    critic_loss     | 0.000635 |
|    ent_coef        | 0.000476 |
|    ent_coef_loss   | 1.87     |
|    learning_rate   | 0.00242  |
|    n_updates       | 12600    |
---------------------------------
Eval num_timesteps=2582000, episode_reward=948.31 +/- 22.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 2582000  |
---------------------------------
Eval num_timesteps=2583000, episode_reward=1019.54 +/- 36.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2583000  |
| train/             |          |
|    actor_loss      | -0.929   |
|    critic_loss     | 0.000701 |
|    ent_coef        | 0.000479 |
|    ent_coef_loss   | 1.86     |
|    learning_rate   | 0.00242  |
|    n_updates       | 12610    |
---------------------------------
Eval num_timesteps=2584000, episode_reward=1046.34 +/- 35.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2584000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 861      |
| time/              |          |
|    episodes        | 2584     |
|    fps             | 643      |
|    time_elapsed    | 4016     |
|    total_timesteps | 2584000  |
---------------------------------
Eval num_timesteps=2585000, episode_reward=1008.62 +/- 18.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2585000  |
| train/             |          |
|    actor_loss      | -0.952   |
|    critic_loss     | 0.000759 |
|    ent_coef        | 0.000482 |
|    ent_coef_loss   | -0.817   |
|    learning_rate   | 0.00242  |
|    n_updates       | 12620    |
---------------------------------
Eval num_timesteps=2586000, episode_reward=1022.82 +/- 36.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2586000  |
---------------------------------
Eval num_timesteps=2587000, episode_reward=1065.15 +/- 31.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2587000  |
| train/             |          |
|    actor_loss      | -0.952   |
|    critic_loss     | 0.000773 |
|    ent_coef        | 0.000482 |
|    ent_coef_loss   | -3.07    |
|    learning_rate   | 0.00241  |
|    n_updates       | 12630    |
---------------------------------
Eval num_timesteps=2588000, episode_reward=6.68 +/- 545.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 6.68     |
| time/              |          |
|    total_timesteps | 2588000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 866      |
| time/              |          |
|    episodes        | 2588     |
|    fps             | 643      |
|    time_elapsed    | 4022     |
|    total_timesteps | 2588000  |
---------------------------------
Eval num_timesteps=2589000, episode_reward=-7.36 +/- 543.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -7.36    |
| time/              |          |
|    total_timesteps | 2589000  |
| train/             |          |
|    actor_loss      | -0.973   |
|    critic_loss     | 0.000734 |
|    ent_coef        | 0.000481 |
|    ent_coef_loss   | 2.8      |
|    learning_rate   | 0.00241  |
|    n_updates       | 12640    |
---------------------------------
Eval num_timesteps=2590000, episode_reward=516.72 +/- 649.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 517      |
| time/              |          |
|    total_timesteps | 2590000  |
---------------------------------
Eval num_timesteps=2591000, episode_reward=-307.89 +/- 0.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 2591000  |
| train/             |          |
|    actor_loss      | -0.724   |
|    critic_loss     | 0.000418 |
|    ent_coef        | 0.000479 |
|    ent_coef_loss   | -14.5    |
|    learning_rate   | 0.00241  |
|    n_updates       | 12650    |
---------------------------------
Eval num_timesteps=2592000, episode_reward=176.19 +/- 593.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 176      |
| time/              |          |
|    total_timesteps | 2592000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 846      |
| time/              |          |
|    episodes        | 2592     |
|    fps             | 643      |
|    time_elapsed    | 4029     |
|    total_timesteps | 2592000  |
---------------------------------
Eval num_timesteps=2593000, episode_reward=938.90 +/- 61.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 2593000  |
| train/             |          |
|    actor_loss      | -0.894   |
|    critic_loss     | 0.000945 |
|    ent_coef        | 0.000473 |
|    ent_coef_loss   | -0.846   |
|    learning_rate   | 0.00241  |
|    n_updates       | 12660    |
---------------------------------
Eval num_timesteps=2594000, episode_reward=929.12 +/- 58.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 2594000  |
---------------------------------
Eval num_timesteps=2595000, episode_reward=838.19 +/- 43.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 2595000  |
| train/             |          |
|    actor_loss      | -0.948   |
|    critic_loss     | 0.000945 |
|    ent_coef        | 0.00047  |
|    ent_coef_loss   | 6.7      |
|    learning_rate   | 0.00241  |
|    n_updates       | 12670    |
---------------------------------
Eval num_timesteps=2596000, episode_reward=787.23 +/- 25.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 787      |
| time/              |          |
|    total_timesteps | 2596000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 842      |
| time/              |          |
|    episodes        | 2596     |
|    fps             | 643      |
|    time_elapsed    | 4035     |
|    total_timesteps | 2596000  |
---------------------------------
Eval num_timesteps=2597000, episode_reward=869.30 +/- 88.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 869      |
| time/              |          |
|    total_timesteps | 2597000  |
| train/             |          |
|    actor_loss      | -0.923   |
|    critic_loss     | 0.000893 |
|    ent_coef        | 0.000472 |
|    ent_coef_loss   | 0.0728   |
|    learning_rate   | 0.0024   |
|    n_updates       | 12680    |
---------------------------------
Eval num_timesteps=2598000, episode_reward=883.84 +/- 45.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 884      |
| time/              |          |
|    total_timesteps | 2598000  |
---------------------------------
Eval num_timesteps=2599000, episode_reward=1072.16 +/- 61.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2599000  |
| train/             |          |
|    actor_loss      | -0.928   |
|    critic_loss     | 0.000819 |
|    ent_coef        | 0.000472 |
|    ent_coef_loss   | -5.32    |
|    learning_rate   | 0.0024   |
|    n_updates       | 12690    |
---------------------------------
Eval num_timesteps=2600000, episode_reward=1041.47 +/- 21.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2600000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 842      |
| time/              |          |
|    episodes        | 2600     |
|    fps             | 643      |
|    time_elapsed    | 4041     |
|    total_timesteps | 2600000  |
---------------------------------
Eval num_timesteps=2601000, episode_reward=891.49 +/- 32.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 891      |
| time/              |          |
|    total_timesteps | 2601000  |
| train/             |          |
|    actor_loss      | -0.962   |
|    critic_loss     | 0.000817 |
|    ent_coef        | 0.000471 |
|    ent_coef_loss   | 5.54     |
|    learning_rate   | 0.0024   |
|    n_updates       | 12700    |
---------------------------------
Eval num_timesteps=2602000, episode_reward=972.48 +/- 44.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 972      |
| time/              |          |
|    total_timesteps | 2602000  |
---------------------------------
Eval num_timesteps=2603000, episode_reward=908.49 +/- 58.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 2603000  |
---------------------------------
Eval num_timesteps=2604000, episode_reward=772.68 +/- 49.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 2604000  |
| train/             |          |
|    actor_loss      | -0.936   |
|    critic_loss     | 0.000812 |
|    ent_coef        | 0.000473 |
|    ent_coef_loss   | 4.58     |
|    learning_rate   | 0.0024   |
|    n_updates       | 12710    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 845      |
| time/              |          |
|    episodes        | 2604     |
|    fps             | 643      |
|    time_elapsed    | 4047     |
|    total_timesteps | 2604000  |
---------------------------------
Eval num_timesteps=2605000, episode_reward=812.70 +/- 73.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 813      |
| time/              |          |
|    total_timesteps | 2605000  |
---------------------------------
Eval num_timesteps=2606000, episode_reward=957.21 +/- 18.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 2606000  |
| train/             |          |
|    actor_loss      | -0.912   |
|    critic_loss     | 0.000863 |
|    ent_coef        | 0.000477 |
|    ent_coef_loss   | 3.86     |
|    learning_rate   | 0.00239  |
|    n_updates       | 12720    |
---------------------------------
Eval num_timesteps=2607000, episode_reward=947.88 +/- 68.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 948      |
| time/              |          |
|    total_timesteps | 2607000  |
---------------------------------
Eval num_timesteps=2608000, episode_reward=1052.77 +/- 19.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2608000  |
| train/             |          |
|    actor_loss      | -0.938   |
|    critic_loss     | 0.00083  |
|    ent_coef        | 0.00048  |
|    ent_coef_loss   | -0.683   |
|    learning_rate   | 0.00239  |
|    n_updates       | 12730    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 845      |
| time/              |          |
|    episodes        | 2608     |
|    fps             | 643      |
|    time_elapsed    | 4053     |
|    total_timesteps | 2608000  |
---------------------------------
Eval num_timesteps=2609000, episode_reward=1061.28 +/- 35.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2609000  |
---------------------------------
Eval num_timesteps=2610000, episode_reward=1000.07 +/- 38.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 2610000  |
| train/             |          |
|    actor_loss      | -0.947   |
|    critic_loss     | 0.000709 |
|    ent_coef        | 0.000481 |
|    ent_coef_loss   | 0.259    |
|    learning_rate   | 0.00239  |
|    n_updates       | 12740    |
---------------------------------
Eval num_timesteps=2611000, episode_reward=933.77 +/- 42.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 934      |
| time/              |          |
|    total_timesteps | 2611000  |
---------------------------------
Eval num_timesteps=2612000, episode_reward=1064.94 +/- 53.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2612000  |
| train/             |          |
|    actor_loss      | -0.938   |
|    critic_loss     | 0.000835 |
|    ent_coef        | 0.000482 |
|    ent_coef_loss   | 9.09     |
|    learning_rate   | 0.00239  |
|    n_updates       | 12750    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 849      |
| time/              |          |
|    episodes        | 2612     |
|    fps             | 643      |
|    time_elapsed    | 4059     |
|    total_timesteps | 2612000  |
---------------------------------
Eval num_timesteps=2613000, episode_reward=1075.24 +/- 27.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2613000  |
---------------------------------
Eval num_timesteps=2614000, episode_reward=922.10 +/- 34.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 922      |
| time/              |          |
|    total_timesteps | 2614000  |
| train/             |          |
|    actor_loss      | -0.953   |
|    critic_loss     | 0.000742 |
|    ent_coef        | 0.000488 |
|    ent_coef_loss   | 5.07     |
|    learning_rate   | 0.00239  |
|    n_updates       | 12760    |
---------------------------------
Eval num_timesteps=2615000, episode_reward=977.56 +/- 19.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 978      |
| time/              |          |
|    total_timesteps | 2615000  |
---------------------------------
Eval num_timesteps=2616000, episode_reward=931.09 +/- 19.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 931      |
| time/              |          |
|    total_timesteps | 2616000  |
| train/             |          |
|    actor_loss      | -0.949   |
|    critic_loss     | 0.00069  |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | 3.02     |
|    learning_rate   | 0.00238  |
|    n_updates       | 12770    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 850      |
| time/              |          |
|    episodes        | 2616     |
|    fps             | 643      |
|    time_elapsed    | 4066     |
|    total_timesteps | 2616000  |
---------------------------------
Eval num_timesteps=2617000, episode_reward=905.24 +/- 70.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 2617000  |
---------------------------------
Eval num_timesteps=2618000, episode_reward=999.68 +/- 23.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 2618000  |
| train/             |          |
|    actor_loss      | -0.917   |
|    critic_loss     | 0.00081  |
|    ent_coef        | 0.000498 |
|    ent_coef_loss   | 7.66     |
|    learning_rate   | 0.00238  |
|    n_updates       | 12780    |
---------------------------------
Eval num_timesteps=2619000, episode_reward=1040.07 +/- 74.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2619000  |
---------------------------------
Eval num_timesteps=2620000, episode_reward=1074.60 +/- 25.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2620000  |
| train/             |          |
|    actor_loss      | -0.946   |
|    critic_loss     | 0.000777 |
|    ent_coef        | 0.000505 |
|    ent_coef_loss   | 1.95     |
|    learning_rate   | 0.00238  |
|    n_updates       | 12790    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 848      |
| time/              |          |
|    episodes        | 2620     |
|    fps             | 643      |
|    time_elapsed    | 4072     |
|    total_timesteps | 2620000  |
---------------------------------
Eval num_timesteps=2621000, episode_reward=1059.61 +/- 32.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2621000  |
---------------------------------
Eval num_timesteps=2622000, episode_reward=994.95 +/- 31.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 995      |
| time/              |          |
|    total_timesteps | 2622000  |
| train/             |          |
|    actor_loss      | -0.954   |
|    critic_loss     | 0.000804 |
|    ent_coef        | 0.000508 |
|    ent_coef_loss   | -3.41    |
|    learning_rate   | 0.00238  |
|    n_updates       | 12800    |
---------------------------------
Eval num_timesteps=2623000, episode_reward=988.12 +/- 62.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 2623000  |
---------------------------------
Eval num_timesteps=2624000, episode_reward=1076.19 +/- 37.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2624000  |
| train/             |          |
|    actor_loss      | -0.939   |
|    critic_loss     | 0.000858 |
|    ent_coef        | 0.000507 |
|    ent_coef_loss   | -1.24    |
|    learning_rate   | 0.00238  |
|    n_updates       | 12810    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 853      |
| time/              |          |
|    episodes        | 2624     |
|    fps             | 643      |
|    time_elapsed    | 4078     |
|    total_timesteps | 2624000  |
---------------------------------
Eval num_timesteps=2625000, episode_reward=1080.81 +/- 51.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2625000  |
---------------------------------
Eval num_timesteps=2626000, episode_reward=967.76 +/- 95.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 968      |
| time/              |          |
|    total_timesteps | 2626000  |
| train/             |          |
|    actor_loss      | -0.968   |
|    critic_loss     | 0.000878 |
|    ent_coef        | 0.000506 |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.00237  |
|    n_updates       | 12820    |
---------------------------------
Eval num_timesteps=2627000, episode_reward=987.81 +/- 38.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 2627000  |
---------------------------------
Eval num_timesteps=2628000, episode_reward=840.77 +/- 53.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 841      |
| time/              |          |
|    total_timesteps | 2628000  |
| train/             |          |
|    actor_loss      | -0.936   |
|    critic_loss     | 0.000905 |
|    ent_coef        | 0.000506 |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 0.00237  |
|    n_updates       | 12830    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 861      |
| time/              |          |
|    episodes        | 2628     |
|    fps             | 643      |
|    time_elapsed    | 4084     |
|    total_timesteps | 2628000  |
---------------------------------
Eval num_timesteps=2629000, episode_reward=871.37 +/- 13.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 871      |
| time/              |          |
|    total_timesteps | 2629000  |
---------------------------------
Eval num_timesteps=2630000, episode_reward=860.75 +/- 39.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 861      |
| time/              |          |
|    total_timesteps | 2630000  |
| train/             |          |
|    actor_loss      | -0.919   |
|    critic_loss     | 0.000966 |
|    ent_coef        | 0.000508 |
|    ent_coef_loss   | 0.222    |
|    learning_rate   | 0.00237  |
|    n_updates       | 12840    |
---------------------------------
Eval num_timesteps=2631000, episode_reward=850.71 +/- 64.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 851      |
| time/              |          |
|    total_timesteps | 2631000  |
---------------------------------
Eval num_timesteps=2632000, episode_reward=1008.73 +/- 17.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2632000  |
| train/             |          |
|    actor_loss      | -0.916   |
|    critic_loss     | 0.000895 |
|    ent_coef        | 0.000508 |
|    ent_coef_loss   | -3.37    |
|    learning_rate   | 0.00237  |
|    n_updates       | 12850    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 861      |
| time/              |          |
|    episodes        | 2632     |
|    fps             | 643      |
|    time_elapsed    | 4090     |
|    total_timesteps | 2632000  |
---------------------------------
Eval num_timesteps=2633000, episode_reward=1027.37 +/- 52.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2633000  |
---------------------------------
Eval num_timesteps=2634000, episode_reward=1080.06 +/- 31.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2634000  |
| train/             |          |
|    actor_loss      | -0.94    |
|    critic_loss     | 0.000924 |
|    ent_coef        | 0.000506 |
|    ent_coef_loss   | -0.708   |
|    learning_rate   | 0.00237  |
|    n_updates       | 12860    |
---------------------------------
Eval num_timesteps=2635000, episode_reward=1076.10 +/- 60.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2635000  |
---------------------------------
Eval num_timesteps=2636000, episode_reward=898.22 +/- 66.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 898      |
| time/              |          |
|    total_timesteps | 2636000  |
| train/             |          |
|    actor_loss      | -0.95    |
|    critic_loss     | 0.000833 |
|    ent_coef        | 0.000505 |
|    ent_coef_loss   | 1.68     |
|    learning_rate   | 0.00236  |
|    n_updates       | 12870    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 861      |
| time/              |          |
|    episodes        | 2636     |
|    fps             | 643      |
|    time_elapsed    | 4096     |
|    total_timesteps | 2636000  |
---------------------------------
Eval num_timesteps=2637000, episode_reward=893.15 +/- 53.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 893      |
| time/              |          |
|    total_timesteps | 2637000  |
---------------------------------
Eval num_timesteps=2638000, episode_reward=992.52 +/- 21.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 993      |
| time/              |          |
|    total_timesteps | 2638000  |
| train/             |          |
|    actor_loss      | -0.922   |
|    critic_loss     | 0.000857 |
|    ent_coef        | 0.000506 |
|    ent_coef_loss   | 1.96     |
|    learning_rate   | 0.00236  |
|    n_updates       | 12880    |
---------------------------------
Eval num_timesteps=2639000, episode_reward=985.06 +/- 49.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 985      |
| time/              |          |
|    total_timesteps | 2639000  |
---------------------------------
Eval num_timesteps=2640000, episode_reward=1001.28 +/- 30.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 2640000  |
| train/             |          |
|    actor_loss      | -0.927   |
|    critic_loss     | 0.000854 |
|    ent_coef        | 0.000508 |
|    ent_coef_loss   | 4.14     |
|    learning_rate   | 0.00236  |
|    n_updates       | 12890    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 863      |
| time/              |          |
|    episodes        | 2640     |
|    fps             | 643      |
|    time_elapsed    | 4102     |
|    total_timesteps | 2640000  |
---------------------------------
Eval num_timesteps=2641000, episode_reward=1010.10 +/- 22.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2641000  |
---------------------------------
Eval num_timesteps=2642000, episode_reward=831.41 +/- 16.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 2642000  |
| train/             |          |
|    actor_loss      | -0.939   |
|    critic_loss     | 0.000841 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | 6        |
|    learning_rate   | 0.00236  |
|    n_updates       | 12900    |
---------------------------------
Eval num_timesteps=2643000, episode_reward=916.40 +/- 42.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 916      |
| time/              |          |
|    total_timesteps | 2643000  |
---------------------------------
Eval num_timesteps=2644000, episode_reward=1023.52 +/- 51.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2644000  |
| train/             |          |
|    actor_loss      | -0.909   |
|    critic_loss     | 0.000887 |
|    ent_coef        | 0.000517 |
|    ent_coef_loss   | 3.75     |
|    learning_rate   | 0.00236  |
|    n_updates       | 12910    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 867      |
| time/              |          |
|    episodes        | 2644     |
|    fps             | 643      |
|    time_elapsed    | 4109     |
|    total_timesteps | 2644000  |
---------------------------------
Eval num_timesteps=2645000, episode_reward=1012.04 +/- 26.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2645000  |
---------------------------------
Eval num_timesteps=2646000, episode_reward=998.52 +/- 55.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 2646000  |
---------------------------------
Eval num_timesteps=2647000, episode_reward=1093.98 +/- 15.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2647000  |
| train/             |          |
|    actor_loss      | -0.942   |
|    critic_loss     | 0.000747 |
|    ent_coef        | 0.000522 |
|    ent_coef_loss   | 3.11     |
|    learning_rate   | 0.00235  |
|    n_updates       | 12920    |
---------------------------------
Eval num_timesteps=2648000, episode_reward=1073.15 +/- 29.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2648000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 876      |
| time/              |          |
|    episodes        | 2648     |
|    fps             | 643      |
|    time_elapsed    | 4115     |
|    total_timesteps | 2648000  |
---------------------------------
Eval num_timesteps=2649000, episode_reward=900.63 +/- 32.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 2649000  |
| train/             |          |
|    actor_loss      | -0.945   |
|    critic_loss     | 0.000853 |
|    ent_coef        | 0.000526 |
|    ent_coef_loss   | 2.35     |
|    learning_rate   | 0.00235  |
|    n_updates       | 12930    |
---------------------------------
Eval num_timesteps=2650000, episode_reward=871.55 +/- 65.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 872      |
| time/              |          |
|    total_timesteps | 2650000  |
---------------------------------
Eval num_timesteps=2651000, episode_reward=1038.98 +/- 17.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2651000  |
| train/             |          |
|    actor_loss      | -0.917   |
|    critic_loss     | 0.000911 |
|    ent_coef        | 0.000529 |
|    ent_coef_loss   | 3.17     |
|    learning_rate   | 0.00235  |
|    n_updates       | 12940    |
---------------------------------
Eval num_timesteps=2652000, episode_reward=1063.99 +/- 34.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2652000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 885      |
| time/              |          |
|    episodes        | 2652     |
|    fps             | 643      |
|    time_elapsed    | 4121     |
|    total_timesteps | 2652000  |
---------------------------------
Eval num_timesteps=2653000, episode_reward=1070.91 +/- 47.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2653000  |
| train/             |          |
|    actor_loss      | -0.938   |
|    critic_loss     | 0.000842 |
|    ent_coef        | 0.000533 |
|    ent_coef_loss   | -0.894   |
|    learning_rate   | 0.00235  |
|    n_updates       | 12950    |
---------------------------------
Eval num_timesteps=2654000, episode_reward=1094.29 +/- 45.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2654000  |
---------------------------------
Eval num_timesteps=2655000, episode_reward=1035.60 +/- 41.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2655000  |
| train/             |          |
|    actor_loss      | -0.953   |
|    critic_loss     | 0.000826 |
|    ent_coef        | 0.000533 |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.00235  |
|    n_updates       | 12960    |
---------------------------------
Eval num_timesteps=2656000, episode_reward=1035.54 +/- 46.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2656000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 889      |
| time/              |          |
|    episodes        | 2656     |
|    fps             | 643      |
|    time_elapsed    | 4127     |
|    total_timesteps | 2656000  |
---------------------------------
Eval num_timesteps=2657000, episode_reward=1040.29 +/- 42.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2657000  |
| train/             |          |
|    actor_loss      | -0.934   |
|    critic_loss     | 0.000784 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.00234  |
|    n_updates       | 12970    |
---------------------------------
Eval num_timesteps=2658000, episode_reward=1001.95 +/- 41.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 2658000  |
---------------------------------
Eval num_timesteps=2659000, episode_reward=1039.09 +/- 26.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2659000  |
| train/             |          |
|    actor_loss      | -0.938   |
|    critic_loss     | 0.000887 |
|    ent_coef        | 0.000531 |
|    ent_coef_loss   | -1.59    |
|    learning_rate   | 0.00234  |
|    n_updates       | 12980    |
---------------------------------
Eval num_timesteps=2660000, episode_reward=990.10 +/- 34.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 990      |
| time/              |          |
|    total_timesteps | 2660000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 891      |
| time/              |          |
|    episodes        | 2660     |
|    fps             | 643      |
|    time_elapsed    | 4133     |
|    total_timesteps | 2660000  |
---------------------------------
Eval num_timesteps=2661000, episode_reward=945.68 +/- 55.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 2661000  |
| train/             |          |
|    actor_loss      | -0.924   |
|    critic_loss     | 0.000946 |
|    ent_coef        | 0.00053  |
|    ent_coef_loss   | 0.583    |
|    learning_rate   | 0.00234  |
|    n_updates       | 12990    |
---------------------------------
Eval num_timesteps=2662000, episode_reward=903.05 +/- 32.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 2662000  |
---------------------------------
Eval num_timesteps=2663000, episode_reward=1033.78 +/- 62.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2663000  |
| train/             |          |
|    actor_loss      | -0.915   |
|    critic_loss     | 0.000838 |
|    ent_coef        | 0.000529 |
|    ent_coef_loss   | 0.793    |
|    learning_rate   | 0.00234  |
|    n_updates       | 13000    |
---------------------------------
Eval num_timesteps=2664000, episode_reward=1016.87 +/- 25.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2664000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 903      |
| time/              |          |
|    episodes        | 2664     |
|    fps             | 643      |
|    time_elapsed    | 4139     |
|    total_timesteps | 2664000  |
---------------------------------
Eval num_timesteps=2665000, episode_reward=1008.00 +/- 35.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2665000  |
| train/             |          |
|    actor_loss      | -0.934   |
|    critic_loss     | 0.000863 |
|    ent_coef        | 0.00053  |
|    ent_coef_loss   | 2.26     |
|    learning_rate   | 0.00234  |
|    n_updates       | 13010    |
---------------------------------
Eval num_timesteps=2666000, episode_reward=1047.78 +/- 37.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2666000  |
---------------------------------
Eval num_timesteps=2667000, episode_reward=1067.74 +/- 30.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2667000  |
| train/             |          |
|    actor_loss      | -0.928   |
|    critic_loss     | 0.000926 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.00233  |
|    n_updates       | 13020    |
---------------------------------
Eval num_timesteps=2668000, episode_reward=1056.84 +/- 42.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2668000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 910      |
| time/              |          |
|    episodes        | 2668     |
|    fps             | 643      |
|    time_elapsed    | 4145     |
|    total_timesteps | 2668000  |
---------------------------------
Eval num_timesteps=2669000, episode_reward=998.45 +/- 25.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 998      |
| time/              |          |
|    total_timesteps | 2669000  |
| train/             |          |
|    actor_loss      | -0.934   |
|    critic_loss     | 0.000812 |
|    ent_coef        | 0.000533 |
|    ent_coef_loss   | -1.41    |
|    learning_rate   | 0.00233  |
|    n_updates       | 13030    |
---------------------------------
Eval num_timesteps=2670000, episode_reward=1041.59 +/- 29.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2670000  |
---------------------------------
Eval num_timesteps=2671000, episode_reward=941.56 +/- 34.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 942      |
| time/              |          |
|    total_timesteps | 2671000  |
| train/             |          |
|    actor_loss      | -0.907   |
|    critic_loss     | 0.000755 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | -5.29    |
|    learning_rate   | 0.00233  |
|    n_updates       | 13040    |
---------------------------------
Eval num_timesteps=2672000, episode_reward=928.24 +/- 12.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 928      |
| time/              |          |
|    total_timesteps | 2672000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 907      |
| time/              |          |
|    episodes        | 2672     |
|    fps             | 643      |
|    time_elapsed    | 4152     |
|    total_timesteps | 2672000  |
---------------------------------
Eval num_timesteps=2673000, episode_reward=962.92 +/- 34.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 963      |
| time/              |          |
|    total_timesteps | 2673000  |
| train/             |          |
|    actor_loss      | -0.919   |
|    critic_loss     | 0.000879 |
|    ent_coef        | 0.000529 |
|    ent_coef_loss   | 0.762    |
|    learning_rate   | 0.00233  |
|    n_updates       | 13050    |
---------------------------------
Eval num_timesteps=2674000, episode_reward=1016.98 +/- 26.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2674000  |
---------------------------------
Eval num_timesteps=2675000, episode_reward=51.60 +/- 429.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 51.6     |
| time/              |          |
|    total_timesteps | 2675000  |
| train/             |          |
|    actor_loss      | -0.918   |
|    critic_loss     | 0.000796 |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | -3.31    |
|    learning_rate   | 0.00233  |
|    n_updates       | 13060    |
---------------------------------
Eval num_timesteps=2676000, episode_reward=285.09 +/- 550.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 285      |
| time/              |          |
|    total_timesteps | 2676000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 917      |
| time/              |          |
|    episodes        | 2676     |
|    fps             | 643      |
|    time_elapsed    | 4158     |
|    total_timesteps | 2676000  |
---------------------------------
Eval num_timesteps=2677000, episode_reward=-210.66 +/- 0.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -211     |
| time/              |          |
|    total_timesteps | 2677000  |
| train/             |          |
|    actor_loss      | -0.904   |
|    critic_loss     | 0.000773 |
|    ent_coef        | 0.000524 |
|    ent_coef_loss   | -3.31    |
|    learning_rate   | 0.00232  |
|    n_updates       | 13070    |
---------------------------------
Eval num_timesteps=2678000, episode_reward=-209.80 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -210     |
| time/              |          |
|    total_timesteps | 2678000  |
---------------------------------
Eval num_timesteps=2679000, episode_reward=-342.81 +/- 1.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -343     |
| time/              |          |
|    total_timesteps | 2679000  |
| train/             |          |
|    actor_loss      | -0.702   |
|    critic_loss     | 0.00063  |
|    ent_coef        | 0.000517 |
|    ent_coef_loss   | -15.6    |
|    learning_rate   | 0.00232  |
|    n_updates       | 13080    |
---------------------------------
Eval num_timesteps=2680000, episode_reward=-344.05 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 2680000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 879      |
| time/              |          |
|    episodes        | 2680     |
|    fps             | 643      |
|    time_elapsed    | 4164     |
|    total_timesteps | 2680000  |
---------------------------------
Eval num_timesteps=2681000, episode_reward=-578.60 +/- 18.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -579     |
| time/              |          |
|    total_timesteps | 2681000  |
| train/             |          |
|    actor_loss      | -0.673   |
|    critic_loss     | 0.000219 |
|    ent_coef        | 0.000506 |
|    ent_coef_loss   | -14.5    |
|    learning_rate   | 0.00232  |
|    n_updates       | 13090    |
---------------------------------
Eval num_timesteps=2682000, episode_reward=-519.72 +/- 31.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -520     |
| time/              |          |
|    total_timesteps | 2682000  |
---------------------------------
Eval num_timesteps=2683000, episode_reward=-305.23 +/- 2.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 2683000  |
| train/             |          |
|    actor_loss      | -0.88    |
|    critic_loss     | 0.00159  |
|    ent_coef        | 0.000493 |
|    ent_coef_loss   | -0.955   |
|    learning_rate   | 0.00232  |
|    n_updates       | 13100    |
---------------------------------
Eval num_timesteps=2684000, episode_reward=-296.40 +/- 18.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 2684000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 865      |
| time/              |          |
|    episodes        | 2684     |
|    fps             | 643      |
|    time_elapsed    | 4171     |
|    total_timesteps | 2684000  |
---------------------------------
Eval num_timesteps=2685000, episode_reward=969.99 +/- 28.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 970      |
| time/              |          |
|    total_timesteps | 2685000  |
| train/             |          |
|    actor_loss      | -0.843   |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.000486 |
|    ent_coef_loss   | 3.24     |
|    learning_rate   | 0.00232  |
|    n_updates       | 13110    |
---------------------------------
Eval num_timesteps=2686000, episode_reward=1031.61 +/- 24.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2686000  |
---------------------------------
Eval num_timesteps=2687000, episode_reward=1019.48 +/- 28.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2687000  |
| train/             |          |
|    actor_loss      | -0.901   |
|    critic_loss     | 0.000997 |
|    ent_coef        | 0.000487 |
|    ent_coef_loss   | 1.02     |
|    learning_rate   | 0.00231  |
|    n_updates       | 13120    |
---------------------------------
Eval num_timesteps=2688000, episode_reward=1037.90 +/- 40.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2688000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 860      |
| time/              |          |
|    episodes        | 2688     |
|    fps             | 643      |
|    time_elapsed    | 4177     |
|    total_timesteps | 2688000  |
---------------------------------
Eval num_timesteps=2689000, episode_reward=1003.80 +/- 42.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 2689000  |
---------------------------------
Eval num_timesteps=2690000, episode_reward=-155.39 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 2690000  |
| train/             |          |
|    actor_loss      | -0.909   |
|    critic_loss     | 0.000932 |
|    ent_coef        | 0.000487 |
|    ent_coef_loss   | -3.29    |
|    learning_rate   | 0.00231  |
|    n_updates       | 13130    |
---------------------------------
Eval num_timesteps=2691000, episode_reward=-155.56 +/- 1.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 2691000  |
---------------------------------
Eval num_timesteps=2692000, episode_reward=-192.69 +/- 2.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 2692000  |
| train/             |          |
|    actor_loss      | -0.915   |
|    critic_loss     | 0.000794 |
|    ent_coef        | 0.000485 |
|    ent_coef_loss   | 3.22     |
|    learning_rate   | 0.00231  |
|    n_updates       | 13140    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 886      |
| time/              |          |
|    episodes        | 2692     |
|    fps             | 643      |
|    time_elapsed    | 4183     |
|    total_timesteps | 2692000  |
---------------------------------
Eval num_timesteps=2693000, episode_reward=-191.55 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 2693000  |
---------------------------------
Eval num_timesteps=2694000, episode_reward=23.96 +/- 456.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 24       |
| time/              |          |
|    total_timesteps | 2694000  |
| train/             |          |
|    actor_loss      | -0.882   |
|    critic_loss     | 0.000794 |
|    ent_coef        | 0.000486 |
|    ent_coef_loss   | -0.49    |
|    learning_rate   | 0.00231  |
|    n_updates       | 13150    |
---------------------------------
Eval num_timesteps=2695000, episode_reward=21.04 +/- 450.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 21       |
| time/              |          |
|    total_timesteps | 2695000  |
---------------------------------
Eval num_timesteps=2696000, episode_reward=912.93 +/- 31.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 913      |
| time/              |          |
|    total_timesteps | 2696000  |
| train/             |          |
|    actor_loss      | -0.928   |
|    critic_loss     | 0.000873 |
|    ent_coef        | 0.000487 |
|    ent_coef_loss   | 3.26     |
|    learning_rate   | 0.0023   |
|    n_updates       | 13160    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 888      |
| time/              |          |
|    episodes        | 2696     |
|    fps             | 643      |
|    time_elapsed    | 4190     |
|    total_timesteps | 2696000  |
---------------------------------
Eval num_timesteps=2697000, episode_reward=852.82 +/- 35.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 853      |
| time/              |          |
|    total_timesteps | 2697000  |
---------------------------------
Eval num_timesteps=2698000, episode_reward=810.73 +/- 71.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 811      |
| time/              |          |
|    total_timesteps | 2698000  |
| train/             |          |
|    actor_loss      | -0.894   |
|    critic_loss     | 0.000857 |
|    ent_coef        | 0.00049  |
|    ent_coef_loss   | 7.58     |
|    learning_rate   | 0.0023   |
|    n_updates       | 13170    |
---------------------------------
Eval num_timesteps=2699000, episode_reward=830.82 +/- 39.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 831      |
| time/              |          |
|    total_timesteps | 2699000  |
---------------------------------
Eval num_timesteps=2700000, episode_reward=852.37 +/- 34.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 852      |
| time/              |          |
|    total_timesteps | 2700000  |
| train/             |          |
|    actor_loss      | -0.889   |
|    critic_loss     | 0.000812 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | 2.72     |
|    learning_rate   | 0.0023   |
|    n_updates       | 13180    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 888      |
| time/              |          |
|    episodes        | 2700     |
|    fps             | 643      |
|    time_elapsed    | 4196     |
|    total_timesteps | 2700000  |
---------------------------------
Eval num_timesteps=2701000, episode_reward=902.79 +/- 35.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 903      |
| time/              |          |
|    total_timesteps | 2701000  |
---------------------------------
Eval num_timesteps=2702000, episode_reward=987.79 +/- 34.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 988      |
| time/              |          |
|    total_timesteps | 2702000  |
| train/             |          |
|    actor_loss      | -0.89    |
|    critic_loss     | 0.000775 |
|    ent_coef        | 0.0005   |
|    ent_coef_loss   | -4.92    |
|    learning_rate   | 0.0023   |
|    n_updates       | 13190    |
---------------------------------
Eval num_timesteps=2703000, episode_reward=986.37 +/- 31.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 986      |
| time/              |          |
|    total_timesteps | 2703000  |
---------------------------------
Eval num_timesteps=2704000, episode_reward=1109.97 +/- 57.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2704000  |
| train/             |          |
|    actor_loss      | -0.895   |
|    critic_loss     | 0.000816 |
|    ent_coef        | 0.000498 |
|    ent_coef_loss   | -2.36    |
|    learning_rate   | 0.0023   |
|    n_updates       | 13200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 887      |
| time/              |          |
|    episodes        | 2704     |
|    fps             | 643      |
|    time_elapsed    | 4202     |
|    total_timesteps | 2704000  |
---------------------------------
Eval num_timesteps=2705000, episode_reward=1094.67 +/- 29.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2705000  |
---------------------------------
Eval num_timesteps=2706000, episode_reward=1083.31 +/- 17.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2706000  |
| train/             |          |
|    actor_loss      | -0.922   |
|    critic_loss     | 0.000698 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | -2.1     |
|    learning_rate   | 0.00229  |
|    n_updates       | 13210    |
---------------------------------
Eval num_timesteps=2707000, episode_reward=1105.52 +/- 13.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2707000  |
---------------------------------
Eval num_timesteps=2708000, episode_reward=1097.01 +/- 67.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 2708000  |
| train/             |          |
|    actor_loss      | -0.918   |
|    critic_loss     | 0.000676 |
|    ent_coef        | 0.000493 |
|    ent_coef_loss   | 2.4      |
|    learning_rate   | 0.00229  |
|    n_updates       | 13220    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 892      |
| time/              |          |
|    episodes        | 2708     |
|    fps             | 643      |
|    time_elapsed    | 4208     |
|    total_timesteps | 2708000  |
---------------------------------
Eval num_timesteps=2709000, episode_reward=1083.95 +/- 29.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2709000  |
---------------------------------
Eval num_timesteps=2710000, episode_reward=1080.66 +/- 25.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2710000  |
| train/             |          |
|    actor_loss      | -0.922   |
|    critic_loss     | 0.000791 |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | 2.54     |
|    learning_rate   | 0.00229  |
|    n_updates       | 13230    |
---------------------------------
Eval num_timesteps=2711000, episode_reward=1110.87 +/- 19.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2711000  |
---------------------------------
Eval num_timesteps=2712000, episode_reward=1031.21 +/- 50.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2712000  |
| train/             |          |
|    actor_loss      | -0.91    |
|    critic_loss     | 0.00075  |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | -0.749   |
|    learning_rate   | 0.00229  |
|    n_updates       | 13240    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 897      |
| time/              |          |
|    episodes        | 2712     |
|    fps             | 643      |
|    time_elapsed    | 4214     |
|    total_timesteps | 2712000  |
---------------------------------
Eval num_timesteps=2713000, episode_reward=1033.90 +/- 27.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2713000  |
---------------------------------
Eval num_timesteps=2714000, episode_reward=1151.70 +/- 37.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2714000  |
| train/             |          |
|    actor_loss      | -0.907   |
|    critic_loss     | 0.000767 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | -0.112   |
|    learning_rate   | 0.00229  |
|    n_updates       | 13250    |
---------------------------------
Eval num_timesteps=2715000, episode_reward=1112.16 +/- 40.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2715000  |
---------------------------------
Eval num_timesteps=2716000, episode_reward=1178.52 +/- 37.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2716000  |
| train/             |          |
|    actor_loss      | -0.927   |
|    critic_loss     | 0.000785 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | -0.761   |
|    learning_rate   | 0.00228  |
|    n_updates       | 13260    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 902      |
| time/              |          |
|    episodes        | 2716     |
|    fps             | 643      |
|    time_elapsed    | 4220     |
|    total_timesteps | 2716000  |
---------------------------------
Eval num_timesteps=2717000, episode_reward=1192.52 +/- 46.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2717000  |
---------------------------------
Eval num_timesteps=2718000, episode_reward=993.90 +/- 99.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 994      |
| time/              |          |
|    total_timesteps | 2718000  |
| train/             |          |
|    actor_loss      | -0.923   |
|    critic_loss     | 0.000801 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | -1.23    |
|    learning_rate   | 0.00228  |
|    n_updates       | 13270    |
---------------------------------
Eval num_timesteps=2719000, episode_reward=1058.34 +/- 46.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2719000  |
---------------------------------
Eval num_timesteps=2720000, episode_reward=1029.01 +/- 49.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 2720000  |
| train/             |          |
|    actor_loss      | -0.897   |
|    critic_loss     | 0.000845 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | 2.72     |
|    learning_rate   | 0.00228  |
|    n_updates       | 13280    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 907      |
| time/              |          |
|    episodes        | 2720     |
|    fps             | 643      |
|    time_elapsed    | 4226     |
|    total_timesteps | 2720000  |
---------------------------------
Eval num_timesteps=2721000, episode_reward=981.23 +/- 35.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 981      |
| time/              |          |
|    total_timesteps | 2721000  |
---------------------------------
Eval num_timesteps=2722000, episode_reward=1086.24 +/- 41.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2722000  |
| train/             |          |
|    actor_loss      | -0.901   |
|    critic_loss     | 0.000895 |
|    ent_coef        | 0.000497 |
|    ent_coef_loss   | 2.36     |
|    learning_rate   | 0.00228  |
|    n_updates       | 13290    |
---------------------------------
Eval num_timesteps=2723000, episode_reward=1052.65 +/- 50.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2723000  |
---------------------------------
Eval num_timesteps=2724000, episode_reward=1066.10 +/- 34.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2724000  |
| train/             |          |
|    actor_loss      | -0.909   |
|    critic_loss     | 0.000937 |
|    ent_coef        | 0.000499 |
|    ent_coef_loss   | 2.43     |
|    learning_rate   | 0.00228  |
|    n_updates       | 13300    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 908      |
| time/              |          |
|    episodes        | 2724     |
|    fps             | 643      |
|    time_elapsed    | 4233     |
|    total_timesteps | 2724000  |
---------------------------------
Eval num_timesteps=2725000, episode_reward=1058.79 +/- 47.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2725000  |
---------------------------------
Eval num_timesteps=2726000, episode_reward=1043.41 +/- 42.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2726000  |
| train/             |          |
|    actor_loss      | -0.904   |
|    critic_loss     | 0.000834 |
|    ent_coef        | 0.000502 |
|    ent_coef_loss   | 4.8      |
|    learning_rate   | 0.00227  |
|    n_updates       | 13310    |
---------------------------------
Eval num_timesteps=2727000, episode_reward=1060.00 +/- 44.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2727000  |
---------------------------------
Eval num_timesteps=2728000, episode_reward=389.97 +/- 487.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 390      |
| time/              |          |
|    total_timesteps | 2728000  |
| train/             |          |
|    actor_loss      | -0.899   |
|    critic_loss     | 0.000922 |
|    ent_coef        | 0.000507 |
|    ent_coef_loss   | 5.27     |
|    learning_rate   | 0.00227  |
|    n_updates       | 13320    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 908      |
| time/              |          |
|    episodes        | 2728     |
|    fps             | 643      |
|    time_elapsed    | 4239     |
|    total_timesteps | 2728000  |
---------------------------------
Eval num_timesteps=2729000, episode_reward=199.32 +/- 497.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 199      |
| time/              |          |
|    total_timesteps | 2729000  |
---------------------------------
Eval num_timesteps=2730000, episode_reward=-225.38 +/- 1.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 2730000  |
| train/             |          |
|    actor_loss      | -0.867   |
|    critic_loss     | 0.00093  |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | 0.889    |
|    learning_rate   | 0.00227  |
|    n_updates       | 13330    |
---------------------------------
Eval num_timesteps=2731000, episode_reward=-225.41 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 2731000  |
---------------------------------
Eval num_timesteps=2732000, episode_reward=-223.74 +/- 1.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -224     |
| time/              |          |
|    total_timesteps | 2732000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 908      |
| time/              |          |
|    episodes        | 2732     |
|    fps             | 643      |
|    time_elapsed    | 4245     |
|    total_timesteps | 2732000  |
---------------------------------
Eval num_timesteps=2733000, episode_reward=-275.06 +/- 1.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 2733000  |
| train/             |          |
|    actor_loss      | -0.875   |
|    critic_loss     | 0.001    |
|    ent_coef        | 0.000516 |
|    ent_coef_loss   | 2.72     |
|    learning_rate   | 0.00227  |
|    n_updates       | 13340    |
---------------------------------
Eval num_timesteps=2734000, episode_reward=-274.95 +/- 1.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -275     |
| time/              |          |
|    total_timesteps | 2734000  |
---------------------------------
Eval num_timesteps=2735000, episode_reward=1117.53 +/- 50.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2735000  |
| train/             |          |
|    actor_loss      | -0.648   |
|    critic_loss     | 0.000128 |
|    ent_coef        | 0.000517 |
|    ent_coef_loss   | -10      |
|    learning_rate   | 0.00227  |
|    n_updates       | 13350    |
---------------------------------
Eval num_timesteps=2736000, episode_reward=1134.49 +/- 29.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2736000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 884      |
| time/              |          |
|    episodes        | 2736     |
|    fps             | 643      |
|    time_elapsed    | 4251     |
|    total_timesteps | 2736000  |
---------------------------------
Eval num_timesteps=2737000, episode_reward=1087.93 +/- 36.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2737000  |
| train/             |          |
|    actor_loss      | -0.891   |
|    critic_loss     | 0.00337  |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | 0.889    |
|    learning_rate   | 0.00226  |
|    n_updates       | 13360    |
---------------------------------
Eval num_timesteps=2738000, episode_reward=1090.60 +/- 47.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2738000  |
---------------------------------
Eval num_timesteps=2739000, episode_reward=995.58 +/- 64.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 996      |
| time/              |          |
|    total_timesteps | 2739000  |
| train/             |          |
|    actor_loss      | -0.9     |
|    critic_loss     | 0.00167  |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | 7.33     |
|    learning_rate   | 0.00226  |
|    n_updates       | 13370    |
---------------------------------
Eval num_timesteps=2740000, episode_reward=948.53 +/- 49.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 949      |
| time/              |          |
|    total_timesteps | 2740000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 888      |
| time/              |          |
|    episodes        | 2740     |
|    fps             | 643      |
|    time_elapsed    | 4257     |
|    total_timesteps | 2740000  |
---------------------------------
Eval num_timesteps=2741000, episode_reward=912.49 +/- 61.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 912      |
| time/              |          |
|    total_timesteps | 2741000  |
| train/             |          |
|    actor_loss      | -0.89    |
|    critic_loss     | 0.00123  |
|    ent_coef        | 0.000516 |
|    ent_coef_loss   | 4.12     |
|    learning_rate   | 0.00226  |
|    n_updates       | 13380    |
---------------------------------
Eval num_timesteps=2742000, episode_reward=958.91 +/- 73.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 959      |
| time/              |          |
|    total_timesteps | 2742000  |
---------------------------------
Eval num_timesteps=2743000, episode_reward=900.61 +/- 19.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 901      |
| time/              |          |
|    total_timesteps | 2743000  |
| train/             |          |
|    actor_loss      | -0.856   |
|    critic_loss     | 0.00104  |
|    ent_coef        | 0.000521 |
|    ent_coef_loss   | 2.14     |
|    learning_rate   | 0.00226  |
|    n_updates       | 13390    |
---------------------------------
Eval num_timesteps=2744000, episode_reward=905.00 +/- 59.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 905      |
| time/              |          |
|    total_timesteps | 2744000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 885      |
| time/              |          |
|    episodes        | 2744     |
|    fps             | 643      |
|    time_elapsed    | 4263     |
|    total_timesteps | 2744000  |
---------------------------------
Eval num_timesteps=2745000, episode_reward=1076.34 +/- 62.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2745000  |
| train/             |          |
|    actor_loss      | -0.864   |
|    critic_loss     | 0.000832 |
|    ent_coef        | 0.000524 |
|    ent_coef_loss   | -4.13    |
|    learning_rate   | 0.00226  |
|    n_updates       | 13400    |
---------------------------------
Eval num_timesteps=2746000, episode_reward=1068.96 +/- 49.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2746000  |
---------------------------------
Eval num_timesteps=2747000, episode_reward=999.20 +/- 40.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 999      |
| time/              |          |
|    total_timesteps | 2747000  |
| train/             |          |
|    actor_loss      | -0.901   |
|    critic_loss     | 0.000952 |
|    ent_coef        | 0.000523 |
|    ent_coef_loss   | 3.1      |
|    learning_rate   | 0.00225  |
|    n_updates       | 13410    |
---------------------------------
Eval num_timesteps=2748000, episode_reward=1021.28 +/- 39.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2748000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 883      |
| time/              |          |
|    episodes        | 2748     |
|    fps             | 643      |
|    time_elapsed    | 4270     |
|    total_timesteps | 2748000  |
---------------------------------
Eval num_timesteps=2749000, episode_reward=1084.98 +/- 30.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2749000  |
| train/             |          |
|    actor_loss      | -0.887   |
|    critic_loss     | 0.000924 |
|    ent_coef        | 0.000524 |
|    ent_coef_loss   | -0.605   |
|    learning_rate   | 0.00225  |
|    n_updates       | 13420    |
---------------------------------
Eval num_timesteps=2750000, episode_reward=1102.26 +/- 43.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 2750000  |
---------------------------------
Eval num_timesteps=2751000, episode_reward=1120.84 +/- 23.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2751000  |
| train/             |          |
|    actor_loss      | -0.901   |
|    critic_loss     | 0.000925 |
|    ent_coef        | 0.000525 |
|    ent_coef_loss   | 2.98     |
|    learning_rate   | 0.00225  |
|    n_updates       | 13430    |
---------------------------------
Eval num_timesteps=2752000, episode_reward=1091.48 +/- 19.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2752000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 889      |
| time/              |          |
|    episodes        | 2752     |
|    fps             | 643      |
|    time_elapsed    | 4276     |
|    total_timesteps | 2752000  |
---------------------------------
Eval num_timesteps=2753000, episode_reward=1158.83 +/- 18.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2753000  |
| train/             |          |
|    actor_loss      | -0.908   |
|    critic_loss     | 0.000739 |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.00225  |
|    n_updates       | 13440    |
---------------------------------
Eval num_timesteps=2754000, episode_reward=1147.99 +/- 9.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2754000  |
---------------------------------
Eval num_timesteps=2755000, episode_reward=1009.45 +/- 19.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 2755000  |
| train/             |          |
|    actor_loss      | -0.899   |
|    critic_loss     | 0.000922 |
|    ent_coef        | 0.000529 |
|    ent_coef_loss   | -1.82    |
|    learning_rate   | 0.00225  |
|    n_updates       | 13450    |
---------------------------------
Eval num_timesteps=2756000, episode_reward=979.83 +/- 68.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 980      |
| time/              |          |
|    total_timesteps | 2756000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 888      |
| time/              |          |
|    episodes        | 2756     |
|    fps             | 643      |
|    time_elapsed    | 4282     |
|    total_timesteps | 2756000  |
---------------------------------
Eval num_timesteps=2757000, episode_reward=1090.17 +/- 15.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2757000  |
| train/             |          |
|    actor_loss      | -0.874   |
|    critic_loss     | 0.000892 |
|    ent_coef        | 0.000528 |
|    ent_coef_loss   | -0.177   |
|    learning_rate   | 0.00224  |
|    n_updates       | 13460    |
---------------------------------
Eval num_timesteps=2758000, episode_reward=1145.23 +/- 39.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2758000  |
---------------------------------
Eval num_timesteps=2759000, episode_reward=1152.02 +/- 54.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2759000  |
| train/             |          |
|    actor_loss      | -0.897   |
|    critic_loss     | 0.000855 |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | -5.9     |
|    learning_rate   | 0.00224  |
|    n_updates       | 13470    |
---------------------------------
Eval num_timesteps=2760000, episode_reward=1134.35 +/- 41.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2760000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 890      |
| time/              |          |
|    episodes        | 2760     |
|    fps             | 643      |
|    time_elapsed    | 4288     |
|    total_timesteps | 2760000  |
---------------------------------
Eval num_timesteps=2761000, episode_reward=1092.65 +/- 26.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2761000  |
| train/             |          |
|    actor_loss      | -0.903   |
|    critic_loss     | 0.000863 |
|    ent_coef        | 0.000522 |
|    ent_coef_loss   | -3.1     |
|    learning_rate   | 0.00224  |
|    n_updates       | 13480    |
---------------------------------
Eval num_timesteps=2762000, episode_reward=1067.42 +/- 7.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2762000  |
---------------------------------
Eval num_timesteps=2763000, episode_reward=1125.83 +/- 6.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2763000  |
| train/             |          |
|    actor_loss      | -0.876   |
|    critic_loss     | 0.000896 |
|    ent_coef        | 0.000518 |
|    ent_coef_loss   | -0.906   |
|    learning_rate   | 0.00224  |
|    n_updates       | 13490    |
---------------------------------
Eval num_timesteps=2764000, episode_reward=1095.74 +/- 33.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 2764000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 892      |
| time/              |          |
|    episodes        | 2764     |
|    fps             | 643      |
|    time_elapsed    | 4294     |
|    total_timesteps | 2764000  |
---------------------------------
Eval num_timesteps=2765000, episode_reward=1203.63 +/- 30.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2765000  |
| train/             |          |
|    actor_loss      | -0.887   |
|    critic_loss     | 0.000799 |
|    ent_coef        | 0.000516 |
|    ent_coef_loss   | -1.94    |
|    learning_rate   | 0.00224  |
|    n_updates       | 13500    |
---------------------------------
Eval num_timesteps=2766000, episode_reward=1214.59 +/- 9.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2766000  |
---------------------------------
Eval num_timesteps=2767000, episode_reward=113.91 +/- 560.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 114      |
| time/              |          |
|    total_timesteps | 2767000  |
| train/             |          |
|    actor_loss      | -0.912   |
|    critic_loss     | 0.000838 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | -0.276   |
|    learning_rate   | 0.00223  |
|    n_updates       | 13510    |
---------------------------------
Eval num_timesteps=2768000, episode_reward=-164.02 +/- 1.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 2768000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 896      |
| time/              |          |
|    episodes        | 2768     |
|    fps             | 643      |
|    time_elapsed    | 4300     |
|    total_timesteps | 2768000  |
---------------------------------
Eval num_timesteps=2769000, episode_reward=104.10 +/- 528.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 104      |
| time/              |          |
|    total_timesteps | 2769000  |
| train/             |          |
|    actor_loss      | -0.9     |
|    critic_loss     | 0.00085  |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | 0.449    |
|    learning_rate   | 0.00223  |
|    n_updates       | 13520    |
---------------------------------
Eval num_timesteps=2770000, episode_reward=-157.25 +/- 1.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 2770000  |
---------------------------------
Eval num_timesteps=2771000, episode_reward=1124.70 +/- 29.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2771000  |
| train/             |          |
|    actor_loss      | -0.732   |
|    critic_loss     | 0.000489 |
|    ent_coef        | 0.000509 |
|    ent_coef_loss   | -15.4    |
|    learning_rate   | 0.00223  |
|    n_updates       | 13530    |
---------------------------------
Eval num_timesteps=2772000, episode_reward=1106.93 +/- 82.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2772000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 888      |
| time/              |          |
|    episodes        | 2772     |
|    fps             | 643      |
|    time_elapsed    | 4306     |
|    total_timesteps | 2772000  |
---------------------------------
Eval num_timesteps=2773000, episode_reward=882.70 +/- 101.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 883      |
| time/              |          |
|    total_timesteps | 2773000  |
| train/             |          |
|    actor_loss      | -0.905   |
|    critic_loss     | 0.000869 |
|    ent_coef        | 0.000499 |
|    ent_coef_loss   | 1.74     |
|    learning_rate   | 0.00223  |
|    n_updates       | 13540    |
---------------------------------
Eval num_timesteps=2774000, episode_reward=891.52 +/- 83.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 892      |
| time/              |          |
|    total_timesteps | 2774000  |
---------------------------------
Eval num_timesteps=2775000, episode_reward=907.16 +/- 62.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 2775000  |
---------------------------------
Eval num_timesteps=2776000, episode_reward=1051.62 +/- 37.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2776000  |
| train/             |          |
|    actor_loss      | -0.865   |
|    critic_loss     | 0.000808 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | -0.781   |
|    learning_rate   | 0.00222  |
|    n_updates       | 13550    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 890      |
| time/              |          |
|    episodes        | 2776     |
|    fps             | 643      |
|    time_elapsed    | 4313     |
|    total_timesteps | 2776000  |
---------------------------------
Eval num_timesteps=2777000, episode_reward=1072.26 +/- 84.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2777000  |
---------------------------------
Eval num_timesteps=2778000, episode_reward=1089.55 +/- 50.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2778000  |
| train/             |          |
|    actor_loss      | -0.899   |
|    critic_loss     | 0.000831 |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | -1.34    |
|    learning_rate   | 0.00222  |
|    n_updates       | 13560    |
---------------------------------
Eval num_timesteps=2779000, episode_reward=1086.85 +/- 38.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2779000  |
---------------------------------
Eval num_timesteps=2780000, episode_reward=1080.35 +/- 33.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2780000  |
| train/             |          |
|    actor_loss      | -0.673   |
|    critic_loss     | 0.000635 |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | 3.67     |
|    learning_rate   | 0.00222  |
|    n_updates       | 13570    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 916      |
| time/              |          |
|    episodes        | 2780     |
|    fps             | 643      |
|    time_elapsed    | 4319     |
|    total_timesteps | 2780000  |
---------------------------------
Eval num_timesteps=2781000, episode_reward=1089.70 +/- 44.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2781000  |
---------------------------------
Eval num_timesteps=2782000, episode_reward=275.02 +/- 629.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 275      |
| time/              |          |
|    total_timesteps | 2782000  |
| train/             |          |
|    actor_loss      | -0.871   |
|    critic_loss     | 0.000804 |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | -8.03    |
|    learning_rate   | 0.00222  |
|    n_updates       | 13580    |
---------------------------------
Eval num_timesteps=2783000, episode_reward=15.71 +/- 506.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 15.7     |
| time/              |          |
|    total_timesteps | 2783000  |
---------------------------------
Eval num_timesteps=2784000, episode_reward=824.24 +/- 566.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 824      |
| time/              |          |
|    total_timesteps | 2784000  |
| train/             |          |
|    actor_loss      | -0.832   |
|    critic_loss     | 0.000833 |
|    ent_coef        | 0.000488 |
|    ent_coef_loss   | -6.5     |
|    learning_rate   | 0.00222  |
|    n_updates       | 13590    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 932      |
| time/              |          |
|    episodes        | 2784     |
|    fps             | 643      |
|    time_elapsed    | 4325     |
|    total_timesteps | 2784000  |
---------------------------------
Eval num_timesteps=2785000, episode_reward=1130.41 +/- 44.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2785000  |
---------------------------------
Eval num_timesteps=2786000, episode_reward=1231.39 +/- 20.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2786000  |
| train/             |          |
|    actor_loss      | -0.872   |
|    critic_loss     | 0.000934 |
|    ent_coef        | 0.000482 |
|    ent_coef_loss   | 2.17     |
|    learning_rate   | 0.00221  |
|    n_updates       | 13600    |
---------------------------------
Eval num_timesteps=2787000, episode_reward=1215.02 +/- 48.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2787000  |
---------------------------------
Eval num_timesteps=2788000, episode_reward=1174.75 +/- 25.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2788000  |
| train/             |          |
|    actor_loss      | -0.902   |
|    critic_loss     | 0.000826 |
|    ent_coef        | 0.00048  |
|    ent_coef_loss   | -3.09    |
|    learning_rate   | 0.00221  |
|    n_updates       | 13610    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 941      |
| time/              |          |
|    episodes        | 2788     |
|    fps             | 643      |
|    time_elapsed    | 4331     |
|    total_timesteps | 2788000  |
---------------------------------
Eval num_timesteps=2789000, episode_reward=1178.50 +/- 32.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2789000  |
---------------------------------
Eval num_timesteps=2790000, episode_reward=1156.14 +/- 19.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2790000  |
| train/             |          |
|    actor_loss      | -0.902   |
|    critic_loss     | 0.000703 |
|    ent_coef        | 0.000478 |
|    ent_coef_loss   | -1.06    |
|    learning_rate   | 0.00221  |
|    n_updates       | 13620    |
---------------------------------
Eval num_timesteps=2791000, episode_reward=1180.12 +/- 16.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2791000  |
---------------------------------
Eval num_timesteps=2792000, episode_reward=-146.86 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 2792000  |
| train/             |          |
|    actor_loss      | -0.893   |
|    critic_loss     | 0.00069  |
|    ent_coef        | 0.000476 |
|    ent_coef_loss   | -2.52    |
|    learning_rate   | 0.00221  |
|    n_updates       | 13630    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 948      |
| time/              |          |
|    episodes        | 2792     |
|    fps             | 643      |
|    time_elapsed    | 4337     |
|    total_timesteps | 2792000  |
---------------------------------
Eval num_timesteps=2793000, episode_reward=-147.23 +/- 1.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 2793000  |
---------------------------------
Eval num_timesteps=2794000, episode_reward=-404.97 +/- 1.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 2794000  |
| train/             |          |
|    actor_loss      | -0.686   |
|    critic_loss     | 0.000464 |
|    ent_coef        | 0.000469 |
|    ent_coef_loss   | -16.8    |
|    learning_rate   | 0.00221  |
|    n_updates       | 13640    |
---------------------------------
Eval num_timesteps=2795000, episode_reward=-406.30 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -406     |
| time/              |          |
|    total_timesteps | 2795000  |
---------------------------------
Eval num_timesteps=2796000, episode_reward=1243.42 +/- 19.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2796000  |
| train/             |          |
|    actor_loss      | -0.622   |
|    critic_loss     | 4.68e-05 |
|    ent_coef        | 0.000461 |
|    ent_coef_loss   | 2.18     |
|    learning_rate   | 0.0022   |
|    n_updates       | 13650    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 906      |
| time/              |          |
|    episodes        | 2796     |
|    fps             | 643      |
|    time_elapsed    | 4344     |
|    total_timesteps | 2796000  |
---------------------------------
Eval num_timesteps=2797000, episode_reward=1248.66 +/- 32.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 2797000  |
---------------------------------
Eval num_timesteps=2798000, episode_reward=1171.52 +/- 37.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2798000  |
| train/             |          |
|    actor_loss      | -0.896   |
|    critic_loss     | 0.000828 |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | 0.198    |
|    learning_rate   | 0.0022   |
|    n_updates       | 13660    |
---------------------------------
Eval num_timesteps=2799000, episode_reward=1187.01 +/- 31.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2799000  |
---------------------------------
Eval num_timesteps=2800000, episode_reward=217.22 +/- 465.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 217      |
| time/              |          |
|    total_timesteps | 2800000  |
| train/             |          |
|    actor_loss      | -0.882   |
|    critic_loss     | 0.000833 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | -2.47    |
|    learning_rate   | 0.0022   |
|    n_updates       | 13670    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 916      |
| time/              |          |
|    episodes        | 2800     |
|    fps             | 643      |
|    time_elapsed    | 4350     |
|    total_timesteps | 2800000  |
---------------------------------
Eval num_timesteps=2801000, episode_reward=728.44 +/- 604.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 728      |
| time/              |          |
|    total_timesteps | 2801000  |
---------------------------------
Eval num_timesteps=2802000, episode_reward=662.27 +/- 643.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 2802000  |
| train/             |          |
|    actor_loss      | -0.89    |
|    critic_loss     | 0.0008   |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | 1.61     |
|    learning_rate   | 0.0022   |
|    n_updates       | 13680    |
---------------------------------
Eval num_timesteps=2803000, episode_reward=661.60 +/- 644.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 662      |
| time/              |          |
|    total_timesteps | 2803000  |
---------------------------------
Eval num_timesteps=2804000, episode_reward=1256.72 +/- 31.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2804000  |
| train/             |          |
|    actor_loss      | -0.892   |
|    critic_loss     | 0.000755 |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | -1.57    |
|    learning_rate   | 0.0022   |
|    n_updates       | 13690    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 927      |
| time/              |          |
|    episodes        | 2804     |
|    fps             | 643      |
|    time_elapsed    | 4356     |
|    total_timesteps | 2804000  |
---------------------------------
Eval num_timesteps=2805000, episode_reward=1247.52 +/- 22.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 2805000  |
---------------------------------
Eval num_timesteps=2806000, episode_reward=1263.86 +/- 45.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2806000  |
| train/             |          |
|    actor_loss      | -0.888   |
|    critic_loss     | 0.000747 |
|    ent_coef        | 0.000451 |
|    ent_coef_loss   | -6.06    |
|    learning_rate   | 0.00219  |
|    n_updates       | 13700    |
---------------------------------
New best mean reward!
Eval num_timesteps=2807000, episode_reward=1214.87 +/- 39.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2807000  |
---------------------------------
Eval num_timesteps=2808000, episode_reward=1077.63 +/- 92.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2808000  |
| train/             |          |
|    actor_loss      | -0.901   |
|    critic_loss     | 0.000896 |
|    ent_coef        | 0.000448 |
|    ent_coef_loss   | 3.07     |
|    learning_rate   | 0.00219  |
|    n_updates       | 13710    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 932      |
| time/              |          |
|    episodes        | 2808     |
|    fps             | 643      |
|    time_elapsed    | 4363     |
|    total_timesteps | 2808000  |
---------------------------------
Eval num_timesteps=2809000, episode_reward=1038.37 +/- 82.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2809000  |
---------------------------------
Eval num_timesteps=2810000, episode_reward=1002.33 +/- 57.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 2810000  |
| train/             |          |
|    actor_loss      | -0.867   |
|    critic_loss     | 0.000847 |
|    ent_coef        | 0.000447 |
|    ent_coef_loss   | -2.15    |
|    learning_rate   | 0.00219  |
|    n_updates       | 13720    |
---------------------------------
Eval num_timesteps=2811000, episode_reward=991.36 +/- 39.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 991      |
| time/              |          |
|    total_timesteps | 2811000  |
---------------------------------
Eval num_timesteps=2812000, episode_reward=1016.96 +/- 26.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.02e+03 |
| time/              |          |
|    total_timesteps | 2812000  |
| train/             |          |
|    actor_loss      | -0.851   |
|    critic_loss     | 0.000908 |
|    ent_coef        | 0.000446 |
|    ent_coef_loss   | -0.42    |
|    learning_rate   | 0.00219  |
|    n_updates       | 13730    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 930      |
| time/              |          |
|    episodes        | 2812     |
|    fps             | 643      |
|    time_elapsed    | 4369     |
|    total_timesteps | 2812000  |
---------------------------------
Eval num_timesteps=2813000, episode_reward=1043.35 +/- 33.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 2813000  |
---------------------------------
Eval num_timesteps=2814000, episode_reward=1191.80 +/- 54.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2814000  |
| train/             |          |
|    actor_loss      | -0.855   |
|    critic_loss     | 0.000877 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | -0.943   |
|    learning_rate   | 0.00219  |
|    n_updates       | 13740    |
---------------------------------
Eval num_timesteps=2815000, episode_reward=1208.84 +/- 28.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2815000  |
---------------------------------
Eval num_timesteps=2816000, episode_reward=1195.59 +/- 41.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2816000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 930      |
| time/              |          |
|    episodes        | 2816     |
|    fps             | 643      |
|    time_elapsed    | 4375     |
|    total_timesteps | 2816000  |
---------------------------------
Eval num_timesteps=2817000, episode_reward=1265.81 +/- 45.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2817000  |
| train/             |          |
|    actor_loss      | -0.894   |
|    critic_loss     | 0.000793 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 1.34     |
|    learning_rate   | 0.00218  |
|    n_updates       | 13750    |
---------------------------------
New best mean reward!
Eval num_timesteps=2818000, episode_reward=1269.70 +/- 31.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2818000  |
---------------------------------
New best mean reward!
Eval num_timesteps=2819000, episode_reward=1289.49 +/- 26.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2819000  |
| train/             |          |
|    actor_loss      | -0.89    |
|    critic_loss     | 0.000765 |
|    ent_coef        | 0.000446 |
|    ent_coef_loss   | 3.49     |
|    learning_rate   | 0.00218  |
|    n_updates       | 13760    |
---------------------------------
New best mean reward!
Eval num_timesteps=2820000, episode_reward=1308.48 +/- 29.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2820000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 938      |
| time/              |          |
|    episodes        | 2820     |
|    fps             | 643      |
|    time_elapsed    | 4381     |
|    total_timesteps | 2820000  |
---------------------------------
Eval num_timesteps=2821000, episode_reward=1192.29 +/- 79.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2821000  |
| train/             |          |
|    actor_loss      | -0.906   |
|    critic_loss     | 0.00081  |
|    ent_coef        | 0.000449 |
|    ent_coef_loss   | 3.24     |
|    learning_rate   | 0.00218  |
|    n_updates       | 13770    |
---------------------------------
Eval num_timesteps=2822000, episode_reward=1243.16 +/- 48.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2822000  |
---------------------------------
Eval num_timesteps=2823000, episode_reward=1120.32 +/- 82.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2823000  |
| train/             |          |
|    actor_loss      | -0.881   |
|    critic_loss     | 0.000862 |
|    ent_coef        | 0.000452 |
|    ent_coef_loss   | 1.34     |
|    learning_rate   | 0.00218  |
|    n_updates       | 13780    |
---------------------------------
Eval num_timesteps=2824000, episode_reward=1050.34 +/- 74.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2824000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 942      |
| time/              |          |
|    episodes        | 2824     |
|    fps             | 643      |
|    time_elapsed    | 4388     |
|    total_timesteps | 2824000  |
---------------------------------
Eval num_timesteps=2825000, episode_reward=1275.95 +/- 25.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2825000  |
| train/             |          |
|    actor_loss      | -0.862   |
|    critic_loss     | 0.000882 |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | 3.07     |
|    learning_rate   | 0.00218  |
|    n_updates       | 13790    |
---------------------------------
Eval num_timesteps=2826000, episode_reward=1281.33 +/- 28.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2826000  |
---------------------------------
Eval num_timesteps=2827000, episode_reward=1318.89 +/- 16.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2827000  |
| train/             |          |
|    actor_loss      | -0.902   |
|    critic_loss     | 0.000867 |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | 5.76     |
|    learning_rate   | 0.00217  |
|    n_updates       | 13800    |
---------------------------------
New best mean reward!
Eval num_timesteps=2828000, episode_reward=1294.89 +/- 19.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2828000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 951      |
| time/              |          |
|    episodes        | 2828     |
|    fps             | 643      |
|    time_elapsed    | 4394     |
|    total_timesteps | 2828000  |
---------------------------------
Eval num_timesteps=2829000, episode_reward=1296.31 +/- 47.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 2829000  |
| train/             |          |
|    actor_loss      | -0.891   |
|    critic_loss     | 0.000774 |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | 0.38     |
|    learning_rate   | 0.00217  |
|    n_updates       | 13810    |
---------------------------------
Eval num_timesteps=2830000, episode_reward=1295.90 +/- 38.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 2830000  |
---------------------------------
Eval num_timesteps=2831000, episode_reward=1287.56 +/- 19.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2831000  |
| train/             |          |
|    actor_loss      | -0.885   |
|    critic_loss     | 0.000826 |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | 4.81     |
|    learning_rate   | 0.00217  |
|    n_updates       | 13820    |
---------------------------------
Eval num_timesteps=2832000, episode_reward=1287.59 +/- 31.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2832000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 964      |
| time/              |          |
|    episodes        | 2832     |
|    fps             | 643      |
|    time_elapsed    | 4400     |
|    total_timesteps | 2832000  |
---------------------------------
Eval num_timesteps=2833000, episode_reward=1320.02 +/- 17.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2833000  |
| train/             |          |
|    actor_loss      | -0.9     |
|    critic_loss     | 0.00081  |
|    ent_coef        | 0.000471 |
|    ent_coef_loss   | 4.9      |
|    learning_rate   | 0.00217  |
|    n_updates       | 13830    |
---------------------------------
New best mean reward!
Eval num_timesteps=2834000, episode_reward=1325.65 +/- 25.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2834000  |
---------------------------------
New best mean reward!
Eval num_timesteps=2835000, episode_reward=1272.60 +/- 14.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2835000  |
| train/             |          |
|    actor_loss      | -0.903   |
|    critic_loss     | 0.000758 |
|    ent_coef        | 0.000476 |
|    ent_coef_loss   | 1.38     |
|    learning_rate   | 0.00217  |
|    n_updates       | 13840    |
---------------------------------
Eval num_timesteps=2836000, episode_reward=1288.74 +/- 20.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2836000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 998      |
| time/              |          |
|    episodes        | 2836     |
|    fps             | 643      |
|    time_elapsed    | 4407     |
|    total_timesteps | 2836000  |
---------------------------------
Eval num_timesteps=2837000, episode_reward=1292.18 +/- 17.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2837000  |
| train/             |          |
|    actor_loss      | -0.894   |
|    critic_loss     | 0.000698 |
|    ent_coef        | 0.000479 |
|    ent_coef_loss   | 2.49     |
|    learning_rate   | 0.00216  |
|    n_updates       | 13850    |
---------------------------------
Eval num_timesteps=2838000, episode_reward=1277.05 +/- 17.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2838000  |
---------------------------------
Eval num_timesteps=2839000, episode_reward=1192.27 +/- 58.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2839000  |
| train/             |          |
|    actor_loss      | -0.894   |
|    critic_loss     | 0.00074  |
|    ent_coef        | 0.000483 |
|    ent_coef_loss   | 1.67     |
|    learning_rate   | 0.00216  |
|    n_updates       | 13860    |
---------------------------------
Eval num_timesteps=2840000, episode_reward=1110.47 +/- 79.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1e+03    |
| time/              |          |
|    episodes        | 2840     |
|    fps             | 643      |
|    time_elapsed    | 4413     |
|    total_timesteps | 2840000  |
---------------------------------
Eval num_timesteps=2841000, episode_reward=1100.99 +/- 61.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 2841000  |
| train/             |          |
|    actor_loss      | -0.852   |
|    critic_loss     | 0.000753 |
|    ent_coef        | 0.000484 |
|    ent_coef_loss   | -2.24    |
|    learning_rate   | 0.00216  |
|    n_updates       | 13870    |
---------------------------------
Eval num_timesteps=2842000, episode_reward=1100.95 +/- 59.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 2842000  |
---------------------------------
Eval num_timesteps=2843000, episode_reward=908.17 +/- 20.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 908      |
| time/              |          |
|    total_timesteps | 2843000  |
| train/             |          |
|    actor_loss      | -0.869   |
|    critic_loss     | 0.000937 |
|    ent_coef        | 0.000484 |
|    ent_coef_loss   | 1.56     |
|    learning_rate   | 0.00216  |
|    n_updates       | 13880    |
---------------------------------
Eval num_timesteps=2844000, episode_reward=933.20 +/- 69.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 933      |
| time/              |          |
|    total_timesteps | 2844000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    episodes        | 2844     |
|    fps             | 643      |
|    time_elapsed    | 4420     |
|    total_timesteps | 2844000  |
---------------------------------
Eval num_timesteps=2845000, episode_reward=1279.73 +/- 49.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2845000  |
| train/             |          |
|    actor_loss      | -0.837   |
|    critic_loss     | 0.000895 |
|    ent_coef        | 0.000484 |
|    ent_coef_loss   | -4.85    |
|    learning_rate   | 0.00216  |
|    n_updates       | 13890    |
---------------------------------
Eval num_timesteps=2846000, episode_reward=1289.72 +/- 37.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2846000  |
---------------------------------
Eval num_timesteps=2847000, episode_reward=1288.06 +/- 54.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2847000  |
| train/             |          |
|    actor_loss      | -0.882   |
|    critic_loss     | 0.000832 |
|    ent_coef        | 0.000481 |
|    ent_coef_loss   | -1.82    |
|    learning_rate   | 0.00215  |
|    n_updates       | 13900    |
---------------------------------
Eval num_timesteps=2848000, episode_reward=1290.33 +/- 55.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2848000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    episodes        | 2848     |
|    fps             | 643      |
|    time_elapsed    | 4426     |
|    total_timesteps | 2848000  |
---------------------------------
Eval num_timesteps=2849000, episode_reward=1241.87 +/- 22.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2849000  |
| train/             |          |
|    actor_loss      | -0.894   |
|    critic_loss     | 0.000828 |
|    ent_coef        | 0.000478 |
|    ent_coef_loss   | 2.18     |
|    learning_rate   | 0.00215  |
|    n_updates       | 13910    |
---------------------------------
Eval num_timesteps=2850000, episode_reward=1186.38 +/- 42.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2850000  |
---------------------------------
Eval num_timesteps=2851000, episode_reward=1170.91 +/- 30.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2851000  |
| train/             |          |
|    actor_loss      | -0.861   |
|    critic_loss     | 0.000868 |
|    ent_coef        | 0.000479 |
|    ent_coef_loss   | 2.58     |
|    learning_rate   | 0.00215  |
|    n_updates       | 13920    |
---------------------------------
Eval num_timesteps=2852000, episode_reward=1148.63 +/- 31.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2852000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    episodes        | 2852     |
|    fps             | 643      |
|    time_elapsed    | 4432     |
|    total_timesteps | 2852000  |
---------------------------------
Eval num_timesteps=2853000, episode_reward=1171.80 +/- 30.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2853000  |
| train/             |          |
|    actor_loss      | -0.864   |
|    critic_loss     | 0.000972 |
|    ent_coef        | 0.000481 |
|    ent_coef_loss   | 0.533    |
|    learning_rate   | 0.00215  |
|    n_updates       | 13930    |
---------------------------------
Eval num_timesteps=2854000, episode_reward=1180.07 +/- 143.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2854000  |
---------------------------------
Eval num_timesteps=2855000, episode_reward=1304.53 +/- 48.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 2855000  |
| train/             |          |
|    actor_loss      | -0.876   |
|    critic_loss     | 0.000845 |
|    ent_coef        | 0.000483 |
|    ent_coef_loss   | 2.46     |
|    learning_rate   | 0.00215  |
|    n_updates       | 13940    |
---------------------------------
Eval num_timesteps=2856000, episode_reward=1302.45 +/- 28.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 2856000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    episodes        | 2856     |
|    fps             | 643      |
|    time_elapsed    | 4438     |
|    total_timesteps | 2856000  |
---------------------------------
Eval num_timesteps=2857000, episode_reward=1178.29 +/- 33.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2857000  |
| train/             |          |
|    actor_loss      | -0.888   |
|    critic_loss     | 0.000724 |
|    ent_coef        | 0.000486 |
|    ent_coef_loss   | 1.81     |
|    learning_rate   | 0.00214  |
|    n_updates       | 13950    |
---------------------------------
Eval num_timesteps=2858000, episode_reward=1167.42 +/- 40.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2858000  |
---------------------------------
Eval num_timesteps=2859000, episode_reward=1194.44 +/- 41.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2859000  |
---------------------------------
Eval num_timesteps=2860000, episode_reward=1219.79 +/- 47.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2860000  |
| train/             |          |
|    actor_loss      | -0.861   |
|    critic_loss     | 0.000729 |
|    ent_coef        | 0.000487 |
|    ent_coef_loss   | -2.58    |
|    learning_rate   | 0.00214  |
|    n_updates       | 13960    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    episodes        | 2860     |
|    fps             | 643      |
|    time_elapsed    | 4444     |
|    total_timesteps | 2860000  |
---------------------------------
Eval num_timesteps=2861000, episode_reward=1226.02 +/- 40.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2861000  |
---------------------------------
Eval num_timesteps=2862000, episode_reward=1089.82 +/- 55.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 2862000  |
| train/             |          |
|    actor_loss      | -0.865   |
|    critic_loss     | 0.000789 |
|    ent_coef        | 0.000487 |
|    ent_coef_loss   | 0.626    |
|    learning_rate   | 0.00214  |
|    n_updates       | 13970    |
---------------------------------
Eval num_timesteps=2863000, episode_reward=1126.29 +/- 69.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.13e+03 |
| time/              |          |
|    total_timesteps | 2863000  |
---------------------------------
Eval num_timesteps=2864000, episode_reward=1073.20 +/- 64.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 2864000  |
| train/             |          |
|    actor_loss      | -0.853   |
|    critic_loss     | 0.000839 |
|    ent_coef        | 0.000487 |
|    ent_coef_loss   | -0.927   |
|    learning_rate   | 0.00214  |
|    n_updates       | 13980    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    episodes        | 2864     |
|    fps             | 643      |
|    time_elapsed    | 4450     |
|    total_timesteps | 2864000  |
---------------------------------
Eval num_timesteps=2865000, episode_reward=1105.77 +/- 85.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 2865000  |
---------------------------------
Eval num_timesteps=2866000, episode_reward=1207.57 +/- 28.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2866000  |
| train/             |          |
|    actor_loss      | -0.839   |
|    critic_loss     | 0.000909 |
|    ent_coef        | 0.000486 |
|    ent_coef_loss   | -2.79    |
|    learning_rate   | 0.00213  |
|    n_updates       | 13990    |
---------------------------------
Eval num_timesteps=2867000, episode_reward=1180.99 +/- 87.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2867000  |
---------------------------------
Eval num_timesteps=2868000, episode_reward=1254.65 +/- 41.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 2868000  |
| train/             |          |
|    actor_loss      | -0.857   |
|    critic_loss     | 0.000858 |
|    ent_coef        | 0.000483 |
|    ent_coef_loss   | -2.07    |
|    learning_rate   | 0.00213  |
|    n_updates       | 14000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.03e+03 |
| time/              |          |
|    episodes        | 2868     |
|    fps             | 643      |
|    time_elapsed    | 4457     |
|    total_timesteps | 2868000  |
---------------------------------
Eval num_timesteps=2869000, episode_reward=1286.26 +/- 6.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2869000  |
---------------------------------
Eval num_timesteps=2870000, episode_reward=1280.99 +/- 40.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2870000  |
| train/             |          |
|    actor_loss      | -0.876   |
|    critic_loss     | 0.000911 |
|    ent_coef        | 0.000481 |
|    ent_coef_loss   | 3.9      |
|    learning_rate   | 0.00213  |
|    n_updates       | 14010    |
---------------------------------
Eval num_timesteps=2871000, episode_reward=1270.19 +/- 65.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2871000  |
---------------------------------
Eval num_timesteps=2872000, episode_reward=1296.32 +/- 36.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 2872000  |
| train/             |          |
|    actor_loss      | -0.886   |
|    critic_loss     | 0.000824 |
|    ent_coef        | 0.000484 |
|    ent_coef_loss   | 4.39     |
|    learning_rate   | 0.00213  |
|    n_updates       | 14020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 2872     |
|    fps             | 643      |
|    time_elapsed    | 4463     |
|    total_timesteps | 2872000  |
---------------------------------
Eval num_timesteps=2873000, episode_reward=1277.44 +/- 23.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2873000  |
---------------------------------
Eval num_timesteps=2874000, episode_reward=1259.77 +/- 27.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2874000  |
| train/             |          |
|    actor_loss      | -0.878   |
|    critic_loss     | 0.000879 |
|    ent_coef        | 0.000489 |
|    ent_coef_loss   | 6.47     |
|    learning_rate   | 0.00213  |
|    n_updates       | 14030    |
---------------------------------
Eval num_timesteps=2875000, episode_reward=1276.73 +/- 32.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2875000  |
---------------------------------
Eval num_timesteps=2876000, episode_reward=1184.21 +/- 75.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2876000  |
| train/             |          |
|    actor_loss      | -0.879   |
|    critic_loss     | 0.000818 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | 5.43     |
|    learning_rate   | 0.00212  |
|    n_updates       | 14040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 2876     |
|    fps             | 643      |
|    time_elapsed    | 4469     |
|    total_timesteps | 2876000  |
---------------------------------
Eval num_timesteps=2877000, episode_reward=1211.14 +/- 66.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2877000  |
---------------------------------
Eval num_timesteps=2878000, episode_reward=1314.52 +/- 47.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2878000  |
| train/             |          |
|    actor_loss      | -0.852   |
|    critic_loss     | 0.000934 |
|    ent_coef        | 0.000503 |
|    ent_coef_loss   | 1.71     |
|    learning_rate   | 0.00212  |
|    n_updates       | 14050    |
---------------------------------
Eval num_timesteps=2879000, episode_reward=1330.61 +/- 30.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2879000  |
---------------------------------
New best mean reward!
Eval num_timesteps=2880000, episode_reward=1319.14 +/- 48.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2880000  |
| train/             |          |
|    actor_loss      | -0.887   |
|    critic_loss     | 0.000843 |
|    ent_coef        | 0.000508 |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.00212  |
|    n_updates       | 14060    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    episodes        | 2880     |
|    fps             | 643      |
|    time_elapsed    | 4475     |
|    total_timesteps | 2880000  |
---------------------------------
Eval num_timesteps=2881000, episode_reward=1318.98 +/- 43.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2881000  |
---------------------------------
Eval num_timesteps=2882000, episode_reward=1235.56 +/- 25.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2882000  |
| train/             |          |
|    actor_loss      | -0.867   |
|    critic_loss     | 0.000867 |
|    ent_coef        | 0.00051  |
|    ent_coef_loss   | -6.03    |
|    learning_rate   | 0.00212  |
|    n_updates       | 14070    |
---------------------------------
Eval num_timesteps=2883000, episode_reward=1255.99 +/- 25.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2883000  |
---------------------------------
Eval num_timesteps=2884000, episode_reward=1249.91 +/- 15.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 2884000  |
| train/             |          |
|    actor_loss      | -0.87    |
|    critic_loss     | 0.000848 |
|    ent_coef        | 0.000505 |
|    ent_coef_loss   | -3.75    |
|    learning_rate   | 0.00212  |
|    n_updates       | 14080    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    episodes        | 2884     |
|    fps             | 643      |
|    time_elapsed    | 4481     |
|    total_timesteps | 2884000  |
---------------------------------
Eval num_timesteps=2885000, episode_reward=1268.57 +/- 26.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2885000  |
---------------------------------
Eval num_timesteps=2886000, episode_reward=1313.26 +/- 60.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2886000  |
| train/             |          |
|    actor_loss      | -0.862   |
|    critic_loss     | 0.000784 |
|    ent_coef        | 0.0005   |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.00211  |
|    n_updates       | 14090    |
---------------------------------
Eval num_timesteps=2887000, episode_reward=1201.10 +/- 7.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2887000  |
---------------------------------
Eval num_timesteps=2888000, episode_reward=1184.01 +/- 64.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2888000  |
| train/             |          |
|    actor_loss      | -0.863   |
|    critic_loss     | 0.000862 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | 0.477    |
|    learning_rate   | 0.00211  |
|    n_updates       | 14100    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    episodes        | 2888     |
|    fps             | 643      |
|    time_elapsed    | 4488     |
|    total_timesteps | 2888000  |
---------------------------------
Eval num_timesteps=2889000, episode_reward=1152.98 +/- 39.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2889000  |
---------------------------------
Eval num_timesteps=2890000, episode_reward=1229.24 +/- 33.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2890000  |
| train/             |          |
|    actor_loss      | -0.86    |
|    critic_loss     | 0.000998 |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | -0.626   |
|    learning_rate   | 0.00211  |
|    n_updates       | 14110    |
---------------------------------
Eval num_timesteps=2891000, episode_reward=1199.46 +/- 48.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2891000  |
---------------------------------
Eval num_timesteps=2892000, episode_reward=1280.85 +/- 47.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2892000  |
| train/             |          |
|    actor_loss      | -0.873   |
|    critic_loss     | 0.000877 |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | 0.472    |
|    learning_rate   | 0.00211  |
|    n_updates       | 14120    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    episodes        | 2892     |
|    fps             | 643      |
|    time_elapsed    | 4494     |
|    total_timesteps | 2892000  |
---------------------------------
Eval num_timesteps=2893000, episode_reward=1298.49 +/- 67.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 2893000  |
---------------------------------
Eval num_timesteps=2894000, episode_reward=-358.80 +/- 1.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -359     |
| time/              |          |
|    total_timesteps | 2894000  |
| train/             |          |
|    actor_loss      | -0.871   |
|    critic_loss     | 0.000892 |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | -0.205   |
|    learning_rate   | 0.00211  |
|    n_updates       | 14130    |
---------------------------------
Eval num_timesteps=2895000, episode_reward=-358.73 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -359     |
| time/              |          |
|    total_timesteps | 2895000  |
---------------------------------
Eval num_timesteps=2896000, episode_reward=-307.44 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 2896000  |
| train/             |          |
|    actor_loss      | -0.591   |
|    critic_loss     | 0.000345 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | 6.58     |
|    learning_rate   | 0.0021   |
|    n_updates       | 14140    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    episodes        | 2896     |
|    fps             | 643      |
|    time_elapsed    | 4500     |
|    total_timesteps | 2896000  |
---------------------------------
Eval num_timesteps=2897000, episode_reward=-307.31 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 2897000  |
---------------------------------
Eval num_timesteps=2898000, episode_reward=-263.94 +/- 1.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 2898000  |
| train/             |          |
|    actor_loss      | -0.543   |
|    critic_loss     | 0.000275 |
|    ent_coef        | 0.000493 |
|    ent_coef_loss   | -32.4    |
|    learning_rate   | 0.0021   |
|    n_updates       | 14150    |
---------------------------------
Eval num_timesteps=2899000, episode_reward=-265.12 +/- 1.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -265     |
| time/              |          |
|    total_timesteps | 2899000  |
---------------------------------
Eval num_timesteps=2900000, episode_reward=-345.18 +/- 2.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -345     |
| time/              |          |
|    total_timesteps | 2900000  |
| train/             |          |
|    actor_loss      | -0.549   |
|    critic_loss     | 0.000116 |
|    ent_coef        | 0.000472 |
|    ent_coef_loss   | -6       |
|    learning_rate   | 0.0021   |
|    n_updates       | 14160    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 2900     |
|    fps             | 643      |
|    time_elapsed    | 4506     |
|    total_timesteps | 2900000  |
---------------------------------
Eval num_timesteps=2901000, episode_reward=-344.99 +/- 1.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -345     |
| time/              |          |
|    total_timesteps | 2901000  |
---------------------------------
Eval num_timesteps=2902000, episode_reward=-341.77 +/- 2.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -342     |
| time/              |          |
|    total_timesteps | 2902000  |
---------------------------------
Eval num_timesteps=2903000, episode_reward=-308.04 +/- 1.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 2903000  |
| train/             |          |
|    actor_loss      | -0.542   |
|    critic_loss     | 7.76e-05 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | -11.9    |
|    learning_rate   | 0.0021   |
|    n_updates       | 14170    |
---------------------------------
Eval num_timesteps=2904000, episode_reward=-307.60 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 2904000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 994      |
| time/              |          |
|    episodes        | 2904     |
|    fps             | 643      |
|    time_elapsed    | 4513     |
|    total_timesteps | 2904000  |
---------------------------------
Eval num_timesteps=2905000, episode_reward=-373.32 +/- 2.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 2905000  |
| train/             |          |
|    actor_loss      | -0.55    |
|    critic_loss     | 0.000105 |
|    ent_coef        | 0.000446 |
|    ent_coef_loss   | -9.55    |
|    learning_rate   | 0.0021   |
|    n_updates       | 14180    |
---------------------------------
Eval num_timesteps=2906000, episode_reward=-374.46 +/- 1.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -374     |
| time/              |          |
|    total_timesteps | 2906000  |
---------------------------------
Eval num_timesteps=2907000, episode_reward=-369.63 +/- 1.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -370     |
| time/              |          |
|    total_timesteps | 2907000  |
| train/             |          |
|    actor_loss      | -0.553   |
|    critic_loss     | 5.67e-05 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | -6.35    |
|    learning_rate   | 0.00209  |
|    n_updates       | 14190    |
---------------------------------
Eval num_timesteps=2908000, episode_reward=-366.62 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -367     |
| time/              |          |
|    total_timesteps | 2908000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 963      |
| time/              |          |
|    episodes        | 2908     |
|    fps             | 643      |
|    time_elapsed    | 4519     |
|    total_timesteps | 2908000  |
---------------------------------
Eval num_timesteps=2909000, episode_reward=1347.17 +/- 43.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2909000  |
| train/             |          |
|    actor_loss      | -0.827   |
|    critic_loss     | 0.00259  |
|    ent_coef        | 0.000427 |
|    ent_coef_loss   | 1.49     |
|    learning_rate   | 0.00209  |
|    n_updates       | 14200    |
---------------------------------
New best mean reward!
Eval num_timesteps=2910000, episode_reward=1316.77 +/- 42.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2910000  |
---------------------------------
Eval num_timesteps=2911000, episode_reward=-194.28 +/- 2.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 2911000  |
| train/             |          |
|    actor_loss      | -0.844   |
|    critic_loss     | 0.00149  |
|    ent_coef        | 0.000425 |
|    ent_coef_loss   | 4.88     |
|    learning_rate   | 0.00209  |
|    n_updates       | 14210    |
---------------------------------
Eval num_timesteps=2912000, episode_reward=213.25 +/- 365.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 213      |
| time/              |          |
|    total_timesteps | 2912000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 965      |
| time/              |          |
|    episodes        | 2912     |
|    fps             | 643      |
|    time_elapsed    | 4525     |
|    total_timesteps | 2912000  |
---------------------------------
Eval num_timesteps=2913000, episode_reward=-177.92 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -178     |
| time/              |          |
|    total_timesteps | 2913000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.000999 |
|    ent_coef        | 0.000427 |
|    ent_coef_loss   | -1.72    |
|    learning_rate   | 0.00209  |
|    n_updates       | 14220    |
---------------------------------
Eval num_timesteps=2914000, episode_reward=-177.30 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 2914000  |
---------------------------------
Eval num_timesteps=2915000, episode_reward=-227.35 +/- 1.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 2915000  |
| train/             |          |
|    actor_loss      | -0.754   |
|    critic_loss     | 0.000895 |
|    ent_coef        | 0.000426 |
|    ent_coef_loss   | -9.57    |
|    learning_rate   | 0.00209  |
|    n_updates       | 14230    |
---------------------------------
Eval num_timesteps=2916000, episode_reward=-226.64 +/- 0.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -227     |
| time/              |          |
|    total_timesteps | 2916000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 935      |
| time/              |          |
|    episodes        | 2916     |
|    fps             | 643      |
|    time_elapsed    | 4532     |
|    total_timesteps | 2916000  |
---------------------------------
Eval num_timesteps=2917000, episode_reward=-217.68 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -218     |
| time/              |          |
|    total_timesteps | 2917000  |
| train/             |          |
|    actor_loss      | -0.543   |
|    critic_loss     | 0.000106 |
|    ent_coef        | 0.000416 |
|    ent_coef_loss   | -34.6    |
|    learning_rate   | 0.00208  |
|    n_updates       | 14240    |
---------------------------------
Eval num_timesteps=2918000, episode_reward=-219.37 +/- 1.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 2918000  |
---------------------------------
Eval num_timesteps=2919000, episode_reward=-342.80 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -343     |
| time/              |          |
|    total_timesteps | 2919000  |
| train/             |          |
|    actor_loss      | -0.69    |
|    critic_loss     | 0.000804 |
|    ent_coef        | 0.000395 |
|    ent_coef_loss   | -15.1    |
|    learning_rate   | 0.00208  |
|    n_updates       | 14250    |
---------------------------------
Eval num_timesteps=2920000, episode_reward=-342.55 +/- 1.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -343     |
| time/              |          |
|    total_timesteps | 2920000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 911      |
| time/              |          |
|    episodes        | 2920     |
|    fps             | 643      |
|    time_elapsed    | 4538     |
|    total_timesteps | 2920000  |
---------------------------------
Eval num_timesteps=2921000, episode_reward=1075.35 +/- 67.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 2921000  |
| train/             |          |
|    actor_loss      | -0.796   |
|    critic_loss     | 0.000906 |
|    ent_coef        | 0.00038  |
|    ent_coef_loss   | -3.3     |
|    learning_rate   | 0.00208  |
|    n_updates       | 14260    |
---------------------------------
Eval num_timesteps=2922000, episode_reward=964.99 +/- 75.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 965      |
| time/              |          |
|    total_timesteps | 2922000  |
---------------------------------
Eval num_timesteps=2923000, episode_reward=1153.30 +/- 77.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2923000  |
| train/             |          |
|    actor_loss      | -0.807   |
|    critic_loss     | 0.000901 |
|    ent_coef        | 0.000372 |
|    ent_coef_loss   | 0.252    |
|    learning_rate   | 0.00208  |
|    n_updates       | 14270    |
---------------------------------
Eval num_timesteps=2924000, episode_reward=1202.83 +/- 85.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2924000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 909      |
| time/              |          |
|    episodes        | 2924     |
|    fps             | 643      |
|    time_elapsed    | 4544     |
|    total_timesteps | 2924000  |
---------------------------------
Eval num_timesteps=2925000, episode_reward=1216.20 +/- 74.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2925000  |
| train/             |          |
|    actor_loss      | -0.824   |
|    critic_loss     | 0.000945 |
|    ent_coef        | 0.00037  |
|    ent_coef_loss   | 4.13     |
|    learning_rate   | 0.00208  |
|    n_updates       | 14280    |
---------------------------------
Eval num_timesteps=2926000, episode_reward=1226.41 +/- 44.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2926000  |
---------------------------------
Eval num_timesteps=2927000, episode_reward=1159.76 +/- 103.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2927000  |
| train/             |          |
|    actor_loss      | -0.843   |
|    critic_loss     | 0.000914 |
|    ent_coef        | 0.000371 |
|    ent_coef_loss   | 1.89     |
|    learning_rate   | 0.00207  |
|    n_updates       | 14290    |
---------------------------------
Eval num_timesteps=2928000, episode_reward=1187.75 +/- 23.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2928000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 908      |
| time/              |          |
|    episodes        | 2928     |
|    fps             | 643      |
|    time_elapsed    | 4550     |
|    total_timesteps | 2928000  |
---------------------------------
Eval num_timesteps=2929000, episode_reward=1309.57 +/- 48.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2929000  |
| train/             |          |
|    actor_loss      | -0.811   |
|    critic_loss     | 0.001    |
|    ent_coef        | 0.000373 |
|    ent_coef_loss   | 5.87     |
|    learning_rate   | 0.00207  |
|    n_updates       | 14300    |
---------------------------------
Eval num_timesteps=2930000, episode_reward=1283.67 +/- 46.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2930000  |
---------------------------------
Eval num_timesteps=2931000, episode_reward=1265.40 +/- 52.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 2931000  |
| train/             |          |
|    actor_loss      | -0.843   |
|    critic_loss     | 0.000936 |
|    ent_coef        | 0.000377 |
|    ent_coef_loss   | 7.36     |
|    learning_rate   | 0.00207  |
|    n_updates       | 14310    |
---------------------------------
Eval num_timesteps=2932000, episode_reward=1293.45 +/- 49.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2932000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 907      |
| time/              |          |
|    episodes        | 2932     |
|    fps             | 643      |
|    time_elapsed    | 4556     |
|    total_timesteps | 2932000  |
---------------------------------
Eval num_timesteps=2933000, episode_reward=270.56 +/- 525.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 271      |
| time/              |          |
|    total_timesteps | 2933000  |
| train/             |          |
|    actor_loss      | -0.842   |
|    critic_loss     | 0.00093  |
|    ent_coef        | 0.000382 |
|    ent_coef_loss   | 5.64     |
|    learning_rate   | 0.00207  |
|    n_updates       | 14320    |
---------------------------------
Eval num_timesteps=2934000, episode_reward=743.79 +/- 695.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 744      |
| time/              |          |
|    total_timesteps | 2934000  |
---------------------------------
Eval num_timesteps=2935000, episode_reward=1336.24 +/- 17.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2935000  |
| train/             |          |
|    actor_loss      | -0.82    |
|    critic_loss     | 0.000859 |
|    ent_coef        | 0.000387 |
|    ent_coef_loss   | 3.5      |
|    learning_rate   | 0.00207  |
|    n_updates       | 14330    |
---------------------------------
Eval num_timesteps=2936000, episode_reward=1327.32 +/- 12.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2936000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 906      |
| time/              |          |
|    episodes        | 2936     |
|    fps             | 643      |
|    time_elapsed    | 4563     |
|    total_timesteps | 2936000  |
---------------------------------
Eval num_timesteps=2937000, episode_reward=1206.63 +/- 51.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2937000  |
| train/             |          |
|    actor_loss      | -0.826   |
|    critic_loss     | 0.000896 |
|    ent_coef        | 0.000391 |
|    ent_coef_loss   | 2.3      |
|    learning_rate   | 0.00206  |
|    n_updates       | 14340    |
---------------------------------
Eval num_timesteps=2938000, episode_reward=1234.08 +/- 21.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 2938000  |
---------------------------------
Eval num_timesteps=2939000, episode_reward=1329.07 +/- 30.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2939000  |
| train/             |          |
|    actor_loss      | -0.817   |
|    critic_loss     | 0.000982 |
|    ent_coef        | 0.000395 |
|    ent_coef_loss   | 8.94     |
|    learning_rate   | 0.00206  |
|    n_updates       | 14350    |
---------------------------------
Eval num_timesteps=2940000, episode_reward=1346.64 +/- 26.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2940000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 906      |
| time/              |          |
|    episodes        | 2940     |
|    fps             | 643      |
|    time_elapsed    | 4569     |
|    total_timesteps | 2940000  |
---------------------------------
Eval num_timesteps=2941000, episode_reward=1348.63 +/- 30.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2941000  |
| train/             |          |
|    actor_loss      | -0.848   |
|    critic_loss     | 0.00094  |
|    ent_coef        | 0.000401 |
|    ent_coef_loss   | 7.49     |
|    learning_rate   | 0.00206  |
|    n_updates       | 14360    |
---------------------------------
New best mean reward!
Eval num_timesteps=2942000, episode_reward=1318.55 +/- 32.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2942000  |
---------------------------------
Eval num_timesteps=2943000, episode_reward=1323.59 +/- 35.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2943000  |
| train/             |          |
|    actor_loss      | -0.845   |
|    critic_loss     | 0.000877 |
|    ent_coef        | 0.000408 |
|    ent_coef_loss   | 7.99     |
|    learning_rate   | 0.00206  |
|    n_updates       | 14370    |
---------------------------------
Eval num_timesteps=2944000, episode_reward=1308.51 +/- 36.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2944000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 916      |
| time/              |          |
|    episodes        | 2944     |
|    fps             | 643      |
|    time_elapsed    | 4575     |
|    total_timesteps | 2944000  |
---------------------------------
Eval num_timesteps=2945000, episode_reward=1330.70 +/- 48.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2945000  |
---------------------------------
Eval num_timesteps=2946000, episode_reward=1305.08 +/- 18.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2946000  |
| train/             |          |
|    actor_loss      | -0.838   |
|    critic_loss     | 0.000924 |
|    ent_coef        | 0.000416 |
|    ent_coef_loss   | 9.61     |
|    learning_rate   | 0.00205  |
|    n_updates       | 14380    |
---------------------------------
Eval num_timesteps=2947000, episode_reward=1283.19 +/- 24.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2947000  |
---------------------------------
Eval num_timesteps=2948000, episode_reward=1320.55 +/- 20.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2948000  |
| train/             |          |
|    actor_loss      | -0.84    |
|    critic_loss     | 0.000926 |
|    ent_coef        | 0.000425 |
|    ent_coef_loss   | 11.3     |
|    learning_rate   | 0.00205  |
|    n_updates       | 14390    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 921      |
| time/              |          |
|    episodes        | 2948     |
|    fps             | 643      |
|    time_elapsed    | 4582     |
|    total_timesteps | 2948000  |
---------------------------------
Eval num_timesteps=2949000, episode_reward=1353.84 +/- 41.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2949000  |
---------------------------------
New best mean reward!
Eval num_timesteps=2950000, episode_reward=1321.77 +/- 23.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2950000  |
| train/             |          |
|    actor_loss      | -0.833   |
|    critic_loss     | 0.00103  |
|    ent_coef        | 0.000436 |
|    ent_coef_loss   | 9.97     |
|    learning_rate   | 0.00205  |
|    n_updates       | 14400    |
---------------------------------
Eval num_timesteps=2951000, episode_reward=1329.93 +/- 10.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 2951000  |
---------------------------------
Eval num_timesteps=2952000, episode_reward=1306.98 +/- 26.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2952000  |
| train/             |          |
|    actor_loss      | -0.846   |
|    critic_loss     | 0.000918 |
|    ent_coef        | 0.000446 |
|    ent_coef_loss   | 6.95     |
|    learning_rate   | 0.00205  |
|    n_updates       | 14410    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 927      |
| time/              |          |
|    episodes        | 2952     |
|    fps             | 643      |
|    time_elapsed    | 4589     |
|    total_timesteps | 2952000  |
---------------------------------
Eval num_timesteps=2953000, episode_reward=1306.40 +/- 16.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2953000  |
---------------------------------
Eval num_timesteps=2954000, episode_reward=1305.95 +/- 21.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2954000  |
| train/             |          |
|    actor_loss      | -0.847   |
|    critic_loss     | 0.000927 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | 7.17     |
|    learning_rate   | 0.00205  |
|    n_updates       | 14420    |
---------------------------------
Eval num_timesteps=2955000, episode_reward=1318.39 +/- 29.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2955000  |
---------------------------------
Eval num_timesteps=2956000, episode_reward=1202.97 +/- 23.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2956000  |
| train/             |          |
|    actor_loss      | -0.839   |
|    critic_loss     | 0.000877 |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | 5.18     |
|    learning_rate   | 0.00204  |
|    n_updates       | 14430    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 933      |
| time/              |          |
|    episodes        | 2956     |
|    fps             | 643      |
|    time_elapsed    | 4595     |
|    total_timesteps | 2956000  |
---------------------------------
Eval num_timesteps=2957000, episode_reward=1193.44 +/- 35.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 2957000  |
---------------------------------
Eval num_timesteps=2958000, episode_reward=1063.46 +/- 25.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 2958000  |
| train/             |          |
|    actor_loss      | -0.819   |
|    critic_loss     | 0.000915 |
|    ent_coef        | 0.000469 |
|    ent_coef_loss   | 3.76     |
|    learning_rate   | 0.00204  |
|    n_updates       | 14440    |
---------------------------------
Eval num_timesteps=2959000, episode_reward=1052.87 +/- 62.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.05e+03 |
| time/              |          |
|    total_timesteps | 2959000  |
---------------------------------
Eval num_timesteps=2960000, episode_reward=957.41 +/- 29.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 957      |
| time/              |          |
|    total_timesteps | 2960000  |
| train/             |          |
|    actor_loss      | -0.8     |
|    critic_loss     | 0.000947 |
|    ent_coef        | 0.000475 |
|    ent_coef_loss   | 5.45     |
|    learning_rate   | 0.00204  |
|    n_updates       | 14450    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 929      |
| time/              |          |
|    episodes        | 2960     |
|    fps             | 643      |
|    time_elapsed    | 4601     |
|    total_timesteps | 2960000  |
---------------------------------
Eval num_timesteps=2961000, episode_reward=915.15 +/- 36.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 915      |
| time/              |          |
|    total_timesteps | 2961000  |
---------------------------------
Eval num_timesteps=2962000, episode_reward=1256.86 +/- 36.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2962000  |
| train/             |          |
|    actor_loss      | -0.768   |
|    critic_loss     | 0.000963 |
|    ent_coef        | 0.00048  |
|    ent_coef_loss   | 0.192    |
|    learning_rate   | 0.00204  |
|    n_updates       | 14460    |
---------------------------------
Eval num_timesteps=2963000, episode_reward=1296.71 +/- 27.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 2963000  |
---------------------------------
Eval num_timesteps=2964000, episode_reward=1347.69 +/- 25.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2964000  |
| train/             |          |
|    actor_loss      | -0.818   |
|    critic_loss     | 0.000828 |
|    ent_coef        | 0.000482 |
|    ent_coef_loss   | -2.79    |
|    learning_rate   | 0.00204  |
|    n_updates       | 14470    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 932      |
| time/              |          |
|    episodes        | 2964     |
|    fps             | 643      |
|    time_elapsed    | 4607     |
|    total_timesteps | 2964000  |
---------------------------------
Eval num_timesteps=2965000, episode_reward=1366.99 +/- 21.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 2965000  |
---------------------------------
New best mean reward!
Eval num_timesteps=2966000, episode_reward=1317.63 +/- 21.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 2966000  |
| train/             |          |
|    actor_loss      | -0.846   |
|    critic_loss     | 0.000906 |
|    ent_coef        | 0.000482 |
|    ent_coef_loss   | 3.09     |
|    learning_rate   | 0.00203  |
|    n_updates       | 14480    |
---------------------------------
Eval num_timesteps=2967000, episode_reward=1305.78 +/- 29.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 2967000  |
---------------------------------
Eval num_timesteps=2968000, episode_reward=1345.00 +/- 33.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2968000  |
| train/             |          |
|    actor_loss      | -0.838   |
|    critic_loss     | 0.000825 |
|    ent_coef        | 0.000483 |
|    ent_coef_loss   | 2.19     |
|    learning_rate   | 0.00203  |
|    n_updates       | 14490    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 942      |
| time/              |          |
|    episodes        | 2968     |
|    fps             | 643      |
|    time_elapsed    | 4614     |
|    total_timesteps | 2968000  |
---------------------------------
Eval num_timesteps=2969000, episode_reward=1350.29 +/- 31.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2969000  |
---------------------------------
Eval num_timesteps=2970000, episode_reward=1393.35 +/- 56.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 2970000  |
| train/             |          |
|    actor_loss      | -0.85    |
|    critic_loss     | 0.000892 |
|    ent_coef        | 0.000486 |
|    ent_coef_loss   | 4.9      |
|    learning_rate   | 0.00203  |
|    n_updates       | 14500    |
---------------------------------
New best mean reward!
Eval num_timesteps=2971000, episode_reward=1339.47 +/- 80.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2971000  |
---------------------------------
Eval num_timesteps=2972000, episode_reward=1337.43 +/- 56.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2972000  |
| train/             |          |
|    actor_loss      | -0.821   |
|    critic_loss     | 0.000971 |
|    ent_coef        | 0.00049  |
|    ent_coef_loss   | 2.89     |
|    learning_rate   | 0.00203  |
|    n_updates       | 14510    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 941      |
| time/              |          |
|    episodes        | 2972     |
|    fps             | 643      |
|    time_elapsed    | 4620     |
|    total_timesteps | 2972000  |
---------------------------------
Eval num_timesteps=2973000, episode_reward=1337.67 +/- 98.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 2973000  |
---------------------------------
Eval num_timesteps=2974000, episode_reward=-97.46 +/- 1.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -97.5    |
| time/              |          |
|    total_timesteps | 2974000  |
| train/             |          |
|    actor_loss      | -0.829   |
|    critic_loss     | 0.000942 |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | -0.258   |
|    learning_rate   | 0.00203  |
|    n_updates       | 14520    |
---------------------------------
Eval num_timesteps=2975000, episode_reward=-97.90 +/- 1.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -97.9    |
| time/              |          |
|    total_timesteps | 2975000  |
---------------------------------
Eval num_timesteps=2976000, episode_reward=185.11 +/- 577.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 185      |
| time/              |          |
|    total_timesteps | 2976000  |
| train/             |          |
|    actor_loss      | -0.798   |
|    critic_loss     | 0.00081  |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | -5.84    |
|    learning_rate   | 0.00202  |
|    n_updates       | 14530    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 941      |
| time/              |          |
|    episodes        | 2976     |
|    fps             | 643      |
|    time_elapsed    | 4627     |
|    total_timesteps | 2976000  |
---------------------------------
Eval num_timesteps=2977000, episode_reward=-105.52 +/- 1.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 2977000  |
---------------------------------
Eval num_timesteps=2978000, episode_reward=1171.69 +/- 58.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2978000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.00095  |
|    ent_coef        | 0.000491 |
|    ent_coef_loss   | -1.7     |
|    learning_rate   | 0.00202  |
|    n_updates       | 14540    |
---------------------------------
Eval num_timesteps=2979000, episode_reward=1120.05 +/- 15.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 2979000  |
---------------------------------
Eval num_timesteps=2980000, episode_reward=1169.12 +/- 50.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2980000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.000987 |
|    ent_coef        | 0.000488 |
|    ent_coef_loss   | -0.273   |
|    learning_rate   | 0.00202  |
|    n_updates       | 14550    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 939      |
| time/              |          |
|    episodes        | 2980     |
|    fps             | 643      |
|    time_elapsed    | 4633     |
|    total_timesteps | 2980000  |
---------------------------------
Eval num_timesteps=2981000, episode_reward=1249.31 +/- 48.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 2981000  |
---------------------------------
Eval num_timesteps=2982000, episode_reward=1164.86 +/- 18.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 2982000  |
| train/             |          |
|    actor_loss      | -0.808   |
|    critic_loss     | 0.000913 |
|    ent_coef        | 0.000487 |
|    ent_coef_loss   | -1.94    |
|    learning_rate   | 0.00202  |
|    n_updates       | 14560    |
---------------------------------
Eval num_timesteps=2983000, episode_reward=1203.57 +/- 22.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 2983000  |
---------------------------------
Eval num_timesteps=2984000, episode_reward=1175.73 +/- 47.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2984000  |
| train/             |          |
|    actor_loss      | -0.825   |
|    critic_loss     | 0.00104  |
|    ent_coef        | 0.000485 |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.00202  |
|    n_updates       | 14570    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 939      |
| time/              |          |
|    episodes        | 2984     |
|    fps             | 643      |
|    time_elapsed    | 4639     |
|    total_timesteps | 2984000  |
---------------------------------
Eval num_timesteps=2985000, episode_reward=1167.67 +/- 34.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2985000  |
---------------------------------
Eval num_timesteps=2986000, episode_reward=1177.08 +/- 59.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2986000  |
| train/             |          |
|    actor_loss      | -0.794   |
|    critic_loss     | 0.000905 |
|    ent_coef        | 0.000486 |
|    ent_coef_loss   | -2.16    |
|    learning_rate   | 0.00201  |
|    n_updates       | 14580    |
---------------------------------
Eval num_timesteps=2987000, episode_reward=1153.64 +/- 49.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 2987000  |
---------------------------------
Eval num_timesteps=2988000, episode_reward=1168.52 +/- 49.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 2988000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 934      |
| time/              |          |
|    episodes        | 2988     |
|    fps             | 643      |
|    time_elapsed    | 4645     |
|    total_timesteps | 2988000  |
---------------------------------
Eval num_timesteps=2989000, episode_reward=1209.94 +/- 72.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 2989000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.000844 |
|    ent_coef        | 0.000483 |
|    ent_coef_loss   | -6.75    |
|    learning_rate   | 0.00201  |
|    n_updates       | 14590    |
---------------------------------
Eval num_timesteps=2990000, episode_reward=1184.61 +/- 72.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 2990000  |
---------------------------------
Eval num_timesteps=2991000, episode_reward=1369.58 +/- 81.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 2991000  |
| train/             |          |
|    actor_loss      | -0.812   |
|    critic_loss     | 0.00089  |
|    ent_coef        | 0.000478 |
|    ent_coef_loss   | 0.6      |
|    learning_rate   | 0.00201  |
|    n_updates       | 14600    |
---------------------------------
Eval num_timesteps=2992000, episode_reward=1420.70 +/- 27.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 2992000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 935      |
| time/              |          |
|    episodes        | 2992     |
|    fps             | 643      |
|    time_elapsed    | 4651     |
|    total_timesteps | 2992000  |
---------------------------------
Eval num_timesteps=2993000, episode_reward=1353.21 +/- 33.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 2993000  |
| train/             |          |
|    actor_loss      | -0.833   |
|    critic_loss     | 0.000911 |
|    ent_coef        | 0.000478 |
|    ent_coef_loss   | 5.86     |
|    learning_rate   | 0.00201  |
|    n_updates       | 14610    |
---------------------------------
Eval num_timesteps=2994000, episode_reward=1413.58 +/- 49.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 2994000  |
---------------------------------
Eval num_timesteps=2995000, episode_reward=1224.35 +/- 30.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 2995000  |
| train/             |          |
|    actor_loss      | -0.837   |
|    critic_loss     | 0.000976 |
|    ent_coef        | 0.000481 |
|    ent_coef_loss   | 3.45     |
|    learning_rate   | 0.00201  |
|    n_updates       | 14620    |
---------------------------------
Eval num_timesteps=2996000, episode_reward=1263.64 +/- 38.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 2996000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 965      |
| time/              |          |
|    episodes        | 2996     |
|    fps             | 643      |
|    time_elapsed    | 4658     |
|    total_timesteps | 2996000  |
---------------------------------
Eval num_timesteps=2997000, episode_reward=1285.50 +/- 70.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 2997000  |
| train/             |          |
|    actor_loss      | -0.816   |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000485 |
|    ent_coef_loss   | 2.36     |
|    learning_rate   | 0.002    |
|    n_updates       | 14630    |
---------------------------------
Eval num_timesteps=2998000, episode_reward=1279.84 +/- 56.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 2998000  |
---------------------------------
Eval num_timesteps=2999000, episode_reward=1235.13 +/- 25.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 2999000  |
| train/             |          |
|    actor_loss      | -0.826   |
|    critic_loss     | 0.00104  |
|    ent_coef        | 0.000488 |
|    ent_coef_loss   | 2.07     |
|    learning_rate   | 0.002    |
|    n_updates       | 14640    |
---------------------------------
Eval num_timesteps=3000000, episode_reward=1220.17 +/- 21.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3000000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.02e+03 |
| time/              |          |
|    episodes        | 3000     |
|    fps             | 643      |
|    time_elapsed    | 4664     |
|    total_timesteps | 3000000  |
---------------------------------
Eval num_timesteps=3001000, episode_reward=1251.34 +/- 19.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3001000  |
| train/             |          |
|    actor_loss      | -0.808   |
|    critic_loss     | 0.00115  |
|    ent_coef        | 0.000492 |
|    ent_coef_loss   | 2.76     |
|    learning_rate   | 0.002    |
|    n_updates       | 14650    |
---------------------------------
Eval num_timesteps=3002000, episode_reward=1237.82 +/- 36.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3002000  |
---------------------------------
Eval num_timesteps=3003000, episode_reward=1235.97 +/- 38.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3003000  |
| train/             |          |
|    actor_loss      | -0.809   |
|    critic_loss     | 0.000989 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | 1.72     |
|    learning_rate   | 0.002    |
|    n_updates       | 14660    |
---------------------------------
Eval num_timesteps=3004000, episode_reward=1245.53 +/- 57.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3004000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.08e+03 |
| time/              |          |
|    episodes        | 3004     |
|    fps             | 643      |
|    time_elapsed    | 4670     |
|    total_timesteps | 3004000  |
---------------------------------
Eval num_timesteps=3005000, episode_reward=1248.13 +/- 39.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3005000  |
| train/             |          |
|    actor_loss      | -0.805   |
|    critic_loss     | 0.000972 |
|    ent_coef        | 0.000497 |
|    ent_coef_loss   | 1.03     |
|    learning_rate   | 0.002    |
|    n_updates       | 14670    |
---------------------------------
Eval num_timesteps=3006000, episode_reward=1231.14 +/- 25.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3006000  |
---------------------------------
Eval num_timesteps=3007000, episode_reward=1256.93 +/- 91.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3007000  |
| train/             |          |
|    actor_loss      | -0.809   |
|    critic_loss     | 0.000972 |
|    ent_coef        | 0.000499 |
|    ent_coef_loss   | 3.84     |
|    learning_rate   | 0.00199  |
|    n_updates       | 14680    |
---------------------------------
Eval num_timesteps=3008000, episode_reward=1257.26 +/- 25.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3008000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    episodes        | 3008     |
|    fps             | 643      |
|    time_elapsed    | 4676     |
|    total_timesteps | 3008000  |
---------------------------------
Eval num_timesteps=3009000, episode_reward=1212.54 +/- 40.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3009000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.001    |
|    ent_coef        | 0.000503 |
|    ent_coef_loss   | 1.71     |
|    learning_rate   | 0.00199  |
|    n_updates       | 14690    |
---------------------------------
Eval num_timesteps=3010000, episode_reward=1235.10 +/- 18.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3010000  |
---------------------------------
Eval num_timesteps=3011000, episode_reward=773.26 +/- 741.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 773      |
| time/              |          |
|    total_timesteps | 3011000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000931 |
|    ent_coef        | 0.000506 |
|    ent_coef_loss   | 5.87     |
|    learning_rate   | 0.00199  |
|    n_updates       | 14700    |
---------------------------------
Eval num_timesteps=3012000, episode_reward=1001.51 +/- 568.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1e+03    |
| time/              |          |
|    total_timesteps | 3012000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.12e+03 |
| time/              |          |
|    episodes        | 3012     |
|    fps             | 643      |
|    time_elapsed    | 4682     |
|    total_timesteps | 3012000  |
---------------------------------
Eval num_timesteps=3013000, episode_reward=-141.50 +/- 1.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 3013000  |
| train/             |          |
|    actor_loss      | -0.812   |
|    critic_loss     | 0.000978 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | 5.03     |
|    learning_rate   | 0.00199  |
|    n_updates       | 14710    |
---------------------------------
Eval num_timesteps=3014000, episode_reward=-140.24 +/- 1.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 3014000  |
---------------------------------
Eval num_timesteps=3015000, episode_reward=394.96 +/- 664.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 395      |
| time/              |          |
|    total_timesteps | 3015000  |
| train/             |          |
|    actor_loss      | -0.747   |
|    critic_loss     | 0.000826 |
|    ent_coef        | 0.000517 |
|    ent_coef_loss   | -6.44    |
|    learning_rate   | 0.00199  |
|    n_updates       | 14720    |
---------------------------------
Eval num_timesteps=3016000, episode_reward=384.85 +/- 649.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 385      |
| time/              |          |
|    total_timesteps | 3016000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    episodes        | 3016     |
|    fps             | 643      |
|    time_elapsed    | 4689     |
|    total_timesteps | 3016000  |
---------------------------------
Eval num_timesteps=3017000, episode_reward=1380.64 +/- 34.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 3017000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.00107  |
|    ent_coef        | 0.000516 |
|    ent_coef_loss   | 3.83     |
|    learning_rate   | 0.00198  |
|    n_updates       | 14730    |
---------------------------------
Eval num_timesteps=3018000, episode_reward=1390.12 +/- 37.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3018000  |
---------------------------------
Eval num_timesteps=3019000, episode_reward=-136.73 +/- 1.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 3019000  |
| train/             |          |
|    actor_loss      | -0.813   |
|    critic_loss     | 0.00111  |
|    ent_coef        | 0.000517 |
|    ent_coef_loss   | -1.24    |
|    learning_rate   | 0.00198  |
|    n_updates       | 14740    |
---------------------------------
Eval num_timesteps=3020000, episode_reward=-137.47 +/- 1.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 3020000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.17e+03 |
| time/              |          |
|    episodes        | 3020     |
|    fps             | 643      |
|    time_elapsed    | 4695     |
|    total_timesteps | 3020000  |
---------------------------------
Eval num_timesteps=3021000, episode_reward=1340.13 +/- 19.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 3021000  |
| train/             |          |
|    actor_loss      | -0.815   |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.000517 |
|    ent_coef_loss   | -0.633   |
|    learning_rate   | 0.00198  |
|    n_updates       | 14750    |
---------------------------------
Eval num_timesteps=3022000, episode_reward=1373.71 +/- 35.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3022000  |
---------------------------------
Eval num_timesteps=3023000, episode_reward=1322.44 +/- 34.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3023000  |
| train/             |          |
|    actor_loss      | -0.813   |
|    critic_loss     | 0.00113  |
|    ent_coef        | 0.000516 |
|    ent_coef_loss   | 3.36     |
|    learning_rate   | 0.00198  |
|    n_updates       | 14760    |
---------------------------------
Eval num_timesteps=3024000, episode_reward=1275.69 +/- 20.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3024000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 3024     |
|    fps             | 643      |
|    time_elapsed    | 4701     |
|    total_timesteps | 3024000  |
---------------------------------
Eval num_timesteps=3025000, episode_reward=1319.75 +/- 35.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3025000  |
| train/             |          |
|    actor_loss      | -0.821   |
|    critic_loss     | 0.00106  |
|    ent_coef        | 0.00052  |
|    ent_coef_loss   | 5.86     |
|    learning_rate   | 0.00198  |
|    n_updates       | 14770    |
---------------------------------
Eval num_timesteps=3026000, episode_reward=1346.63 +/- 75.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3026000  |
---------------------------------
Eval num_timesteps=3027000, episode_reward=1212.27 +/- 96.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3027000  |
| train/             |          |
|    actor_loss      | -0.804   |
|    critic_loss     | 0.00101  |
|    ent_coef        | 0.000526 |
|    ent_coef_loss   | 4.49     |
|    learning_rate   | 0.00197  |
|    n_updates       | 14780    |
---------------------------------
Eval num_timesteps=3028000, episode_reward=1193.76 +/- 61.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3028000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 3028     |
|    fps             | 643      |
|    time_elapsed    | 4707     |
|    total_timesteps | 3028000  |
---------------------------------
Eval num_timesteps=3029000, episode_reward=1257.46 +/- 40.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3029000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.00101  |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | 3.29     |
|    learning_rate   | 0.00197  |
|    n_updates       | 14790    |
---------------------------------
Eval num_timesteps=3030000, episode_reward=1301.46 +/- 41.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3030000  |
---------------------------------
Eval num_timesteps=3031000, episode_reward=1326.27 +/- 65.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3031000  |
---------------------------------
Eval num_timesteps=3032000, episode_reward=1282.15 +/- 96.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3032000  |
| train/             |          |
|    actor_loss      | -0.775   |
|    critic_loss     | 0.000959 |
|    ent_coef        | 0.000536 |
|    ent_coef_loss   | -2.3     |
|    learning_rate   | 0.00197  |
|    n_updates       | 14800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.18e+03 |
| time/              |          |
|    episodes        | 3032     |
|    fps             | 643      |
|    time_elapsed    | 4713     |
|    total_timesteps | 3032000  |
---------------------------------
Eval num_timesteps=3033000, episode_reward=1273.51 +/- 87.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3033000  |
---------------------------------
Eval num_timesteps=3034000, episode_reward=-115.59 +/- 1.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 3034000  |
| train/             |          |
|    actor_loss      | -0.698   |
|    critic_loss     | 0.00069  |
|    ent_coef        | 0.000535 |
|    ent_coef_loss   | -12.6    |
|    learning_rate   | 0.00197  |
|    n_updates       | 14810    |
---------------------------------
Eval num_timesteps=3035000, episode_reward=-115.73 +/- 1.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 3035000  |
---------------------------------
Eval num_timesteps=3036000, episode_reward=-145.52 +/- 1.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 3036000  |
| train/             |          |
|    actor_loss      | -0.49    |
|    critic_loss     | 0.000152 |
|    ent_coef        | 0.000519 |
|    ent_coef_loss   | -37.2    |
|    learning_rate   | 0.00196  |
|    n_updates       | 14820    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.15e+03 |
| time/              |          |
|    episodes        | 3036     |
|    fps             | 643      |
|    time_elapsed    | 4720     |
|    total_timesteps | 3036000  |
---------------------------------
Eval num_timesteps=3037000, episode_reward=-146.58 +/- 1.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 3037000  |
---------------------------------
Eval num_timesteps=3038000, episode_reward=-192.73 +/- 1.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 3038000  |
| train/             |          |
|    actor_loss      | -0.605   |
|    critic_loss     | 0.000484 |
|    ent_coef        | 0.00049  |
|    ent_coef_loss   | -20.4    |
|    learning_rate   | 0.00196  |
|    n_updates       | 14830    |
---------------------------------
Eval num_timesteps=3039000, episode_reward=-193.38 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -193     |
| time/              |          |
|    total_timesteps | 3039000  |
---------------------------------
Eval num_timesteps=3040000, episode_reward=-143.72 +/- 1.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 3040000  |
| train/             |          |
|    actor_loss      | -0.481   |
|    critic_loss     | 6.39e-05 |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | -34.1    |
|    learning_rate   | 0.00196  |
|    n_updates       | 14840    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.1e+03  |
| time/              |          |
|    episodes        | 3040     |
|    fps             | 643      |
|    time_elapsed    | 4726     |
|    total_timesteps | 3040000  |
---------------------------------
Eval num_timesteps=3041000, episode_reward=-144.28 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 3041000  |
---------------------------------
Eval num_timesteps=3042000, episode_reward=1294.21 +/- 22.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3042000  |
| train/             |          |
|    actor_loss      | -0.47    |
|    critic_loss     | 4.48e-05 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | -35.4    |
|    learning_rate   | 0.00196  |
|    n_updates       | 14850    |
---------------------------------
Eval num_timesteps=3043000, episode_reward=1275.15 +/- 65.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3043000  |
---------------------------------
Eval num_timesteps=3044000, episode_reward=163.17 +/- 546.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 163      |
| time/              |          |
|    total_timesteps | 3044000  |
| train/             |          |
|    actor_loss      | -0.757   |
|    critic_loss     | 0.00134  |
|    ent_coef        | 0.00041  |
|    ent_coef_loss   | 1.62     |
|    learning_rate   | 0.00196  |
|    n_updates       | 14860    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.07e+03 |
| time/              |          |
|    episodes        | 3044     |
|    fps             | 643      |
|    time_elapsed    | 4732     |
|    total_timesteps | 3044000  |
---------------------------------
Eval num_timesteps=3045000, episode_reward=-110.76 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 3045000  |
---------------------------------
Eval num_timesteps=3046000, episode_reward=1406.02 +/- 21.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 3046000  |
| train/             |          |
|    actor_loss      | -0.618   |
|    critic_loss     | 0.000522 |
|    ent_coef        | 0.000399 |
|    ent_coef_loss   | -13.8    |
|    learning_rate   | 0.00195  |
|    n_updates       | 14870    |
---------------------------------
Eval num_timesteps=3047000, episode_reward=1368.00 +/- 71.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3047000  |
---------------------------------
Eval num_timesteps=3048000, episode_reward=938.51 +/- 535.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 939      |
| time/              |          |
|    total_timesteps | 3048000  |
| train/             |          |
|    actor_loss      | -0.81    |
|    critic_loss     | 0.0011   |
|    ent_coef        | 0.000392 |
|    ent_coef_loss   | 14       |
|    learning_rate   | 0.00195  |
|    n_updates       | 14880    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    episodes        | 3048     |
|    fps             | 643      |
|    time_elapsed    | 4738     |
|    total_timesteps | 3048000  |
---------------------------------
Eval num_timesteps=3049000, episode_reward=407.40 +/- 657.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 407      |
| time/              |          |
|    total_timesteps | 3049000  |
---------------------------------
Eval num_timesteps=3050000, episode_reward=1242.20 +/- 56.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3050000  |
| train/             |          |
|    actor_loss      | -0.745   |
|    critic_loss     | 0.000797 |
|    ent_coef        | 0.000394 |
|    ent_coef_loss   | 2.35     |
|    learning_rate   | 0.00195  |
|    n_updates       | 14890    |
---------------------------------
Eval num_timesteps=3051000, episode_reward=1217.34 +/- 33.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3051000  |
---------------------------------
Eval num_timesteps=3052000, episode_reward=1273.59 +/- 53.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3052000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000397 |
|    ent_coef_loss   | 9.88     |
|    learning_rate   | 0.00195  |
|    n_updates       | 14900    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 3052     |
|    fps             | 643      |
|    time_elapsed    | 4745     |
|    total_timesteps | 3052000  |
---------------------------------
Eval num_timesteps=3053000, episode_reward=1262.90 +/- 57.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3053000  |
---------------------------------
Eval num_timesteps=3054000, episode_reward=1306.18 +/- 48.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3054000  |
| train/             |          |
|    actor_loss      | -0.802   |
|    critic_loss     | 0.00119  |
|    ent_coef        | 0.000403 |
|    ent_coef_loss   | 11.3     |
|    learning_rate   | 0.00195  |
|    n_updates       | 14910    |
---------------------------------
Eval num_timesteps=3055000, episode_reward=1324.30 +/- 60.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3055000  |
---------------------------------
Eval num_timesteps=3056000, episode_reward=1118.47 +/- 29.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3056000  |
| train/             |          |
|    actor_loss      | -0.802   |
|    critic_loss     | 0.00103  |
|    ent_coef        | 0.00041  |
|    ent_coef_loss   | 9.44     |
|    learning_rate   | 0.00194  |
|    n_updates       | 14920    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 3056     |
|    fps             | 643      |
|    time_elapsed    | 4751     |
|    total_timesteps | 3056000  |
---------------------------------
Eval num_timesteps=3057000, episode_reward=1153.69 +/- 46.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 3057000  |
---------------------------------
Eval num_timesteps=3058000, episode_reward=1227.40 +/- 19.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3058000  |
| train/             |          |
|    actor_loss      | -0.763   |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.000418 |
|    ent_coef_loss   | 5.78     |
|    learning_rate   | 0.00194  |
|    n_updates       | 14930    |
---------------------------------
Eval num_timesteps=3059000, episode_reward=1246.92 +/- 33.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3059000  |
---------------------------------
Eval num_timesteps=3060000, episode_reward=1293.84 +/- 43.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3060000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.0011   |
|    ent_coef        | 0.000424 |
|    ent_coef_loss   | 11.2     |
|    learning_rate   | 0.00194  |
|    n_updates       | 14940    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 3060     |
|    fps             | 643      |
|    time_elapsed    | 4757     |
|    total_timesteps | 3060000  |
---------------------------------
Eval num_timesteps=3061000, episode_reward=1252.40 +/- 25.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3061000  |
---------------------------------
Eval num_timesteps=3062000, episode_reward=1336.19 +/- 39.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 3062000  |
| train/             |          |
|    actor_loss      | -0.786   |
|    critic_loss     | 0.00111  |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | 11.6     |
|    learning_rate   | 0.00194  |
|    n_updates       | 14950    |
---------------------------------
Eval num_timesteps=3063000, episode_reward=1363.79 +/- 52.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3063000  |
---------------------------------
Eval num_timesteps=3064000, episode_reward=1244.01 +/- 79.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3064000  |
| train/             |          |
|    actor_loss      | -0.807   |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000441 |
|    ent_coef_loss   | 5.93     |
|    learning_rate   | 0.00194  |
|    n_updates       | 14960    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    episodes        | 3064     |
|    fps             | 643      |
|    time_elapsed    | 4763     |
|    total_timesteps | 3064000  |
---------------------------------
Eval num_timesteps=3065000, episode_reward=1219.20 +/- 55.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3065000  |
---------------------------------
Eval num_timesteps=3066000, episode_reward=1240.01 +/- 48.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3066000  |
| train/             |          |
|    actor_loss      | -0.773   |
|    critic_loss     | 0.00106  |
|    ent_coef        | 0.000448 |
|    ent_coef_loss   | 6.58     |
|    learning_rate   | 0.00193  |
|    n_updates       | 14970    |
---------------------------------
Eval num_timesteps=3067000, episode_reward=1295.65 +/- 27.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3067000  |
---------------------------------
Eval num_timesteps=3068000, episode_reward=1141.94 +/- 51.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 3068000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.00107  |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | 2.34     |
|    learning_rate   | 0.00193  |
|    n_updates       | 14980    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 3068     |
|    fps             | 643      |
|    time_elapsed    | 4769     |
|    total_timesteps | 3068000  |
---------------------------------
Eval num_timesteps=3069000, episode_reward=1092.38 +/- 39.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3069000  |
---------------------------------
Eval num_timesteps=3070000, episode_reward=1120.84 +/- 18.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3070000  |
| train/             |          |
|    actor_loss      | -0.76    |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | 5.74     |
|    learning_rate   | 0.00193  |
|    n_updates       | 14990    |
---------------------------------
Eval num_timesteps=3071000, episode_reward=1098.39 +/- 28.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3071000  |
---------------------------------
Eval num_timesteps=3072000, episode_reward=1106.40 +/- 31.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3072000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.05e+03 |
| time/              |          |
|    episodes        | 3072     |
|    fps             | 643      |
|    time_elapsed    | 4775     |
|    total_timesteps | 3072000  |
---------------------------------
Eval num_timesteps=3073000, episode_reward=1231.77 +/- 40.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3073000  |
| train/             |          |
|    actor_loss      | -0.751   |
|    critic_loss     | 0.000918 |
|    ent_coef        | 0.000462 |
|    ent_coef_loss   | 3.15     |
|    learning_rate   | 0.00193  |
|    n_updates       | 15000    |
---------------------------------
Eval num_timesteps=3074000, episode_reward=1207.47 +/- 47.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3074000  |
---------------------------------
Eval num_timesteps=3075000, episode_reward=-132.20 +/- 1.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 3075000  |
| train/             |          |
|    actor_loss      | -0.772   |
|    critic_loss     | 0.00104  |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | 4.17     |
|    learning_rate   | 0.00193  |
|    n_updates       | 15010    |
---------------------------------
Eval num_timesteps=3076000, episode_reward=-131.27 +/- 1.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 3076000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.04e+03 |
| time/              |          |
|    episodes        | 3076     |
|    fps             | 643      |
|    time_elapsed    | 4782     |
|    total_timesteps | 3076000  |
---------------------------------
Eval num_timesteps=3077000, episode_reward=-140.34 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 3077000  |
| train/             |          |
|    actor_loss      | -0.724   |
|    critic_loss     | 0.000845 |
|    ent_coef        | 0.000469 |
|    ent_coef_loss   | -1.72    |
|    learning_rate   | 0.00192  |
|    n_updates       | 15020    |
---------------------------------
Eval num_timesteps=3078000, episode_reward=-141.18 +/- 1.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 3078000  |
---------------------------------
Eval num_timesteps=3079000, episode_reward=-162.88 +/- 1.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 3079000  |
| train/             |          |
|    actor_loss      | -0.55    |
|    critic_loss     | 0.000349 |
|    ent_coef        | 0.000467 |
|    ent_coef_loss   | -19.9    |
|    learning_rate   | 0.00192  |
|    n_updates       | 15030    |
---------------------------------
Eval num_timesteps=3080000, episode_reward=-163.11 +/- 1.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 3080000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    episodes        | 3080     |
|    fps             | 643      |
|    time_elapsed    | 4788     |
|    total_timesteps | 3080000  |
---------------------------------
Eval num_timesteps=3081000, episode_reward=-168.70 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 3081000  |
| train/             |          |
|    actor_loss      | -0.577   |
|    critic_loss     | 0.000429 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | -15.3    |
|    learning_rate   | 0.00192  |
|    n_updates       | 15040    |
---------------------------------
Eval num_timesteps=3082000, episode_reward=-169.38 +/- 1.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 3082000  |
---------------------------------
Eval num_timesteps=3083000, episode_reward=-160.30 +/- 1.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 3083000  |
| train/             |          |
|    actor_loss      | -0.458   |
|    critic_loss     | 2.82e-05 |
|    ent_coef        | 0.000442 |
|    ent_coef_loss   | -31.7    |
|    learning_rate   | 0.00192  |
|    n_updates       | 15050    |
---------------------------------
Eval num_timesteps=3084000, episode_reward=-163.11 +/- 0.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 3084000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 961      |
| time/              |          |
|    episodes        | 3084     |
|    fps             | 643      |
|    time_elapsed    | 4794     |
|    total_timesteps | 3084000  |
---------------------------------
Eval num_timesteps=3085000, episode_reward=-140.52 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 3085000  |
| train/             |          |
|    actor_loss      | -0.452   |
|    critic_loss     | 2.32e-05 |
|    ent_coef        | 0.000422 |
|    ent_coef_loss   | -36.1    |
|    learning_rate   | 0.00192  |
|    n_updates       | 15060    |
---------------------------------
Eval num_timesteps=3086000, episode_reward=-140.92 +/- 1.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 3086000  |
---------------------------------
Eval num_timesteps=3087000, episode_reward=-102.82 +/- 1.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -103     |
| time/              |          |
|    total_timesteps | 3087000  |
| train/             |          |
|    actor_loss      | -0.451   |
|    critic_loss     | 2.38e-05 |
|    ent_coef        | 0.000398 |
|    ent_coef_loss   | -37.5    |
|    learning_rate   | 0.00191  |
|    n_updates       | 15070    |
---------------------------------
Eval num_timesteps=3088000, episode_reward=-103.10 +/- 0.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -103     |
| time/              |          |
|    total_timesteps | 3088000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 913      |
| time/              |          |
|    episodes        | 3088     |
|    fps             | 643      |
|    time_elapsed    | 4801     |
|    total_timesteps | 3088000  |
---------------------------------
Eval num_timesteps=3089000, episode_reward=-118.15 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 3089000  |
| train/             |          |
|    actor_loss      | -0.448   |
|    critic_loss     | 3.43e-05 |
|    ent_coef        | 0.000375 |
|    ent_coef_loss   | -39.4    |
|    learning_rate   | 0.00191  |
|    n_updates       | 15080    |
---------------------------------
Eval num_timesteps=3090000, episode_reward=-118.18 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 3090000  |
---------------------------------
Eval num_timesteps=3091000, episode_reward=-132.61 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 3091000  |
| train/             |          |
|    actor_loss      | -0.614   |
|    critic_loss     | 0.000583 |
|    ent_coef        | 0.000356 |
|    ent_coef_loss   | -10.5    |
|    learning_rate   | 0.00191  |
|    n_updates       | 15090    |
---------------------------------
Eval num_timesteps=3092000, episode_reward=-131.12 +/- 1.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 3092000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 882      |
| time/              |          |
|    episodes        | 3092     |
|    fps             | 643      |
|    time_elapsed    | 4807     |
|    total_timesteps | 3092000  |
---------------------------------
Eval num_timesteps=3093000, episode_reward=-232.01 +/- 2.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -232     |
| time/              |          |
|    total_timesteps | 3093000  |
| train/             |          |
|    actor_loss      | -0.528   |
|    critic_loss     | 0.000311 |
|    ent_coef        | 0.000344 |
|    ent_coef_loss   | -16.1    |
|    learning_rate   | 0.00191  |
|    n_updates       | 15100    |
---------------------------------
Eval num_timesteps=3094000, episode_reward=-230.87 +/- 2.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -231     |
| time/              |          |
|    total_timesteps | 3094000  |
---------------------------------
Eval num_timesteps=3095000, episode_reward=-191.52 +/- 2.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 3095000  |
| train/             |          |
|    actor_loss      | -0.434   |
|    critic_loss     | 1.59e-05 |
|    ent_coef        | 0.000334 |
|    ent_coef_loss   | -20.5    |
|    learning_rate   | 0.00191  |
|    n_updates       | 15110    |
---------------------------------
Eval num_timesteps=3096000, episode_reward=-194.07 +/- 0.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 3096000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 825      |
| time/              |          |
|    episodes        | 3096     |
|    fps             | 643      |
|    time_elapsed    | 4813     |
|    total_timesteps | 3096000  |
---------------------------------
Eval num_timesteps=3097000, episode_reward=-164.09 +/- 1.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -164     |
| time/              |          |
|    total_timesteps | 3097000  |
| train/             |          |
|    actor_loss      | -0.44    |
|    critic_loss     | 1.28e-05 |
|    ent_coef        | 0.000324 |
|    ent_coef_loss   | -30.1    |
|    learning_rate   | 0.0019   |
|    n_updates       | 15120    |
---------------------------------
Eval num_timesteps=3098000, episode_reward=-165.05 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 3098000  |
---------------------------------
Eval num_timesteps=3099000, episode_reward=-180.31 +/- 1.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -180     |
| time/              |          |
|    total_timesteps | 3099000  |
| train/             |          |
|    actor_loss      | -0.442   |
|    critic_loss     | 9.63e-06 |
|    ent_coef        | 0.000312 |
|    ent_coef_loss   | -24.4    |
|    learning_rate   | 0.0019   |
|    n_updates       | 15130    |
---------------------------------
Eval num_timesteps=3100000, episode_reward=-178.93 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -179     |
| time/              |          |
|    total_timesteps | 3100000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 770      |
| time/              |          |
|    episodes        | 3100     |
|    fps             | 643      |
|    time_elapsed    | 4820     |
|    total_timesteps | 3100000  |
---------------------------------
Eval num_timesteps=3101000, episode_reward=-154.27 +/- 1.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 3101000  |
| train/             |          |
|    actor_loss      | -0.443   |
|    critic_loss     | 7.56e-06 |
|    ent_coef        | 0.000301 |
|    ent_coef_loss   | -24.5    |
|    learning_rate   | 0.0019   |
|    n_updates       | 15140    |
---------------------------------
Eval num_timesteps=3102000, episode_reward=-153.73 +/- 0.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -154     |
| time/              |          |
|    total_timesteps | 3102000  |
---------------------------------
Eval num_timesteps=3103000, episode_reward=-127.78 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 3103000  |
| train/             |          |
|    actor_loss      | -0.534   |
|    critic_loss     | 0.000358 |
|    ent_coef        | 0.000291 |
|    ent_coef_loss   | -13.2    |
|    learning_rate   | 0.0019   |
|    n_updates       | 15150    |
---------------------------------
Eval num_timesteps=3104000, episode_reward=-127.76 +/- 0.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 3104000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 738      |
| time/              |          |
|    episodes        | 3104     |
|    fps             | 643      |
|    time_elapsed    | 4826     |
|    total_timesteps | 3104000  |
---------------------------------
Eval num_timesteps=3105000, episode_reward=1157.95 +/- 60.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3105000  |
| train/             |          |
|    actor_loss      | -0.658   |
|    critic_loss     | 0.000862 |
|    ent_coef        | 0.000285 |
|    ent_coef_loss   | 3.73     |
|    learning_rate   | 0.0019   |
|    n_updates       | 15160    |
---------------------------------
Eval num_timesteps=3106000, episode_reward=1171.32 +/- 44.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3106000  |
---------------------------------
Eval num_timesteps=3107000, episode_reward=1137.32 +/- 45.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.14e+03 |
| time/              |          |
|    total_timesteps | 3107000  |
| train/             |          |
|    actor_loss      | -0.753   |
|    critic_loss     | 0.00116  |
|    ent_coef        | 0.000284 |
|    ent_coef_loss   | 16.2     |
|    learning_rate   | 0.00189  |
|    n_updates       | 15170    |
---------------------------------
Eval num_timesteps=3108000, episode_reward=1095.34 +/- 37.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3108000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 734      |
| time/              |          |
|    episodes        | 3108     |
|    fps             | 643      |
|    time_elapsed    | 4832     |
|    total_timesteps | 3108000  |
---------------------------------
Eval num_timesteps=3109000, episode_reward=1120.95 +/- 47.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3109000  |
| train/             |          |
|    actor_loss      | -0.74    |
|    critic_loss     | 0.00114  |
|    ent_coef        | 0.000288 |
|    ent_coef_loss   | 24.7     |
|    learning_rate   | 0.00189  |
|    n_updates       | 15180    |
---------------------------------
Eval num_timesteps=3110000, episode_reward=1094.68 +/- 27.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3110000  |
---------------------------------
Eval num_timesteps=3111000, episode_reward=1109.91 +/- 38.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3111000  |
| train/             |          |
|    actor_loss      | -0.731   |
|    critic_loss     | 0.00105  |
|    ent_coef        | 0.000295 |
|    ent_coef_loss   | 15.6     |
|    learning_rate   | 0.00189  |
|    n_updates       | 15190    |
---------------------------------
Eval num_timesteps=3112000, episode_reward=1122.30 +/- 29.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3112000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 721      |
| time/              |          |
|    episodes        | 3112     |
|    fps             | 643      |
|    time_elapsed    | 4838     |
|    total_timesteps | 3112000  |
---------------------------------
Eval num_timesteps=3113000, episode_reward=1116.70 +/- 31.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3113000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 0.000364 |
|    ent_coef        | 0.0003   |
|    ent_coef_loss   | -17.1    |
|    learning_rate   | 0.00189  |
|    n_updates       | 15200    |
---------------------------------
Eval num_timesteps=3114000, episode_reward=1097.40 +/- 50.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.1e+03  |
| time/              |          |
|    total_timesteps | 3114000  |
---------------------------------
Eval num_timesteps=3115000, episode_reward=1106.50 +/- 33.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.11e+03 |
| time/              |          |
|    total_timesteps | 3115000  |
---------------------------------
Eval num_timesteps=3116000, episode_reward=931.66 +/- 31.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 932      |
| time/              |          |
|    total_timesteps | 3116000  |
| train/             |          |
|    actor_loss      | -0.739   |
|    critic_loss     | 0.00115  |
|    ent_coef        | 0.0003   |
|    ent_coef_loss   | 16.6     |
|    learning_rate   | 0.00188  |
|    n_updates       | 15210    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 714      |
| time/              |          |
|    episodes        | 3116     |
|    fps             | 643      |
|    time_elapsed    | 4844     |
|    total_timesteps | 3116000  |
---------------------------------
Eval num_timesteps=3117000, episode_reward=969.08 +/- 50.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 969      |
| time/              |          |
|    total_timesteps | 3117000  |
---------------------------------
Eval num_timesteps=3118000, episode_reward=1077.37 +/- 20.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3118000  |
| train/             |          |
|    actor_loss      | -0.71    |
|    critic_loss     | 0.00106  |
|    ent_coef        | 0.000304 |
|    ent_coef_loss   | 15.8     |
|    learning_rate   | 0.00188  |
|    n_updates       | 15220    |
---------------------------------
Eval num_timesteps=3119000, episode_reward=1121.67 +/- 64.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3119000  |
---------------------------------
Eval num_timesteps=3120000, episode_reward=907.24 +/- 51.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 907      |
| time/              |          |
|    total_timesteps | 3120000  |
| train/             |          |
|    actor_loss      | -0.75    |
|    critic_loss     | 0.00126  |
|    ent_coef        | 0.00031  |
|    ent_coef_loss   | 22.6     |
|    learning_rate   | 0.00188  |
|    n_updates       | 15230    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 704      |
| time/              |          |
|    episodes        | 3120     |
|    fps             | 643      |
|    time_elapsed    | 4850     |
|    total_timesteps | 3120000  |
---------------------------------
Eval num_timesteps=3121000, episode_reward=946.48 +/- 39.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 946      |
| time/              |          |
|    total_timesteps | 3121000  |
---------------------------------
Eval num_timesteps=3122000, episode_reward=1195.12 +/- 62.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3122000  |
| train/             |          |
|    actor_loss      | -0.701   |
|    critic_loss     | 0.00114  |
|    ent_coef        | 0.000318 |
|    ent_coef_loss   | 20.4     |
|    learning_rate   | 0.00188  |
|    n_updates       | 15240    |
---------------------------------
Eval num_timesteps=3123000, episode_reward=1210.35 +/- 25.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3123000  |
---------------------------------
Eval num_timesteps=3124000, episode_reward=1217.98 +/- 42.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3124000  |
| train/             |          |
|    actor_loss      | -0.748   |
|    critic_loss     | 0.00105  |
|    ent_coef        | 0.000327 |
|    ent_coef_loss   | 20.1     |
|    learning_rate   | 0.00188  |
|    n_updates       | 15250    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 696      |
| time/              |          |
|    episodes        | 3124     |
|    fps             | 643      |
|    time_elapsed    | 4857     |
|    total_timesteps | 3124000  |
---------------------------------
Eval num_timesteps=3125000, episode_reward=1197.23 +/- 78.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3125000  |
---------------------------------
Eval num_timesteps=3126000, episode_reward=1227.34 +/- 57.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3126000  |
| train/             |          |
|    actor_loss      | -0.761   |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.000336 |
|    ent_coef_loss   | 21.1     |
|    learning_rate   | 0.00187  |
|    n_updates       | 15260    |
---------------------------------
Eval num_timesteps=3127000, episode_reward=1157.31 +/- 31.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3127000  |
---------------------------------
Eval num_timesteps=3128000, episode_reward=-100.61 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -101     |
| time/              |          |
|    total_timesteps | 3128000  |
| train/             |          |
|    actor_loss      | -0.727   |
|    critic_loss     | 0.000987 |
|    ent_coef        | 0.000345 |
|    ent_coef_loss   | 9.19     |
|    learning_rate   | 0.00187  |
|    n_updates       | 15270    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 693      |
| time/              |          |
|    episodes        | 3128     |
|    fps             | 643      |
|    time_elapsed    | 4863     |
|    total_timesteps | 3128000  |
---------------------------------
Eval num_timesteps=3129000, episode_reward=-99.55 +/- 2.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -99.6    |
| time/              |          |
|    total_timesteps | 3129000  |
---------------------------------
Eval num_timesteps=3130000, episode_reward=-185.15 +/- 1.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -185     |
| time/              |          |
|    total_timesteps | 3130000  |
| train/             |          |
|    actor_loss      | -0.551   |
|    critic_loss     | 0.000457 |
|    ent_coef        | 0.00035  |
|    ent_coef_loss   | -12.2    |
|    learning_rate   | 0.00187  |
|    n_updates       | 15280    |
---------------------------------
Eval num_timesteps=3131000, episode_reward=-186.74 +/- 1.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -187     |
| time/              |          |
|    total_timesteps | 3131000  |
---------------------------------
Eval num_timesteps=3132000, episode_reward=-163.26 +/- 1.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 3132000  |
| train/             |          |
|    actor_loss      | -0.515   |
|    critic_loss     | 0.000296 |
|    ent_coef        | 0.000348 |
|    ent_coef_loss   | -17.3    |
|    learning_rate   | 0.00187  |
|    n_updates       | 15290    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 655      |
| time/              |          |
|    episodes        | 3132     |
|    fps             | 643      |
|    time_elapsed    | 4869     |
|    total_timesteps | 3132000  |
---------------------------------
Eval num_timesteps=3133000, episode_reward=-162.53 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 3133000  |
---------------------------------
Eval num_timesteps=3134000, episode_reward=-140.83 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 3134000  |
| train/             |          |
|    actor_loss      | -0.434   |
|    critic_loss     | 2.1e-05  |
|    ent_coef        | 0.000342 |
|    ent_coef_loss   | -34.5    |
|    learning_rate   | 0.00187  |
|    n_updates       | 15300    |
---------------------------------
Eval num_timesteps=3135000, episode_reward=-139.12 +/- 1.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 3135000  |
---------------------------------
Eval num_timesteps=3136000, episode_reward=-85.34 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -85.3    |
| time/              |          |
|    total_timesteps | 3136000  |
| train/             |          |
|    actor_loss      | -0.434   |
|    critic_loss     | 1.24e-05 |
|    ent_coef        | 0.00033  |
|    ent_coef_loss   | -38.3    |
|    learning_rate   | 0.00186  |
|    n_updates       | 15310    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 629      |
| time/              |          |
|    episodes        | 3136     |
|    fps             | 643      |
|    time_elapsed    | 4876     |
|    total_timesteps | 3136000  |
---------------------------------
Eval num_timesteps=3137000, episode_reward=-85.58 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -85.6    |
| time/              |          |
|    total_timesteps | 3137000  |
---------------------------------
Eval num_timesteps=3138000, episode_reward=-90.73 +/- 1.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -90.7    |
| time/              |          |
|    total_timesteps | 3138000  |
| train/             |          |
|    actor_loss      | -0.433   |
|    critic_loss     | 1.57e-05 |
|    ent_coef        | 0.000316 |
|    ent_coef_loss   | -36.1    |
|    learning_rate   | 0.00186  |
|    n_updates       | 15320    |
---------------------------------
Eval num_timesteps=3139000, episode_reward=-89.32 +/- 1.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -89.3    |
| time/              |          |
|    total_timesteps | 3139000  |
---------------------------------
Eval num_timesteps=3140000, episode_reward=-83.23 +/- 2.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -83.2    |
| time/              |          |
|    total_timesteps | 3140000  |
| train/             |          |
|    actor_loss      | -0.433   |
|    critic_loss     | 1.19e-05 |
|    ent_coef        | 0.000303 |
|    ent_coef_loss   | -34.1    |
|    learning_rate   | 0.00186  |
|    n_updates       | 15330    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 635      |
| time/              |          |
|    episodes        | 3140     |
|    fps             | 643      |
|    time_elapsed    | 4882     |
|    total_timesteps | 3140000  |
---------------------------------
Eval num_timesteps=3141000, episode_reward=-81.63 +/- 1.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -81.6    |
| time/              |          |
|    total_timesteps | 3141000  |
---------------------------------
Eval num_timesteps=3142000, episode_reward=-81.98 +/- 1.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -82      |
| time/              |          |
|    total_timesteps | 3142000  |
| train/             |          |
|    actor_loss      | -0.679   |
|    critic_loss     | 0.000876 |
|    ent_coef        | 0.000292 |
|    ent_coef_loss   | 1.63     |
|    learning_rate   | 0.00186  |
|    n_updates       | 15340    |
---------------------------------
Eval num_timesteps=3143000, episode_reward=-81.88 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -81.9    |
| time/              |          |
|    total_timesteps | 3143000  |
---------------------------------
Eval num_timesteps=3144000, episode_reward=1072.13 +/- 28.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.07e+03 |
| time/              |          |
|    total_timesteps | 3144000  |
| train/             |          |
|    actor_loss      | -0.487   |
|    critic_loss     | 0.000244 |
|    ent_coef        | 0.000286 |
|    ent_coef_loss   | -22.5    |
|    learning_rate   | 0.00186  |
|    n_updates       | 15350    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 629      |
| time/              |          |
|    episodes        | 3144     |
|    fps             | 643      |
|    time_elapsed    | 4888     |
|    total_timesteps | 3144000  |
---------------------------------
Eval num_timesteps=3145000, episode_reward=1056.67 +/- 56.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.06e+03 |
| time/              |          |
|    total_timesteps | 3145000  |
---------------------------------
Eval num_timesteps=3146000, episode_reward=1276.88 +/- 27.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3146000  |
| train/             |          |
|    actor_loss      | -0.721   |
|    critic_loss     | 0.00115  |
|    ent_coef        | 0.000282 |
|    ent_coef_loss   | 19.8     |
|    learning_rate   | 0.00185  |
|    n_updates       | 15360    |
---------------------------------
Eval num_timesteps=3147000, episode_reward=1244.88 +/- 82.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3147000  |
---------------------------------
Eval num_timesteps=3148000, episode_reward=1234.93 +/- 43.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3148000  |
| train/             |          |
|    actor_loss      | -0.756   |
|    critic_loss     | 0.00114  |
|    ent_coef        | 0.000284 |
|    ent_coef_loss   | 23.9     |
|    learning_rate   | 0.00185  |
|    n_updates       | 15370    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 640      |
| time/              |          |
|    episodes        | 3148     |
|    fps             | 643      |
|    time_elapsed    | 4894     |
|    total_timesteps | 3148000  |
---------------------------------
Eval num_timesteps=3149000, episode_reward=1269.73 +/- 28.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3149000  |
---------------------------------
Eval num_timesteps=3150000, episode_reward=1251.45 +/- 30.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3150000  |
| train/             |          |
|    actor_loss      | -0.748   |
|    critic_loss     | 0.0011   |
|    ent_coef        | 0.00029  |
|    ent_coef_loss   | 13       |
|    learning_rate   | 0.00185  |
|    n_updates       | 15380    |
---------------------------------
Eval num_timesteps=3151000, episode_reward=1260.27 +/- 29.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3151000  |
---------------------------------
Eval num_timesteps=3152000, episode_reward=1090.20 +/- 38.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.09e+03 |
| time/              |          |
|    total_timesteps | 3152000  |
| train/             |          |
|    actor_loss      | -0.745   |
|    critic_loss     | 0.00115  |
|    ent_coef        | 0.000295 |
|    ent_coef_loss   | 15.6     |
|    learning_rate   | 0.00185  |
|    n_updates       | 15390    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 645      |
| time/              |          |
|    episodes        | 3152     |
|    fps             | 643      |
|    time_elapsed    | 4900     |
|    total_timesteps | 3152000  |
---------------------------------
Eval num_timesteps=3153000, episode_reward=1082.17 +/- 34.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.08e+03 |
| time/              |          |
|    total_timesteps | 3153000  |
---------------------------------
Eval num_timesteps=3154000, episode_reward=1285.96 +/- 45.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3154000  |
| train/             |          |
|    actor_loss      | -0.72    |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.0003   |
|    ent_coef_loss   | 20.4     |
|    learning_rate   | 0.00185  |
|    n_updates       | 15400    |
---------------------------------
Eval num_timesteps=3155000, episode_reward=1269.03 +/- 33.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3155000  |
---------------------------------
Eval num_timesteps=3156000, episode_reward=1202.41 +/- 44.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3156000  |
| train/             |          |
|    actor_loss      | -0.75    |
|    critic_loss     | 0.00102  |
|    ent_coef        | 0.000307 |
|    ent_coef_loss   | 17       |
|    learning_rate   | 0.00184  |
|    n_updates       | 15410    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 642      |
| time/              |          |
|    episodes        | 3156     |
|    fps             | 643      |
|    time_elapsed    | 4907     |
|    total_timesteps | 3156000  |
---------------------------------
Eval num_timesteps=3157000, episode_reward=1193.60 +/- 49.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3157000  |
---------------------------------
Eval num_timesteps=3158000, episode_reward=1167.12 +/- 22.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3158000  |
---------------------------------
Eval num_timesteps=3159000, episode_reward=1232.11 +/- 35.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3159000  |
| train/             |          |
|    actor_loss      | -0.729   |
|    critic_loss     | 0.0011   |
|    ent_coef        | 0.000313 |
|    ent_coef_loss   | 15.7     |
|    learning_rate   | 0.00184  |
|    n_updates       | 15420    |
---------------------------------
Eval num_timesteps=3160000, episode_reward=1233.25 +/- 38.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3160000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 644      |
| time/              |          |
|    episodes        | 3160     |
|    fps             | 643      |
|    time_elapsed    | 4913     |
|    total_timesteps | 3160000  |
---------------------------------
Eval num_timesteps=3161000, episode_reward=1296.66 +/- 34.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3161000  |
| train/             |          |
|    actor_loss      | -0.746   |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.000319 |
|    ent_coef_loss   | 17.4     |
|    learning_rate   | 0.00184  |
|    n_updates       | 15430    |
---------------------------------
Eval num_timesteps=3162000, episode_reward=1288.34 +/- 65.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3162000  |
---------------------------------
Eval num_timesteps=3163000, episode_reward=1230.88 +/- 46.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3163000  |
| train/             |          |
|    actor_loss      | -0.758   |
|    critic_loss     | 0.00103  |
|    ent_coef        | 0.000325 |
|    ent_coef_loss   | 14.1     |
|    learning_rate   | 0.00184  |
|    n_updates       | 15440    |
---------------------------------
Eval num_timesteps=3164000, episode_reward=1232.71 +/- 53.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3164000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 645      |
| time/              |          |
|    episodes        | 3164     |
|    fps             | 643      |
|    time_elapsed    | 4919     |
|    total_timesteps | 3164000  |
---------------------------------
Eval num_timesteps=3165000, episode_reward=1259.79 +/- 45.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3165000  |
| train/             |          |
|    actor_loss      | -0.737   |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000331 |
|    ent_coef_loss   | 14.1     |
|    learning_rate   | 0.00184  |
|    n_updates       | 15450    |
---------------------------------
Eval num_timesteps=3166000, episode_reward=1291.69 +/- 18.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3166000  |
---------------------------------
Eval num_timesteps=3167000, episode_reward=1291.74 +/- 14.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3167000  |
| train/             |          |
|    actor_loss      | -0.737   |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.000337 |
|    ent_coef_loss   | 13.3     |
|    learning_rate   | 0.00183  |
|    n_updates       | 15460    |
---------------------------------
Eval num_timesteps=3168000, episode_reward=1263.54 +/- 40.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.26e+03 |
| time/              |          |
|    total_timesteps | 3168000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 648      |
| time/              |          |
|    episodes        | 3168     |
|    fps             | 643      |
|    time_elapsed    | 4925     |
|    total_timesteps | 3168000  |
---------------------------------
Eval num_timesteps=3169000, episode_reward=1167.81 +/- 38.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3169000  |
| train/             |          |
|    actor_loss      | -0.742   |
|    critic_loss     | 0.00113  |
|    ent_coef        | 0.000343 |
|    ent_coef_loss   | 15.5     |
|    learning_rate   | 0.00183  |
|    n_updates       | 15470    |
---------------------------------
Eval num_timesteps=3170000, episode_reward=1168.49 +/- 51.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.17e+03 |
| time/              |          |
|    total_timesteps | 3170000  |
---------------------------------
Eval num_timesteps=3171000, episode_reward=1286.11 +/- 63.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3171000  |
| train/             |          |
|    actor_loss      | -0.721   |
|    critic_loss     | 0.000951 |
|    ent_coef        | 0.000349 |
|    ent_coef_loss   | 13.2     |
|    learning_rate   | 0.00183  |
|    n_updates       | 15480    |
---------------------------------
Eval num_timesteps=3172000, episode_reward=1286.65 +/- 54.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3172000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 651      |
| time/              |          |
|    episodes        | 3172     |
|    fps             | 643      |
|    time_elapsed    | 4931     |
|    total_timesteps | 3172000  |
---------------------------------
Eval num_timesteps=3173000, episode_reward=1213.36 +/- 63.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3173000  |
| train/             |          |
|    actor_loss      | -0.733   |
|    critic_loss     | 0.00113  |
|    ent_coef        | 0.000354 |
|    ent_coef_loss   | 8.93     |
|    learning_rate   | 0.00183  |
|    n_updates       | 15490    |
---------------------------------
Eval num_timesteps=3174000, episode_reward=1210.96 +/- 28.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3174000  |
---------------------------------
Eval num_timesteps=3175000, episode_reward=1236.27 +/- 38.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3175000  |
| train/             |          |
|    actor_loss      | -0.708   |
|    critic_loss     | 0.0011   |
|    ent_coef        | 0.000359 |
|    ent_coef_loss   | 10.9     |
|    learning_rate   | 0.00183  |
|    n_updates       | 15500    |
---------------------------------
Eval num_timesteps=3176000, episode_reward=1268.61 +/- 47.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3176000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 653      |
| time/              |          |
|    episodes        | 3176     |
|    fps             | 643      |
|    time_elapsed    | 4937     |
|    total_timesteps | 3176000  |
---------------------------------
Eval num_timesteps=3177000, episode_reward=1216.19 +/- 51.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3177000  |
| train/             |          |
|    actor_loss      | -0.728   |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000364 |
|    ent_coef_loss   | 9.49     |
|    learning_rate   | 0.00182  |
|    n_updates       | 15510    |
---------------------------------
Eval num_timesteps=3178000, episode_reward=1185.80 +/- 46.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3178000  |
---------------------------------
Eval num_timesteps=3179000, episode_reward=1188.33 +/- 59.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3179000  |
| train/             |          |
|    actor_loss      | -0.731   |
|    critic_loss     | 0.00113  |
|    ent_coef        | 0.000368 |
|    ent_coef_loss   | 10.3     |
|    learning_rate   | 0.00182  |
|    n_updates       | 15520    |
---------------------------------
Eval num_timesteps=3180000, episode_reward=1180.88 +/- 61.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3180000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 685      |
| time/              |          |
|    episodes        | 3180     |
|    fps             | 643      |
|    time_elapsed    | 4943     |
|    total_timesteps | 3180000  |
---------------------------------
Eval num_timesteps=3181000, episode_reward=1302.83 +/- 49.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.3e+03  |
| time/              |          |
|    total_timesteps | 3181000  |
| train/             |          |
|    actor_loss      | -0.705   |
|    critic_loss     | 0.00108  |
|    ent_coef        | 0.000372 |
|    ent_coef_loss   | 7.7      |
|    learning_rate   | 0.00182  |
|    n_updates       | 15530    |
---------------------------------
Eval num_timesteps=3182000, episode_reward=1274.83 +/- 40.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.27e+03 |
| time/              |          |
|    total_timesteps | 3182000  |
---------------------------------
Eval num_timesteps=3183000, episode_reward=1122.93 +/- 64.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.12e+03 |
| time/              |          |
|    total_timesteps | 3183000  |
| train/             |          |
|    actor_loss      | -0.714   |
|    critic_loss     | 0.00107  |
|    ent_coef        | 0.000376 |
|    ent_coef_loss   | 4.73     |
|    learning_rate   | 0.00182  |
|    n_updates       | 15540    |
---------------------------------
Eval num_timesteps=3184000, episode_reward=1156.53 +/- 55.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3184000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 737      |
| time/              |          |
|    episodes        | 3184     |
|    fps             | 643      |
|    time_elapsed    | 4949     |
|    total_timesteps | 3184000  |
---------------------------------
Eval num_timesteps=3185000, episode_reward=1181.91 +/- 19.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3185000  |
| train/             |          |
|    actor_loss      | -0.697   |
|    critic_loss     | 0.00105  |
|    ent_coef        | 0.000379 |
|    ent_coef_loss   | 10.3     |
|    learning_rate   | 0.00182  |
|    n_updates       | 15550    |
---------------------------------
Eval num_timesteps=3186000, episode_reward=1155.40 +/- 36.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3186000  |
---------------------------------
Eval num_timesteps=3187000, episode_reward=-84.19 +/- 1.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -84.2    |
| time/              |          |
|    total_timesteps | 3187000  |
| train/             |          |
|    actor_loss      | -0.678   |
|    critic_loss     | 0.000919 |
|    ent_coef        | 0.000383 |
|    ent_coef_loss   | 1.75     |
|    learning_rate   | 0.00181  |
|    n_updates       | 15560    |
---------------------------------
Eval num_timesteps=3188000, episode_reward=-83.59 +/- 1.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -83.6    |
| time/              |          |
|    total_timesteps | 3188000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 773      |
| time/              |          |
|    episodes        | 3188     |
|    fps             | 643      |
|    time_elapsed    | 4956     |
|    total_timesteps | 3188000  |
---------------------------------
Eval num_timesteps=3189000, episode_reward=-104.90 +/- 3.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 3189000  |
| train/             |          |
|    actor_loss      | -0.55    |
|    critic_loss     | 0.000475 |
|    ent_coef        | 0.000384 |
|    ent_coef_loss   | -17      |
|    learning_rate   | 0.00181  |
|    n_updates       | 15570    |
---------------------------------
Eval num_timesteps=3190000, episode_reward=-105.50 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 3190000  |
---------------------------------
Eval num_timesteps=3191000, episode_reward=-139.52 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 3191000  |
| train/             |          |
|    actor_loss      | -0.443   |
|    critic_loss     | 0.000123 |
|    ent_coef        | 0.000379 |
|    ent_coef_loss   | -30.2    |
|    learning_rate   | 0.00181  |
|    n_updates       | 15580    |
---------------------------------
Eval num_timesteps=3192000, episode_reward=-139.86 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 3192000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 762      |
| time/              |          |
|    episodes        | 3192     |
|    fps             | 643      |
|    time_elapsed    | 4962     |
|    total_timesteps | 3192000  |
---------------------------------
Eval num_timesteps=3193000, episode_reward=-148.24 +/- 1.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 3193000  |
| train/             |          |
|    actor_loss      | -0.41    |
|    critic_loss     | 3.22e-05 |
|    ent_coef        | 0.000369 |
|    ent_coef_loss   | -31.1    |
|    learning_rate   | 0.00181  |
|    n_updates       | 15590    |
---------------------------------
Eval num_timesteps=3194000, episode_reward=-148.45 +/- 1.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 3194000  |
---------------------------------
Eval num_timesteps=3195000, episode_reward=-243.63 +/- 1.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 3195000  |
| train/             |          |
|    actor_loss      | -0.413   |
|    critic_loss     | 2.43e-05 |
|    ent_coef        | 0.000357 |
|    ent_coef_loss   | -34      |
|    learning_rate   | 0.00181  |
|    n_updates       | 15600    |
---------------------------------
Eval num_timesteps=3196000, episode_reward=-241.67 +/- 4.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 3196000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 763      |
| time/              |          |
|    episodes        | 3196     |
|    fps             | 643      |
|    time_elapsed    | 4968     |
|    total_timesteps | 3196000  |
---------------------------------
Eval num_timesteps=3197000, episode_reward=-165.77 +/- 2.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -166     |
| time/              |          |
|    total_timesteps | 3197000  |
| train/             |          |
|    actor_loss      | -0.416   |
|    critic_loss     | 2.49e-05 |
|    ent_coef        | 0.000344 |
|    ent_coef_loss   | -30.9    |
|    learning_rate   | 0.0018   |
|    n_updates       | 15610    |
---------------------------------
Eval num_timesteps=3198000, episode_reward=-164.63 +/- 1.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -165     |
| time/              |          |
|    total_timesteps | 3198000  |
---------------------------------
Eval num_timesteps=3199000, episode_reward=-176.17 +/- 2.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 3199000  |
| train/             |          |
|    actor_loss      | -0.415   |
|    critic_loss     | 1.89e-05 |
|    ent_coef        | 0.000332 |
|    ent_coef_loss   | -30.4    |
|    learning_rate   | 0.0018   |
|    n_updates       | 15620    |
---------------------------------
Eval num_timesteps=3200000, episode_reward=-176.82 +/- 2.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -177     |
| time/              |          |
|    total_timesteps | 3200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 763      |
| time/              |          |
|    episodes        | 3200     |
|    fps             | 643      |
|    time_elapsed    | 4975     |
|    total_timesteps | 3200000  |
---------------------------------
Eval num_timesteps=3201000, episode_reward=-175.73 +/- 2.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -176     |
| time/              |          |
|    total_timesteps | 3201000  |
---------------------------------
Eval num_timesteps=3202000, episode_reward=-200.51 +/- 2.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -201     |
| time/              |          |
|    total_timesteps | 3202000  |
| train/             |          |
|    actor_loss      | -0.415   |
|    critic_loss     | 1.07e-05 |
|    ent_coef        | 0.000321 |
|    ent_coef_loss   | -31.7    |
|    learning_rate   | 0.0018   |
|    n_updates       | 15630    |
---------------------------------
Eval num_timesteps=3203000, episode_reward=-198.28 +/- 2.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -198     |
| time/              |          |
|    total_timesteps | 3203000  |
---------------------------------
Eval num_timesteps=3204000, episode_reward=-144.73 +/- 1.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 3204000  |
| train/             |          |
|    actor_loss      | -0.416   |
|    critic_loss     | 1.33e-05 |
|    ent_coef        | 0.00031  |
|    ent_coef_loss   | -34      |
|    learning_rate   | 0.0018   |
|    n_updates       | 15640    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 742      |
| time/              |          |
|    episodes        | 3204     |
|    fps             | 643      |
|    time_elapsed    | 4981     |
|    total_timesteps | 3204000  |
---------------------------------
Eval num_timesteps=3205000, episode_reward=-146.10 +/- 2.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 3205000  |
---------------------------------
Eval num_timesteps=3206000, episode_reward=-133.21 +/- 3.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 3206000  |
| train/             |          |
|    actor_loss      | -0.414   |
|    critic_loss     | 1.09e-05 |
|    ent_coef        | 0.000299 |
|    ent_coef_loss   | -36      |
|    learning_rate   | 0.00179  |
|    n_updates       | 15650    |
---------------------------------
Eval num_timesteps=3207000, episode_reward=-134.96 +/- 1.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 3207000  |
---------------------------------
Eval num_timesteps=3208000, episode_reward=-110.18 +/- 2.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 3208000  |
| train/             |          |
|    actor_loss      | -0.412   |
|    critic_loss     | 8.75e-06 |
|    ent_coef        | 0.000289 |
|    ent_coef_loss   | -36      |
|    learning_rate   | 0.00179  |
|    n_updates       | 15660    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 694      |
| time/              |          |
|    episodes        | 3208     |
|    fps             | 643      |
|    time_elapsed    | 4987     |
|    total_timesteps | 3208000  |
---------------------------------
Eval num_timesteps=3209000, episode_reward=-108.42 +/- 2.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 3209000  |
---------------------------------
Eval num_timesteps=3210000, episode_reward=-111.23 +/- 1.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 3210000  |
| train/             |          |
|    actor_loss      | -0.411   |
|    critic_loss     | 9.03e-06 |
|    ent_coef        | 0.000278 |
|    ent_coef_loss   | -37.8    |
|    learning_rate   | 0.00179  |
|    n_updates       | 15670    |
---------------------------------
Eval num_timesteps=3211000, episode_reward=-110.31 +/- 1.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 3211000  |
---------------------------------
Eval num_timesteps=3212000, episode_reward=-109.61 +/- 1.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 3212000  |
| train/             |          |
|    actor_loss      | -0.412   |
|    critic_loss     | 8.33e-06 |
|    ent_coef        | 0.000268 |
|    ent_coef_loss   | -35.7    |
|    learning_rate   | 0.00179  |
|    n_updates       | 15680    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 656      |
| time/              |          |
|    episodes        | 3212     |
|    fps             | 643      |
|    time_elapsed    | 4994     |
|    total_timesteps | 3212000  |
---------------------------------
Eval num_timesteps=3213000, episode_reward=-110.95 +/- 1.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -111     |
| time/              |          |
|    total_timesteps | 3213000  |
---------------------------------
Eval num_timesteps=3214000, episode_reward=-106.27 +/- 2.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 3214000  |
| train/             |          |
|    actor_loss      | -0.414   |
|    critic_loss     | 6.93e-06 |
|    ent_coef        | 0.000259 |
|    ent_coef_loss   | -30.9    |
|    learning_rate   | 0.00179  |
|    n_updates       | 15690    |
---------------------------------
Eval num_timesteps=3215000, episode_reward=-106.80 +/- 1.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 3215000  |
---------------------------------
Eval num_timesteps=3216000, episode_reward=-107.84 +/- 0.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 3216000  |
| train/             |          |
|    actor_loss      | -0.414   |
|    critic_loss     | 9.12e-06 |
|    ent_coef        | 0.000251 |
|    ent_coef_loss   | -28.7    |
|    learning_rate   | 0.00178  |
|    n_updates       | 15700    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 618      |
| time/              |          |
|    episodes        | 3216     |
|    fps             | 643      |
|    time_elapsed    | 5000     |
|    total_timesteps | 3216000  |
---------------------------------
Eval num_timesteps=3217000, episode_reward=-108.42 +/- 1.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 3217000  |
---------------------------------
Eval num_timesteps=3218000, episode_reward=-104.40 +/- 0.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 3218000  |
| train/             |          |
|    actor_loss      | -0.413   |
|    critic_loss     | 5.58e-06 |
|    ent_coef        | 0.000244 |
|    ent_coef_loss   | -29.9    |
|    learning_rate   | 0.00178  |
|    n_updates       | 15710    |
---------------------------------
Eval num_timesteps=3219000, episode_reward=-103.96 +/- 1.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 3219000  |
---------------------------------
Eval num_timesteps=3220000, episode_reward=-87.99 +/- 1.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -88      |
| time/              |          |
|    total_timesteps | 3220000  |
| train/             |          |
|    actor_loss      | -0.409   |
|    critic_loss     | 5.68e-06 |
|    ent_coef        | 0.000237 |
|    ent_coef_loss   | -33.7    |
|    learning_rate   | 0.00178  |
|    n_updates       | 15720    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 573      |
| time/              |          |
|    episodes        | 3220     |
|    fps             | 643      |
|    time_elapsed    | 5006     |
|    total_timesteps | 3220000  |
---------------------------------
Eval num_timesteps=3221000, episode_reward=-87.14 +/- 1.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -87.1    |
| time/              |          |
|    total_timesteps | 3221000  |
---------------------------------
Eval num_timesteps=3222000, episode_reward=-104.72 +/- 1.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -105     |
| time/              |          |
|    total_timesteps | 3222000  |
| train/             |          |
|    actor_loss      | -0.408   |
|    critic_loss     | 5.69e-06 |
|    ent_coef        | 0.00023  |
|    ent_coef_loss   | -34.6    |
|    learning_rate   | 0.00178  |
|    n_updates       | 15730    |
---------------------------------
Eval num_timesteps=3223000, episode_reward=-104.10 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 3223000  |
---------------------------------
Eval num_timesteps=3224000, episode_reward=-85.02 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -85      |
| time/              |          |
|    total_timesteps | 3224000  |
| train/             |          |
|    actor_loss      | -0.411   |
|    critic_loss     | 4.95e-06 |
|    ent_coef        | 0.000223 |
|    ent_coef_loss   | -33.4    |
|    learning_rate   | 0.00178  |
|    n_updates       | 15740    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 525      |
| time/              |          |
|    episodes        | 3224     |
|    fps             | 643      |
|    time_elapsed    | 5013     |
|    total_timesteps | 3224000  |
---------------------------------
Eval num_timesteps=3225000, episode_reward=-83.86 +/- 1.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -83.9    |
| time/              |          |
|    total_timesteps | 3225000  |
---------------------------------
Eval num_timesteps=3226000, episode_reward=-101.74 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -102     |
| time/              |          |
|    total_timesteps | 3226000  |
| train/             |          |
|    actor_loss      | -0.41    |
|    critic_loss     | 6.16e-06 |
|    ent_coef        | 0.000217 |
|    ent_coef_loss   | -31.4    |
|    learning_rate   | 0.00177  |
|    n_updates       | 15750    |
---------------------------------
Eval num_timesteps=3227000, episode_reward=-101.33 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -101     |
| time/              |          |
|    total_timesteps | 3227000  |
---------------------------------
Eval num_timesteps=3228000, episode_reward=-108.97 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 3228000  |
| train/             |          |
|    actor_loss      | -0.41    |
|    critic_loss     | 5.36e-06 |
|    ent_coef        | 0.000211 |
|    ent_coef_loss   | -18.7    |
|    learning_rate   | 0.00177  |
|    n_updates       | 15760    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 476      |
| time/              |          |
|    episodes        | 3228     |
|    fps             | 643      |
|    time_elapsed    | 5019     |
|    total_timesteps | 3228000  |
---------------------------------
Eval num_timesteps=3229000, episode_reward=-109.25 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 3229000  |
---------------------------------
Eval num_timesteps=3230000, episode_reward=-104.19 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 3230000  |
| train/             |          |
|    actor_loss      | -0.409   |
|    critic_loss     | 5.77e-06 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | -19.9    |
|    learning_rate   | 0.00177  |
|    n_updates       | 15770    |
---------------------------------
Eval num_timesteps=3231000, episode_reward=-103.23 +/- 0.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -103     |
| time/              |          |
|    total_timesteps | 3231000  |
---------------------------------
Eval num_timesteps=3232000, episode_reward=-94.96 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -95      |
| time/              |          |
|    total_timesteps | 3232000  |
| train/             |          |
|    actor_loss      | -0.406   |
|    critic_loss     | 4.85e-06 |
|    ent_coef        | 0.000203 |
|    ent_coef_loss   | -25.7    |
|    learning_rate   | 0.00177  |
|    n_updates       | 15780    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 470      |
| time/              |          |
|    episodes        | 3232     |
|    fps             | 643      |
|    time_elapsed    | 5025     |
|    total_timesteps | 3232000  |
---------------------------------
Eval num_timesteps=3233000, episode_reward=-95.66 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -95.7    |
| time/              |          |
|    total_timesteps | 3233000  |
---------------------------------
Eval num_timesteps=3234000, episode_reward=-72.40 +/- 1.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -72.4    |
| time/              |          |
|    total_timesteps | 3234000  |
| train/             |          |
|    actor_loss      | -0.405   |
|    critic_loss     | 5.6e-06  |
|    ent_coef        | 0.000198 |
|    ent_coef_loss   | -31.3    |
|    learning_rate   | 0.00177  |
|    n_updates       | 15790    |
---------------------------------
Eval num_timesteps=3235000, episode_reward=-73.76 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -73.8    |
| time/              |          |
|    total_timesteps | 3235000  |
---------------------------------
Eval num_timesteps=3236000, episode_reward=-39.97 +/- 1.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -40      |
| time/              |          |
|    total_timesteps | 3236000  |
| train/             |          |
|    actor_loss      | -0.405   |
|    critic_loss     | 5.01e-06 |
|    ent_coef        | 0.000193 |
|    ent_coef_loss   | -33.4    |
|    learning_rate   | 0.00176  |
|    n_updates       | 15800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 472      |
| time/              |          |
|    episodes        | 3236     |
|    fps             | 643      |
|    time_elapsed    | 5031     |
|    total_timesteps | 3236000  |
---------------------------------
Eval num_timesteps=3237000, episode_reward=-40.19 +/- 1.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -40.2    |
| time/              |          |
|    total_timesteps | 3237000  |
---------------------------------
Eval num_timesteps=3238000, episode_reward=-52.94 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -52.9    |
| time/              |          |
|    total_timesteps | 3238000  |
| train/             |          |
|    actor_loss      | -0.404   |
|    critic_loss     | 5.64e-06 |
|    ent_coef        | 0.000188 |
|    ent_coef_loss   | -32.9    |
|    learning_rate   | 0.00176  |
|    n_updates       | 15810    |
---------------------------------
Eval num_timesteps=3239000, episode_reward=-52.43 +/- 1.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -52.4    |
| time/              |          |
|    total_timesteps | 3239000  |
---------------------------------
Eval num_timesteps=3240000, episode_reward=-53.07 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -53.1    |
| time/              |          |
|    total_timesteps | 3240000  |
| train/             |          |
|    actor_loss      | -0.403   |
|    critic_loss     | 5.49e-06 |
|    ent_coef        | 0.000183 |
|    ent_coef_loss   | -29      |
|    learning_rate   | 0.00176  |
|    n_updates       | 15820    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 472      |
| time/              |          |
|    episodes        | 3240     |
|    fps             | 643      |
|    time_elapsed    | 5038     |
|    total_timesteps | 3240000  |
---------------------------------
Eval num_timesteps=3241000, episode_reward=-52.59 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -52.6    |
| time/              |          |
|    total_timesteps | 3241000  |
---------------------------------
Eval num_timesteps=3242000, episode_reward=-60.26 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -60.3    |
| time/              |          |
|    total_timesteps | 3242000  |
| train/             |          |
|    actor_loss      | -0.402   |
|    critic_loss     | 5.22e-06 |
|    ent_coef        | 0.000179 |
|    ent_coef_loss   | -25.9    |
|    learning_rate   | 0.00176  |
|    n_updates       | 15830    |
---------------------------------
Eval num_timesteps=3243000, episode_reward=-60.36 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -60.4    |
| time/              |          |
|    total_timesteps | 3243000  |
---------------------------------
Eval num_timesteps=3244000, episode_reward=-60.29 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -60.3    |
| time/              |          |
|    total_timesteps | 3244000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 447      |
| time/              |          |
|    episodes        | 3244     |
|    fps             | 643      |
|    time_elapsed    | 5044     |
|    total_timesteps | 3244000  |
---------------------------------
Eval num_timesteps=3245000, episode_reward=-37.89 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -37.9    |
| time/              |          |
|    total_timesteps | 3245000  |
| train/             |          |
|    actor_loss      | -0.401   |
|    critic_loss     | 4.92e-06 |
|    ent_coef        | 0.000175 |
|    ent_coef_loss   | -23.2    |
|    learning_rate   | 0.00176  |
|    n_updates       | 15840    |
---------------------------------
Eval num_timesteps=3246000, episode_reward=-37.04 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -37      |
| time/              |          |
|    total_timesteps | 3246000  |
---------------------------------
Eval num_timesteps=3247000, episode_reward=-23.79 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -23.8    |
| time/              |          |
|    total_timesteps | 3247000  |
| train/             |          |
|    actor_loss      | -0.399   |
|    critic_loss     | 5.13e-06 |
|    ent_coef        | 0.000171 |
|    ent_coef_loss   | -25      |
|    learning_rate   | 0.00175  |
|    n_updates       | 15850    |
---------------------------------
Eval num_timesteps=3248000, episode_reward=-23.04 +/- 0.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -23      |
| time/              |          |
|    total_timesteps | 3248000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 399      |
| time/              |          |
|    episodes        | 3248     |
|    fps             | 643      |
|    time_elapsed    | 5050     |
|    total_timesteps | 3248000  |
---------------------------------
Eval num_timesteps=3249000, episode_reward=-31.94 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -31.9    |
| time/              |          |
|    total_timesteps | 3249000  |
| train/             |          |
|    actor_loss      | -0.398   |
|    critic_loss     | 6.03e-06 |
|    ent_coef        | 0.000168 |
|    ent_coef_loss   | -24.4    |
|    learning_rate   | 0.00175  |
|    n_updates       | 15860    |
---------------------------------
Eval num_timesteps=3250000, episode_reward=-31.73 +/- 0.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -31.7    |
| time/              |          |
|    total_timesteps | 3250000  |
---------------------------------
Eval num_timesteps=3251000, episode_reward=-42.27 +/- 1.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -42.3    |
| time/              |          |
|    total_timesteps | 3251000  |
| train/             |          |
|    actor_loss      | -0.398   |
|    critic_loss     | 4.02e-06 |
|    ent_coef        | 0.000164 |
|    ent_coef_loss   | -22.3    |
|    learning_rate   | 0.00175  |
|    n_updates       | 15870    |
---------------------------------
Eval num_timesteps=3252000, episode_reward=-41.30 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -41.3    |
| time/              |          |
|    total_timesteps | 3252000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 349      |
| time/              |          |
|    episodes        | 3252     |
|    fps             | 643      |
|    time_elapsed    | 5056     |
|    total_timesteps | 3252000  |
---------------------------------
Eval num_timesteps=3253000, episode_reward=-42.04 +/- 1.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -42      |
| time/              |          |
|    total_timesteps | 3253000  |
| train/             |          |
|    actor_loss      | -0.4     |
|    critic_loss     | 3.71e-06 |
|    ent_coef        | 0.000161 |
|    ent_coef_loss   | -20.4    |
|    learning_rate   | 0.00175  |
|    n_updates       | 15880    |
---------------------------------
Eval num_timesteps=3254000, episode_reward=-41.33 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -41.3    |
| time/              |          |
|    total_timesteps | 3254000  |
---------------------------------
Eval num_timesteps=3255000, episode_reward=-36.54 +/- 1.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -36.5    |
| time/              |          |
|    total_timesteps | 3255000  |
| train/             |          |
|    actor_loss      | -0.401   |
|    critic_loss     | 2.76e-06 |
|    ent_coef        | 0.000159 |
|    ent_coef_loss   | -18.8    |
|    learning_rate   | 0.00175  |
|    n_updates       | 15890    |
---------------------------------
Eval num_timesteps=3256000, episode_reward=-36.00 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -36      |
| time/              |          |
|    total_timesteps | 3256000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 302      |
| time/              |          |
|    episodes        | 3256     |
|    fps             | 643      |
|    time_elapsed    | 5063     |
|    total_timesteps | 3256000  |
---------------------------------
Eval num_timesteps=3257000, episode_reward=-32.57 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -32.6    |
| time/              |          |
|    total_timesteps | 3257000  |
| train/             |          |
|    actor_loss      | -0.398   |
|    critic_loss     | 3.14e-06 |
|    ent_coef        | 0.000156 |
|    ent_coef_loss   | -19.6    |
|    learning_rate   | 0.00174  |
|    n_updates       | 15900    |
---------------------------------
Eval num_timesteps=3258000, episode_reward=-32.39 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -32.4    |
| time/              |          |
|    total_timesteps | 3258000  |
---------------------------------
Eval num_timesteps=3259000, episode_reward=-28.41 +/- 1.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -28.4    |
| time/              |          |
|    total_timesteps | 3259000  |
| train/             |          |
|    actor_loss      | -0.398   |
|    critic_loss     | 3.06e-06 |
|    ent_coef        | 0.000153 |
|    ent_coef_loss   | -22.4    |
|    learning_rate   | 0.00174  |
|    n_updates       | 15910    |
---------------------------------
Eval num_timesteps=3260000, episode_reward=-28.47 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -28.5    |
| time/              |          |
|    total_timesteps | 3260000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 253      |
| time/              |          |
|    episodes        | 3260     |
|    fps             | 643      |
|    time_elapsed    | 5069     |
|    total_timesteps | 3260000  |
---------------------------------
Eval num_timesteps=3261000, episode_reward=-22.63 +/- 0.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.6    |
| time/              |          |
|    total_timesteps | 3261000  |
| train/             |          |
|    actor_loss      | -0.398   |
|    critic_loss     | 2.96e-06 |
|    ent_coef        | 0.000151 |
|    ent_coef_loss   | -19.3    |
|    learning_rate   | 0.00174  |
|    n_updates       | 15920    |
---------------------------------
Eval num_timesteps=3262000, episode_reward=-22.76 +/- 1.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.8    |
| time/              |          |
|    total_timesteps | 3262000  |
---------------------------------
Eval num_timesteps=3263000, episode_reward=-23.90 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -23.9    |
| time/              |          |
|    total_timesteps | 3263000  |
| train/             |          |
|    actor_loss      | -0.396   |
|    critic_loss     | 3.05e-06 |
|    ent_coef        | 0.000148 |
|    ent_coef_loss   | -19.4    |
|    learning_rate   | 0.00174  |
|    n_updates       | 15930    |
---------------------------------
Eval num_timesteps=3264000, episode_reward=-22.74 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.7    |
| time/              |          |
|    total_timesteps | 3264000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 202      |
| time/              |          |
|    episodes        | 3264     |
|    fps             | 643      |
|    time_elapsed    | 5075     |
|    total_timesteps | 3264000  |
---------------------------------
Eval num_timesteps=3265000, episode_reward=-20.76 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -20.8    |
| time/              |          |
|    total_timesteps | 3265000  |
| train/             |          |
|    actor_loss      | -0.396   |
|    critic_loss     | 2.81e-06 |
|    ent_coef        | 0.000146 |
|    ent_coef_loss   | -17.1    |
|    learning_rate   | 0.00174  |
|    n_updates       | 15940    |
---------------------------------
Eval num_timesteps=3266000, episode_reward=-19.73 +/- 0.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -19.7    |
| time/              |          |
|    total_timesteps | 3266000  |
---------------------------------
Eval num_timesteps=3267000, episode_reward=-18.55 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.5    |
| time/              |          |
|    total_timesteps | 3267000  |
| train/             |          |
|    actor_loss      | -0.397   |
|    critic_loss     | 3.41e-06 |
|    ent_coef        | 0.000144 |
|    ent_coef_loss   | -14.3    |
|    learning_rate   | 0.00173  |
|    n_updates       | 15950    |
---------------------------------
Eval num_timesteps=3268000, episode_reward=-17.58 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -17.6    |
| time/              |          |
|    total_timesteps | 3268000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 153      |
| time/              |          |
|    episodes        | 3268     |
|    fps             | 643      |
|    time_elapsed    | 5082     |
|    total_timesteps | 3268000  |
---------------------------------
Eval num_timesteps=3269000, episode_reward=-20.06 +/- 1.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -20.1    |
| time/              |          |
|    total_timesteps | 3269000  |
| train/             |          |
|    actor_loss      | -0.395   |
|    critic_loss     | 3.43e-06 |
|    ent_coef        | 0.000142 |
|    ent_coef_loss   | -15.7    |
|    learning_rate   | 0.00173  |
|    n_updates       | 15960    |
---------------------------------
Eval num_timesteps=3270000, episode_reward=-19.72 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -19.7    |
| time/              |          |
|    total_timesteps | 3270000  |
---------------------------------
Eval num_timesteps=3271000, episode_reward=-22.92 +/- 1.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.9    |
| time/              |          |
|    total_timesteps | 3271000  |
| train/             |          |
|    actor_loss      | -0.395   |
|    critic_loss     | 3.72e-06 |
|    ent_coef        | 0.000141 |
|    ent_coef_loss   | -13.9    |
|    learning_rate   | 0.00173  |
|    n_updates       | 15970    |
---------------------------------
Eval num_timesteps=3272000, episode_reward=-22.91 +/- 1.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.9    |
| time/              |          |
|    total_timesteps | 3272000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 105      |
| time/              |          |
|    episodes        | 3272     |
|    fps             | 643      |
|    time_elapsed    | 5088     |
|    total_timesteps | 3272000  |
---------------------------------
Eval num_timesteps=3273000, episode_reward=-20.60 +/- 0.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -20.6    |
| time/              |          |
|    total_timesteps | 3273000  |
| train/             |          |
|    actor_loss      | -0.396   |
|    critic_loss     | 3.18e-06 |
|    ent_coef        | 0.000139 |
|    ent_coef_loss   | -14.3    |
|    learning_rate   | 0.00173  |
|    n_updates       | 15980    |
---------------------------------
Eval num_timesteps=3274000, episode_reward=-20.03 +/- 1.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -20      |
| time/              |          |
|    total_timesteps | 3274000  |
---------------------------------
Eval num_timesteps=3275000, episode_reward=-26.86 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -26.9    |
| time/              |          |
|    total_timesteps | 3275000  |
| train/             |          |
|    actor_loss      | -0.395   |
|    critic_loss     | 2.53e-06 |
|    ent_coef        | 0.000137 |
|    ent_coef_loss   | -17.4    |
|    learning_rate   | 0.00173  |
|    n_updates       | 15990    |
---------------------------------
Eval num_timesteps=3276000, episode_reward=-26.45 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -26.4    |
| time/              |          |
|    total_timesteps | 3276000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 59.4     |
| time/              |          |
|    episodes        | 3276     |
|    fps             | 643      |
|    time_elapsed    | 5094     |
|    total_timesteps | 3276000  |
---------------------------------
Eval num_timesteps=3277000, episode_reward=-20.53 +/- 0.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -20.5    |
| time/              |          |
|    total_timesteps | 3277000  |
| train/             |          |
|    actor_loss      | -0.394   |
|    critic_loss     | 3.48e-06 |
|    ent_coef        | 0.000136 |
|    ent_coef_loss   | -15.3    |
|    learning_rate   | 0.00172  |
|    n_updates       | 16000    |
---------------------------------
Eval num_timesteps=3278000, episode_reward=-20.64 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -20.6    |
| time/              |          |
|    total_timesteps | 3278000  |
---------------------------------
Eval num_timesteps=3279000, episode_reward=-16.87 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -16.9    |
| time/              |          |
|    total_timesteps | 3279000  |
| train/             |          |
|    actor_loss      | -0.392   |
|    critic_loss     | 2.79e-06 |
|    ent_coef        | 0.000134 |
|    ent_coef_loss   | -15.9    |
|    learning_rate   | 0.00172  |
|    n_updates       | 16010    |
---------------------------------
Eval num_timesteps=3280000, episode_reward=-17.63 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -17.6    |
| time/              |          |
|    total_timesteps | 3280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 11.9     |
| time/              |          |
|    episodes        | 3280     |
|    fps             | 643      |
|    time_elapsed    | 5100     |
|    total_timesteps | 3280000  |
---------------------------------
Eval num_timesteps=3281000, episode_reward=-17.68 +/- 1.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -17.7    |
| time/              |          |
|    total_timesteps | 3281000  |
| train/             |          |
|    actor_loss      | -0.393   |
|    critic_loss     | 2.19e-06 |
|    ent_coef        | 0.000132 |
|    ent_coef_loss   | -15.6    |
|    learning_rate   | 0.00172  |
|    n_updates       | 16020    |
---------------------------------
Eval num_timesteps=3282000, episode_reward=-17.85 +/- 1.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -17.9    |
| time/              |          |
|    total_timesteps | 3282000  |
---------------------------------
Eval num_timesteps=3283000, episode_reward=-16.42 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -16.4    |
| time/              |          |
|    total_timesteps | 3283000  |
| train/             |          |
|    actor_loss      | -0.39    |
|    critic_loss     | 2.35e-06 |
|    ent_coef        | 0.000131 |
|    ent_coef_loss   | -14.2    |
|    learning_rate   | 0.00172  |
|    n_updates       | 16030    |
---------------------------------
Eval num_timesteps=3284000, episode_reward=-16.21 +/- 1.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -16.2    |
| time/              |          |
|    total_timesteps | 3284000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -33.4    |
| time/              |          |
|    episodes        | 3284     |
|    fps             | 643      |
|    time_elapsed    | 5107     |
|    total_timesteps | 3284000  |
---------------------------------
Eval num_timesteps=3285000, episode_reward=-18.89 +/- 0.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.9    |
| time/              |          |
|    total_timesteps | 3285000  |
| train/             |          |
|    actor_loss      | -0.391   |
|    critic_loss     | 2.74e-06 |
|    ent_coef        | 0.000129 |
|    ent_coef_loss   | -14.1    |
|    learning_rate   | 0.00172  |
|    n_updates       | 16040    |
---------------------------------
Eval num_timesteps=3286000, episode_reward=-19.00 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -19      |
| time/              |          |
|    total_timesteps | 3286000  |
---------------------------------
Eval num_timesteps=3287000, episode_reward=-19.24 +/- 0.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -19.2    |
| time/              |          |
|    total_timesteps | 3287000  |
---------------------------------
Eval num_timesteps=3288000, episode_reward=-18.11 +/- 0.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.1    |
| time/              |          |
|    total_timesteps | 3288000  |
| train/             |          |
|    actor_loss      | -0.392   |
|    critic_loss     | 3.72e-06 |
|    ent_coef        | 0.000128 |
|    ent_coef_loss   | -16.3    |
|    learning_rate   | 0.00171  |
|    n_updates       | 16050    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -63.8    |
| time/              |          |
|    episodes        | 3288     |
|    fps             | 643      |
|    time_elapsed    | 5113     |
|    total_timesteps | 3288000  |
---------------------------------
Eval num_timesteps=3289000, episode_reward=-18.13 +/- 0.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.1    |
| time/              |          |
|    total_timesteps | 3289000  |
---------------------------------
Eval num_timesteps=3290000, episode_reward=-18.19 +/- 1.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.2    |
| time/              |          |
|    total_timesteps | 3290000  |
| train/             |          |
|    actor_loss      | -0.392   |
|    critic_loss     | 4.59e-06 |
|    ent_coef        | 0.000126 |
|    ent_coef_loss   | -15.9    |
|    learning_rate   | 0.00171  |
|    n_updates       | 16060    |
---------------------------------
Eval num_timesteps=3291000, episode_reward=-18.03 +/- 0.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18      |
| time/              |          |
|    total_timesteps | 3291000  |
---------------------------------
Eval num_timesteps=3292000, episode_reward=-25.06 +/- 0.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -25.1    |
| time/              |          |
|    total_timesteps | 3292000  |
| train/             |          |
|    actor_loss      | -0.392   |
|    critic_loss     | 4.18e-06 |
|    ent_coef        | 0.000124 |
|    ent_coef_loss   | -16.9    |
|    learning_rate   | 0.00171  |
|    n_updates       | 16070    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -71.5    |
| time/              |          |
|    episodes        | 3292     |
|    fps             | 643      |
|    time_elapsed    | 5119     |
|    total_timesteps | 3292000  |
---------------------------------
Eval num_timesteps=3293000, episode_reward=-25.78 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -25.8    |
| time/              |          |
|    total_timesteps | 3293000  |
---------------------------------
Eval num_timesteps=3294000, episode_reward=-25.15 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -25.2    |
| time/              |          |
|    total_timesteps | 3294000  |
| train/             |          |
|    actor_loss      | -0.392   |
|    critic_loss     | 2.98e-06 |
|    ent_coef        | 0.000123 |
|    ent_coef_loss   | -14.1    |
|    learning_rate   | 0.00171  |
|    n_updates       | 16080    |
---------------------------------
Eval num_timesteps=3295000, episode_reward=-25.69 +/- 1.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -25.7    |
| time/              |          |
|    total_timesteps | 3295000  |
---------------------------------
Eval num_timesteps=3296000, episode_reward=-26.03 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -26      |
| time/              |          |
|    total_timesteps | 3296000  |
| train/             |          |
|    actor_loss      | -0.392   |
|    critic_loss     | 2.63e-06 |
|    ent_coef        | 0.000121 |
|    ent_coef_loss   | -13.7    |
|    learning_rate   | 0.0017   |
|    n_updates       | 16090    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -65.1    |
| time/              |          |
|    episodes        | 3296     |
|    fps             | 643      |
|    time_elapsed    | 5125     |
|    total_timesteps | 3296000  |
---------------------------------
Eval num_timesteps=3297000, episode_reward=-27.21 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -27.2    |
| time/              |          |
|    total_timesteps | 3297000  |
---------------------------------
Eval num_timesteps=3298000, episode_reward=-21.76 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -21.8    |
| time/              |          |
|    total_timesteps | 3298000  |
| train/             |          |
|    actor_loss      | -0.393   |
|    critic_loss     | 2.5e-06  |
|    ent_coef        | 0.00012  |
|    ent_coef_loss   | -13.4    |
|    learning_rate   | 0.0017   |
|    n_updates       | 16100    |
---------------------------------
Eval num_timesteps=3299000, episode_reward=-21.21 +/- 0.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -21.2    |
| time/              |          |
|    total_timesteps | 3299000  |
---------------------------------
Eval num_timesteps=3300000, episode_reward=-21.53 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -21.5    |
| time/              |          |
|    total_timesteps | 3300000  |
| train/             |          |
|    actor_loss      | -0.391   |
|    critic_loss     | 3.48e-06 |
|    ent_coef        | 0.000119 |
|    ent_coef_loss   | -14.4    |
|    learning_rate   | 0.0017   |
|    n_updates       | 16110    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -58.5    |
| time/              |          |
|    episodes        | 3300     |
|    fps             | 642      |
|    time_elapsed    | 5132     |
|    total_timesteps | 3300000  |
---------------------------------
Eval num_timesteps=3301000, episode_reward=-22.30 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -22.3    |
| time/              |          |
|    total_timesteps | 3301000  |
---------------------------------
Eval num_timesteps=3302000, episode_reward=-20.96 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -21      |
| time/              |          |
|    total_timesteps | 3302000  |
| train/             |          |
|    actor_loss      | -0.391   |
|    critic_loss     | 3.93e-06 |
|    ent_coef        | 0.000117 |
|    ent_coef_loss   | -16.4    |
|    learning_rate   | 0.0017   |
|    n_updates       | 16120    |
---------------------------------
Eval num_timesteps=3303000, episode_reward=-20.89 +/- 1.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -20.9    |
| time/              |          |
|    total_timesteps | 3303000  |
---------------------------------
Eval num_timesteps=3304000, episode_reward=-20.83 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -20.8    |
| time/              |          |
|    total_timesteps | 3304000  |
| train/             |          |
|    actor_loss      | -0.392   |
|    critic_loss     | 3.16e-06 |
|    ent_coef        | 0.000116 |
|    ent_coef_loss   | -13.7    |
|    learning_rate   | 0.0017   |
|    n_updates       | 16130    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -52.4    |
| time/              |          |
|    episodes        | 3304     |
|    fps             | 642      |
|    time_elapsed    | 5138     |
|    total_timesteps | 3304000  |
---------------------------------
Eval num_timesteps=3305000, episode_reward=-21.00 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -21      |
| time/              |          |
|    total_timesteps | 3305000  |
---------------------------------
Eval num_timesteps=3306000, episode_reward=-18.58 +/- 1.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.6    |
| time/              |          |
|    total_timesteps | 3306000  |
| train/             |          |
|    actor_loss      | -0.388   |
|    critic_loss     | 2.48e-06 |
|    ent_coef        | 0.000114 |
|    ent_coef_loss   | -13.6    |
|    learning_rate   | 0.00169  |
|    n_updates       | 16140    |
---------------------------------
Eval num_timesteps=3307000, episode_reward=-18.74 +/- 0.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.7    |
| time/              |          |
|    total_timesteps | 3307000  |
---------------------------------
Eval num_timesteps=3308000, episode_reward=-17.87 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -17.9    |
| time/              |          |
|    total_timesteps | 3308000  |
| train/             |          |
|    actor_loss      | -0.388   |
|    critic_loss     | 2.79e-06 |
|    ent_coef        | 0.000113 |
|    ent_coef_loss   | -11.1    |
|    learning_rate   | 0.00169  |
|    n_updates       | 16150    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -47.8    |
| time/              |          |
|    episodes        | 3308     |
|    fps             | 642      |
|    time_elapsed    | 5144     |
|    total_timesteps | 3308000  |
---------------------------------
Eval num_timesteps=3309000, episode_reward=-18.30 +/- 0.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -18.3    |
| time/              |          |
|    total_timesteps | 3309000  |
---------------------------------
Eval num_timesteps=3310000, episode_reward=-19.69 +/- 2.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -19.7    |
| time/              |          |
|    total_timesteps | 3310000  |
| train/             |          |
|    actor_loss      | -0.386   |
|    critic_loss     | 2.86e-06 |
|    ent_coef        | 0.000112 |
|    ent_coef_loss   | -11.6    |
|    learning_rate   | 0.00169  |
|    n_updates       | 16160    |
---------------------------------
Eval num_timesteps=3311000, episode_reward=-20.41 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -20.4    |
| time/              |          |
|    total_timesteps | 3311000  |
---------------------------------
Eval num_timesteps=3312000, episode_reward=-15.68 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -15.7    |
| time/              |          |
|    total_timesteps | 3312000  |
| train/             |          |
|    actor_loss      | -0.387   |
|    critic_loss     | 3.22e-06 |
|    ent_coef        | 0.000111 |
|    ent_coef_loss   | -8.84    |
|    learning_rate   | 0.00169  |
|    n_updates       | 16170    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -44.4    |
| time/              |          |
|    episodes        | 3312     |
|    fps             | 642      |
|    time_elapsed    | 5151     |
|    total_timesteps | 3312000  |
---------------------------------
Eval num_timesteps=3313000, episode_reward=-15.41 +/- 0.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -15.4    |
| time/              |          |
|    total_timesteps | 3313000  |
---------------------------------
Eval num_timesteps=3314000, episode_reward=-12.78 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -12.8    |
| time/              |          |
|    total_timesteps | 3314000  |
| train/             |          |
|    actor_loss      | -0.385   |
|    critic_loss     | 3.01e-06 |
|    ent_coef        | 0.00011  |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.00169  |
|    n_updates       | 16180    |
---------------------------------
Eval num_timesteps=3315000, episode_reward=-13.86 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -13.9    |
| time/              |          |
|    total_timesteps | 3315000  |
---------------------------------
Eval num_timesteps=3316000, episode_reward=-13.35 +/- 1.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -13.3    |
| time/              |          |
|    total_timesteps | 3316000  |
| train/             |          |
|    actor_loss      | -0.385   |
|    critic_loss     | 2.96e-06 |
|    ent_coef        | 0.000109 |
|    ent_coef_loss   | -11.4    |
|    learning_rate   | 0.00168  |
|    n_updates       | 16190    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -40.6    |
| time/              |          |
|    episodes        | 3316     |
|    fps             | 642      |
|    time_elapsed    | 5157     |
|    total_timesteps | 3316000  |
---------------------------------
Eval num_timesteps=3317000, episode_reward=-11.70 +/- 1.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -11.7    |
| time/              |          |
|    total_timesteps | 3317000  |
---------------------------------
Eval num_timesteps=3318000, episode_reward=-39.18 +/- 1.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -39.2    |
| time/              |          |
|    total_timesteps | 3318000  |
| train/             |          |
|    actor_loss      | -0.382   |
|    critic_loss     | 4.92e-06 |
|    ent_coef        | 0.000108 |
|    ent_coef_loss   | -11.8    |
|    learning_rate   | 0.00168  |
|    n_updates       | 16200    |
---------------------------------
Eval num_timesteps=3319000, episode_reward=-41.11 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -41.1    |
| time/              |          |
|    total_timesteps | 3319000  |
---------------------------------
Eval num_timesteps=3320000, episode_reward=1247.03 +/- 31.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.25e+03 |
| time/              |          |
|    total_timesteps | 3320000  |
| train/             |          |
|    actor_loss      | -0.378   |
|    critic_loss     | 5.41e-06 |
|    ent_coef        | 0.000107 |
|    ent_coef_loss   | -13.2    |
|    learning_rate   | 0.00168  |
|    n_updates       | 16210    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -36.6    |
| time/              |          |
|    episodes        | 3320     |
|    fps             | 642      |
|    time_elapsed    | 5163     |
|    total_timesteps | 3320000  |
---------------------------------
Eval num_timesteps=3321000, episode_reward=1186.39 +/- 42.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3321000  |
---------------------------------
Eval num_timesteps=3322000, episode_reward=1282.15 +/- 38.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.28e+03 |
| time/              |          |
|    total_timesteps | 3322000  |
| train/             |          |
|    actor_loss      | -0.375   |
|    critic_loss     | 8.02e-06 |
|    ent_coef        | 0.000106 |
|    ent_coef_loss   | -18.7    |
|    learning_rate   | 0.00168  |
|    n_updates       | 16220    |
---------------------------------
Eval num_timesteps=3323000, episode_reward=1294.18 +/- 52.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.29e+03 |
| time/              |          |
|    total_timesteps | 3323000  |
---------------------------------
Eval num_timesteps=3324000, episode_reward=1341.11 +/- 56.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 3324000  |
| train/             |          |
|    actor_loss      | -0.673   |
|    critic_loss     | 0.00131  |
|    ent_coef        | 0.000105 |
|    ent_coef_loss   | 27.8     |
|    learning_rate   | 0.00168  |
|    n_updates       | 16230    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -5.92    |
| time/              |          |
|    episodes        | 3324     |
|    fps             | 642      |
|    time_elapsed    | 5169     |
|    total_timesteps | 3324000  |
---------------------------------
Eval num_timesteps=3325000, episode_reward=1326.46 +/- 54.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3325000  |
---------------------------------
Eval num_timesteps=3326000, episode_reward=1332.78 +/- 30.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.33e+03 |
| time/              |          |
|    total_timesteps | 3326000  |
| train/             |          |
|    actor_loss      | -0.673   |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.000106 |
|    ent_coef_loss   | 42.4     |
|    learning_rate   | 0.00167  |
|    n_updates       | 16240    |
---------------------------------
Eval num_timesteps=3327000, episode_reward=1323.42 +/- 51.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3327000  |
---------------------------------
Eval num_timesteps=3328000, episode_reward=1353.85 +/- 38.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3328000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 49.4     |
| time/              |          |
|    episodes        | 3328     |
|    fps             | 642      |
|    time_elapsed    | 5175     |
|    total_timesteps | 3328000  |
---------------------------------
Eval num_timesteps=3329000, episode_reward=1356.13 +/- 14.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3329000  |
| train/             |          |
|    actor_loss      | -0.679   |
|    critic_loss     | 0.00139  |
|    ent_coef        | 0.000109 |
|    ent_coef_loss   | 58.8     |
|    learning_rate   | 0.00167  |
|    n_updates       | 16250    |
---------------------------------
Eval num_timesteps=3330000, episode_reward=1354.76 +/- 22.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3330000  |
---------------------------------
Eval num_timesteps=3331000, episode_reward=-97.43 +/- 2.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -97.4    |
| time/              |          |
|    total_timesteps | 3331000  |
| train/             |          |
|    actor_loss      | -0.668   |
|    critic_loss     | 0.00114  |
|    ent_coef        | 0.000113 |
|    ent_coef_loss   | 52.1     |
|    learning_rate   | 0.00167  |
|    n_updates       | 16260    |
---------------------------------
Eval num_timesteps=3332000, episode_reward=-96.47 +/- 1.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -96.5    |
| time/              |          |
|    total_timesteps | 3332000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 77.6     |
| time/              |          |
|    episodes        | 3332     |
|    fps             | 642      |
|    time_elapsed    | 5182     |
|    total_timesteps | 3332000  |
---------------------------------
Eval num_timesteps=3333000, episode_reward=-152.89 +/- 1.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 3333000  |
| train/             |          |
|    actor_loss      | -0.373   |
|    critic_loss     | 6.32e-05 |
|    ent_coef        | 0.000117 |
|    ent_coef_loss   | -5.15    |
|    learning_rate   | 0.00167  |
|    n_updates       | 16270    |
---------------------------------
Eval num_timesteps=3334000, episode_reward=-152.04 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -152     |
| time/              |          |
|    total_timesteps | 3334000  |
---------------------------------
Eval num_timesteps=3335000, episode_reward=-244.26 +/- 0.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 3335000  |
| train/             |          |
|    actor_loss      | -0.389   |
|    critic_loss     | 6.87e-05 |
|    ent_coef        | 0.000119 |
|    ent_coef_loss   | 1.21     |
|    learning_rate   | 0.00167  |
|    n_updates       | 16280    |
---------------------------------
Eval num_timesteps=3336000, episode_reward=-243.54 +/- 0.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -244     |
| time/              |          |
|    total_timesteps | 3336000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 72.6     |
| time/              |          |
|    episodes        | 3336     |
|    fps             | 642      |
|    time_elapsed    | 5188     |
|    total_timesteps | 3336000  |
---------------------------------
Eval num_timesteps=3337000, episode_reward=-232.80 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 3337000  |
| train/             |          |
|    actor_loss      | -0.392   |
|    critic_loss     | 1.98e-05 |
|    ent_coef        | 0.000119 |
|    ent_coef_loss   | 10.3     |
|    learning_rate   | 0.00166  |
|    n_updates       | 16290    |
---------------------------------
Eval num_timesteps=3338000, episode_reward=-232.98 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 3338000  |
---------------------------------
Eval num_timesteps=3339000, episode_reward=-284.59 +/- 0.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 3339000  |
| train/             |          |
|    actor_loss      | -0.394   |
|    critic_loss     | 9.1e-06  |
|    ent_coef        | 0.00012  |
|    ent_coef_loss   | 0.717    |
|    learning_rate   | 0.00166  |
|    n_updates       | 16300    |
---------------------------------
Eval num_timesteps=3340000, episode_reward=-284.35 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 3340000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 64.8     |
| time/              |          |
|    episodes        | 3340     |
|    fps             | 642      |
|    time_elapsed    | 5194     |
|    total_timesteps | 3340000  |
---------------------------------
Eval num_timesteps=3341000, episode_reward=-266.61 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 3341000  |
| train/             |          |
|    actor_loss      | -0.391   |
|    critic_loss     | 6.25e-06 |
|    ent_coef        | 0.00012  |
|    ent_coef_loss   | -0.254   |
|    learning_rate   | 0.00166  |
|    n_updates       | 16310    |
---------------------------------
Eval num_timesteps=3342000, episode_reward=-266.81 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -267     |
| time/              |          |
|    total_timesteps | 3342000  |
---------------------------------
Eval num_timesteps=3343000, episode_reward=-147.01 +/- 0.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 3343000  |
| train/             |          |
|    actor_loss      | -0.388   |
|    critic_loss     | 8.14e-06 |
|    ent_coef        | 0.00012  |
|    ent_coef_loss   | -19.9    |
|    learning_rate   | 0.00166  |
|    n_updates       | 16320    |
---------------------------------
Eval num_timesteps=3344000, episode_reward=-147.23 +/- 0.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 3344000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 58.7     |
| time/              |          |
|    episodes        | 3344     |
|    fps             | 642      |
|    time_elapsed    | 5201     |
|    total_timesteps | 3344000  |
---------------------------------
Eval num_timesteps=3345000, episode_reward=-140.24 +/- 0.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 3345000  |
| train/             |          |
|    actor_loss      | -0.38    |
|    critic_loss     | 3.97e-05 |
|    ent_coef        | 0.000119 |
|    ent_coef_loss   | -49.7    |
|    learning_rate   | 0.00166  |
|    n_updates       | 16330    |
---------------------------------
Eval num_timesteps=3346000, episode_reward=-141.09 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 3346000  |
---------------------------------
Eval num_timesteps=3347000, episode_reward=-259.92 +/- 0.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -260     |
| time/              |          |
|    total_timesteps | 3347000  |
| train/             |          |
|    actor_loss      | -0.461   |
|    critic_loss     | 0.000787 |
|    ent_coef        | 0.000116 |
|    ent_coef_loss   | -17.4    |
|    learning_rate   | 0.00165  |
|    n_updates       | 16340    |
---------------------------------
Eval num_timesteps=3348000, episode_reward=-258.86 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -259     |
| time/              |          |
|    total_timesteps | 3348000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 65.2     |
| time/              |          |
|    episodes        | 3348     |
|    fps             | 642      |
|    time_elapsed    | 5207     |
|    total_timesteps | 3348000  |
---------------------------------
Eval num_timesteps=3349000, episode_reward=-501.18 +/- 1.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -501     |
| time/              |          |
|    total_timesteps | 3349000  |
| train/             |          |
|    actor_loss      | -0.387   |
|    critic_loss     | 8.87e-05 |
|    ent_coef        | 0.000114 |
|    ent_coef_loss   | 29.6     |
|    learning_rate   | 0.00165  |
|    n_updates       | 16350    |
---------------------------------
Eval num_timesteps=3350000, episode_reward=-503.10 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -503     |
| time/              |          |
|    total_timesteps | 3350000  |
---------------------------------
Eval num_timesteps=3351000, episode_reward=-569.65 +/- 0.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 3351000  |
| train/             |          |
|    actor_loss      | -0.378   |
|    critic_loss     | 9.31e-05 |
|    ent_coef        | 0.000115 |
|    ent_coef_loss   | 121      |
|    learning_rate   | 0.00165  |
|    n_updates       | 16360    |
---------------------------------
Eval num_timesteps=3352000, episode_reward=-569.61 +/- 0.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -570     |
| time/              |          |
|    total_timesteps | 3352000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 48.4     |
| time/              |          |
|    episodes        | 3352     |
|    fps             | 642      |
|    time_elapsed    | 5213     |
|    total_timesteps | 3352000  |
---------------------------------
Eval num_timesteps=3353000, episode_reward=-532.64 +/- 1.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 3353000  |
| train/             |          |
|    actor_loss      | -0.38    |
|    critic_loss     | 1.5e-05  |
|    ent_coef        | 0.000122 |
|    ent_coef_loss   | 80.6     |
|    learning_rate   | 0.00165  |
|    n_updates       | 16370    |
---------------------------------
Eval num_timesteps=3354000, episode_reward=-532.52 +/- 0.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 3354000  |
---------------------------------
Eval num_timesteps=3355000, episode_reward=-515.50 +/- 0.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 3355000  |
| train/             |          |
|    actor_loss      | -0.408   |
|    critic_loss     | 0.000307 |
|    ent_coef        | 0.000128 |
|    ent_coef_loss   | 48.3     |
|    learning_rate   | 0.00165  |
|    n_updates       | 16380    |
---------------------------------
Eval num_timesteps=3356000, episode_reward=-516.01 +/- 1.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -516     |
| time/              |          |
|    total_timesteps | 3356000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 27.2     |
| time/              |          |
|    episodes        | 3356     |
|    fps             | 642      |
|    time_elapsed    | 5219     |
|    total_timesteps | 3356000  |
---------------------------------
Eval num_timesteps=3357000, episode_reward=-539.94 +/- 0.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -540     |
| time/              |          |
|    total_timesteps | 3357000  |
| train/             |          |
|    actor_loss      | -0.48    |
|    critic_loss     | 0.000659 |
|    ent_coef        | 0.000133 |
|    ent_coef_loss   | 115      |
|    learning_rate   | 0.00164  |
|    n_updates       | 16390    |
---------------------------------
Eval num_timesteps=3358000, episode_reward=-540.53 +/- 0.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -541     |
| time/              |          |
|    total_timesteps | 3358000  |
---------------------------------
Eval num_timesteps=3359000, episode_reward=-583.58 +/- 0.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -584     |
| time/              |          |
|    total_timesteps | 3359000  |
| train/             |          |
|    actor_loss      | -0.562   |
|    critic_loss     | 0.000128 |
|    ent_coef        | 0.000141 |
|    ent_coef_loss   | 188      |
|    learning_rate   | 0.00164  |
|    n_updates       | 16400    |
---------------------------------
Eval num_timesteps=3360000, episode_reward=-582.93 +/- 1.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -583     |
| time/              |          |
|    total_timesteps | 3360000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 6.7      |
| time/              |          |
|    episodes        | 3360     |
|    fps             | 642      |
|    time_elapsed    | 5226     |
|    total_timesteps | 3360000  |
---------------------------------
Eval num_timesteps=3361000, episode_reward=-576.41 +/- 1.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -576     |
| time/              |          |
|    total_timesteps | 3361000  |
| train/             |          |
|    actor_loss      | -0.552   |
|    critic_loss     | 6.83e-05 |
|    ent_coef        | 0.000151 |
|    ent_coef_loss   | 126      |
|    learning_rate   | 0.00164  |
|    n_updates       | 16410    |
---------------------------------
Eval num_timesteps=3362000, episode_reward=-576.88 +/- 0.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -577     |
| time/              |          |
|    total_timesteps | 3362000  |
---------------------------------
Eval num_timesteps=3363000, episode_reward=-563.32 +/- 0.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -563     |
| time/              |          |
|    total_timesteps | 3363000  |
| train/             |          |
|    actor_loss      | -0.555   |
|    critic_loss     | 2.92e-05 |
|    ent_coef        | 0.000161 |
|    ent_coef_loss   | 103      |
|    learning_rate   | 0.00164  |
|    n_updates       | 16420    |
---------------------------------
Eval num_timesteps=3364000, episode_reward=-564.41 +/- 0.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -564     |
| time/              |          |
|    total_timesteps | 3364000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -14.8    |
| time/              |          |
|    episodes        | 3364     |
|    fps             | 642      |
|    time_elapsed    | 5232     |
|    total_timesteps | 3364000  |
---------------------------------
Eval num_timesteps=3365000, episode_reward=-567.69 +/- 0.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 3365000  |
| train/             |          |
|    actor_loss      | -0.558   |
|    critic_loss     | 3.04e-05 |
|    ent_coef        | 0.000169 |
|    ent_coef_loss   | 79.7     |
|    learning_rate   | 0.00164  |
|    n_updates       | 16430    |
---------------------------------
Eval num_timesteps=3366000, episode_reward=-568.36 +/- 1.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -568     |
| time/              |          |
|    total_timesteps | 3366000  |
---------------------------------
Eval num_timesteps=3367000, episode_reward=-533.11 +/- 1.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 3367000  |
| train/             |          |
|    actor_loss      | -0.556   |
|    critic_loss     | 5.87e-05 |
|    ent_coef        | 0.000176 |
|    ent_coef_loss   | 74.9     |
|    learning_rate   | 0.00163  |
|    n_updates       | 16440    |
---------------------------------
Eval num_timesteps=3368000, episode_reward=-532.51 +/- 1.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 3368000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -35.5    |
| time/              |          |
|    episodes        | 3368     |
|    fps             | 642      |
|    time_elapsed    | 5238     |
|    total_timesteps | 3368000  |
---------------------------------
Eval num_timesteps=3369000, episode_reward=-480.92 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -481     |
| time/              |          |
|    total_timesteps | 3369000  |
| train/             |          |
|    actor_loss      | -0.556   |
|    critic_loss     | 7.36e-05 |
|    ent_coef        | 0.000182 |
|    ent_coef_loss   | 55.5     |
|    learning_rate   | 0.00163  |
|    n_updates       | 16450    |
---------------------------------
Eval num_timesteps=3370000, episode_reward=-480.81 +/- 0.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -481     |
| time/              |          |
|    total_timesteps | 3370000  |
---------------------------------
Eval num_timesteps=3371000, episode_reward=-480.30 +/- 1.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -480     |
| time/              |          |
|    total_timesteps | 3371000  |
---------------------------------
Eval num_timesteps=3372000, episode_reward=-522.95 +/- 0.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -523     |
| time/              |          |
|    total_timesteps | 3372000  |
| train/             |          |
|    actor_loss      | -0.542   |
|    critic_loss     | 8.65e-05 |
|    ent_coef        | 0.000187 |
|    ent_coef_loss   | 60       |
|    learning_rate   | 0.00163  |
|    n_updates       | 16460    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -53.7    |
| time/              |          |
|    episodes        | 3372     |
|    fps             | 642      |
|    time_elapsed    | 5245     |
|    total_timesteps | 3372000  |
---------------------------------
Eval num_timesteps=3373000, episode_reward=-521.84 +/- 0.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -522     |
| time/              |          |
|    total_timesteps | 3373000  |
---------------------------------
Eval num_timesteps=3374000, episode_reward=-557.74 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -558     |
| time/              |          |
|    total_timesteps | 3374000  |
| train/             |          |
|    actor_loss      | -0.545   |
|    critic_loss     | 2.62e-05 |
|    ent_coef        | 0.000192 |
|    ent_coef_loss   | 104      |
|    learning_rate   | 0.00163  |
|    n_updates       | 16470    |
---------------------------------
Eval num_timesteps=3375000, episode_reward=-557.45 +/- 0.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -557     |
| time/              |          |
|    total_timesteps | 3375000  |
---------------------------------
Eval num_timesteps=3376000, episode_reward=-533.60 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -534     |
| time/              |          |
|    total_timesteps | 3376000  |
| train/             |          |
|    actor_loss      | -0.556   |
|    critic_loss     | 3.23e-05 |
|    ent_coef        | 0.000199 |
|    ent_coef_loss   | 65.9     |
|    learning_rate   | 0.00162  |
|    n_updates       | 16480    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -74.2    |
| time/              |          |
|    episodes        | 3376     |
|    fps             | 642      |
|    time_elapsed    | 5251     |
|    total_timesteps | 3376000  |
---------------------------------
Eval num_timesteps=3377000, episode_reward=-533.31 +/- 0.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -533     |
| time/              |          |
|    total_timesteps | 3377000  |
---------------------------------
Eval num_timesteps=3378000, episode_reward=-513.07 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 3378000  |
| train/             |          |
|    actor_loss      | -0.558   |
|    critic_loss     | 3.43e-05 |
|    ent_coef        | 0.000205 |
|    ent_coef_loss   | 55       |
|    learning_rate   | 0.00162  |
|    n_updates       | 16490    |
---------------------------------
Eval num_timesteps=3379000, episode_reward=-513.40 +/- 1.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 3379000  |
---------------------------------
Eval num_timesteps=3380000, episode_reward=-220.04 +/- 1.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 3380000  |
| train/             |          |
|    actor_loss      | -0.558   |
|    critic_loss     | 6e-05    |
|    ent_coef        | 0.00021  |
|    ent_coef_loss   | 37.9     |
|    learning_rate   | 0.00162  |
|    n_updates       | 16500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -89.2    |
| time/              |          |
|    episodes        | 3380     |
|    fps             | 642      |
|    time_elapsed    | 5257     |
|    total_timesteps | 3380000  |
---------------------------------
Eval num_timesteps=3381000, episode_reward=-219.85 +/- 1.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 3381000  |
---------------------------------
Eval num_timesteps=3382000, episode_reward=-422.60 +/- 4.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -423     |
| time/              |          |
|    total_timesteps | 3382000  |
| train/             |          |
|    actor_loss      | -0.565   |
|    critic_loss     | 0.000386 |
|    ent_coef        | 0.000214 |
|    ent_coef_loss   | 48       |
|    learning_rate   | 0.00162  |
|    n_updates       | 16510    |
---------------------------------
Eval num_timesteps=3383000, episode_reward=-423.73 +/- 4.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 3383000  |
---------------------------------
Eval num_timesteps=3384000, episode_reward=40.65 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 40.6     |
| time/              |          |
|    total_timesteps | 3384000  |
| train/             |          |
|    actor_loss      | -0.568   |
|    critic_loss     | 0.000751 |
|    ent_coef        | 0.000218 |
|    ent_coef_loss   | 71.3     |
|    learning_rate   | 0.00162  |
|    n_updates       | 16520    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -94.2    |
| time/              |          |
|    episodes        | 3384     |
|    fps             | 642      |
|    time_elapsed    | 5264     |
|    total_timesteps | 3384000  |
---------------------------------
Eval num_timesteps=3385000, episode_reward=42.01 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 42       |
| time/              |          |
|    total_timesteps | 3385000  |
---------------------------------
Eval num_timesteps=3386000, episode_reward=-71.16 +/- 3.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -71.2    |
| time/              |          |
|    total_timesteps | 3386000  |
| train/             |          |
|    actor_loss      | -0.59    |
|    critic_loss     | 0.000979 |
|    ent_coef        | 0.000224 |
|    ent_coef_loss   | 60.8     |
|    learning_rate   | 0.00161  |
|    n_updates       | 16530    |
---------------------------------
Eval num_timesteps=3387000, episode_reward=-68.06 +/- 5.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -68.1    |
| time/              |          |
|    total_timesteps | 3387000  |
---------------------------------
Eval num_timesteps=3388000, episode_reward=-76.28 +/- 12.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -76.3    |
| time/              |          |
|    total_timesteps | 3388000  |
| train/             |          |
|    actor_loss      | -0.573   |
|    critic_loss     | 0.0013   |
|    ent_coef        | 0.00023  |
|    ent_coef_loss   | 62.7     |
|    learning_rate   | 0.00161  |
|    n_updates       | 16540    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -87.4    |
| time/              |          |
|    episodes        | 3388     |
|    fps             | 642      |
|    time_elapsed    | 5270     |
|    total_timesteps | 3388000  |
---------------------------------
Eval num_timesteps=3389000, episode_reward=-76.24 +/- 3.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -76.2    |
| time/              |          |
|    total_timesteps | 3389000  |
---------------------------------
Eval num_timesteps=3390000, episode_reward=591.27 +/- 10.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 591      |
| time/              |          |
|    total_timesteps | 3390000  |
| train/             |          |
|    actor_loss      | -0.575   |
|    critic_loss     | 0.00129  |
|    ent_coef        | 0.000235 |
|    ent_coef_loss   | 54.5     |
|    learning_rate   | 0.00161  |
|    n_updates       | 16550    |
---------------------------------
Eval num_timesteps=3391000, episode_reward=589.35 +/- 2.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 589      |
| time/              |          |
|    total_timesteps | 3391000  |
---------------------------------
Eval num_timesteps=3392000, episode_reward=584.89 +/- 9.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 585      |
| time/              |          |
|    total_timesteps | 3392000  |
| train/             |          |
|    actor_loss      | -0.562   |
|    critic_loss     | 0.00149  |
|    ent_coef        | 0.000241 |
|    ent_coef_loss   | 53.4     |
|    learning_rate   | 0.00161  |
|    n_updates       | 16560    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -73.2    |
| time/              |          |
|    episodes        | 3392     |
|    fps             | 642      |
|    time_elapsed    | 5276     |
|    total_timesteps | 3392000  |
---------------------------------
Eval num_timesteps=3393000, episode_reward=575.81 +/- 8.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 576      |
| time/              |          |
|    total_timesteps | 3393000  |
---------------------------------
Eval num_timesteps=3394000, episode_reward=147.61 +/- 39.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 148      |
| time/              |          |
|    total_timesteps | 3394000  |
| train/             |          |
|    actor_loss      | -0.582   |
|    critic_loss     | 0.000965 |
|    ent_coef        | 0.000246 |
|    ent_coef_loss   | 47.1     |
|    learning_rate   | 0.00161  |
|    n_updates       | 16570    |
---------------------------------
Eval num_timesteps=3395000, episode_reward=113.81 +/- 1.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 114      |
| time/              |          |
|    total_timesteps | 3395000  |
---------------------------------
Eval num_timesteps=3396000, episode_reward=-349.35 +/- 2.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -349     |
| time/              |          |
|    total_timesteps | 3396000  |
| train/             |          |
|    actor_loss      | -0.579   |
|    critic_loss     | 0.000652 |
|    ent_coef        | 0.000251 |
|    ent_coef_loss   | 60.3     |
|    learning_rate   | 0.0016   |
|    n_updates       | 16580    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -63.8    |
| time/              |          |
|    episodes        | 3396     |
|    fps             | 642      |
|    time_elapsed    | 5282     |
|    total_timesteps | 3396000  |
---------------------------------
Eval num_timesteps=3397000, episode_reward=-331.47 +/- 23.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -331     |
| time/              |          |
|    total_timesteps | 3397000  |
---------------------------------
Eval num_timesteps=3398000, episode_reward=-174.88 +/- 12.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -175     |
| time/              |          |
|    total_timesteps | 3398000  |
| train/             |          |
|    actor_loss      | -0.564   |
|    critic_loss     | 0.000703 |
|    ent_coef        | 0.000257 |
|    ent_coef_loss   | 50.8     |
|    learning_rate   | 0.0016   |
|    n_updates       | 16590    |
---------------------------------
Eval num_timesteps=3399000, episode_reward=-168.61 +/- 10.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -169     |
| time/              |          |
|    total_timesteps | 3399000  |
---------------------------------
Eval num_timesteps=3400000, episode_reward=16.99 +/- 3.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 17       |
| time/              |          |
|    total_timesteps | 3400000  |
| train/             |          |
|    actor_loss      | -0.567   |
|    critic_loss     | 0.000692 |
|    ent_coef        | 0.000262 |
|    ent_coef_loss   | 40.3     |
|    learning_rate   | 0.0016   |
|    n_updates       | 16600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -70.6    |
| time/              |          |
|    episodes        | 3400     |
|    fps             | 642      |
|    time_elapsed    | 5289     |
|    total_timesteps | 3400000  |
---------------------------------
Eval num_timesteps=3401000, episode_reward=17.08 +/- 4.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 17.1     |
| time/              |          |
|    total_timesteps | 3401000  |
---------------------------------
Eval num_timesteps=3402000, episode_reward=-71.92 +/- 3.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -71.9    |
| time/              |          |
|    total_timesteps | 3402000  |
| train/             |          |
|    actor_loss      | -0.575   |
|    critic_loss     | 0.00065  |
|    ent_coef        | 0.000267 |
|    ent_coef_loss   | 47       |
|    learning_rate   | 0.0016   |
|    n_updates       | 16610    |
---------------------------------
Eval num_timesteps=3403000, episode_reward=-68.28 +/- 5.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -68.3    |
| time/              |          |
|    total_timesteps | 3403000  |
---------------------------------
Eval num_timesteps=3404000, episode_reward=-37.97 +/- 2.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -38      |
| time/              |          |
|    total_timesteps | 3404000  |
| train/             |          |
|    actor_loss      | -0.572   |
|    critic_loss     | 0.000587 |
|    ent_coef        | 0.000271 |
|    ent_coef_loss   | 49.5     |
|    learning_rate   | 0.0016   |
|    n_updates       | 16620    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -67.4    |
| time/              |          |
|    episodes        | 3404     |
|    fps             | 642      |
|    time_elapsed    | 5295     |
|    total_timesteps | 3404000  |
---------------------------------
Eval num_timesteps=3405000, episode_reward=-40.75 +/- 0.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -40.8    |
| time/              |          |
|    total_timesteps | 3405000  |
---------------------------------
Eval num_timesteps=3406000, episode_reward=23.29 +/- 1.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 23.3     |
| time/              |          |
|    total_timesteps | 3406000  |
| train/             |          |
|    actor_loss      | -0.573   |
|    critic_loss     | 0.000494 |
|    ent_coef        | 0.000277 |
|    ent_coef_loss   | 42.3     |
|    learning_rate   | 0.00159  |
|    n_updates       | 16630    |
---------------------------------
Eval num_timesteps=3407000, episode_reward=24.67 +/- 1.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 24.7     |
| time/              |          |
|    total_timesteps | 3407000  |
---------------------------------
Eval num_timesteps=3408000, episode_reward=-129.20 +/- 3.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 3408000  |
| train/             |          |
|    actor_loss      | -0.573   |
|    critic_loss     | 0.000398 |
|    ent_coef        | 0.000281 |
|    ent_coef_loss   | 39.5     |
|    learning_rate   | 0.00159  |
|    n_updates       | 16640    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -64.3    |
| time/              |          |
|    episodes        | 3408     |
|    fps             | 642      |
|    time_elapsed    | 5302     |
|    total_timesteps | 3408000  |
---------------------------------
Eval num_timesteps=3409000, episode_reward=-126.87 +/- 1.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 3409000  |
---------------------------------
Eval num_timesteps=3410000, episode_reward=-57.67 +/- 1.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -57.7    |
| time/              |          |
|    total_timesteps | 3410000  |
| train/             |          |
|    actor_loss      | -0.571   |
|    critic_loss     | 0.000382 |
|    ent_coef        | 0.000286 |
|    ent_coef_loss   | 37.6     |
|    learning_rate   | 0.00159  |
|    n_updates       | 16650    |
---------------------------------
Eval num_timesteps=3411000, episode_reward=-57.40 +/- 2.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -57.4    |
| time/              |          |
|    total_timesteps | 3411000  |
---------------------------------
Eval num_timesteps=3412000, episode_reward=-40.91 +/- 2.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -40.9    |
| time/              |          |
|    total_timesteps | 3412000  |
| train/             |          |
|    actor_loss      | -0.569   |
|    critic_loss     | 0.000356 |
|    ent_coef        | 0.000291 |
|    ent_coef_loss   | 36.6     |
|    learning_rate   | 0.00159  |
|    n_updates       | 16660    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -66.2    |
| time/              |          |
|    episodes        | 3412     |
|    fps             | 642      |
|    time_elapsed    | 5308     |
|    total_timesteps | 3412000  |
---------------------------------
Eval num_timesteps=3413000, episode_reward=-39.59 +/- 2.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -39.6    |
| time/              |          |
|    total_timesteps | 3413000  |
---------------------------------
Eval num_timesteps=3414000, episode_reward=-40.57 +/- 2.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -40.6    |
| time/              |          |
|    total_timesteps | 3414000  |
---------------------------------
Eval num_timesteps=3415000, episode_reward=-52.45 +/- 4.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -52.5    |
| time/              |          |
|    total_timesteps | 3415000  |
| train/             |          |
|    actor_loss      | -0.571   |
|    critic_loss     | 0.000327 |
|    ent_coef        | 0.000295 |
|    ent_coef_loss   | 33.3     |
|    learning_rate   | 0.00159  |
|    n_updates       | 16670    |
---------------------------------
Eval num_timesteps=3416000, episode_reward=-48.76 +/- 4.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -48.8    |
| time/              |          |
|    total_timesteps | 3416000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -65.7    |
| time/              |          |
|    episodes        | 3416     |
|    fps             | 642      |
|    time_elapsed    | 5314     |
|    total_timesteps | 3416000  |
---------------------------------
Eval num_timesteps=3417000, episode_reward=2.51 +/- 1.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.51     |
| time/              |          |
|    total_timesteps | 3417000  |
| train/             |          |
|    actor_loss      | -0.57    |
|    critic_loss     | 0.000319 |
|    ent_coef        | 0.000299 |
|    ent_coef_loss   | 33.7     |
|    learning_rate   | 0.00158  |
|    n_updates       | 16680    |
---------------------------------
Eval num_timesteps=3418000, episode_reward=5.55 +/- 5.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 5.55     |
| time/              |          |
|    total_timesteps | 3418000  |
---------------------------------
Eval num_timesteps=3419000, episode_reward=-31.42 +/- 4.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -31.4    |
| time/              |          |
|    total_timesteps | 3419000  |
| train/             |          |
|    actor_loss      | -0.568   |
|    critic_loss     | 0.000263 |
|    ent_coef        | 0.000303 |
|    ent_coef_loss   | 30       |
|    learning_rate   | 0.00158  |
|    n_updates       | 16690    |
---------------------------------
Eval num_timesteps=3420000, episode_reward=-37.26 +/- 2.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -37.3    |
| time/              |          |
|    total_timesteps | 3420000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -67.7    |
| time/              |          |
|    episodes        | 3420     |
|    fps             | 642      |
|    time_elapsed    | 5320     |
|    total_timesteps | 3420000  |
---------------------------------
Eval num_timesteps=3421000, episode_reward=-255.19 +/- 4.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 3421000  |
| train/             |          |
|    actor_loss      | -0.564   |
|    critic_loss     | 0.000194 |
|    ent_coef        | 0.000307 |
|    ent_coef_loss   | 27.1     |
|    learning_rate   | 0.00158  |
|    n_updates       | 16700    |
---------------------------------
Eval num_timesteps=3422000, episode_reward=-255.23 +/- 6.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 3422000  |
---------------------------------
Eval num_timesteps=3423000, episode_reward=-426.70 +/- 1.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 3423000  |
| train/             |          |
|    actor_loss      | -0.559   |
|    critic_loss     | 0.00017  |
|    ent_coef        | 0.00031  |
|    ent_coef_loss   | 25.3     |
|    learning_rate   | 0.00158  |
|    n_updates       | 16710    |
---------------------------------
Eval num_timesteps=3424000, episode_reward=-424.44 +/- 0.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -424     |
| time/              |          |
|    total_timesteps | 3424000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -105     |
| time/              |          |
|    episodes        | 3424     |
|    fps             | 642      |
|    time_elapsed    | 5327     |
|    total_timesteps | 3424000  |
---------------------------------
Eval num_timesteps=3425000, episode_reward=-400.19 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 3425000  |
| train/             |          |
|    actor_loss      | -0.552   |
|    critic_loss     | 0.000113 |
|    ent_coef        | 0.000314 |
|    ent_coef_loss   | 26.9     |
|    learning_rate   | 0.00158  |
|    n_updates       | 16720    |
---------------------------------
Eval num_timesteps=3426000, episode_reward=-401.09 +/- 0.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -401     |
| time/              |          |
|    total_timesteps | 3426000  |
---------------------------------
Eval num_timesteps=3427000, episode_reward=-400.11 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 3427000  |
| train/             |          |
|    actor_loss      | -0.552   |
|    critic_loss     | 8.31e-05 |
|    ent_coef        | 0.000317 |
|    ent_coef_loss   | 20       |
|    learning_rate   | 0.00157  |
|    n_updates       | 16730    |
---------------------------------
Eval num_timesteps=3428000, episode_reward=-399.63 +/- 0.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -400     |
| time/              |          |
|    total_timesteps | 3428000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -170     |
| time/              |          |
|    episodes        | 3428     |
|    fps             | 642      |
|    time_elapsed    | 5333     |
|    total_timesteps | 3428000  |
---------------------------------
Eval num_timesteps=3429000, episode_reward=-228.85 +/- 13.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -229     |
| time/              |          |
|    total_timesteps | 3429000  |
| train/             |          |
|    actor_loss      | -0.553   |
|    critic_loss     | 0.000107 |
|    ent_coef        | 0.00032  |
|    ent_coef_loss   | 16.4     |
|    learning_rate   | 0.00157  |
|    n_updates       | 16740    |
---------------------------------
Eval num_timesteps=3430000, episode_reward=-234.08 +/- 7.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 3430000  |
---------------------------------
Eval num_timesteps=3431000, episode_reward=-299.76 +/- 2.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -300     |
| time/              |          |
|    total_timesteps | 3431000  |
| train/             |          |
|    actor_loss      | -0.553   |
|    critic_loss     | 0.000122 |
|    ent_coef        | 0.000322 |
|    ent_coef_loss   | 13.6     |
|    learning_rate   | 0.00157  |
|    n_updates       | 16750    |
---------------------------------
Eval num_timesteps=3432000, episode_reward=-296.48 +/- 7.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 3432000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -203     |
| time/              |          |
|    episodes        | 3432     |
|    fps             | 642      |
|    time_elapsed    | 5340     |
|    total_timesteps | 3432000  |
---------------------------------
Eval num_timesteps=3433000, episode_reward=-123.32 +/- 5.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 3433000  |
| train/             |          |
|    actor_loss      | -0.556   |
|    critic_loss     | 0.000141 |
|    ent_coef        | 0.000324 |
|    ent_coef_loss   | 13.2     |
|    learning_rate   | 0.00157  |
|    n_updates       | 16760    |
---------------------------------
Eval num_timesteps=3434000, episode_reward=-118.15 +/- 4.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 3434000  |
---------------------------------
Eval num_timesteps=3435000, episode_reward=-309.77 +/- 7.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -310     |
| time/              |          |
|    total_timesteps | 3435000  |
| train/             |          |
|    actor_loss      | -0.554   |
|    critic_loss     | 9.09e-05 |
|    ent_coef        | 0.000326 |
|    ent_coef_loss   | 19.3     |
|    learning_rate   | 0.00157  |
|    n_updates       | 16770    |
---------------------------------
Eval num_timesteps=3436000, episode_reward=-301.84 +/- 4.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 3436000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -204     |
| time/              |          |
|    episodes        | 3436     |
|    fps             | 642      |
|    time_elapsed    | 5346     |
|    total_timesteps | 3436000  |
---------------------------------
Eval num_timesteps=3437000, episode_reward=-372.97 +/- 0.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 3437000  |
| train/             |          |
|    actor_loss      | -0.551   |
|    critic_loss     | 5.56e-05 |
|    ent_coef        | 0.000328 |
|    ent_coef_loss   | 12.8     |
|    learning_rate   | 0.00156  |
|    n_updates       | 16780    |
---------------------------------
Eval num_timesteps=3438000, episode_reward=-373.37 +/- 1.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -373     |
| time/              |          |
|    total_timesteps | 3438000  |
---------------------------------
Eval num_timesteps=3439000, episode_reward=-196.83 +/- 3.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 3439000  |
| train/             |          |
|    actor_loss      | -0.551   |
|    critic_loss     | 5.62e-05 |
|    ent_coef        | 0.00033  |
|    ent_coef_loss   | 12.2     |
|    learning_rate   | 0.00156  |
|    n_updates       | 16790    |
---------------------------------
Eval num_timesteps=3440000, episode_reward=-196.53 +/- 3.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -197     |
| time/              |          |
|    total_timesteps | 3440000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -202     |
| time/              |          |
|    episodes        | 3440     |
|    fps             | 642      |
|    time_elapsed    | 5352     |
|    total_timesteps | 3440000  |
---------------------------------
Eval num_timesteps=3441000, episode_reward=-116.67 +/- 4.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 3441000  |
| train/             |          |
|    actor_loss      | -0.551   |
|    critic_loss     | 6.67e-05 |
|    ent_coef        | 0.000332 |
|    ent_coef_loss   | 16       |
|    learning_rate   | 0.00156  |
|    n_updates       | 16800    |
---------------------------------
Eval num_timesteps=3442000, episode_reward=-115.90 +/- 2.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 3442000  |
---------------------------------
Eval num_timesteps=3443000, episode_reward=-126.16 +/- 2.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 3443000  |
| train/             |          |
|    actor_loss      | -0.552   |
|    critic_loss     | 6.68e-05 |
|    ent_coef        | 0.000334 |
|    ent_coef_loss   | 8.24     |
|    learning_rate   | 0.00156  |
|    n_updates       | 16810    |
---------------------------------
Eval num_timesteps=3444000, episode_reward=-126.81 +/- 2.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 3444000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -202     |
| time/              |          |
|    episodes        | 3444     |
|    fps             | 642      |
|    time_elapsed    | 5359     |
|    total_timesteps | 3444000  |
---------------------------------
Eval num_timesteps=3445000, episode_reward=-294.00 +/- 3.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -294     |
| time/              |          |
|    total_timesteps | 3445000  |
| train/             |          |
|    actor_loss      | -0.549   |
|    critic_loss     | 4.62e-05 |
|    ent_coef        | 0.000335 |
|    ent_coef_loss   | 13.8     |
|    learning_rate   | 0.00156  |
|    n_updates       | 16820    |
---------------------------------
Eval num_timesteps=3446000, episode_reward=-315.41 +/- 44.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 3446000  |
---------------------------------
Eval num_timesteps=3447000, episode_reward=-237.63 +/- 1.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 3447000  |
| train/             |          |
|    actor_loss      | -0.547   |
|    critic_loss     | 3.86e-05 |
|    ent_coef        | 0.000337 |
|    ent_coef_loss   | 7.39     |
|    learning_rate   | 0.00155  |
|    n_updates       | 16830    |
---------------------------------
Eval num_timesteps=3448000, episode_reward=-237.94 +/- 0.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -238     |
| time/              |          |
|    total_timesteps | 3448000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -216     |
| time/              |          |
|    episodes        | 3448     |
|    fps             | 642      |
|    time_elapsed    | 5365     |
|    total_timesteps | 3448000  |
---------------------------------
Eval num_timesteps=3449000, episode_reward=-370.00 +/- 2.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -370     |
| time/              |          |
|    total_timesteps | 3449000  |
| train/             |          |
|    actor_loss      | -0.548   |
|    critic_loss     | 3.78e-05 |
|    ent_coef        | 0.000338 |
|    ent_coef_loss   | 8.35     |
|    learning_rate   | 0.00155  |
|    n_updates       | 16840    |
---------------------------------
Eval num_timesteps=3450000, episode_reward=-370.69 +/- 2.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 3450000  |
---------------------------------
Eval num_timesteps=3451000, episode_reward=-381.67 +/- 14.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -382     |
| time/              |          |
|    total_timesteps | 3451000  |
| train/             |          |
|    actor_loss      | -0.542   |
|    critic_loss     | 3.62e-05 |
|    ent_coef        | 0.00034  |
|    ent_coef_loss   | 12       |
|    learning_rate   | 0.00155  |
|    n_updates       | 16850    |
---------------------------------
Eval num_timesteps=3452000, episode_reward=-404.87 +/- 4.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -405     |
| time/              |          |
|    total_timesteps | 3452000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -209     |
| time/              |          |
|    episodes        | 3452     |
|    fps             | 642      |
|    time_elapsed    | 5372     |
|    total_timesteps | 3452000  |
---------------------------------
Eval num_timesteps=3453000, episode_reward=-370.50 +/- 1.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -370     |
| time/              |          |
|    total_timesteps | 3453000  |
| train/             |          |
|    actor_loss      | -0.544   |
|    critic_loss     | 3.41e-05 |
|    ent_coef        | 0.000341 |
|    ent_coef_loss   | 0.403    |
|    learning_rate   | 0.00155  |
|    n_updates       | 16860    |
---------------------------------
Eval num_timesteps=3454000, episode_reward=-371.81 +/- 1.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -372     |
| time/              |          |
|    total_timesteps | 3454000  |
---------------------------------
Eval num_timesteps=3455000, episode_reward=-367.82 +/- 1.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 3455000  |
| train/             |          |
|    actor_loss      | -0.543   |
|    critic_loss     | 3.37e-05 |
|    ent_coef        | 0.000342 |
|    ent_coef_loss   | 2.56     |
|    learning_rate   | 0.00155  |
|    n_updates       | 16870    |
---------------------------------
Eval num_timesteps=3456000, episode_reward=-369.51 +/- 1.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -370     |
| time/              |          |
|    total_timesteps | 3456000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -198     |
| time/              |          |
|    episodes        | 3456     |
|    fps             | 642      |
|    time_elapsed    | 5378     |
|    total_timesteps | 3456000  |
---------------------------------
Eval num_timesteps=3457000, episode_reward=-367.82 +/- 1.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -368     |
| time/              |          |
|    total_timesteps | 3457000  |
---------------------------------
Eval num_timesteps=3458000, episode_reward=-370.65 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -371     |
| time/              |          |
|    total_timesteps | 3458000  |
| train/             |          |
|    actor_loss      | -0.542   |
|    critic_loss     | 2.88e-05 |
|    ent_coef        | 0.000342 |
|    ent_coef_loss   | 0.853    |
|    learning_rate   | 0.00154  |
|    n_updates       | 16880    |
---------------------------------
Eval num_timesteps=3459000, episode_reward=-370.29 +/- 0.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -370     |
| time/              |          |
|    total_timesteps | 3459000  |
---------------------------------
Eval num_timesteps=3460000, episode_reward=-372.41 +/- 1.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -372     |
| time/              |          |
|    total_timesteps | 3460000  |
| train/             |          |
|    actor_loss      | -0.542   |
|    critic_loss     | 2.71e-05 |
|    ent_coef        | 0.000342 |
|    ent_coef_loss   | -1.02    |
|    learning_rate   | 0.00154  |
|    n_updates       | 16890    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -188     |
| time/              |          |
|    episodes        | 3460     |
|    fps             | 642      |
|    time_elapsed    | 5385     |
|    total_timesteps | 3460000  |
---------------------------------
Eval num_timesteps=3461000, episode_reward=-372.02 +/- 1.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -372     |
| time/              |          |
|    total_timesteps | 3461000  |
---------------------------------
Eval num_timesteps=3462000, episode_reward=-374.60 +/- 1.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -375     |
| time/              |          |
|    total_timesteps | 3462000  |
| train/             |          |
|    actor_loss      | -0.54    |
|    critic_loss     | 2.73e-05 |
|    ent_coef        | 0.000342 |
|    ent_coef_loss   | -2.44    |
|    learning_rate   | 0.00154  |
|    n_updates       | 16900    |
---------------------------------
Eval num_timesteps=3463000, episode_reward=-374.67 +/- 1.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -375     |
| time/              |          |
|    total_timesteps | 3463000  |
---------------------------------
Eval num_timesteps=3464000, episode_reward=-370.23 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -370     |
| time/              |          |
|    total_timesteps | 3464000  |
| train/             |          |
|    actor_loss      | -0.541   |
|    critic_loss     | 2.39e-05 |
|    ent_coef        | 0.000342 |
|    ent_coef_loss   | -1.38    |
|    learning_rate   | 0.00154  |
|    n_updates       | 16910    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -178     |
| time/              |          |
|    episodes        | 3464     |
|    fps             | 642      |
|    time_elapsed    | 5391     |
|    total_timesteps | 3464000  |
---------------------------------
Eval num_timesteps=3465000, episode_reward=-369.11 +/- 0.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -369     |
| time/              |          |
|    total_timesteps | 3465000  |
---------------------------------
Eval num_timesteps=3466000, episode_reward=-353.74 +/- 3.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 3466000  |
| train/             |          |
|    actor_loss      | -0.54    |
|    critic_loss     | 2.23e-05 |
|    ent_coef        | 0.000342 |
|    ent_coef_loss   | -6.66    |
|    learning_rate   | 0.00153  |
|    n_updates       | 16920    |
---------------------------------
Eval num_timesteps=3467000, episode_reward=-352.66 +/- 3.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -353     |
| time/              |          |
|    total_timesteps | 3467000  |
---------------------------------
Eval num_timesteps=3468000, episode_reward=-344.18 +/- 0.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -344     |
| time/              |          |
|    total_timesteps | 3468000  |
| train/             |          |
|    actor_loss      | -0.54    |
|    critic_loss     | 2e-05    |
|    ent_coef        | 0.000341 |
|    ent_coef_loss   | -6.22    |
|    learning_rate   | 0.00153  |
|    n_updates       | 16930    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -168     |
| time/              |          |
|    episodes        | 3468     |
|    fps             | 642      |
|    time_elapsed    | 5398     |
|    total_timesteps | 3468000  |
---------------------------------
Eval num_timesteps=3469000, episode_reward=-344.64 +/- 0.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -345     |
| time/              |          |
|    total_timesteps | 3469000  |
---------------------------------
Eval num_timesteps=3470000, episode_reward=-347.03 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -347     |
| time/              |          |
|    total_timesteps | 3470000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 1.9e-05  |
|    ent_coef        | 0.00034  |
|    ent_coef_loss   | -7.09    |
|    learning_rate   | 0.00153  |
|    n_updates       | 16940    |
---------------------------------
Eval num_timesteps=3471000, episode_reward=-348.03 +/- 1.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -348     |
| time/              |          |
|    total_timesteps | 3471000  |
---------------------------------
Eval num_timesteps=3472000, episode_reward=-350.30 +/- 1.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 3472000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 1.71e-05 |
|    ent_coef        | 0.000339 |
|    ent_coef_loss   | -9.49    |
|    learning_rate   | 0.00153  |
|    n_updates       | 16950    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -162     |
| time/              |          |
|    episodes        | 3472     |
|    fps             | 642      |
|    time_elapsed    | 5404     |
|    total_timesteps | 3472000  |
---------------------------------
Eval num_timesteps=3473000, episode_reward=-350.13 +/- 0.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -350     |
| time/              |          |
|    total_timesteps | 3473000  |
---------------------------------
Eval num_timesteps=3474000, episode_reward=-354.68 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -355     |
| time/              |          |
|    total_timesteps | 3474000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 1.71e-05 |
|    ent_coef        | 0.000338 |
|    ent_coef_loss   | -9.56    |
|    learning_rate   | 0.00153  |
|    n_updates       | 16960    |
---------------------------------
Eval num_timesteps=3475000, episode_reward=-355.00 +/- 1.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -355     |
| time/              |          |
|    total_timesteps | 3475000  |
---------------------------------
Eval num_timesteps=3476000, episode_reward=-352.19 +/- 0.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -352     |
| time/              |          |
|    total_timesteps | 3476000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 1.77e-05 |
|    ent_coef        | 0.000337 |
|    ent_coef_loss   | -11      |
|    learning_rate   | 0.00152  |
|    n_updates       | 16970    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -154     |
| time/              |          |
|    episodes        | 3476     |
|    fps             | 642      |
|    time_elapsed    | 5410     |
|    total_timesteps | 3476000  |
---------------------------------
Eval num_timesteps=3477000, episode_reward=-354.26 +/- 1.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 3477000  |
---------------------------------
Eval num_timesteps=3478000, episode_reward=-335.63 +/- 1.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 3478000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 1.67e-05 |
|    ent_coef        | 0.000335 |
|    ent_coef_loss   | -12.1    |
|    learning_rate   | 0.00152  |
|    n_updates       | 16980    |
---------------------------------
Eval num_timesteps=3479000, episode_reward=-336.35 +/- 1.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -336     |
| time/              |          |
|    total_timesteps | 3479000  |
---------------------------------
Eval num_timesteps=3480000, episode_reward=-340.39 +/- 1.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -340     |
| time/              |          |
|    total_timesteps | 3480000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 1.95e-05 |
|    ent_coef        | 0.000333 |
|    ent_coef_loss   | -13.3    |
|    learning_rate   | 0.00152  |
|    n_updates       | 16990    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -150     |
| time/              |          |
|    episodes        | 3480     |
|    fps             | 642      |
|    time_elapsed    | 5417     |
|    total_timesteps | 3480000  |
---------------------------------
Eval num_timesteps=3481000, episode_reward=-341.86 +/- 1.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -342     |
| time/              |          |
|    total_timesteps | 3481000  |
---------------------------------
Eval num_timesteps=3482000, episode_reward=-343.41 +/- 0.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -343     |
| time/              |          |
|    total_timesteps | 3482000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 1.71e-05 |
|    ent_coef        | 0.000331 |
|    ent_coef_loss   | -15.1    |
|    learning_rate   | 0.00152  |
|    n_updates       | 17000    |
---------------------------------
Eval num_timesteps=3483000, episode_reward=-342.72 +/- 0.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -343     |
| time/              |          |
|    total_timesteps | 3483000  |
---------------------------------
Eval num_timesteps=3484000, episode_reward=-297.67 +/- 1.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -298     |
| time/              |          |
|    total_timesteps | 3484000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 2e-05    |
|    ent_coef        | 0.000329 |
|    ent_coef_loss   | -14.9    |
|    learning_rate   | 0.00152  |
|    n_updates       | 17010    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -156     |
| time/              |          |
|    episodes        | 3484     |
|    fps             | 642      |
|    time_elapsed    | 5423     |
|    total_timesteps | 3484000  |
---------------------------------
Eval num_timesteps=3485000, episode_reward=-296.95 +/- 1.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -297     |
| time/              |          |
|    total_timesteps | 3485000  |
---------------------------------
Eval num_timesteps=3486000, episode_reward=-315.13 +/- 8.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 3486000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 2.19e-05 |
|    ent_coef        | 0.000327 |
|    ent_coef_loss   | -17.7    |
|    learning_rate   | 0.00151  |
|    n_updates       | 17020    |
---------------------------------
Eval num_timesteps=3487000, episode_reward=-315.10 +/- 8.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 3487000  |
---------------------------------
Eval num_timesteps=3488000, episode_reward=-295.60 +/- 12.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 3488000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 2.28e-05 |
|    ent_coef        | 0.000325 |
|    ent_coef_loss   | -17.4    |
|    learning_rate   | 0.00151  |
|    n_updates       | 17030    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -173     |
| time/              |          |
|    episodes        | 3488     |
|    fps             | 642      |
|    time_elapsed    | 5430     |
|    total_timesteps | 3488000  |
---------------------------------
Eval num_timesteps=3489000, episode_reward=-296.15 +/- 12.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -296     |
| time/              |          |
|    total_timesteps | 3489000  |
---------------------------------
Eval num_timesteps=3490000, episode_reward=-307.01 +/- 3.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -307     |
| time/              |          |
|    total_timesteps | 3490000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 2.37e-05 |
|    ent_coef        | 0.000322 |
|    ent_coef_loss   | -15.9    |
|    learning_rate   | 0.00151  |
|    n_updates       | 17040    |
---------------------------------
Eval num_timesteps=3491000, episode_reward=-308.49 +/- 3.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -308     |
| time/              |          |
|    total_timesteps | 3491000  |
---------------------------------
Eval num_timesteps=3492000, episode_reward=-306.47 +/- 3.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 3492000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 2.34e-05 |
|    ent_coef        | 0.00032  |
|    ent_coef_loss   | -14.1    |
|    learning_rate   | 0.00151  |
|    n_updates       | 17050    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -198     |
| time/              |          |
|    episodes        | 3492     |
|    fps             | 642      |
|    time_elapsed    | 5436     |
|    total_timesteps | 3492000  |
---------------------------------
Eval num_timesteps=3493000, episode_reward=-305.82 +/- 3.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 3493000  |
---------------------------------
Eval num_timesteps=3494000, episode_reward=-333.70 +/- 6.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -334     |
| time/              |          |
|    total_timesteps | 3494000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 2.65e-05 |
|    ent_coef        | 0.000318 |
|    ent_coef_loss   | -15.5    |
|    learning_rate   | 0.00151  |
|    n_updates       | 17060    |
---------------------------------
Eval num_timesteps=3495000, episode_reward=-330.16 +/- 8.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 3495000  |
---------------------------------
Eval num_timesteps=3496000, episode_reward=-277.41 +/- 2.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 3496000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 2.44e-05 |
|    ent_coef        | 0.000316 |
|    ent_coef_loss   | -17      |
|    learning_rate   | 0.0015   |
|    n_updates       | 17070    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -217     |
| time/              |          |
|    episodes        | 3496     |
|    fps             | 642      |
|    time_elapsed    | 5443     |
|    total_timesteps | 3496000  |
---------------------------------
Eval num_timesteps=3497000, episode_reward=-277.81 +/- 1.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -278     |
| time/              |          |
|    total_timesteps | 3497000  |
---------------------------------
Eval num_timesteps=3498000, episode_reward=-317.11 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -317     |
| time/              |          |
|    total_timesteps | 3498000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 3.06e-05 |
|    ent_coef        | 0.000313 |
|    ent_coef_loss   | -15.8    |
|    learning_rate   | 0.0015   |
|    n_updates       | 17080    |
---------------------------------
Eval num_timesteps=3499000, episode_reward=-316.33 +/- 2.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -316     |
| time/              |          |
|    total_timesteps | 3499000  |
---------------------------------
Eval num_timesteps=3500000, episode_reward=-315.10 +/- 4.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -315     |
| time/              |          |
|    total_timesteps | 3500000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -218     |
| time/              |          |
|    episodes        | 3500     |
|    fps             | 642      |
|    time_elapsed    | 5449     |
|    total_timesteps | 3500000  |
---------------------------------
Eval num_timesteps=3501000, episode_reward=-283.96 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -284     |
| time/              |          |
|    total_timesteps | 3501000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 3e-05    |
|    ent_coef        | 0.000311 |
|    ent_coef_loss   | -13.4    |
|    learning_rate   | 0.0015   |
|    n_updates       | 17090    |
---------------------------------
Eval num_timesteps=3502000, episode_reward=-285.18 +/- 1.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -285     |
| time/              |          |
|    total_timesteps | 3502000  |
---------------------------------
Eval num_timesteps=3503000, episode_reward=-311.61 +/- 4.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -312     |
| time/              |          |
|    total_timesteps | 3503000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 3.09e-05 |
|    ent_coef        | 0.000309 |
|    ent_coef_loss   | -9.84    |
|    learning_rate   | 0.0015   |
|    n_updates       | 17100    |
---------------------------------
Eval num_timesteps=3504000, episode_reward=-317.50 +/- 2.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -318     |
| time/              |          |
|    total_timesteps | 3504000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -229     |
| time/              |          |
|    episodes        | 3504     |
|    fps             | 642      |
|    time_elapsed    | 5455     |
|    total_timesteps | 3504000  |
---------------------------------
Eval num_timesteps=3505000, episode_reward=-225.79 +/- 0.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -226     |
| time/              |          |
|    total_timesteps | 3505000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 2.99e-05 |
|    ent_coef        | 0.000308 |
|    ent_coef_loss   | -10.6    |
|    learning_rate   | 0.0015   |
|    n_updates       | 17110    |
---------------------------------
Eval num_timesteps=3506000, episode_reward=-225.43 +/- 0.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -225     |
| time/              |          |
|    total_timesteps | 3506000  |
---------------------------------
Eval num_timesteps=3507000, episode_reward=-235.47 +/- 0.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 3507000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 2.54e-05 |
|    ent_coef        | 0.000306 |
|    ent_coef_loss   | -12.4    |
|    learning_rate   | 0.00149  |
|    n_updates       | 17120    |
---------------------------------
Eval num_timesteps=3508000, episode_reward=-233.63 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -234     |
| time/              |          |
|    total_timesteps | 3508000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -239     |
| time/              |          |
|    episodes        | 3508     |
|    fps             | 642      |
|    time_elapsed    | 5462     |
|    total_timesteps | 3508000  |
---------------------------------
Eval num_timesteps=3509000, episode_reward=-349.24 +/- 1.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -349     |
| time/              |          |
|    total_timesteps | 3509000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 2.74e-05 |
|    ent_coef        | 0.000305 |
|    ent_coef_loss   | -11.6    |
|    learning_rate   | 0.00149  |
|    n_updates       | 17130    |
---------------------------------
Eval num_timesteps=3510000, episode_reward=-348.72 +/- 2.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -349     |
| time/              |          |
|    total_timesteps | 3510000  |
---------------------------------
Eval num_timesteps=3511000, episode_reward=-119.90 +/- 1.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 3511000  |
| train/             |          |
|    actor_loss      | -0.534   |
|    critic_loss     | 2.45e-05 |
|    ent_coef        | 0.000303 |
|    ent_coef_loss   | -12.8    |
|    learning_rate   | 0.00149  |
|    n_updates       | 17140    |
---------------------------------
Eval num_timesteps=3512000, episode_reward=-121.08 +/- 1.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 3512000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -244     |
| time/              |          |
|    episodes        | 3512     |
|    fps             | 642      |
|    time_elapsed    | 5468     |
|    total_timesteps | 3512000  |
---------------------------------
Eval num_timesteps=3513000, episode_reward=-233.30 +/- 17.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -233     |
| time/              |          |
|    total_timesteps | 3513000  |
| train/             |          |
|    actor_loss      | -0.535   |
|    critic_loss     | 2.7e-05  |
|    ent_coef        | 0.000301 |
|    ent_coef_loss   | -13.1    |
|    learning_rate   | 0.00149  |
|    n_updates       | 17150    |
---------------------------------
Eval num_timesteps=3514000, episode_reward=-255.02 +/- 24.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 3514000  |
---------------------------------
Eval num_timesteps=3515000, episode_reward=-152.69 +/- 2.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 3515000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 3.05e-05 |
|    ent_coef        | 0.0003   |
|    ent_coef_loss   | -11.2    |
|    learning_rate   | 0.00149  |
|    n_updates       | 17160    |
---------------------------------
Eval num_timesteps=3516000, episode_reward=-155.74 +/- 3.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -156     |
| time/              |          |
|    total_timesteps | 3516000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -250     |
| time/              |          |
|    episodes        | 3516     |
|    fps             | 642      |
|    time_elapsed    | 5475     |
|    total_timesteps | 3516000  |
---------------------------------
Eval num_timesteps=3517000, episode_reward=-335.38 +/- 6.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -335     |
| time/              |          |
|    total_timesteps | 3517000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 2.78e-05 |
|    ent_coef        | 0.000298 |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.00148  |
|    n_updates       | 17170    |
---------------------------------
Eval num_timesteps=3518000, episode_reward=-320.71 +/- 19.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -321     |
| time/              |          |
|    total_timesteps | 3518000  |
---------------------------------
Eval num_timesteps=3519000, episode_reward=-298.94 +/- 5.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -299     |
| time/              |          |
|    total_timesteps | 3519000  |
| train/             |          |
|    actor_loss      | -0.534   |
|    critic_loss     | 2.64e-05 |
|    ent_coef        | 0.000296 |
|    ent_coef_loss   | -10.4    |
|    learning_rate   | 0.00148  |
|    n_updates       | 17180    |
---------------------------------
Eval num_timesteps=3520000, episode_reward=-302.12 +/- 6.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -302     |
| time/              |          |
|    total_timesteps | 3520000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -255     |
| time/              |          |
|    episodes        | 3520     |
|    fps             | 642      |
|    time_elapsed    | 5481     |
|    total_timesteps | 3520000  |
---------------------------------
Eval num_timesteps=3521000, episode_reward=-273.19 +/- 13.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 3521000  |
| train/             |          |
|    actor_loss      | -0.534   |
|    critic_loss     | 2.73e-05 |
|    ent_coef        | 0.000295 |
|    ent_coef_loss   | -16.5    |
|    learning_rate   | 0.00148  |
|    n_updates       | 17190    |
---------------------------------
Eval num_timesteps=3522000, episode_reward=-264.37 +/- 16.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -264     |
| time/              |          |
|    total_timesteps | 3522000  |
---------------------------------
Eval num_timesteps=3523000, episode_reward=60.21 +/- 3.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 60.2     |
| time/              |          |
|    total_timesteps | 3523000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 3.75e-05 |
|    ent_coef        | 0.000293 |
|    ent_coef_loss   | -11      |
|    learning_rate   | 0.00148  |
|    n_updates       | 17200    |
---------------------------------
Eval num_timesteps=3524000, episode_reward=59.28 +/- 0.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 59.3     |
| time/              |          |
|    total_timesteps | 3524000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -248     |
| time/              |          |
|    episodes        | 3524     |
|    fps             | 642      |
|    time_elapsed    | 5487     |
|    total_timesteps | 3524000  |
---------------------------------
Eval num_timesteps=3525000, episode_reward=-128.15 +/- 1.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 3525000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 3.58e-05 |
|    ent_coef        | 0.000291 |
|    ent_coef_loss   | -9.43    |
|    learning_rate   | 0.00148  |
|    n_updates       | 17210    |
---------------------------------
Eval num_timesteps=3526000, episode_reward=-125.65 +/- 3.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 3526000  |
---------------------------------
Eval num_timesteps=3527000, episode_reward=-305.54 +/- 15.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -306     |
| time/              |          |
|    total_timesteps | 3527000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 2.93e-05 |
|    ent_coef        | 0.00029  |
|    ent_coef_loss   | -13      |
|    learning_rate   | 0.00147  |
|    n_updates       | 17220    |
---------------------------------
Eval num_timesteps=3528000, episode_reward=-305.38 +/- 11.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -305     |
| time/              |          |
|    total_timesteps | 3528000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -239     |
| time/              |          |
|    episodes        | 3528     |
|    fps             | 642      |
|    time_elapsed    | 5494     |
|    total_timesteps | 3528000  |
---------------------------------
Eval num_timesteps=3529000, episode_reward=-255.76 +/- 37.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -256     |
| time/              |          |
|    total_timesteps | 3529000  |
| train/             |          |
|    actor_loss      | -0.535   |
|    critic_loss     | 3.13e-05 |
|    ent_coef        | 0.000288 |
|    ent_coef_loss   | -11.5    |
|    learning_rate   | 0.00147  |
|    n_updates       | 17230    |
---------------------------------
Eval num_timesteps=3530000, episode_reward=-254.26 +/- 22.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -254     |
| time/              |          |
|    total_timesteps | 3530000  |
---------------------------------
Eval num_timesteps=3531000, episode_reward=-279.00 +/- 5.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -279     |
| time/              |          |
|    total_timesteps | 3531000  |
| train/             |          |
|    actor_loss      | -0.543   |
|    critic_loss     | 4.85e-05 |
|    ent_coef        | 0.000287 |
|    ent_coef_loss   | -13.4    |
|    learning_rate   | 0.00147  |
|    n_updates       | 17240    |
---------------------------------
Eval num_timesteps=3532000, episode_reward=-276.77 +/- 10.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -277     |
| time/              |          |
|    total_timesteps | 3532000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -230     |
| time/              |          |
|    episodes        | 3532     |
|    fps             | 642      |
|    time_elapsed    | 5500     |
|    total_timesteps | 3532000  |
---------------------------------
Eval num_timesteps=3533000, episode_reward=-270.73 +/- 8.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 3533000  |
| train/             |          |
|    actor_loss      | -0.54    |
|    critic_loss     | 3.87e-05 |
|    ent_coef        | 0.000285 |
|    ent_coef_loss   | -12.9    |
|    learning_rate   | 0.00147  |
|    n_updates       | 17250    |
---------------------------------
Eval num_timesteps=3534000, episode_reward=-273.32 +/- 5.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -273     |
| time/              |          |
|    total_timesteps | 3534000  |
---------------------------------
Eval num_timesteps=3535000, episode_reward=-258.23 +/- 1.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 3535000  |
| train/             |          |
|    actor_loss      | -0.54    |
|    critic_loss     | 3.46e-05 |
|    ent_coef        | 0.000283 |
|    ent_coef_loss   | -13.4    |
|    learning_rate   | 0.00147  |
|    n_updates       | 17260    |
---------------------------------
Eval num_timesteps=3536000, episode_reward=-252.97 +/- 6.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -253     |
| time/              |          |
|    total_timesteps | 3536000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -223     |
| time/              |          |
|    episodes        | 3536     |
|    fps             | 642      |
|    time_elapsed    | 5506     |
|    total_timesteps | 3536000  |
---------------------------------
Eval num_timesteps=3537000, episode_reward=-257.56 +/- 3.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -258     |
| time/              |          |
|    total_timesteps | 3537000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 4.64e-05 |
|    ent_coef        | 0.000281 |
|    ent_coef_loss   | -18.9    |
|    learning_rate   | 0.00146  |
|    n_updates       | 17270    |
---------------------------------
Eval num_timesteps=3538000, episode_reward=-254.52 +/- 8.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -255     |
| time/              |          |
|    total_timesteps | 3538000  |
---------------------------------
Eval num_timesteps=3539000, episode_reward=-265.88 +/- 13.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -266     |
| time/              |          |
|    total_timesteps | 3539000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 3.76e-05 |
|    ent_coef        | 0.000279 |
|    ent_coef_loss   | -15.1    |
|    learning_rate   | 0.00146  |
|    n_updates       | 17280    |
---------------------------------
Eval num_timesteps=3540000, episode_reward=-271.28 +/- 13.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -271     |
| time/              |          |
|    total_timesteps | 3540000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -218     |
| time/              |          |
|    episodes        | 3540     |
|    fps             | 642      |
|    time_elapsed    | 5513     |
|    total_timesteps | 3540000  |
---------------------------------
Eval num_timesteps=3541000, episode_reward=-4.37 +/- 12.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -4.37    |
| time/              |          |
|    total_timesteps | 3541000  |
| train/             |          |
|    actor_loss      | -0.534   |
|    critic_loss     | 3.43e-05 |
|    ent_coef        | 0.000277 |
|    ent_coef_loss   | -23.8    |
|    learning_rate   | 0.00146  |
|    n_updates       | 17290    |
---------------------------------
Eval num_timesteps=3542000, episode_reward=-0.45 +/- 15.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -0.451   |
| time/              |          |
|    total_timesteps | 3542000  |
---------------------------------
Eval num_timesteps=3543000, episode_reward=-11.97 +/- 8.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -12      |
| time/              |          |
|    total_timesteps | 3543000  |
---------------------------------
Eval num_timesteps=3544000, episode_reward=-192.39 +/- 3.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -192     |
| time/              |          |
|    total_timesteps | 3544000  |
| train/             |          |
|    actor_loss      | -0.546   |
|    critic_loss     | 6.74e-05 |
|    ent_coef        | 0.000275 |
|    ent_coef_loss   | -16.5    |
|    learning_rate   | 0.00146  |
|    n_updates       | 17300    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -209     |
| time/              |          |
|    episodes        | 3544     |
|    fps             | 642      |
|    time_elapsed    | 5519     |
|    total_timesteps | 3544000  |
---------------------------------
Eval num_timesteps=3545000, episode_reward=-194.28 +/- 4.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -194     |
| time/              |          |
|    total_timesteps | 3545000  |
---------------------------------
Eval num_timesteps=3546000, episode_reward=-283.29 +/- 0.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -283     |
| time/              |          |
|    total_timesteps | 3546000  |
| train/             |          |
|    actor_loss      | -0.531   |
|    critic_loss     | 5.2e-05  |
|    ent_coef        | 0.000272 |
|    ent_coef_loss   | -1.56    |
|    learning_rate   | 0.00145  |
|    n_updates       | 17310    |
---------------------------------
Eval num_timesteps=3547000, episode_reward=-281.81 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -282     |
| time/              |          |
|    total_timesteps | 3547000  |
---------------------------------
Eval num_timesteps=3548000, episode_reward=-157.15 +/- 168.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 3548000  |
| train/             |          |
|    actor_loss      | -0.527   |
|    critic_loss     | 2.99e-05 |
|    ent_coef        | 0.000271 |
|    ent_coef_loss   | -23.5    |
|    learning_rate   | 0.00145  |
|    n_updates       | 17320    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -207     |
| time/              |          |
|    episodes        | 3548     |
|    fps             | 642      |
|    time_elapsed    | 5525     |
|    total_timesteps | 3548000  |
---------------------------------
Eval num_timesteps=3549000, episode_reward=12.09 +/- 206.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 12.1     |
| time/              |          |
|    total_timesteps | 3549000  |
---------------------------------
Eval num_timesteps=3550000, episode_reward=-236.60 +/- 4.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -237     |
| time/              |          |
|    total_timesteps | 3550000  |
| train/             |          |
|    actor_loss      | -0.525   |
|    critic_loss     | 5.08e-05 |
|    ent_coef        | 0.000269 |
|    ent_coef_loss   | -25.9    |
|    learning_rate   | 0.00145  |
|    n_updates       | 17330    |
---------------------------------
Eval num_timesteps=3551000, episode_reward=-235.43 +/- 5.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -235     |
| time/              |          |
|    total_timesteps | 3551000  |
---------------------------------
Eval num_timesteps=3552000, episode_reward=-114.14 +/- 4.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 3552000  |
| train/             |          |
|    actor_loss      | -0.526   |
|    critic_loss     | 8.07e-05 |
|    ent_coef        | 0.000266 |
|    ent_coef_loss   | -9.17    |
|    learning_rate   | 0.00145  |
|    n_updates       | 17340    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -200     |
| time/              |          |
|    episodes        | 3552     |
|    fps             | 642      |
|    time_elapsed    | 5531     |
|    total_timesteps | 3552000  |
---------------------------------
Eval num_timesteps=3553000, episode_reward=-103.95 +/- 17.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 3553000  |
---------------------------------
Eval num_timesteps=3554000, episode_reward=-336.68 +/- 4.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -337     |
| time/              |          |
|    total_timesteps | 3554000  |
| train/             |          |
|    actor_loss      | -0.533   |
|    critic_loss     | 6.37e-05 |
|    ent_coef        | 0.000265 |
|    ent_coef_loss   | 10.4     |
|    learning_rate   | 0.00145  |
|    n_updates       | 17350    |
---------------------------------
Eval num_timesteps=3555000, episode_reward=-339.33 +/- 6.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 3555000  |
---------------------------------
Eval num_timesteps=3556000, episode_reward=-287.63 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 3556000  |
| train/             |          |
|    actor_loss      | -0.519   |
|    critic_loss     | 4.24e-05 |
|    ent_coef        | 0.000265 |
|    ent_coef_loss   | -15.6    |
|    learning_rate   | 0.00144  |
|    n_updates       | 17360    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -199     |
| time/              |          |
|    episodes        | 3556     |
|    fps             | 642      |
|    time_elapsed    | 5538     |
|    total_timesteps | 3556000  |
---------------------------------
Eval num_timesteps=3557000, episode_reward=-287.56 +/- 1.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -288     |
| time/              |          |
|    total_timesteps | 3557000  |
---------------------------------
Eval num_timesteps=3558000, episode_reward=-339.38 +/- 0.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -339     |
| time/              |          |
|    total_timesteps | 3558000  |
| train/             |          |
|    actor_loss      | -0.513   |
|    critic_loss     | 4.72e-05 |
|    ent_coef        | 0.000263 |
|    ent_coef_loss   | -20      |
|    learning_rate   | 0.00144  |
|    n_updates       | 17370    |
---------------------------------
Eval num_timesteps=3559000, episode_reward=-338.03 +/- 0.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -338     |
| time/              |          |
|    total_timesteps | 3559000  |
---------------------------------
Eval num_timesteps=3560000, episode_reward=-427.32 +/- 0.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 3560000  |
| train/             |          |
|    actor_loss      | -0.506   |
|    critic_loss     | 2.68e-05 |
|    ent_coef        | 0.000261 |
|    ent_coef_loss   | -7.09    |
|    learning_rate   | 0.00144  |
|    n_updates       | 17380    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -200     |
| time/              |          |
|    episodes        | 3560     |
|    fps             | 642      |
|    time_elapsed    | 5544     |
|    total_timesteps | 3560000  |
---------------------------------
Eval num_timesteps=3561000, episode_reward=-426.68 +/- 0.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -427     |
| time/              |          |
|    total_timesteps | 3561000  |
---------------------------------
Eval num_timesteps=3562000, episode_reward=-441.97 +/- 0.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 3562000  |
| train/             |          |
|    actor_loss      | -0.512   |
|    critic_loss     | 1.81e-05 |
|    ent_coef        | 0.00026  |
|    ent_coef_loss   | 7.48     |
|    learning_rate   | 0.00144  |
|    n_updates       | 17390    |
---------------------------------
Eval num_timesteps=3563000, episode_reward=-442.10 +/- 1.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -442     |
| time/              |          |
|    total_timesteps | 3563000  |
---------------------------------
Eval num_timesteps=3564000, episode_reward=-453.94 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -454     |
| time/              |          |
|    total_timesteps | 3564000  |
| train/             |          |
|    actor_loss      | -0.514   |
|    critic_loss     | 1.18e-05 |
|    ent_coef        | 0.000261 |
|    ent_coef_loss   | 5.87     |
|    learning_rate   | 0.00144  |
|    n_updates       | 17400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -203     |
| time/              |          |
|    episodes        | 3564     |
|    fps             | 642      |
|    time_elapsed    | 5550     |
|    total_timesteps | 3564000  |
---------------------------------
Eval num_timesteps=3565000, episode_reward=-455.28 +/- 0.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -455     |
| time/              |          |
|    total_timesteps | 3565000  |
---------------------------------
Eval num_timesteps=3566000, episode_reward=-442.77 +/- 0.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -443     |
| time/              |          |
|    total_timesteps | 3566000  |
| train/             |          |
|    actor_loss      | -0.513   |
|    critic_loss     | 6.69e-06 |
|    ent_coef        | 0.000261 |
|    ent_coef_loss   | 2.22     |
|    learning_rate   | 0.00143  |
|    n_updates       | 17410    |
---------------------------------
Eval num_timesteps=3567000, episode_reward=-444.08 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -444     |
| time/              |          |
|    total_timesteps | 3567000  |
---------------------------------
Eval num_timesteps=3568000, episode_reward=-437.98 +/- 0.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 3568000  |
| train/             |          |
|    actor_loss      | -0.509   |
|    critic_loss     | 8.54e-06 |
|    ent_coef        | 0.000261 |
|    ent_coef_loss   | -3.29    |
|    learning_rate   | 0.00143  |
|    n_updates       | 17420    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -207     |
| time/              |          |
|    episodes        | 3568     |
|    fps             | 642      |
|    time_elapsed    | 5557     |
|    total_timesteps | 3568000  |
---------------------------------
Eval num_timesteps=3569000, episode_reward=-438.10 +/- 0.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -438     |
| time/              |          |
|    total_timesteps | 3569000  |
---------------------------------
Eval num_timesteps=3570000, episode_reward=-436.75 +/- 1.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 3570000  |
| train/             |          |
|    actor_loss      | -0.509   |
|    critic_loss     | 8.56e-06 |
|    ent_coef        | 0.000261 |
|    ent_coef_loss   | -3.41    |
|    learning_rate   | 0.00143  |
|    n_updates       | 17430    |
---------------------------------
Eval num_timesteps=3571000, episode_reward=-436.84 +/- 0.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -437     |
| time/              |          |
|    total_timesteps | 3571000  |
---------------------------------
Eval num_timesteps=3572000, episode_reward=-427.98 +/- 0.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 3572000  |
| train/             |          |
|    actor_loss      | -0.509   |
|    critic_loss     | 7.01e-06 |
|    ent_coef        | 0.000261 |
|    ent_coef_loss   | -6.33    |
|    learning_rate   | 0.00143  |
|    n_updates       | 17440    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -210     |
| time/              |          |
|    episodes        | 3572     |
|    fps             | 642      |
|    time_elapsed    | 5563     |
|    total_timesteps | 3572000  |
---------------------------------
Eval num_timesteps=3573000, episode_reward=-428.00 +/- 0.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -428     |
| time/              |          |
|    total_timesteps | 3573000  |
---------------------------------
Eval num_timesteps=3574000, episode_reward=-412.13 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -412     |
| time/              |          |
|    total_timesteps | 3574000  |
| train/             |          |
|    actor_loss      | -0.508   |
|    critic_loss     | 8.86e-06 |
|    ent_coef        | 0.00026  |
|    ent_coef_loss   | -7.57    |
|    learning_rate   | 0.00143  |
|    n_updates       | 17450    |
---------------------------------
Eval num_timesteps=3575000, episode_reward=-411.03 +/- 0.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -411     |
| time/              |          |
|    total_timesteps | 3575000  |
---------------------------------
Eval num_timesteps=3576000, episode_reward=-388.26 +/- 1.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 3576000  |
| train/             |          |
|    actor_loss      | -0.507   |
|    critic_loss     | 8.86e-06 |
|    ent_coef        | 0.000259 |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.00142  |
|    n_updates       | 17460    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -211     |
| time/              |          |
|    episodes        | 3576     |
|    fps             | 642      |
|    time_elapsed    | 5569     |
|    total_timesteps | 3576000  |
---------------------------------
Eval num_timesteps=3577000, episode_reward=-387.61 +/- 0.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -388     |
| time/              |          |
|    total_timesteps | 3577000  |
---------------------------------
Eval num_timesteps=3578000, episode_reward=-376.94 +/- 0.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -377     |
| time/              |          |
|    total_timesteps | 3578000  |
| train/             |          |
|    actor_loss      | -0.508   |
|    critic_loss     | 9.41e-06 |
|    ent_coef        | 0.000258 |
|    ent_coef_loss   | -10.8    |
|    learning_rate   | 0.00142  |
|    n_updates       | 17470    |
---------------------------------
Eval num_timesteps=3579000, episode_reward=-377.59 +/- 0.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -378     |
| time/              |          |
|    total_timesteps | 3579000  |
---------------------------------
Eval num_timesteps=3580000, episode_reward=-381.33 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -381     |
| time/              |          |
|    total_timesteps | 3580000  |
| train/             |          |
|    actor_loss      | -0.509   |
|    critic_loss     | 8.28e-06 |
|    ent_coef        | 0.000257 |
|    ent_coef_loss   | -8.79    |
|    learning_rate   | 0.00142  |
|    n_updates       | 17480    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -212     |
| time/              |          |
|    episodes        | 3580     |
|    fps             | 642      |
|    time_elapsed    | 5576     |
|    total_timesteps | 3580000  |
---------------------------------
Eval num_timesteps=3581000, episode_reward=-382.54 +/- 0.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -383     |
| time/              |          |
|    total_timesteps | 3581000  |
---------------------------------
Eval num_timesteps=3582000, episode_reward=-354.34 +/- 0.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 3582000  |
| train/             |          |
|    actor_loss      | -0.506   |
|    critic_loss     | 8.59e-06 |
|    ent_coef        | 0.000256 |
|    ent_coef_loss   | -11.4    |
|    learning_rate   | 0.00142  |
|    n_updates       | 17490    |
---------------------------------
Eval num_timesteps=3583000, episode_reward=-353.07 +/- 0.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -353     |
| time/              |          |
|    total_timesteps | 3583000  |
---------------------------------
Eval num_timesteps=3584000, episode_reward=-353.79 +/- 0.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -354     |
| time/              |          |
|    total_timesteps | 3584000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -212     |
| time/              |          |
|    episodes        | 3584     |
|    fps             | 642      |
|    time_elapsed    | 5582     |
|    total_timesteps | 3584000  |
---------------------------------
Eval num_timesteps=3585000, episode_reward=-359.49 +/- 1.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -359     |
| time/              |          |
|    total_timesteps | 3585000  |
| train/             |          |
|    actor_loss      | -0.505   |
|    critic_loss     | 8.82e-06 |
|    ent_coef        | 0.000254 |
|    ent_coef_loss   | -10.1    |
|    learning_rate   | 0.00142  |
|    n_updates       | 17500    |
---------------------------------
Eval num_timesteps=3586000, episode_reward=-357.45 +/- 0.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -357     |
| time/              |          |
|    total_timesteps | 3586000  |
---------------------------------
Eval num_timesteps=3587000, episode_reward=-349.04 +/- 1.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -349     |
| time/              |          |
|    total_timesteps | 3587000  |
| train/             |          |
|    actor_loss      | -0.503   |
|    critic_loss     | 9.91e-06 |
|    ent_coef        | 0.000253 |
|    ent_coef_loss   | -10.3    |
|    learning_rate   | 0.00141  |
|    n_updates       | 17510    |
---------------------------------
Eval num_timesteps=3588000, episode_reward=-349.20 +/- 1.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -349     |
| time/              |          |
|    total_timesteps | 3588000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -213     |
| time/              |          |
|    episodes        | 3588     |
|    fps             | 642      |
|    time_elapsed    | 5588     |
|    total_timesteps | 3588000  |
---------------------------------
Eval num_timesteps=3589000, episode_reward=-329.71 +/- 1.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -330     |
| time/              |          |
|    total_timesteps | 3589000  |
| train/             |          |
|    actor_loss      | -0.503   |
|    critic_loss     | 1.06e-05 |
|    ent_coef        | 0.000252 |
|    ent_coef_loss   | -13.3    |
|    learning_rate   | 0.00141  |
|    n_updates       | 17520    |
---------------------------------
Eval num_timesteps=3590000, episode_reward=-329.19 +/- 0.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -329     |
| time/              |          |
|    total_timesteps | 3590000  |
---------------------------------
Eval num_timesteps=3591000, episode_reward=-227.80 +/- 0.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 3591000  |
| train/             |          |
|    actor_loss      | -0.503   |
|    critic_loss     | 1.44e-05 |
|    ent_coef        | 0.00025  |
|    ent_coef_loss   | -12.7    |
|    learning_rate   | 0.00141  |
|    n_updates       | 17530    |
---------------------------------
Eval num_timesteps=3592000, episode_reward=-228.14 +/- 1.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -228     |
| time/              |          |
|    total_timesteps | 3592000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -212     |
| time/              |          |
|    episodes        | 3592     |
|    fps             | 641      |
|    time_elapsed    | 5595     |
|    total_timesteps | 3592000  |
---------------------------------
Eval num_timesteps=3593000, episode_reward=-218.70 +/- 2.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 3593000  |
| train/             |          |
|    actor_loss      | -0.504   |
|    critic_loss     | 1.77e-05 |
|    ent_coef        | 0.000249 |
|    ent_coef_loss   | -17.4    |
|    learning_rate   | 0.00141  |
|    n_updates       | 17540    |
---------------------------------
Eval num_timesteps=3594000, episode_reward=-218.61 +/- 1.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -219     |
| time/              |          |
|    total_timesteps | 3594000  |
---------------------------------
Eval num_timesteps=3595000, episode_reward=-220.35 +/- 18.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -220     |
| time/              |          |
|    total_timesteps | 3595000  |
| train/             |          |
|    actor_loss      | -0.508   |
|    critic_loss     | 2.98e-05 |
|    ent_coef        | 0.000247 |
|    ent_coef_loss   | -18.4    |
|    learning_rate   | 0.00141  |
|    n_updates       | 17550    |
---------------------------------
Eval num_timesteps=3596000, episode_reward=-242.09 +/- 15.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -242     |
| time/              |          |
|    total_timesteps | 3596000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -207     |
| time/              |          |
|    episodes        | 3596     |
|    fps             | 641      |
|    time_elapsed    | 5601     |
|    total_timesteps | 3596000  |
---------------------------------
Eval num_timesteps=3597000, episode_reward=-190.94 +/- 10.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -191     |
| time/              |          |
|    total_timesteps | 3597000  |
| train/             |          |
|    actor_loss      | -0.507   |
|    critic_loss     | 3.52e-05 |
|    ent_coef        | 0.000245 |
|    ent_coef_loss   | -12.9    |
|    learning_rate   | 0.0014   |
|    n_updates       | 17560    |
---------------------------------
Eval num_timesteps=3598000, episode_reward=-195.04 +/- 5.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -195     |
| time/              |          |
|    total_timesteps | 3598000  |
---------------------------------
Eval num_timesteps=3599000, episode_reward=-203.25 +/- 17.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -203     |
| time/              |          |
|    total_timesteps | 3599000  |
| train/             |          |
|    actor_loss      | -0.507   |
|    critic_loss     | 3.43e-05 |
|    ent_coef        | 0.000243 |
|    ent_coef_loss   | -15.2    |
|    learning_rate   | 0.0014   |
|    n_updates       | 17570    |
---------------------------------
Eval num_timesteps=3600000, episode_reward=-211.96 +/- 16.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -212     |
| time/              |          |
|    total_timesteps | 3600000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -204     |
| time/              |          |
|    episodes        | 3600     |
|    fps             | 641      |
|    time_elapsed    | 5607     |
|    total_timesteps | 3600000  |
---------------------------------
Eval num_timesteps=3601000, episode_reward=7.38 +/- 15.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 7.38     |
| time/              |          |
|    total_timesteps | 3601000  |
| train/             |          |
|    actor_loss      | -0.508   |
|    critic_loss     | 4.67e-05 |
|    ent_coef        | 0.000241 |
|    ent_coef_loss   | -15.8    |
|    learning_rate   | 0.0014   |
|    n_updates       | 17580    |
---------------------------------
Eval num_timesteps=3602000, episode_reward=5.66 +/- 12.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 5.66     |
| time/              |          |
|    total_timesteps | 3602000  |
---------------------------------
Eval num_timesteps=3603000, episode_reward=-80.03 +/- 9.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -80      |
| time/              |          |
|    total_timesteps | 3603000  |
| train/             |          |
|    actor_loss      | -0.514   |
|    critic_loss     | 4.59e-05 |
|    ent_coef        | 0.000239 |
|    ent_coef_loss   | -14.7    |
|    learning_rate   | 0.0014   |
|    n_updates       | 17590    |
---------------------------------
Eval num_timesteps=3604000, episode_reward=-83.98 +/- 10.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -84      |
| time/              |          |
|    total_timesteps | 3604000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -199     |
| time/              |          |
|    episodes        | 3604     |
|    fps             | 641      |
|    time_elapsed    | 5613     |
|    total_timesteps | 3604000  |
---------------------------------
Eval num_timesteps=3605000, episode_reward=-106.47 +/- 6.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 3605000  |
| train/             |          |
|    actor_loss      | -0.512   |
|    critic_loss     | 3.66e-05 |
|    ent_coef        | 0.000237 |
|    ent_coef_loss   | -20.7    |
|    learning_rate   | 0.0014   |
|    n_updates       | 17600    |
---------------------------------
Eval num_timesteps=3606000, episode_reward=-103.89 +/- 9.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 3606000  |
---------------------------------
Eval num_timesteps=3607000, episode_reward=308.44 +/- 5.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 308      |
| time/              |          |
|    total_timesteps | 3607000  |
| train/             |          |
|    actor_loss      | -0.51    |
|    critic_loss     | 3.47e-05 |
|    ent_coef        | 0.000235 |
|    ent_coef_loss   | -25.7    |
|    learning_rate   | 0.00139  |
|    n_updates       | 17610    |
---------------------------------
Eval num_timesteps=3608000, episode_reward=305.00 +/- 2.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 305      |
| time/              |          |
|    total_timesteps | 3608000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -191     |
| time/              |          |
|    episodes        | 3608     |
|    fps             | 641      |
|    time_elapsed    | 5620     |
|    total_timesteps | 3608000  |
---------------------------------
Eval num_timesteps=3609000, episode_reward=436.77 +/- 5.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 437      |
| time/              |          |
|    total_timesteps | 3609000  |
| train/             |          |
|    actor_loss      | -0.518   |
|    critic_loss     | 4.07e-05 |
|    ent_coef        | 0.000233 |
|    ent_coef_loss   | -21.7    |
|    learning_rate   | 0.00139  |
|    n_updates       | 17620    |
---------------------------------
Eval num_timesteps=3610000, episode_reward=431.23 +/- 7.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 431      |
| time/              |          |
|    total_timesteps | 3610000  |
---------------------------------
Eval num_timesteps=3611000, episode_reward=421.02 +/- 2.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 421      |
| time/              |          |
|    total_timesteps | 3611000  |
| train/             |          |
|    actor_loss      | -0.524   |
|    critic_loss     | 5.09e-05 |
|    ent_coef        | 0.00023  |
|    ent_coef_loss   | -18.9    |
|    learning_rate   | 0.00139  |
|    n_updates       | 17630    |
---------------------------------
Eval num_timesteps=3612000, episode_reward=419.34 +/- 2.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 419      |
| time/              |          |
|    total_timesteps | 3612000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -173     |
| time/              |          |
|    episodes        | 3612     |
|    fps             | 641      |
|    time_elapsed    | 5626     |
|    total_timesteps | 3612000  |
---------------------------------
Eval num_timesteps=3613000, episode_reward=397.95 +/- 7.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 398      |
| time/              |          |
|    total_timesteps | 3613000  |
| train/             |          |
|    actor_loss      | -0.532   |
|    critic_loss     | 5.68e-05 |
|    ent_coef        | 0.000228 |
|    ent_coef_loss   | -15.2    |
|    learning_rate   | 0.00139  |
|    n_updates       | 17640    |
---------------------------------
Eval num_timesteps=3614000, episode_reward=398.85 +/- 10.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 399      |
| time/              |          |
|    total_timesteps | 3614000  |
---------------------------------
Eval num_timesteps=3615000, episode_reward=313.56 +/- 1.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 314      |
| time/              |          |
|    total_timesteps | 3615000  |
| train/             |          |
|    actor_loss      | -0.532   |
|    critic_loss     | 4.92e-05 |
|    ent_coef        | 0.000226 |
|    ent_coef_loss   | -13.7    |
|    learning_rate   | 0.00139  |
|    n_updates       | 17650    |
---------------------------------
Eval num_timesteps=3616000, episode_reward=314.91 +/- 2.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 315      |
| time/              |          |
|    total_timesteps | 3616000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -155     |
| time/              |          |
|    episodes        | 3616     |
|    fps             | 641      |
|    time_elapsed    | 5632     |
|    total_timesteps | 3616000  |
---------------------------------
Eval num_timesteps=3617000, episode_reward=426.08 +/- 5.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 426      |
| time/              |          |
|    total_timesteps | 3617000  |
| train/             |          |
|    actor_loss      | -0.525   |
|    critic_loss     | 4.07e-05 |
|    ent_coef        | 0.000224 |
|    ent_coef_loss   | -15.4    |
|    learning_rate   | 0.00138  |
|    n_updates       | 17660    |
---------------------------------
Eval num_timesteps=3618000, episode_reward=423.38 +/- 6.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 423      |
| time/              |          |
|    total_timesteps | 3618000  |
---------------------------------
Eval num_timesteps=3619000, episode_reward=545.12 +/- 10.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 545      |
| time/              |          |
|    total_timesteps | 3619000  |
| train/             |          |
|    actor_loss      | -0.541   |
|    critic_loss     | 7.49e-05 |
|    ent_coef        | 0.000223 |
|    ent_coef_loss   | -15.5    |
|    learning_rate   | 0.00138  |
|    n_updates       | 17670    |
---------------------------------
Eval num_timesteps=3620000, episode_reward=548.05 +/- 11.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 548      |
| time/              |          |
|    total_timesteps | 3620000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -130     |
| time/              |          |
|    episodes        | 3620     |
|    fps             | 641      |
|    time_elapsed    | 5639     |
|    total_timesteps | 3620000  |
---------------------------------
Eval num_timesteps=3621000, episode_reward=348.43 +/- 5.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 348      |
| time/              |          |
|    total_timesteps | 3621000  |
| train/             |          |
|    actor_loss      | -0.538   |
|    critic_loss     | 7.84e-05 |
|    ent_coef        | 0.000221 |
|    ent_coef_loss   | -11.8    |
|    learning_rate   | 0.00138  |
|    n_updates       | 17680    |
---------------------------------
Eval num_timesteps=3622000, episode_reward=352.19 +/- 3.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 352      |
| time/              |          |
|    total_timesteps | 3622000  |
---------------------------------
Eval num_timesteps=3623000, episode_reward=517.61 +/- 5.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 3623000  |
| train/             |          |
|    actor_loss      | -0.536   |
|    critic_loss     | 5.54e-05 |
|    ent_coef        | 0.000219 |
|    ent_coef_loss   | -13.8    |
|    learning_rate   | 0.00138  |
|    n_updates       | 17690    |
---------------------------------
Eval num_timesteps=3624000, episode_reward=517.81 +/- 5.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 518      |
| time/              |          |
|    total_timesteps | 3624000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -111     |
| time/              |          |
|    episodes        | 3624     |
|    fps             | 641      |
|    time_elapsed    | 5645     |
|    total_timesteps | 3624000  |
---------------------------------
Eval num_timesteps=3625000, episode_reward=539.96 +/- 2.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 540      |
| time/              |          |
|    total_timesteps | 3625000  |
| train/             |          |
|    actor_loss      | -0.53    |
|    critic_loss     | 5.98e-05 |
|    ent_coef        | 0.000218 |
|    ent_coef_loss   | -15.7    |
|    learning_rate   | 0.00138  |
|    n_updates       | 17700    |
---------------------------------
Eval num_timesteps=3626000, episode_reward=540.46 +/- 3.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 540      |
| time/              |          |
|    total_timesteps | 3626000  |
---------------------------------
Eval num_timesteps=3627000, episode_reward=535.70 +/- 6.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 536      |
| time/              |          |
|    total_timesteps | 3627000  |
---------------------------------
Eval num_timesteps=3628000, episode_reward=655.76 +/- 9.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 3628000  |
| train/             |          |
|    actor_loss      | -0.532   |
|    critic_loss     | 7.8e-05  |
|    ent_coef        | 0.000216 |
|    ent_coef_loss   | -14.2    |
|    learning_rate   | 0.00137  |
|    n_updates       | 17710    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -88.8    |
| time/              |          |
|    episodes        | 3628     |
|    fps             | 641      |
|    time_elapsed    | 5651     |
|    total_timesteps | 3628000  |
---------------------------------
Eval num_timesteps=3629000, episode_reward=651.40 +/- 7.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 651      |
| time/              |          |
|    total_timesteps | 3629000  |
---------------------------------
Eval num_timesteps=3630000, episode_reward=446.60 +/- 4.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 447      |
| time/              |          |
|    total_timesteps | 3630000  |
| train/             |          |
|    actor_loss      | -0.543   |
|    critic_loss     | 5.99e-05 |
|    ent_coef        | 0.000215 |
|    ent_coef_loss   | -2.75    |
|    learning_rate   | 0.00137  |
|    n_updates       | 17720    |
---------------------------------
Eval num_timesteps=3631000, episode_reward=442.34 +/- 15.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 442      |
| time/              |          |
|    total_timesteps | 3631000  |
---------------------------------
Eval num_timesteps=3632000, episode_reward=396.51 +/- 12.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 397      |
| time/              |          |
|    total_timesteps | 3632000  |
| train/             |          |
|    actor_loss      | -0.537   |
|    critic_loss     | 5.46e-05 |
|    ent_coef        | 0.000214 |
|    ent_coef_loss   | -2.21    |
|    learning_rate   | 0.00137  |
|    n_updates       | 17730    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -69.3    |
| time/              |          |
|    episodes        | 3632     |
|    fps             | 641      |
|    time_elapsed    | 5658     |
|    total_timesteps | 3632000  |
---------------------------------
Eval num_timesteps=3633000, episode_reward=394.12 +/- 13.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 394      |
| time/              |          |
|    total_timesteps | 3633000  |
---------------------------------
Eval num_timesteps=3634000, episode_reward=596.65 +/- 13.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 597      |
| time/              |          |
|    total_timesteps | 3634000  |
| train/             |          |
|    actor_loss      | -0.54    |
|    critic_loss     | 6.95e-05 |
|    ent_coef        | 0.000214 |
|    ent_coef_loss   | -14      |
|    learning_rate   | 0.00137  |
|    n_updates       | 17740    |
---------------------------------
Eval num_timesteps=3635000, episode_reward=599.87 +/- 7.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 600      |
| time/              |          |
|    total_timesteps | 3635000  |
---------------------------------
Eval num_timesteps=3636000, episode_reward=656.36 +/- 9.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 656      |
| time/              |          |
|    total_timesteps | 3636000  |
| train/             |          |
|    actor_loss      | -0.545   |
|    critic_loss     | 8.34e-05 |
|    ent_coef        | 0.000213 |
|    ent_coef_loss   | -15.5    |
|    learning_rate   | 0.00136  |
|    n_updates       | 17750    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -46.4    |
| time/              |          |
|    episodes        | 3636     |
|    fps             | 641      |
|    time_elapsed    | 5664     |
|    total_timesteps | 3636000  |
---------------------------------
Eval num_timesteps=3637000, episode_reward=649.49 +/- 13.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 649      |
| time/              |          |
|    total_timesteps | 3637000  |
---------------------------------
Eval num_timesteps=3638000, episode_reward=642.46 +/- 11.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 642      |
| time/              |          |
|    total_timesteps | 3638000  |
| train/             |          |
|    actor_loss      | -0.546   |
|    critic_loss     | 7.06e-05 |
|    ent_coef        | 0.000211 |
|    ent_coef_loss   | -8.75    |
|    learning_rate   | 0.00136  |
|    n_updates       | 17760    |
---------------------------------
Eval num_timesteps=3639000, episode_reward=647.42 +/- 11.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 647      |
| time/              |          |
|    total_timesteps | 3639000  |
---------------------------------
Eval num_timesteps=3640000, episode_reward=600.64 +/- 13.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 601      |
| time/              |          |
|    total_timesteps | 3640000  |
| train/             |          |
|    actor_loss      | -0.545   |
|    critic_loss     | 6.62e-05 |
|    ent_coef        | 0.00021  |
|    ent_coef_loss   | -6.99    |
|    learning_rate   | 0.00136  |
|    n_updates       | 17770    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | -20.8    |
| time/              |          |
|    episodes        | 3640     |
|    fps             | 641      |
|    time_elapsed    | 5670     |
|    total_timesteps | 3640000  |
---------------------------------
Eval num_timesteps=3641000, episode_reward=615.45 +/- 8.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 615      |
| time/              |          |
|    total_timesteps | 3641000  |
---------------------------------
Eval num_timesteps=3642000, episode_reward=630.29 +/- 12.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 630      |
| time/              |          |
|    total_timesteps | 3642000  |
| train/             |          |
|    actor_loss      | -0.545   |
|    critic_loss     | 6.59e-05 |
|    ent_coef        | 0.000209 |
|    ent_coef_loss   | -8.38    |
|    learning_rate   | 0.00136  |
|    n_updates       | 17780    |
---------------------------------
Eval num_timesteps=3643000, episode_reward=618.22 +/- 6.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 618      |
| time/              |          |
|    total_timesteps | 3643000  |
---------------------------------
Eval num_timesteps=3644000, episode_reward=746.81 +/- 14.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 747      |
| time/              |          |
|    total_timesteps | 3644000  |
| train/             |          |
|    actor_loss      | -0.548   |
|    critic_loss     | 7.99e-05 |
|    ent_coef        | 0.000208 |
|    ent_coef_loss   | -6.59    |
|    learning_rate   | 0.00136  |
|    n_updates       | 17790    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.51     |
| time/              |          |
|    episodes        | 3644     |
|    fps             | 641      |
|    time_elapsed    | 5676     |
|    total_timesteps | 3644000  |
---------------------------------
Eval num_timesteps=3645000, episode_reward=733.47 +/- 14.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 733      |
| time/              |          |
|    total_timesteps | 3645000  |
---------------------------------
Eval num_timesteps=3646000, episode_reward=928.54 +/- 8.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 929      |
| time/              |          |
|    total_timesteps | 3646000  |
| train/             |          |
|    actor_loss      | -0.553   |
|    critic_loss     | 8.28e-05 |
|    ent_coef        | 0.000207 |
|    ent_coef_loss   | -5.92    |
|    learning_rate   | 0.00135  |
|    n_updates       | 17800    |
---------------------------------
Eval num_timesteps=3647000, episode_reward=916.95 +/- 5.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 917      |
| time/              |          |
|    total_timesteps | 3647000  |
---------------------------------
Eval num_timesteps=3648000, episode_reward=1013.78 +/- 28.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.01e+03 |
| time/              |          |
|    total_timesteps | 3648000  |
| train/             |          |
|    actor_loss      | -0.557   |
|    critic_loss     | 9.73e-05 |
|    ent_coef        | 0.000207 |
|    ent_coef_loss   | -6.87    |
|    learning_rate   | 0.00135  |
|    n_updates       | 17810    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 40.1     |
| time/              |          |
|    episodes        | 3648     |
|    fps             | 641      |
|    time_elapsed    | 5683     |
|    total_timesteps | 3648000  |
---------------------------------
Eval num_timesteps=3649000, episode_reward=1033.99 +/- 18.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.03e+03 |
| time/              |          |
|    total_timesteps | 3649000  |
---------------------------------
Eval num_timesteps=3650000, episode_reward=768.48 +/- 26.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 768      |
| time/              |          |
|    total_timesteps | 3650000  |
| train/             |          |
|    actor_loss      | -0.565   |
|    critic_loss     | 0.000121 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | -3.01    |
|    learning_rate   | 0.00135  |
|    n_updates       | 17820    |
---------------------------------
Eval num_timesteps=3651000, episode_reward=766.73 +/- 6.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 767      |
| time/              |          |
|    total_timesteps | 3651000  |
---------------------------------
Eval num_timesteps=3652000, episode_reward=844.74 +/- 15.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 845      |
| time/              |          |
|    total_timesteps | 3652000  |
| train/             |          |
|    actor_loss      | -0.554   |
|    critic_loss     | 8.23e-05 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | -1.05    |
|    learning_rate   | 0.00135  |
|    n_updates       | 17830    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 73.1     |
| time/              |          |
|    episodes        | 3652     |
|    fps             | 641      |
|    time_elapsed    | 5689     |
|    total_timesteps | 3652000  |
---------------------------------
Eval num_timesteps=3653000, episode_reward=837.86 +/- 18.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 838      |
| time/              |          |
|    total_timesteps | 3653000  |
---------------------------------
Eval num_timesteps=3654000, episode_reward=758.19 +/- 20.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 758      |
| time/              |          |
|    total_timesteps | 3654000  |
| train/             |          |
|    actor_loss      | -0.566   |
|    critic_loss     | 9.17e-05 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 0.00135  |
|    n_updates       | 17840    |
---------------------------------
Eval num_timesteps=3655000, episode_reward=753.44 +/- 22.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 753      |
| time/              |          |
|    total_timesteps | 3655000  |
---------------------------------
Eval num_timesteps=3656000, episode_reward=709.41 +/- 15.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 709      |
| time/              |          |
|    total_timesteps | 3656000  |
| train/             |          |
|    actor_loss      | -0.559   |
|    critic_loss     | 8.16e-05 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | 0.865    |
|    learning_rate   | 0.00134  |
|    n_updates       | 17850    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 114      |
| time/              |          |
|    episodes        | 3656     |
|    fps             | 641      |
|    time_elapsed    | 5695     |
|    total_timesteps | 3656000  |
---------------------------------
Eval num_timesteps=3657000, episode_reward=681.87 +/- 18.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 682      |
| time/              |          |
|    total_timesteps | 3657000  |
---------------------------------
Eval num_timesteps=3658000, episode_reward=829.46 +/- 17.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 829      |
| time/              |          |
|    total_timesteps | 3658000  |
| train/             |          |
|    actor_loss      | -0.555   |
|    critic_loss     | 7.32e-05 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | 0.000747 |
|    learning_rate   | 0.00134  |
|    n_updates       | 17860    |
---------------------------------
Eval num_timesteps=3659000, episode_reward=830.13 +/- 14.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 830      |
| time/              |          |
|    total_timesteps | 3659000  |
---------------------------------
Eval num_timesteps=3660000, episode_reward=1177.79 +/- 26.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.18e+03 |
| time/              |          |
|    total_timesteps | 3660000  |
| train/             |          |
|    actor_loss      | -0.563   |
|    critic_loss     | 7.44e-05 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | 1.74     |
|    learning_rate   | 0.00134  |
|    n_updates       | 17870    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 157      |
| time/              |          |
|    episodes        | 3660     |
|    fps             | 641      |
|    time_elapsed    | 5701     |
|    total_timesteps | 3660000  |
---------------------------------
Eval num_timesteps=3661000, episode_reward=1201.74 +/- 24.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3661000  |
---------------------------------
Eval num_timesteps=3662000, episode_reward=1217.67 +/- 18.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.22e+03 |
| time/              |          |
|    total_timesteps | 3662000  |
| train/             |          |
|    actor_loss      | -0.58    |
|    critic_loss     | 0.000122 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | 2.72     |
|    learning_rate   | 0.00134  |
|    n_updates       | 17880    |
---------------------------------
Eval num_timesteps=3663000, episode_reward=1210.53 +/- 19.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3663000  |
---------------------------------
Eval num_timesteps=3664000, episode_reward=1151.49 +/- 36.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.15e+03 |
| time/              |          |
|    total_timesteps | 3664000  |
| train/             |          |
|    actor_loss      | -0.585   |
|    critic_loss     | 0.000137 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | 2.24     |
|    learning_rate   | 0.00134  |
|    n_updates       | 17890    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 214      |
| time/              |          |
|    episodes        | 3664     |
|    fps             | 641      |
|    time_elapsed    | 5708     |
|    total_timesteps | 3664000  |
---------------------------------
Eval num_timesteps=3665000, episode_reward=1157.52 +/- 20.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.16e+03 |
| time/              |          |
|    total_timesteps | 3665000  |
---------------------------------
Eval num_timesteps=3666000, episode_reward=1323.26 +/- 9.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.32e+03 |
| time/              |          |
|    total_timesteps | 3666000  |
| train/             |          |
|    actor_loss      | -0.577   |
|    critic_loss     | 0.000126 |
|    ent_coef        | 0.000206 |
|    ent_coef_loss   | 0.123    |
|    learning_rate   | 0.00133  |
|    n_updates       | 17900    |
---------------------------------
Eval num_timesteps=3667000, episode_reward=1347.60 +/- 17.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3667000  |
---------------------------------
Eval num_timesteps=3668000, episode_reward=1337.11 +/- 20.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.34e+03 |
| time/              |          |
|    total_timesteps | 3668000  |
| train/             |          |
|    actor_loss      | -0.594   |
|    critic_loss     | 0.000144 |
|    ent_coef        | 0.000207 |
|    ent_coef_loss   | 4.31     |
|    learning_rate   | 0.00133  |
|    n_updates       | 17910    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 274      |
| time/              |          |
|    episodes        | 3668     |
|    fps             | 641      |
|    time_elapsed    | 5714     |
|    total_timesteps | 3668000  |
---------------------------------
Eval num_timesteps=3669000, episode_reward=1348.82 +/- 19.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3669000  |
---------------------------------
Eval num_timesteps=3670000, episode_reward=1346.61 +/- 15.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.35e+03 |
| time/              |          |
|    total_timesteps | 3670000  |
---------------------------------
Eval num_timesteps=3671000, episode_reward=1308.55 +/- 16.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3671000  |
| train/             |          |
|    actor_loss      | -0.597   |
|    critic_loss     | 0.000135 |
|    ent_coef        | 0.000207 |
|    ent_coef_loss   | 7.12     |
|    learning_rate   | 0.00133  |
|    n_updates       | 17920    |
---------------------------------
Eval num_timesteps=3672000, episode_reward=1307.31 +/- 25.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.31e+03 |
| time/              |          |
|    total_timesteps | 3672000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 339      |
| time/              |          |
|    episodes        | 3672     |
|    fps             | 641      |
|    time_elapsed    | 5720     |
|    total_timesteps | 3672000  |
---------------------------------
Eval num_timesteps=3673000, episode_reward=1399.91 +/- 15.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 3673000  |
| train/             |          |
|    actor_loss      | -0.602   |
|    critic_loss     | 0.000141 |
|    ent_coef        | 0.000207 |
|    ent_coef_loss   | 6.46     |
|    learning_rate   | 0.00133  |
|    n_updates       | 17930    |
---------------------------------
Eval num_timesteps=3674000, episode_reward=1415.01 +/- 17.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 3674000  |
---------------------------------
Eval num_timesteps=3675000, episode_reward=1190.40 +/- 20.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.19e+03 |
| time/              |          |
|    total_timesteps | 3675000  |
| train/             |          |
|    actor_loss      | -0.605   |
|    critic_loss     | 0.000124 |
|    ent_coef        | 0.000208 |
|    ent_coef_loss   | 11.2     |
|    learning_rate   | 0.00133  |
|    n_updates       | 17940    |
---------------------------------
Eval num_timesteps=3676000, episode_reward=1195.29 +/- 11.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3676000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 404      |
| time/              |          |
|    episodes        | 3676     |
|    fps             | 641      |
|    time_elapsed    | 5726     |
|    total_timesteps | 3676000  |
---------------------------------
Eval num_timesteps=3677000, episode_reward=1204.21 +/- 13.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.2e+03  |
| time/              |          |
|    total_timesteps | 3677000  |
| train/             |          |
|    actor_loss      | -0.596   |
|    critic_loss     | 0.000125 |
|    ent_coef        | 0.000209 |
|    ent_coef_loss   | 7.6      |
|    learning_rate   | 0.00132  |
|    n_updates       | 17950    |
---------------------------------
Eval num_timesteps=3678000, episode_reward=1205.57 +/- 12.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.21e+03 |
| time/              |          |
|    total_timesteps | 3678000  |
---------------------------------
Eval num_timesteps=3679000, episode_reward=1231.70 +/- 16.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.23e+03 |
| time/              |          |
|    total_timesteps | 3679000  |
| train/             |          |
|    actor_loss      | -0.592   |
|    critic_loss     | 0.00013  |
|    ent_coef        | 0.00021  |
|    ent_coef_loss   | 5.5      |
|    learning_rate   | 0.00132  |
|    n_updates       | 17960    |
---------------------------------
Eval num_timesteps=3680000, episode_reward=1243.78 +/- 27.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.24e+03 |
| time/              |          |
|    total_timesteps | 3680000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 464      |
| time/              |          |
|    episodes        | 3680     |
|    fps             | 641      |
|    time_elapsed    | 5732     |
|    total_timesteps | 3680000  |
---------------------------------
Eval num_timesteps=3681000, episode_reward=1420.90 +/- 11.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 3681000  |
| train/             |          |
|    actor_loss      | -0.598   |
|    critic_loss     | 0.000141 |
|    ent_coef        | 0.000211 |
|    ent_coef_loss   | 5.13     |
|    learning_rate   | 0.00132  |
|    n_updates       | 17970    |
---------------------------------
New best mean reward!
Eval num_timesteps=3682000, episode_reward=1403.99 +/- 24.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.4e+03  |
| time/              |          |
|    total_timesteps | 3682000  |
---------------------------------
Eval num_timesteps=3683000, episode_reward=1415.09 +/- 25.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 3683000  |
| train/             |          |
|    actor_loss      | -0.609   |
|    critic_loss     | 0.000156 |
|    ent_coef        | 0.000212 |
|    ent_coef_loss   | 8.49     |
|    learning_rate   | 0.00132  |
|    n_updates       | 17980    |
---------------------------------
Eval num_timesteps=3684000, episode_reward=1427.76 +/- 11.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.43e+03 |
| time/              |          |
|    total_timesteps | 3684000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 530      |
| time/              |          |
|    episodes        | 3684     |
|    fps             | 641      |
|    time_elapsed    | 5739     |
|    total_timesteps | 3684000  |
---------------------------------
Eval num_timesteps=3685000, episode_reward=1485.51 +/- 21.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.49e+03 |
| time/              |          |
|    total_timesteps | 3685000  |
| train/             |          |
|    actor_loss      | -0.609   |
|    critic_loss     | 0.000154 |
|    ent_coef        | 0.000212 |
|    ent_coef_loss   | 8.03     |
|    learning_rate   | 0.00132  |
|    n_updates       | 17990    |
---------------------------------
New best mean reward!
Eval num_timesteps=3686000, episode_reward=1495.83 +/- 9.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.5e+03  |
| time/              |          |
|    total_timesteps | 3686000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3687000, episode_reward=1416.33 +/- 8.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.42e+03 |
| time/              |          |
|    total_timesteps | 3687000  |
| train/             |          |
|    actor_loss      | -0.619   |
|    critic_loss     | 0.000166 |
|    ent_coef        | 0.000213 |
|    ent_coef_loss   | 10.2     |
|    learning_rate   | 0.00131  |
|    n_updates       | 18000    |
---------------------------------
Eval num_timesteps=3688000, episode_reward=1405.14 +/- 21.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.41e+03 |
| time/              |          |
|    total_timesteps | 3688000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 597      |
| time/              |          |
|    episodes        | 3688     |
|    fps             | 641      |
|    time_elapsed    | 5746     |
|    total_timesteps | 3688000  |
---------------------------------
Eval num_timesteps=3689000, episode_reward=1457.50 +/- 21.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3689000  |
| train/             |          |
|    actor_loss      | -0.606   |
|    critic_loss     | 0.000151 |
|    ent_coef        | 0.000214 |
|    ent_coef_loss   | 2.25     |
|    learning_rate   | 0.00131  |
|    n_updates       | 18010    |
---------------------------------
Eval num_timesteps=3690000, episode_reward=1463.14 +/- 27.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3690000  |
---------------------------------
Eval num_timesteps=3691000, episode_reward=1374.71 +/- 34.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3691000  |
| train/             |          |
|    actor_loss      | -0.615   |
|    critic_loss     | 0.000164 |
|    ent_coef        | 0.000215 |
|    ent_coef_loss   | 5.42     |
|    learning_rate   | 0.00131  |
|    n_updates       | 18020    |
---------------------------------
Eval num_timesteps=3692000, episode_reward=1386.75 +/- 27.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3692000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 662      |
| time/              |          |
|    episodes        | 3692     |
|    fps             | 641      |
|    time_elapsed    | 5752     |
|    total_timesteps | 3692000  |
---------------------------------
Eval num_timesteps=3693000, episode_reward=1448.01 +/- 30.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.45e+03 |
| time/              |          |
|    total_timesteps | 3693000  |
| train/             |          |
|    actor_loss      | -0.61    |
|    critic_loss     | 0.000141 |
|    ent_coef        | 0.000215 |
|    ent_coef_loss   | 2.35     |
|    learning_rate   | 0.00131  |
|    n_updates       | 18030    |
---------------------------------
Eval num_timesteps=3694000, episode_reward=1436.68 +/- 8.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 3694000  |
---------------------------------
Eval num_timesteps=3695000, episode_reward=1370.42 +/- 8.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3695000  |
| train/             |          |
|    actor_loss      | -0.617   |
|    critic_loss     | 0.000154 |
|    ent_coef        | 0.000216 |
|    ent_coef_loss   | 8.89     |
|    learning_rate   | 0.00131  |
|    n_updates       | 18040    |
---------------------------------
Eval num_timesteps=3696000, episode_reward=1357.32 +/- 15.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3696000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 723      |
| time/              |          |
|    episodes        | 3696     |
|    fps             | 641      |
|    time_elapsed    | 5759     |
|    total_timesteps | 3696000  |
---------------------------------
Eval num_timesteps=3697000, episode_reward=1455.02 +/- 24.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.46e+03 |
| time/              |          |
|    total_timesteps | 3697000  |
| train/             |          |
|    actor_loss      | -0.608   |
|    critic_loss     | 0.000149 |
|    ent_coef        | 0.000217 |
|    ent_coef_loss   | 5.81     |
|    learning_rate   | 0.0013   |
|    n_updates       | 18050    |
---------------------------------
Eval num_timesteps=3698000, episode_reward=1443.54 +/- 26.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.44e+03 |
| time/              |          |
|    total_timesteps | 3698000  |
---------------------------------
Eval num_timesteps=3699000, episode_reward=1543.83 +/- 32.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.54e+03 |
| time/              |          |
|    total_timesteps | 3699000  |
| train/             |          |
|    actor_loss      | -0.622   |
|    critic_loss     | 0.000171 |
|    ent_coef        | 0.000218 |
|    ent_coef_loss   | 14.2     |
|    learning_rate   | 0.0013   |
|    n_updates       | 18060    |
---------------------------------
New best mean reward!
Eval num_timesteps=3700000, episode_reward=1547.01 +/- 31.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.55e+03 |
| time/              |          |
|    total_timesteps | 3700000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 786      |
| time/              |          |
|    episodes        | 3700     |
|    fps             | 641      |
|    time_elapsed    | 5765     |
|    total_timesteps | 3700000  |
---------------------------------
Eval num_timesteps=3701000, episode_reward=1561.14 +/- 25.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3701000  |
| train/             |          |
|    actor_loss      | -0.622   |
|    critic_loss     | 0.000176 |
|    ent_coef        | 0.000219 |
|    ent_coef_loss   | 9.15     |
|    learning_rate   | 0.0013   |
|    n_updates       | 18070    |
---------------------------------
New best mean reward!
Eval num_timesteps=3702000, episode_reward=1557.36 +/- 19.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 3702000  |
---------------------------------
Eval num_timesteps=3703000, episode_reward=1386.66 +/- 12.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.39e+03 |
| time/              |          |
|    total_timesteps | 3703000  |
| train/             |          |
|    actor_loss      | -0.628   |
|    critic_loss     | 0.000176 |
|    ent_coef        | 0.00022  |
|    ent_coef_loss   | 15.9     |
|    learning_rate   | 0.0013   |
|    n_updates       | 18080    |
---------------------------------
Eval num_timesteps=3704000, episode_reward=1372.46 +/- 29.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.37e+03 |
| time/              |          |
|    total_timesteps | 3704000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 847      |
| time/              |          |
|    episodes        | 3704     |
|    fps             | 641      |
|    time_elapsed    | 5773     |
|    total_timesteps | 3704000  |
---------------------------------
Eval num_timesteps=3705000, episode_reward=1520.81 +/- 11.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.52e+03 |
| time/              |          |
|    total_timesteps | 3705000  |
| train/             |          |
|    actor_loss      | -0.622   |
|    critic_loss     | 0.000155 |
|    ent_coef        | 0.000222 |
|    ent_coef_loss   | 8.85     |
|    learning_rate   | 0.0013   |
|    n_updates       | 18090    |
---------------------------------
Eval num_timesteps=3706000, episode_reward=1530.64 +/- 22.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.53e+03 |
| time/              |          |
|    total_timesteps | 3706000  |
---------------------------------
Eval num_timesteps=3707000, episode_reward=1608.68 +/- 23.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.61e+03 |
| time/              |          |
|    total_timesteps | 3707000  |
| train/             |          |
|    actor_loss      | -0.628   |
|    critic_loss     | 0.000187 |
|    ent_coef        | 0.000223 |
|    ent_coef_loss   | 12.7     |
|    learning_rate   | 0.00129  |
|    n_updates       | 18100    |
---------------------------------
New best mean reward!
Eval num_timesteps=3708000, episode_reward=1601.58 +/- 28.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 3708000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 907      |
| time/              |          |
|    episodes        | 3708     |
|    fps             | 641      |
|    time_elapsed    | 5780     |
|    total_timesteps | 3708000  |
---------------------------------
Eval num_timesteps=3709000, episode_reward=1618.17 +/- 10.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 3709000  |
| train/             |          |
|    actor_loss      | -0.638   |
|    critic_loss     | 0.000179 |
|    ent_coef        | 0.000225 |
|    ent_coef_loss   | 16.3     |
|    learning_rate   | 0.00129  |
|    n_updates       | 18110    |
---------------------------------
New best mean reward!
Eval num_timesteps=3710000, episode_reward=1618.23 +/- 10.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 3710000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3711000, episode_reward=1660.36 +/- 38.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 3711000  |
| train/             |          |
|    actor_loss      | -0.635   |
|    critic_loss     | 0.000174 |
|    ent_coef        | 0.000227 |
|    ent_coef_loss   | 9.34     |
|    learning_rate   | 0.00129  |
|    n_updates       | 18120    |
---------------------------------
New best mean reward!
Eval num_timesteps=3712000, episode_reward=1649.55 +/- 35.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.65e+03 |
| time/              |          |
|    total_timesteps | 3712000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 961      |
| time/              |          |
|    episodes        | 3712     |
|    fps             | 641      |
|    time_elapsed    | 5786     |
|    total_timesteps | 3712000  |
---------------------------------
Eval num_timesteps=3713000, episode_reward=1667.87 +/- 32.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.67e+03 |
| time/              |          |
|    total_timesteps | 3713000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3714000, episode_reward=1576.40 +/- 14.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.58e+03 |
| time/              |          |
|    total_timesteps | 3714000  |
| train/             |          |
|    actor_loss      | -0.64    |
|    critic_loss     | 0.000214 |
|    ent_coef        | 0.000228 |
|    ent_coef_loss   | 14.3     |
|    learning_rate   | 0.00129  |
|    n_updates       | 18130    |
---------------------------------
Eval num_timesteps=3715000, episode_reward=1595.79 +/- 32.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.6e+03  |
| time/              |          |
|    total_timesteps | 3715000  |
---------------------------------
Eval num_timesteps=3716000, episode_reward=1620.73 +/- 10.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 3716000  |
| train/             |          |
|    actor_loss      | -0.633   |
|    critic_loss     | 0.000178 |
|    ent_coef        | 0.00023  |
|    ent_coef_loss   | 9.03     |
|    learning_rate   | 0.00128  |
|    n_updates       | 18140    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.01e+03 |
| time/              |          |
|    episodes        | 3716     |
|    fps             | 641      |
|    time_elapsed    | 5793     |
|    total_timesteps | 3716000  |
---------------------------------
Eval num_timesteps=3717000, episode_reward=1634.75 +/- 28.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.63e+03 |
| time/              |          |
|    total_timesteps | 3717000  |
---------------------------------
Eval num_timesteps=3718000, episode_reward=1805.71 +/- 36.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 3718000  |
| train/             |          |
|    actor_loss      | -0.632   |
|    critic_loss     | 0.000204 |
|    ent_coef        | 0.000231 |
|    ent_coef_loss   | 9.67     |
|    learning_rate   | 0.00128  |
|    n_updates       | 18150    |
---------------------------------
New best mean reward!
Eval num_timesteps=3719000, episode_reward=1796.90 +/- 19.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3719000  |
---------------------------------
Eval num_timesteps=3720000, episode_reward=1759.76 +/- 17.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3720000  |
| train/             |          |
|    actor_loss      | -0.646   |
|    critic_loss     | 0.000217 |
|    ent_coef        | 0.000232 |
|    ent_coef_loss   | 13.4     |
|    learning_rate   | 0.00128  |
|    n_updates       | 18160    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.06e+03 |
| time/              |          |
|    episodes        | 3720     |
|    fps             | 641      |
|    time_elapsed    | 5799     |
|    total_timesteps | 3720000  |
---------------------------------
Eval num_timesteps=3721000, episode_reward=1743.50 +/- 26.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3721000  |
---------------------------------
Eval num_timesteps=3722000, episode_reward=1788.64 +/- 16.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 3722000  |
| train/             |          |
|    actor_loss      | -0.647   |
|    critic_loss     | 0.000206 |
|    ent_coef        | 0.000234 |
|    ent_coef_loss   | 8.4      |
|    learning_rate   | 0.00128  |
|    n_updates       | 18170    |
---------------------------------
Eval num_timesteps=3723000, episode_reward=1788.36 +/- 36.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 3723000  |
---------------------------------
Eval num_timesteps=3724000, episode_reward=1773.95 +/- 21.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3724000  |
| train/             |          |
|    actor_loss      | -0.649   |
|    critic_loss     | 0.000239 |
|    ent_coef        | 0.000235 |
|    ent_coef_loss   | 6.09     |
|    learning_rate   | 0.00128  |
|    n_updates       | 18180    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.11e+03 |
| time/              |          |
|    episodes        | 3724     |
|    fps             | 641      |
|    time_elapsed    | 5805     |
|    total_timesteps | 3724000  |
---------------------------------
Eval num_timesteps=3725000, episode_reward=1768.25 +/- 31.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.77e+03 |
| time/              |          |
|    total_timesteps | 3725000  |
---------------------------------
Eval num_timesteps=3726000, episode_reward=1736.33 +/- 36.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3726000  |
| train/             |          |
|    actor_loss      | -0.648   |
|    critic_loss     | 0.000209 |
|    ent_coef        | 0.000236 |
|    ent_coef_loss   | 11.6     |
|    learning_rate   | 0.00127  |
|    n_updates       | 18190    |
---------------------------------
Eval num_timesteps=3727000, episode_reward=1755.33 +/- 35.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3727000  |
---------------------------------
Eval num_timesteps=3728000, episode_reward=1839.40 +/- 33.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 3728000  |
| train/             |          |
|    actor_loss      | -0.645   |
|    critic_loss     | 0.000185 |
|    ent_coef        | 0.000238 |
|    ent_coef_loss   | 6.55     |
|    learning_rate   | 0.00127  |
|    n_updates       | 18200    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.16e+03 |
| time/              |          |
|    episodes        | 3728     |
|    fps             | 641      |
|    time_elapsed    | 5812     |
|    total_timesteps | 3728000  |
---------------------------------
Eval num_timesteps=3729000, episode_reward=1804.81 +/- 26.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3729000  |
---------------------------------
Eval num_timesteps=3730000, episode_reward=1723.87 +/- 9.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.72e+03 |
| time/              |          |
|    total_timesteps | 3730000  |
| train/             |          |
|    actor_loss      | -0.655   |
|    critic_loss     | 0.000227 |
|    ent_coef        | 0.000239 |
|    ent_coef_loss   | 14.5     |
|    learning_rate   | 0.00127  |
|    n_updates       | 18210    |
---------------------------------
Eval num_timesteps=3731000, episode_reward=1698.72 +/- 40.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.7e+03  |
| time/              |          |
|    total_timesteps | 3731000  |
---------------------------------
Eval num_timesteps=3732000, episode_reward=1762.73 +/- 21.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3732000  |
| train/             |          |
|    actor_loss      | -0.65    |
|    critic_loss     | 0.000204 |
|    ent_coef        | 0.000241 |
|    ent_coef_loss   | 9.72     |
|    learning_rate   | 0.00127  |
|    n_updates       | 18220    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.21e+03 |
| time/              |          |
|    episodes        | 3732     |
|    fps             | 641      |
|    time_elapsed    | 5818     |
|    total_timesteps | 3732000  |
---------------------------------
Eval num_timesteps=3733000, episode_reward=1754.54 +/- 20.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 3733000  |
---------------------------------
Eval num_timesteps=3734000, episode_reward=1799.58 +/- 13.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3734000  |
| train/             |          |
|    actor_loss      | -0.657   |
|    critic_loss     | 0.000235 |
|    ent_coef        | 0.000242 |
|    ent_coef_loss   | 11.9     |
|    learning_rate   | 0.00127  |
|    n_updates       | 18230    |
---------------------------------
Eval num_timesteps=3735000, episode_reward=1794.74 +/- 28.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 3735000  |
---------------------------------
Eval num_timesteps=3736000, episode_reward=1833.79 +/- 42.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3736000  |
| train/             |          |
|    actor_loss      | -0.652   |
|    critic_loss     | 0.00024  |
|    ent_coef        | 0.000244 |
|    ent_coef_loss   | 14.1     |
|    learning_rate   | 0.00126  |
|    n_updates       | 18240    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.26e+03 |
| time/              |          |
|    episodes        | 3736     |
|    fps             | 641      |
|    time_elapsed    | 5824     |
|    total_timesteps | 3736000  |
---------------------------------
Eval num_timesteps=3737000, episode_reward=1827.38 +/- 31.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3737000  |
---------------------------------
Eval num_timesteps=3738000, episode_reward=1764.90 +/- 38.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3738000  |
| train/             |          |
|    actor_loss      | -0.665   |
|    critic_loss     | 0.000237 |
|    ent_coef        | 0.000246 |
|    ent_coef_loss   | 13.4     |
|    learning_rate   | 0.00126  |
|    n_updates       | 18250    |
---------------------------------
Eval num_timesteps=3739000, episode_reward=1750.85 +/- 15.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.75e+03 |
| time/              |          |
|    total_timesteps | 3739000  |
---------------------------------
Eval num_timesteps=3740000, episode_reward=1968.94 +/- 45.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3740000  |
| train/             |          |
|    actor_loss      | -0.657   |
|    critic_loss     | 0.000216 |
|    ent_coef        | 0.000247 |
|    ent_coef_loss   | 5.33     |
|    learning_rate   | 0.00126  |
|    n_updates       | 18260    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.31e+03 |
| time/              |          |
|    episodes        | 3740     |
|    fps             | 641      |
|    time_elapsed    | 5830     |
|    total_timesteps | 3740000  |
---------------------------------
Eval num_timesteps=3741000, episode_reward=1983.02 +/- 7.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3741000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3742000, episode_reward=1957.33 +/- 15.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3742000  |
| train/             |          |
|    actor_loss      | -0.674   |
|    critic_loss     | 0.00026  |
|    ent_coef        | 0.000249 |
|    ent_coef_loss   | 14.5     |
|    learning_rate   | 0.00126  |
|    n_updates       | 18270    |
---------------------------------
Eval num_timesteps=3743000, episode_reward=1992.03 +/- 33.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3743000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3744000, episode_reward=1941.72 +/- 19.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 3744000  |
| train/             |          |
|    actor_loss      | -0.667   |
|    critic_loss     | 0.000291 |
|    ent_coef        | 0.00025  |
|    ent_coef_loss   | 17.5     |
|    learning_rate   | 0.00126  |
|    n_updates       | 18280    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.36e+03 |
| time/              |          |
|    episodes        | 3744     |
|    fps             | 641      |
|    time_elapsed    | 5837     |
|    total_timesteps | 3744000  |
---------------------------------
Eval num_timesteps=3745000, episode_reward=1919.36 +/- 30.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3745000  |
---------------------------------
Eval num_timesteps=3746000, episode_reward=1755.82 +/- 34.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 3746000  |
| train/             |          |
|    actor_loss      | -0.673   |
|    critic_loss     | 0.000234 |
|    ent_coef        | 0.000253 |
|    ent_coef_loss   | 18       |
|    learning_rate   | 0.00125  |
|    n_updates       | 18290    |
---------------------------------
Eval num_timesteps=3747000, episode_reward=1743.97 +/- 40.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.74e+03 |
| time/              |          |
|    total_timesteps | 3747000  |
---------------------------------
Eval num_timesteps=3748000, episode_reward=1980.36 +/- 18.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3748000  |
| train/             |          |
|    actor_loss      | -0.655   |
|    critic_loss     | 0.000254 |
|    ent_coef        | 0.000255 |
|    ent_coef_loss   | 8.05     |
|    learning_rate   | 0.00125  |
|    n_updates       | 18300    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.4e+03  |
| time/              |          |
|    episodes        | 3748     |
|    fps             | 641      |
|    time_elapsed    | 5843     |
|    total_timesteps | 3748000  |
---------------------------------
Eval num_timesteps=3749000, episode_reward=1973.13 +/- 15.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3749000  |
---------------------------------
Eval num_timesteps=3750000, episode_reward=1827.52 +/- 19.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3750000  |
| train/             |          |
|    actor_loss      | -0.676   |
|    critic_loss     | 0.000264 |
|    ent_coef        | 0.000257 |
|    ent_coef_loss   | 20.2     |
|    learning_rate   | 0.00125  |
|    n_updates       | 18310    |
---------------------------------
Eval num_timesteps=3751000, episode_reward=1834.12 +/- 20.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3751000  |
---------------------------------
Eval num_timesteps=3752000, episode_reward=1889.86 +/- 51.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 3752000  |
| train/             |          |
|    actor_loss      | -0.664   |
|    critic_loss     | 0.00025  |
|    ent_coef        | 0.00026  |
|    ent_coef_loss   | 7.89     |
|    learning_rate   | 0.00125  |
|    n_updates       | 18320    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.45e+03 |
| time/              |          |
|    episodes        | 3752     |
|    fps             | 641      |
|    time_elapsed    | 5849     |
|    total_timesteps | 3752000  |
---------------------------------
Eval num_timesteps=3753000, episode_reward=1871.48 +/- 31.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 3753000  |
---------------------------------
Eval num_timesteps=3754000, episode_reward=1786.49 +/- 12.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 3754000  |
| train/             |          |
|    actor_loss      | -0.669   |
|    critic_loss     | 0.000264 |
|    ent_coef        | 0.000261 |
|    ent_coef_loss   | 9.64     |
|    learning_rate   | 0.00125  |
|    n_updates       | 18330    |
---------------------------------
Eval num_timesteps=3755000, episode_reward=1798.21 +/- 42.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3755000  |
---------------------------------
Eval num_timesteps=3756000, episode_reward=1792.40 +/- 22.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.79e+03 |
| time/              |          |
|    total_timesteps | 3756000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.49e+03 |
| time/              |          |
|    episodes        | 3756     |
|    fps             | 641      |
|    time_elapsed    | 5856     |
|    total_timesteps | 3756000  |
---------------------------------
Eval num_timesteps=3757000, episode_reward=1963.22 +/- 56.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3757000  |
| train/             |          |
|    actor_loss      | -0.666   |
|    critic_loss     | 0.000289 |
|    ent_coef        | 0.000263 |
|    ent_coef_loss   | 10.2     |
|    learning_rate   | 0.00124  |
|    n_updates       | 18340    |
---------------------------------
Eval num_timesteps=3758000, episode_reward=1941.16 +/- 38.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 3758000  |
---------------------------------
Eval num_timesteps=3759000, episode_reward=1869.71 +/- 15.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.87e+03 |
| time/              |          |
|    total_timesteps | 3759000  |
| train/             |          |
|    actor_loss      | -0.675   |
|    critic_loss     | 0.000281 |
|    ent_coef        | 0.000265 |
|    ent_coef_loss   | 12       |
|    learning_rate   | 0.00124  |
|    n_updates       | 18350    |
---------------------------------
Eval num_timesteps=3760000, episode_reward=1893.88 +/- 12.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.89e+03 |
| time/              |          |
|    total_timesteps | 3760000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.54e+03 |
| time/              |          |
|    episodes        | 3760     |
|    fps             | 641      |
|    time_elapsed    | 5862     |
|    total_timesteps | 3760000  |
---------------------------------
Eval num_timesteps=3761000, episode_reward=1858.39 +/- 21.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.86e+03 |
| time/              |          |
|    total_timesteps | 3761000  |
| train/             |          |
|    actor_loss      | -0.668   |
|    critic_loss     | 0.000306 |
|    ent_coef        | 0.000266 |
|    ent_coef_loss   | 12.5     |
|    learning_rate   | 0.00124  |
|    n_updates       | 18360    |
---------------------------------
Eval num_timesteps=3762000, episode_reward=1840.24 +/- 28.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.84e+03 |
| time/              |          |
|    total_timesteps | 3762000  |
---------------------------------
Eval num_timesteps=3763000, episode_reward=1993.97 +/- 64.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3763000  |
| train/             |          |
|    actor_loss      | -0.664   |
|    critic_loss     | 0.000275 |
|    ent_coef        | 0.000268 |
|    ent_coef_loss   | 12.1     |
|    learning_rate   | 0.00124  |
|    n_updates       | 18370    |
---------------------------------
New best mean reward!
Eval num_timesteps=3764000, episode_reward=1932.44 +/- 28.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3764000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.57e+03 |
| time/              |          |
|    episodes        | 3764     |
|    fps             | 641      |
|    time_elapsed    | 5868     |
|    total_timesteps | 3764000  |
---------------------------------
Eval num_timesteps=3765000, episode_reward=2040.24 +/- 9.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 3765000  |
| train/             |          |
|    actor_loss      | -0.678   |
|    critic_loss     | 0.00029  |
|    ent_coef        | 0.00027  |
|    ent_coef_loss   | 9.26     |
|    learning_rate   | 0.00124  |
|    n_updates       | 18380    |
---------------------------------
New best mean reward!
Eval num_timesteps=3766000, episode_reward=2058.55 +/- 44.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.06e+03 |
| time/              |          |
|    total_timesteps | 3766000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3767000, episode_reward=1990.59 +/- 23.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3767000  |
| train/             |          |
|    actor_loss      | -0.687   |
|    critic_loss     | 0.000247 |
|    ent_coef        | 0.000272 |
|    ent_coef_loss   | 12.6     |
|    learning_rate   | 0.00123  |
|    n_updates       | 18390    |
---------------------------------
Eval num_timesteps=3768000, episode_reward=1974.59 +/- 30.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3768000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.6e+03  |
| time/              |          |
|    episodes        | 3768     |
|    fps             | 641      |
|    time_elapsed    | 5875     |
|    total_timesteps | 3768000  |
---------------------------------
Eval num_timesteps=3769000, episode_reward=1990.49 +/- 25.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3769000  |
| train/             |          |
|    actor_loss      | -0.678   |
|    critic_loss     | 0.000319 |
|    ent_coef        | 0.000274 |
|    ent_coef_loss   | 9.4      |
|    learning_rate   | 0.00123  |
|    n_updates       | 18400    |
---------------------------------
Eval num_timesteps=3770000, episode_reward=1984.79 +/- 31.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3770000  |
---------------------------------
Eval num_timesteps=3771000, episode_reward=1966.08 +/- 50.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3771000  |
| train/             |          |
|    actor_loss      | -0.684   |
|    critic_loss     | 0.000266 |
|    ent_coef        | 0.000276 |
|    ent_coef_loss   | 13.9     |
|    learning_rate   | 0.00123  |
|    n_updates       | 18410    |
---------------------------------
Eval num_timesteps=3772000, episode_reward=1981.57 +/- 37.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3772000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.63e+03 |
| time/              |          |
|    episodes        | 3772     |
|    fps             | 641      |
|    time_elapsed    | 5881     |
|    total_timesteps | 3772000  |
---------------------------------
Eval num_timesteps=3773000, episode_reward=1937.84 +/- 37.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 3773000  |
| train/             |          |
|    actor_loss      | -0.684   |
|    critic_loss     | 0.000268 |
|    ent_coef        | 0.000278 |
|    ent_coef_loss   | 10       |
|    learning_rate   | 0.00123  |
|    n_updates       | 18420    |
---------------------------------
Eval num_timesteps=3774000, episode_reward=1931.58 +/- 40.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3774000  |
---------------------------------
Eval num_timesteps=3775000, episode_reward=1928.51 +/- 25.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3775000  |
| train/             |          |
|    actor_loss      | -0.674   |
|    critic_loss     | 0.000309 |
|    ent_coef        | 0.000279 |
|    ent_coef_loss   | 8.45     |
|    learning_rate   | 0.00123  |
|    n_updates       | 18430    |
---------------------------------
Eval num_timesteps=3776000, episode_reward=1915.15 +/- 36.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3776000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.65e+03 |
| time/              |          |
|    episodes        | 3776     |
|    fps             | 641      |
|    time_elapsed    | 5887     |
|    total_timesteps | 3776000  |
---------------------------------
Eval num_timesteps=3777000, episode_reward=1910.34 +/- 17.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 3777000  |
| train/             |          |
|    actor_loss      | -0.676   |
|    critic_loss     | 0.000334 |
|    ent_coef        | 0.000281 |
|    ent_coef_loss   | 10.9     |
|    learning_rate   | 0.00122  |
|    n_updates       | 18440    |
---------------------------------
Eval num_timesteps=3778000, episode_reward=1922.37 +/- 29.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.92e+03 |
| time/              |          |
|    total_timesteps | 3778000  |
---------------------------------
Eval num_timesteps=3779000, episode_reward=1959.34 +/- 44.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.96e+03 |
| time/              |          |
|    total_timesteps | 3779000  |
| train/             |          |
|    actor_loss      | -0.677   |
|    critic_loss     | 0.000305 |
|    ent_coef        | 0.000283 |
|    ent_coef_loss   | 8.93     |
|    learning_rate   | 0.00122  |
|    n_updates       | 18450    |
---------------------------------
Eval num_timesteps=3780000, episode_reward=1914.20 +/- 21.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 3780000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.68e+03 |
| time/              |          |
|    episodes        | 3780     |
|    fps             | 641      |
|    time_elapsed    | 5894     |
|    total_timesteps | 3780000  |
---------------------------------
Eval num_timesteps=3781000, episode_reward=1913.92 +/- 17.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 3781000  |
| train/             |          |
|    actor_loss      | -0.671   |
|    critic_loss     | 0.000329 |
|    ent_coef        | 0.000285 |
|    ent_coef_loss   | 11.6     |
|    learning_rate   | 0.00122  |
|    n_updates       | 18460    |
---------------------------------
Eval num_timesteps=3782000, episode_reward=1878.93 +/- 20.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 3782000  |
---------------------------------
Eval num_timesteps=3783000, episode_reward=1971.38 +/- 26.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3783000  |
| train/             |          |
|    actor_loss      | -0.677   |
|    critic_loss     | 0.000309 |
|    ent_coef        | 0.000286 |
|    ent_coef_loss   | 12.4     |
|    learning_rate   | 0.00122  |
|    n_updates       | 18470    |
---------------------------------
Eval num_timesteps=3784000, episode_reward=1984.05 +/- 28.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3784000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.7e+03  |
| time/              |          |
|    episodes        | 3784     |
|    fps             | 641      |
|    time_elapsed    | 5900     |
|    total_timesteps | 3784000  |
---------------------------------
Eval num_timesteps=3785000, episode_reward=1908.61 +/- 49.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.91e+03 |
| time/              |          |
|    total_timesteps | 3785000  |
| train/             |          |
|    actor_loss      | -0.681   |
|    critic_loss     | 0.000286 |
|    ent_coef        | 0.000289 |
|    ent_coef_loss   | 7.73     |
|    learning_rate   | 0.00122  |
|    n_updates       | 18480    |
---------------------------------
Eval num_timesteps=3786000, episode_reward=1926.58 +/- 24.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3786000  |
---------------------------------
Eval num_timesteps=3787000, episode_reward=1803.61 +/- 31.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.8e+03  |
| time/              |          |
|    total_timesteps | 3787000  |
| train/             |          |
|    actor_loss      | -0.678   |
|    critic_loss     | 0.000275 |
|    ent_coef        | 0.00029  |
|    ent_coef_loss   | 3.81     |
|    learning_rate   | 0.00121  |
|    n_updates       | 18490    |
---------------------------------
Eval num_timesteps=3788000, episode_reward=1826.69 +/- 22.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3788000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.72e+03 |
| time/              |          |
|    episodes        | 3788     |
|    fps             | 641      |
|    time_elapsed    | 5906     |
|    total_timesteps | 3788000  |
---------------------------------
Eval num_timesteps=3789000, episode_reward=1808.21 +/- 37.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.81e+03 |
| time/              |          |
|    total_timesteps | 3789000  |
| train/             |          |
|    actor_loss      | -0.672   |
|    critic_loss     | 0.000293 |
|    ent_coef        | 0.000291 |
|    ent_coef_loss   | 4.25     |
|    learning_rate   | 0.00121  |
|    n_updates       | 18500    |
---------------------------------
Eval num_timesteps=3790000, episode_reward=1829.89 +/- 65.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.83e+03 |
| time/              |          |
|    total_timesteps | 3790000  |
---------------------------------
Eval num_timesteps=3791000, episode_reward=2067.55 +/- 56.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 3791000  |
| train/             |          |
|    actor_loss      | -0.675   |
|    critic_loss     | 0.000287 |
|    ent_coef        | 0.000292 |
|    ent_coef_loss   | 11.8     |
|    learning_rate   | 0.00121  |
|    n_updates       | 18510    |
---------------------------------
New best mean reward!
Eval num_timesteps=3792000, episode_reward=2072.72 +/- 38.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 3792000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.74e+03 |
| time/              |          |
|    episodes        | 3792     |
|    fps             | 641      |
|    time_elapsed    | 5912     |
|    total_timesteps | 3792000  |
---------------------------------
Eval num_timesteps=3793000, episode_reward=1937.64 +/- 69.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.94e+03 |
| time/              |          |
|    total_timesteps | 3793000  |
| train/             |          |
|    actor_loss      | -0.685   |
|    critic_loss     | 0.00028  |
|    ent_coef        | 0.000294 |
|    ent_coef_loss   | 12.4     |
|    learning_rate   | 0.00121  |
|    n_updates       | 18520    |
---------------------------------
Eval num_timesteps=3794000, episode_reward=2030.83 +/- 26.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.03e+03 |
| time/              |          |
|    total_timesteps | 3794000  |
---------------------------------
Eval num_timesteps=3795000, episode_reward=2019.40 +/- 19.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3795000  |
| train/             |          |
|    actor_loss      | -0.691   |
|    critic_loss     | 0.000321 |
|    ent_coef        | 0.000296 |
|    ent_coef_loss   | 13.2     |
|    learning_rate   | 0.00121  |
|    n_updates       | 18530    |
---------------------------------
Eval num_timesteps=3796000, episode_reward=2008.10 +/- 30.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 3796000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.76e+03 |
| time/              |          |
|    episodes        | 3796     |
|    fps             | 641      |
|    time_elapsed    | 5919     |
|    total_timesteps | 3796000  |
---------------------------------
Eval num_timesteps=3797000, episode_reward=2095.38 +/- 19.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 3797000  |
| train/             |          |
|    actor_loss      | -0.69    |
|    critic_loss     | 0.000282 |
|    ent_coef        | 0.000298 |
|    ent_coef_loss   | 12.3     |
|    learning_rate   | 0.0012   |
|    n_updates       | 18540    |
---------------------------------
New best mean reward!
Eval num_timesteps=3798000, episode_reward=2141.87 +/- 74.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 3798000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3799000, episode_reward=2173.17 +/- 39.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 3799000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3800000, episode_reward=2092.23 +/- 62.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 3800000  |
| train/             |          |
|    actor_loss      | -0.689   |
|    critic_loss     | 0.000332 |
|    ent_coef        | 0.000301 |
|    ent_coef_loss   | 11.2     |
|    learning_rate   | 0.0012   |
|    n_updates       | 18550    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.78e+03 |
| time/              |          |
|    episodes        | 3800     |
|    fps             | 641      |
|    time_elapsed    | 5925     |
|    total_timesteps | 3800000  |
---------------------------------
Eval num_timesteps=3801000, episode_reward=2138.55 +/- 108.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 3801000  |
---------------------------------
Eval num_timesteps=3802000, episode_reward=2132.42 +/- 57.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 3802000  |
| train/             |          |
|    actor_loss      | -0.697   |
|    critic_loss     | 0.000319 |
|    ent_coef        | 0.000303 |
|    ent_coef_loss   | 10.7     |
|    learning_rate   | 0.0012   |
|    n_updates       | 18560    |
---------------------------------
Eval num_timesteps=3803000, episode_reward=2190.37 +/- 57.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3803000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3804000, episode_reward=1993.04 +/- 33.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.99e+03 |
| time/              |          |
|    total_timesteps | 3804000  |
| train/             |          |
|    actor_loss      | -0.694   |
|    critic_loss     | 0.000307 |
|    ent_coef        | 0.000305 |
|    ent_coef_loss   | 11       |
|    learning_rate   | 0.0012   |
|    n_updates       | 18570    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.8e+03  |
| time/              |          |
|    episodes        | 3804     |
|    fps             | 641      |
|    time_elapsed    | 5932     |
|    total_timesteps | 3804000  |
---------------------------------
Eval num_timesteps=3805000, episode_reward=2036.97 +/- 45.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.04e+03 |
| time/              |          |
|    total_timesteps | 3805000  |
---------------------------------
Eval num_timesteps=3806000, episode_reward=2091.93 +/- 16.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 3806000  |
| train/             |          |
|    actor_loss      | -0.689   |
|    critic_loss     | 0.000317 |
|    ent_coef        | 0.000307 |
|    ent_coef_loss   | 9.23     |
|    learning_rate   | 0.00119  |
|    n_updates       | 18580    |
---------------------------------
Eval num_timesteps=3807000, episode_reward=2069.13 +/- 33.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 3807000  |
---------------------------------
Eval num_timesteps=3808000, episode_reward=2265.01 +/- 53.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 3808000  |
| train/             |          |
|    actor_loss      | -0.7     |
|    critic_loss     | 0.00032  |
|    ent_coef        | 0.000309 |
|    ent_coef_loss   | 12.9     |
|    learning_rate   | 0.00119  |
|    n_updates       | 18590    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.82e+03 |
| time/              |          |
|    episodes        | 3808     |
|    fps             | 641      |
|    time_elapsed    | 5938     |
|    total_timesteps | 3808000  |
---------------------------------
Eval num_timesteps=3809000, episode_reward=2266.79 +/- 30.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 3809000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3810000, episode_reward=2296.06 +/- 36.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 3810000  |
| train/             |          |
|    actor_loss      | -0.713   |
|    critic_loss     | 0.000326 |
|    ent_coef        | 0.000311 |
|    ent_coef_loss   | 13.1     |
|    learning_rate   | 0.00119  |
|    n_updates       | 18600    |
---------------------------------
New best mean reward!
Eval num_timesteps=3811000, episode_reward=2281.92 +/- 29.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 3811000  |
---------------------------------
Eval num_timesteps=3812000, episode_reward=2186.60 +/- 21.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3812000  |
| train/             |          |
|    actor_loss      | -0.709   |
|    critic_loss     | 0.000331 |
|    ent_coef        | 0.000314 |
|    ent_coef_loss   | 11       |
|    learning_rate   | 0.00119  |
|    n_updates       | 18610    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.84e+03 |
| time/              |          |
|    episodes        | 3812     |
|    fps             | 641      |
|    time_elapsed    | 5945     |
|    total_timesteps | 3812000  |
---------------------------------
Eval num_timesteps=3813000, episode_reward=2149.59 +/- 61.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 3813000  |
---------------------------------
Eval num_timesteps=3814000, episode_reward=2118.39 +/- 107.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 3814000  |
| train/             |          |
|    actor_loss      | -0.694   |
|    critic_loss     | 0.00035  |
|    ent_coef        | 0.000316 |
|    ent_coef_loss   | 11.6     |
|    learning_rate   | 0.00119  |
|    n_updates       | 18620    |
---------------------------------
Eval num_timesteps=3815000, episode_reward=2096.09 +/- 73.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 3815000  |
---------------------------------
Eval num_timesteps=3816000, episode_reward=2137.21 +/- 53.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 3816000  |
| train/             |          |
|    actor_loss      | -0.703   |
|    critic_loss     | 0.000303 |
|    ent_coef        | 0.000318 |
|    ent_coef_loss   | 12.2     |
|    learning_rate   | 0.00118  |
|    n_updates       | 18630    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.86e+03 |
| time/              |          |
|    episodes        | 3816     |
|    fps             | 641      |
|    time_elapsed    | 5952     |
|    total_timesteps | 3816000  |
---------------------------------
Eval num_timesteps=3817000, episode_reward=2101.68 +/- 29.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 3817000  |
---------------------------------
Eval num_timesteps=3818000, episode_reward=2185.31 +/- 34.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3818000  |
| train/             |          |
|    actor_loss      | -0.704   |
|    critic_loss     | 0.000343 |
|    ent_coef        | 0.000321 |
|    ent_coef_loss   | 11.2     |
|    learning_rate   | 0.00118  |
|    n_updates       | 18640    |
---------------------------------
Eval num_timesteps=3819000, episode_reward=2181.33 +/- 73.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 3819000  |
---------------------------------
Eval num_timesteps=3820000, episode_reward=2279.92 +/- 66.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 3820000  |
| train/             |          |
|    actor_loss      | -0.71    |
|    critic_loss     | 0.000335 |
|    ent_coef        | 0.000323 |
|    ent_coef_loss   | 14.1     |
|    learning_rate   | 0.00118  |
|    n_updates       | 18650    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.88e+03 |
| time/              |          |
|    episodes        | 3820     |
|    fps             | 641      |
|    time_elapsed    | 5958     |
|    total_timesteps | 3820000  |
---------------------------------
Eval num_timesteps=3821000, episode_reward=2243.43 +/- 28.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 3821000  |
---------------------------------
Eval num_timesteps=3822000, episode_reward=2322.24 +/- 26.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 3822000  |
| train/             |          |
|    actor_loss      | -0.71    |
|    critic_loss     | 0.000317 |
|    ent_coef        | 0.000326 |
|    ent_coef_loss   | 10.6     |
|    learning_rate   | 0.00118  |
|    n_updates       | 18660    |
---------------------------------
New best mean reward!
Eval num_timesteps=3823000, episode_reward=2322.95 +/- 38.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 3823000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3824000, episode_reward=2330.94 +/- 24.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 3824000  |
| train/             |          |
|    actor_loss      | -0.715   |
|    critic_loss     | 0.000333 |
|    ent_coef        | 0.000329 |
|    ent_coef_loss   | 14.8     |
|    learning_rate   | 0.00118  |
|    n_updates       | 18670    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.9e+03  |
| time/              |          |
|    episodes        | 3824     |
|    fps             | 641      |
|    time_elapsed    | 5965     |
|    total_timesteps | 3824000  |
---------------------------------
Eval num_timesteps=3825000, episode_reward=2332.46 +/- 36.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 3825000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3826000, episode_reward=2253.35 +/- 26.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 3826000  |
| train/             |          |
|    actor_loss      | -0.725   |
|    critic_loss     | 0.000352 |
|    ent_coef        | 0.000331 |
|    ent_coef_loss   | 15.7     |
|    learning_rate   | 0.00117  |
|    n_updates       | 18680    |
---------------------------------
Eval num_timesteps=3827000, episode_reward=2212.82 +/- 69.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 3827000  |
---------------------------------
Eval num_timesteps=3828000, episode_reward=2145.09 +/- 20.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 3828000  |
| train/             |          |
|    actor_loss      | -0.703   |
|    critic_loss     | 0.000361 |
|    ent_coef        | 0.000335 |
|    ent_coef_loss   | 11.1     |
|    learning_rate   | 0.00117  |
|    n_updates       | 18690    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.92e+03 |
| time/              |          |
|    episodes        | 3828     |
|    fps             | 641      |
|    time_elapsed    | 5971     |
|    total_timesteps | 3828000  |
---------------------------------
Eval num_timesteps=3829000, episode_reward=2115.10 +/- 85.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 3829000  |
---------------------------------
Eval num_timesteps=3830000, episode_reward=2155.84 +/- 42.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 3830000  |
| train/             |          |
|    actor_loss      | -0.704   |
|    critic_loss     | 0.000315 |
|    ent_coef        | 0.000337 |
|    ent_coef_loss   | 5.74     |
|    learning_rate   | 0.00117  |
|    n_updates       | 18700    |
---------------------------------
Eval num_timesteps=3831000, episode_reward=2161.20 +/- 26.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 3831000  |
---------------------------------
Eval num_timesteps=3832000, episode_reward=2119.54 +/- 49.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.12e+03 |
| time/              |          |
|    total_timesteps | 3832000  |
| train/             |          |
|    actor_loss      | -0.702   |
|    critic_loss     | 0.000324 |
|    ent_coef        | 0.000339 |
|    ent_coef_loss   | 6.56     |
|    learning_rate   | 0.00117  |
|    n_updates       | 18710    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.93e+03 |
| time/              |          |
|    episodes        | 3832     |
|    fps             | 641      |
|    time_elapsed    | 5977     |
|    total_timesteps | 3832000  |
---------------------------------
Eval num_timesteps=3833000, episode_reward=2172.51 +/- 42.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 3833000  |
---------------------------------
Eval num_timesteps=3834000, episode_reward=2233.87 +/- 30.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 3834000  |
| train/             |          |
|    actor_loss      | -0.701   |
|    critic_loss     | 0.00034  |
|    ent_coef        | 0.000341 |
|    ent_coef_loss   | 8.7      |
|    learning_rate   | 0.00117  |
|    n_updates       | 18720    |
---------------------------------
Eval num_timesteps=3835000, episode_reward=2219.39 +/- 66.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 3835000  |
---------------------------------
Eval num_timesteps=3836000, episode_reward=2132.85 +/- 71.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 3836000  |
| train/             |          |
|    actor_loss      | -0.7     |
|    critic_loss     | 0.000297 |
|    ent_coef        | 0.000342 |
|    ent_coef_loss   | 7.15     |
|    learning_rate   | 0.00116  |
|    n_updates       | 18730    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.94e+03 |
| time/              |          |
|    episodes        | 3836     |
|    fps             | 641      |
|    time_elapsed    | 5984     |
|    total_timesteps | 3836000  |
---------------------------------
Eval num_timesteps=3837000, episode_reward=2125.02 +/- 18.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 3837000  |
---------------------------------
Eval num_timesteps=3838000, episode_reward=1966.73 +/- 70.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.97e+03 |
| time/              |          |
|    total_timesteps | 3838000  |
| train/             |          |
|    actor_loss      | -0.708   |
|    critic_loss     | 0.000345 |
|    ent_coef        | 0.000344 |
|    ent_coef_loss   | 10.6     |
|    learning_rate   | 0.00116  |
|    n_updates       | 18740    |
---------------------------------
Eval num_timesteps=3839000, episode_reward=1999.34 +/- 32.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 3839000  |
---------------------------------
Eval num_timesteps=3840000, episode_reward=2023.86 +/- 90.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3840000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.94e+03 |
| time/              |          |
|    episodes        | 3840     |
|    fps             | 641      |
|    time_elapsed    | 5990     |
|    total_timesteps | 3840000  |
---------------------------------
Eval num_timesteps=3841000, episode_reward=2100.03 +/- 34.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.1e+03  |
| time/              |          |
|    total_timesteps | 3841000  |
| train/             |          |
|    actor_loss      | -0.695   |
|    critic_loss     | 0.000323 |
|    ent_coef        | 0.000346 |
|    ent_coef_loss   | 7.86     |
|    learning_rate   | 0.00116  |
|    n_updates       | 18750    |
---------------------------------
Eval num_timesteps=3842000, episode_reward=2086.77 +/- 57.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.09e+03 |
| time/              |          |
|    total_timesteps | 3842000  |
---------------------------------
Eval num_timesteps=3843000, episode_reward=2181.58 +/- 98.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 3843000  |
| train/             |          |
|    actor_loss      | -0.698   |
|    critic_loss     | 0.000348 |
|    ent_coef        | 0.000348 |
|    ent_coef_loss   | 5.56     |
|    learning_rate   | 0.00116  |
|    n_updates       | 18760    |
---------------------------------
Eval num_timesteps=3844000, episode_reward=2125.34 +/- 69.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 3844000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.95e+03 |
| time/              |          |
|    episodes        | 3844     |
|    fps             | 641      |
|    time_elapsed    | 5996     |
|    total_timesteps | 3844000  |
---------------------------------
Eval num_timesteps=3845000, episode_reward=2018.98 +/- 23.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 3845000  |
| train/             |          |
|    actor_loss      | -0.705   |
|    critic_loss     | 0.000385 |
|    ent_coef        | 0.00035  |
|    ent_coef_loss   | 6.94     |
|    learning_rate   | 0.00116  |
|    n_updates       | 18770    |
---------------------------------
Eval num_timesteps=3846000, episode_reward=2011.28 +/- 59.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.01e+03 |
| time/              |          |
|    total_timesteps | 3846000  |
---------------------------------
Eval num_timesteps=3847000, episode_reward=2069.11 +/- 57.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 3847000  |
| train/             |          |
|    actor_loss      | -0.702   |
|    critic_loss     | 0.00037  |
|    ent_coef        | 0.000351 |
|    ent_coef_loss   | 7.18     |
|    learning_rate   | 0.00115  |
|    n_updates       | 18780    |
---------------------------------
Eval num_timesteps=3848000, episode_reward=2053.88 +/- 45.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 3848000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.95e+03 |
| time/              |          |
|    episodes        | 3848     |
|    fps             | 641      |
|    time_elapsed    | 6002     |
|    total_timesteps | 3848000  |
---------------------------------
Eval num_timesteps=3849000, episode_reward=2194.21 +/- 20.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3849000  |
| train/             |          |
|    actor_loss      | -0.698   |
|    critic_loss     | 0.000299 |
|    ent_coef        | 0.000353 |
|    ent_coef_loss   | 3.82     |
|    learning_rate   | 0.00115  |
|    n_updates       | 18790    |
---------------------------------
Eval num_timesteps=3850000, episode_reward=2193.84 +/- 34.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3850000  |
---------------------------------
Eval num_timesteps=3851000, episode_reward=2151.09 +/- 29.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 3851000  |
| train/             |          |
|    actor_loss      | -0.708   |
|    critic_loss     | 0.00033  |
|    ent_coef        | 0.000354 |
|    ent_coef_loss   | 8.81     |
|    learning_rate   | 0.00115  |
|    n_updates       | 18800    |
---------------------------------
Eval num_timesteps=3852000, episode_reward=2146.15 +/- 31.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 3852000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.96e+03 |
| time/              |          |
|    episodes        | 3852     |
|    fps             | 641      |
|    time_elapsed    | 6008     |
|    total_timesteps | 3852000  |
---------------------------------
Eval num_timesteps=3853000, episode_reward=2176.06 +/- 32.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 3853000  |
| train/             |          |
|    actor_loss      | -0.71    |
|    critic_loss     | 0.000293 |
|    ent_coef        | 0.000356 |
|    ent_coef_loss   | 8.54     |
|    learning_rate   | 0.00115  |
|    n_updates       | 18810    |
---------------------------------
Eval num_timesteps=3854000, episode_reward=2157.65 +/- 39.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 3854000  |
---------------------------------
Eval num_timesteps=3855000, episode_reward=2215.60 +/- 44.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 3855000  |
| train/             |          |
|    actor_loss      | -0.707   |
|    critic_loss     | 0.000327 |
|    ent_coef        | 0.000358 |
|    ent_coef_loss   | 4.98     |
|    learning_rate   | 0.00115  |
|    n_updates       | 18820    |
---------------------------------
Eval num_timesteps=3856000, episode_reward=2239.08 +/- 28.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 3856000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.97e+03 |
| time/              |          |
|    episodes        | 3856     |
|    fps             | 641      |
|    time_elapsed    | 6015     |
|    total_timesteps | 3856000  |
---------------------------------
Eval num_timesteps=3857000, episode_reward=2249.70 +/- 20.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 3857000  |
| train/             |          |
|    actor_loss      | -0.72    |
|    critic_loss     | 0.000306 |
|    ent_coef        | 0.000359 |
|    ent_coef_loss   | 9.09     |
|    learning_rate   | 0.00114  |
|    n_updates       | 18830    |
---------------------------------
Eval num_timesteps=3858000, episode_reward=2214.36 +/- 41.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 3858000  |
---------------------------------
Eval num_timesteps=3859000, episode_reward=2279.86 +/- 66.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 3859000  |
| train/             |          |
|    actor_loss      | -0.731   |
|    critic_loss     | 0.000291 |
|    ent_coef        | 0.000361 |
|    ent_coef_loss   | 13.1     |
|    learning_rate   | 0.00114  |
|    n_updates       | 18840    |
---------------------------------
Eval num_timesteps=3860000, episode_reward=2274.18 +/- 39.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 3860000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.98e+03 |
| time/              |          |
|    episodes        | 3860     |
|    fps             | 641      |
|    time_elapsed    | 6021     |
|    total_timesteps | 3860000  |
---------------------------------
Eval num_timesteps=3861000, episode_reward=2160.98 +/- 24.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 3861000  |
| train/             |          |
|    actor_loss      | -0.718   |
|    critic_loss     | 0.000329 |
|    ent_coef        | 0.000364 |
|    ent_coef_loss   | 5.53     |
|    learning_rate   | 0.00114  |
|    n_updates       | 18850    |
---------------------------------
Eval num_timesteps=3862000, episode_reward=2194.15 +/- 44.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3862000  |
---------------------------------
Eval num_timesteps=3863000, episode_reward=2234.06 +/- 69.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 3863000  |
| train/             |          |
|    actor_loss      | -0.707   |
|    critic_loss     | 0.000295 |
|    ent_coef        | 0.000366 |
|    ent_coef_loss   | 2.63     |
|    learning_rate   | 0.00114  |
|    n_updates       | 18860    |
---------------------------------
Eval num_timesteps=3864000, episode_reward=2201.19 +/- 45.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 3864000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 1.99e+03 |
| time/              |          |
|    episodes        | 3864     |
|    fps             | 641      |
|    time_elapsed    | 6027     |
|    total_timesteps | 3864000  |
---------------------------------
Eval num_timesteps=3865000, episode_reward=2189.16 +/- 24.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 3865000  |
| train/             |          |
|    actor_loss      | -0.713   |
|    critic_loss     | 0.000328 |
|    ent_coef        | 0.000367 |
|    ent_coef_loss   | 2.9      |
|    learning_rate   | 0.00114  |
|    n_updates       | 18870    |
---------------------------------
Eval num_timesteps=3866000, episode_reward=2219.73 +/- 53.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 3866000  |
---------------------------------
Eval num_timesteps=3867000, episode_reward=2271.35 +/- 7.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 3867000  |
| train/             |          |
|    actor_loss      | -0.724   |
|    critic_loss     | 0.00036  |
|    ent_coef        | 0.000368 |
|    ent_coef_loss   | 6.79     |
|    learning_rate   | 0.00113  |
|    n_updates       | 18880    |
---------------------------------
Eval num_timesteps=3868000, episode_reward=2254.60 +/- 63.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 3868000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2e+03    |
| time/              |          |
|    episodes        | 3868     |
|    fps             | 641      |
|    time_elapsed    | 6033     |
|    total_timesteps | 3868000  |
---------------------------------
Eval num_timesteps=3869000, episode_reward=2219.15 +/- 53.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 3869000  |
| train/             |          |
|    actor_loss      | -0.727   |
|    critic_loss     | 0.000337 |
|    ent_coef        | 0.000369 |
|    ent_coef_loss   | 8.39     |
|    learning_rate   | 0.00113  |
|    n_updates       | 18890    |
---------------------------------
Eval num_timesteps=3870000, episode_reward=2228.20 +/- 48.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 3870000  |
---------------------------------
Eval num_timesteps=3871000, episode_reward=2333.90 +/- 49.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 3871000  |
| train/             |          |
|    actor_loss      | -0.726   |
|    critic_loss     | 0.000337 |
|    ent_coef        | 0.000371 |
|    ent_coef_loss   | 6.72     |
|    learning_rate   | 0.00113  |
|    n_updates       | 18900    |
---------------------------------
New best mean reward!
Eval num_timesteps=3872000, episode_reward=2382.41 +/- 52.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 3872000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.01e+03 |
| time/              |          |
|    episodes        | 3872     |
|    fps             | 641      |
|    time_elapsed    | 6040     |
|    total_timesteps | 3872000  |
---------------------------------
Eval num_timesteps=3873000, episode_reward=2408.46 +/- 26.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 3873000  |
| train/             |          |
|    actor_loss      | -0.733   |
|    critic_loss     | 0.000349 |
|    ent_coef        | 0.000373 |
|    ent_coef_loss   | 8.97     |
|    learning_rate   | 0.00113  |
|    n_updates       | 18910    |
---------------------------------
New best mean reward!
Eval num_timesteps=3874000, episode_reward=2403.05 +/- 23.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3874000  |
---------------------------------
Eval num_timesteps=3875000, episode_reward=2380.88 +/- 43.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 3875000  |
| train/             |          |
|    actor_loss      | -0.739   |
|    critic_loss     | 0.00033  |
|    ent_coef        | 0.000375 |
|    ent_coef_loss   | 8.29     |
|    learning_rate   | 0.00113  |
|    n_updates       | 18920    |
---------------------------------
Eval num_timesteps=3876000, episode_reward=2362.29 +/- 51.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 3876000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.03e+03 |
| time/              |          |
|    episodes        | 3876     |
|    fps             | 641      |
|    time_elapsed    | 6046     |
|    total_timesteps | 3876000  |
---------------------------------
Eval num_timesteps=3877000, episode_reward=2263.44 +/- 47.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 3877000  |
| train/             |          |
|    actor_loss      | -0.737   |
|    critic_loss     | 0.000342 |
|    ent_coef        | 0.000377 |
|    ent_coef_loss   | 9.45     |
|    learning_rate   | 0.00112  |
|    n_updates       | 18930    |
---------------------------------
Eval num_timesteps=3878000, episode_reward=2261.25 +/- 34.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 3878000  |
---------------------------------
Eval num_timesteps=3879000, episode_reward=2475.29 +/- 66.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3879000  |
| train/             |          |
|    actor_loss      | -0.73    |
|    critic_loss     | 0.000358 |
|    ent_coef        | 0.00038  |
|    ent_coef_loss   | 11       |
|    learning_rate   | 0.00112  |
|    n_updates       | 18940    |
---------------------------------
New best mean reward!
Eval num_timesteps=3880000, episode_reward=2466.95 +/- 54.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 3880000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.04e+03 |
| time/              |          |
|    episodes        | 3880     |
|    fps             | 641      |
|    time_elapsed    | 6053     |
|    total_timesteps | 3880000  |
---------------------------------
Eval num_timesteps=3881000, episode_reward=2524.13 +/- 53.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3881000  |
| train/             |          |
|    actor_loss      | -0.745   |
|    critic_loss     | 0.000347 |
|    ent_coef        | 0.000382 |
|    ent_coef_loss   | 12.8     |
|    learning_rate   | 0.00112  |
|    n_updates       | 18950    |
---------------------------------
New best mean reward!
Eval num_timesteps=3882000, episode_reward=2493.43 +/- 32.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 3882000  |
---------------------------------
Eval num_timesteps=3883000, episode_reward=2487.71 +/- 42.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.49e+03 |
| time/              |          |
|    total_timesteps | 3883000  |
---------------------------------
Eval num_timesteps=3884000, episode_reward=2325.79 +/- 66.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 3884000  |
| train/             |          |
|    actor_loss      | -0.747   |
|    critic_loss     | 0.000363 |
|    ent_coef        | 0.000386 |
|    ent_coef_loss   | 11.5     |
|    learning_rate   | 0.00112  |
|    n_updates       | 18960    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.06e+03 |
| time/              |          |
|    episodes        | 3884     |
|    fps             | 640      |
|    time_elapsed    | 6059     |
|    total_timesteps | 3884000  |
---------------------------------
Eval num_timesteps=3885000, episode_reward=2299.03 +/- 81.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 3885000  |
---------------------------------
Eval num_timesteps=3886000, episode_reward=2432.20 +/- 48.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 3886000  |
| train/             |          |
|    actor_loss      | -0.729   |
|    critic_loss     | 0.000322 |
|    ent_coef        | 0.000388 |
|    ent_coef_loss   | 5.32     |
|    learning_rate   | 0.00111  |
|    n_updates       | 18970    |
---------------------------------
Eval num_timesteps=3887000, episode_reward=2380.47 +/- 54.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 3887000  |
---------------------------------
Eval num_timesteps=3888000, episode_reward=2527.44 +/- 58.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 3888000  |
| train/             |          |
|    actor_loss      | -0.73    |
|    critic_loss     | 0.000318 |
|    ent_coef        | 0.000391 |
|    ent_coef_loss   | 6.86     |
|    learning_rate   | 0.00111  |
|    n_updates       | 18980    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.08e+03 |
| time/              |          |
|    episodes        | 3888     |
|    fps             | 640      |
|    time_elapsed    | 6065     |
|    total_timesteps | 3888000  |
---------------------------------
Eval num_timesteps=3889000, episode_reward=2469.52 +/- 47.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 3889000  |
---------------------------------
Eval num_timesteps=3890000, episode_reward=2476.16 +/- 48.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3890000  |
| train/             |          |
|    actor_loss      | -0.733   |
|    critic_loss     | 0.000308 |
|    ent_coef        | 0.000393 |
|    ent_coef_loss   | 8.9      |
|    learning_rate   | 0.00111  |
|    n_updates       | 18990    |
---------------------------------
Eval num_timesteps=3891000, episode_reward=2402.12 +/- 75.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3891000  |
---------------------------------
Eval num_timesteps=3892000, episode_reward=2404.78 +/- 52.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3892000  |
| train/             |          |
|    actor_loss      | -0.752   |
|    critic_loss     | 0.000354 |
|    ent_coef        | 0.000395 |
|    ent_coef_loss   | 11.6     |
|    learning_rate   | 0.00111  |
|    n_updates       | 19000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.1e+03  |
| time/              |          |
|    episodes        | 3892     |
|    fps             | 640      |
|    time_elapsed    | 6071     |
|    total_timesteps | 3892000  |
---------------------------------
Eval num_timesteps=3893000, episode_reward=2401.45 +/- 23.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 3893000  |
---------------------------------
Eval num_timesteps=3894000, episode_reward=2469.54 +/- 50.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 3894000  |
| train/             |          |
|    actor_loss      | -0.74    |
|    critic_loss     | 0.000345 |
|    ent_coef        | 0.000398 |
|    ent_coef_loss   | 9.26     |
|    learning_rate   | 0.00111  |
|    n_updates       | 19010    |
---------------------------------
Eval num_timesteps=3895000, episode_reward=2476.93 +/- 24.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3895000  |
---------------------------------
Eval num_timesteps=3896000, episode_reward=2320.90 +/- 27.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 3896000  |
| train/             |          |
|    actor_loss      | -0.744   |
|    critic_loss     | 0.000329 |
|    ent_coef        | 0.000401 |
|    ent_coef_loss   | 8.71     |
|    learning_rate   | 0.0011   |
|    n_updates       | 19020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.11e+03 |
| time/              |          |
|    episodes        | 3896     |
|    fps             | 640      |
|    time_elapsed    | 6078     |
|    total_timesteps | 3896000  |
---------------------------------
Eval num_timesteps=3897000, episode_reward=2340.08 +/- 55.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 3897000  |
---------------------------------
Eval num_timesteps=3898000, episode_reward=2459.10 +/- 39.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 3898000  |
| train/             |          |
|    actor_loss      | -0.734   |
|    critic_loss     | 0.00033  |
|    ent_coef        | 0.000403 |
|    ent_coef_loss   | 7.76     |
|    learning_rate   | 0.0011   |
|    n_updates       | 19030    |
---------------------------------
Eval num_timesteps=3899000, episode_reward=2430.70 +/- 69.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 3899000  |
---------------------------------
Eval num_timesteps=3900000, episode_reward=2510.38 +/- 19.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 3900000  |
| train/             |          |
|    actor_loss      | -0.749   |
|    critic_loss     | 0.000385 |
|    ent_coef        | 0.000405 |
|    ent_coef_loss   | 7.83     |
|    learning_rate   | 0.0011   |
|    n_updates       | 19040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.13e+03 |
| time/              |          |
|    episodes        | 3900     |
|    fps             | 640      |
|    time_elapsed    | 6084     |
|    total_timesteps | 3900000  |
---------------------------------
Eval num_timesteps=3901000, episode_reward=2470.03 +/- 23.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 3901000  |
---------------------------------
Eval num_timesteps=3902000, episode_reward=2323.23 +/- 49.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 3902000  |
| train/             |          |
|    actor_loss      | -0.742   |
|    critic_loss     | 0.000344 |
|    ent_coef        | 0.000408 |
|    ent_coef_loss   | 9.59     |
|    learning_rate   | 0.0011   |
|    n_updates       | 19050    |
---------------------------------
Eval num_timesteps=3903000, episode_reward=2334.57 +/- 40.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 3903000  |
---------------------------------
Eval num_timesteps=3904000, episode_reward=2552.99 +/- 32.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 3904000  |
| train/             |          |
|    actor_loss      | -0.736   |
|    critic_loss     | 0.00034  |
|    ent_coef        | 0.00041  |
|    ent_coef_loss   | 5.83     |
|    learning_rate   | 0.0011   |
|    n_updates       | 19060    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.14e+03 |
| time/              |          |
|    episodes        | 3904     |
|    fps             | 640      |
|    time_elapsed    | 6090     |
|    total_timesteps | 3904000  |
---------------------------------
Eval num_timesteps=3905000, episode_reward=2561.58 +/- 56.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 3905000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3906000, episode_reward=2483.73 +/- 88.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3906000  |
| train/             |          |
|    actor_loss      | -0.754   |
|    critic_loss     | 0.000351 |
|    ent_coef        | 0.000413 |
|    ent_coef_loss   | 13.1     |
|    learning_rate   | 0.00109  |
|    n_updates       | 19070    |
---------------------------------
Eval num_timesteps=3907000, episode_reward=2536.42 +/- 60.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 3907000  |
---------------------------------
Eval num_timesteps=3908000, episode_reward=2524.19 +/- 36.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.52e+03 |
| time/              |          |
|    total_timesteps | 3908000  |
| train/             |          |
|    actor_loss      | -0.752   |
|    critic_loss     | 0.000356 |
|    ent_coef        | 0.000416 |
|    ent_coef_loss   | 7.95     |
|    learning_rate   | 0.00109  |
|    n_updates       | 19080    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.15e+03 |
| time/              |          |
|    episodes        | 3908     |
|    fps             | 640      |
|    time_elapsed    | 6097     |
|    total_timesteps | 3908000  |
---------------------------------
Eval num_timesteps=3909000, episode_reward=2554.47 +/- 53.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 3909000  |
---------------------------------
Eval num_timesteps=3910000, episode_reward=2476.48 +/- 45.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3910000  |
| train/             |          |
|    actor_loss      | -0.739   |
|    critic_loss     | 0.000362 |
|    ent_coef        | 0.000418 |
|    ent_coef_loss   | 4.78     |
|    learning_rate   | 0.00109  |
|    n_updates       | 19090    |
---------------------------------
Eval num_timesteps=3911000, episode_reward=2479.91 +/- 48.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3911000  |
---------------------------------
Eval num_timesteps=3912000, episode_reward=2697.60 +/- 30.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3912000  |
| train/             |          |
|    actor_loss      | -0.75    |
|    critic_loss     | 0.000326 |
|    ent_coef        | 0.00042  |
|    ent_coef_loss   | 4.99     |
|    learning_rate   | 0.00109  |
|    n_updates       | 19100    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.16e+03 |
| time/              |          |
|    episodes        | 3912     |
|    fps             | 640      |
|    time_elapsed    | 6103     |
|    total_timesteps | 3912000  |
---------------------------------
Eval num_timesteps=3913000, episode_reward=2667.16 +/- 16.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 3913000  |
---------------------------------
Eval num_timesteps=3914000, episode_reward=2758.65 +/- 28.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3914000  |
| train/             |          |
|    actor_loss      | -0.761   |
|    critic_loss     | 0.000335 |
|    ent_coef        | 0.000422 |
|    ent_coef_loss   | 4.71     |
|    learning_rate   | 0.00109  |
|    n_updates       | 19110    |
---------------------------------
New best mean reward!
Eval num_timesteps=3915000, episode_reward=2761.38 +/- 67.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3915000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3916000, episode_reward=2667.79 +/- 40.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 3916000  |
| train/             |          |
|    actor_loss      | -0.766   |
|    critic_loss     | 0.000331 |
|    ent_coef        | 0.000423 |
|    ent_coef_loss   | 6.15     |
|    learning_rate   | 0.00108  |
|    n_updates       | 19120    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.18e+03 |
| time/              |          |
|    episodes        | 3916     |
|    fps             | 640      |
|    time_elapsed    | 6110     |
|    total_timesteps | 3916000  |
---------------------------------
Eval num_timesteps=3917000, episode_reward=2689.35 +/- 35.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 3917000  |
---------------------------------
Eval num_timesteps=3918000, episode_reward=2671.45 +/- 14.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 3918000  |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.000322 |
|    ent_coef        | 0.000425 |
|    ent_coef_loss   | 7.01     |
|    learning_rate   | 0.00108  |
|    n_updates       | 19130    |
---------------------------------
Eval num_timesteps=3919000, episode_reward=2646.20 +/- 56.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 3919000  |
---------------------------------
Eval num_timesteps=3920000, episode_reward=2759.93 +/- 21.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3920000  |
| train/             |          |
|    actor_loss      | -0.768   |
|    critic_loss     | 0.00034  |
|    ent_coef        | 0.000427 |
|    ent_coef_loss   | 5.53     |
|    learning_rate   | 0.00108  |
|    n_updates       | 19140    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.19e+03 |
| time/              |          |
|    episodes        | 3920     |
|    fps             | 640      |
|    time_elapsed    | 6116     |
|    total_timesteps | 3920000  |
---------------------------------
Eval num_timesteps=3921000, episode_reward=2743.70 +/- 22.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 3921000  |
---------------------------------
Eval num_timesteps=3922000, episode_reward=2701.35 +/- 37.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3922000  |
| train/             |          |
|    actor_loss      | -0.773   |
|    critic_loss     | 0.000334 |
|    ent_coef        | 0.000429 |
|    ent_coef_loss   | 7.5      |
|    learning_rate   | 0.00108  |
|    n_updates       | 19150    |
---------------------------------
Eval num_timesteps=3923000, episode_reward=2717.22 +/- 52.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 3923000  |
---------------------------------
Eval num_timesteps=3924000, episode_reward=2672.98 +/- 60.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 3924000  |
| train/             |          |
|    actor_loss      | -0.765   |
|    critic_loss     | 0.000336 |
|    ent_coef        | 0.000431 |
|    ent_coef_loss   | 4.69     |
|    learning_rate   | 0.00108  |
|    n_updates       | 19160    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.21e+03 |
| time/              |          |
|    episodes        | 3924     |
|    fps             | 640      |
|    time_elapsed    | 6123     |
|    total_timesteps | 3924000  |
---------------------------------
Eval num_timesteps=3925000, episode_reward=2673.43 +/- 27.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 3925000  |
---------------------------------
Eval num_timesteps=3926000, episode_reward=2727.67 +/- 30.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 3926000  |
---------------------------------
Eval num_timesteps=3927000, episode_reward=2693.76 +/- 42.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 3927000  |
| train/             |          |
|    actor_loss      | -0.761   |
|    critic_loss     | 0.000338 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 3.49     |
|    learning_rate   | 0.00107  |
|    n_updates       | 19170    |
---------------------------------
Eval num_timesteps=3928000, episode_reward=2761.96 +/- 71.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3928000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.23e+03 |
| time/              |          |
|    episodes        | 3928     |
|    fps             | 640      |
|    time_elapsed    | 6129     |
|    total_timesteps | 3928000  |
---------------------------------
Eval num_timesteps=3929000, episode_reward=2742.35 +/- 14.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 3929000  |
| train/             |          |
|    actor_loss      | -0.762   |
|    critic_loss     | 0.000367 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | 2.57     |
|    learning_rate   | 0.00107  |
|    n_updates       | 19180    |
---------------------------------
Eval num_timesteps=3930000, episode_reward=2724.38 +/- 27.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 3930000  |
---------------------------------
Eval num_timesteps=3931000, episode_reward=2790.58 +/- 54.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 3931000  |
| train/             |          |
|    actor_loss      | -0.766   |
|    critic_loss     | 0.000388 |
|    ent_coef        | 0.000435 |
|    ent_coef_loss   | 2.69     |
|    learning_rate   | 0.00107  |
|    n_updates       | 19190    |
---------------------------------
New best mean reward!
Eval num_timesteps=3932000, episode_reward=2790.15 +/- 50.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 3932000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.25e+03 |
| time/              |          |
|    episodes        | 3932     |
|    fps             | 640      |
|    time_elapsed    | 6135     |
|    total_timesteps | 3932000  |
---------------------------------
Eval num_timesteps=3933000, episode_reward=2773.57 +/- 46.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 3933000  |
| train/             |          |
|    actor_loss      | -0.773   |
|    critic_loss     | 0.000316 |
|    ent_coef        | 0.000436 |
|    ent_coef_loss   | 4.19     |
|    learning_rate   | 0.00107  |
|    n_updates       | 19200    |
---------------------------------
Eval num_timesteps=3934000, episode_reward=2774.01 +/- 46.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 3934000  |
---------------------------------
Eval num_timesteps=3935000, episode_reward=2761.40 +/- 63.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3935000  |
| train/             |          |
|    actor_loss      | -0.766   |
|    critic_loss     | 0.000408 |
|    ent_coef        | 0.000437 |
|    ent_coef_loss   | 7.92     |
|    learning_rate   | 0.00107  |
|    n_updates       | 19210    |
---------------------------------
Eval num_timesteps=3936000, episode_reward=2768.21 +/- 38.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 3936000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.27e+03 |
| time/              |          |
|    episodes        | 3936     |
|    fps             | 640      |
|    time_elapsed    | 6142     |
|    total_timesteps | 3936000  |
---------------------------------
Eval num_timesteps=3937000, episode_reward=2797.90 +/- 53.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 3937000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.000346 |
|    ent_coef        | 0.000439 |
|    ent_coef_loss   | 8.89     |
|    learning_rate   | 0.00106  |
|    n_updates       | 19220    |
---------------------------------
New best mean reward!
Eval num_timesteps=3938000, episode_reward=2794.79 +/- 44.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 3938000  |
---------------------------------
Eval num_timesteps=3939000, episode_reward=2746.00 +/- 36.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 3939000  |
| train/             |          |
|    actor_loss      | -0.778   |
|    critic_loss     | 0.000339 |
|    ent_coef        | 0.000442 |
|    ent_coef_loss   | 5.98     |
|    learning_rate   | 0.00106  |
|    n_updates       | 19230    |
---------------------------------
Eval num_timesteps=3940000, episode_reward=2769.26 +/- 39.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 3940000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.29e+03 |
| time/              |          |
|    episodes        | 3940     |
|    fps             | 640      |
|    time_elapsed    | 6149     |
|    total_timesteps | 3940000  |
---------------------------------
Eval num_timesteps=3941000, episode_reward=2743.42 +/- 33.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 3941000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.000354 |
|    ent_coef        | 0.000444 |
|    ent_coef_loss   | 5.41     |
|    learning_rate   | 0.00106  |
|    n_updates       | 19240    |
---------------------------------
Eval num_timesteps=3942000, episode_reward=2799.25 +/- 33.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 3942000  |
---------------------------------
New best mean reward!
Eval num_timesteps=3943000, episode_reward=2791.84 +/- 45.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 3943000  |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.000377 |
|    ent_coef        | 0.000446 |
|    ent_coef_loss   | 4.89     |
|    learning_rate   | 0.00106  |
|    n_updates       | 19250    |
---------------------------------
Eval num_timesteps=3944000, episode_reward=2750.14 +/- 15.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 3944000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.32e+03 |
| time/              |          |
|    episodes        | 3944     |
|    fps             | 640      |
|    time_elapsed    | 6155     |
|    total_timesteps | 3944000  |
---------------------------------
Eval num_timesteps=3945000, episode_reward=2751.37 +/- 10.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 3945000  |
| train/             |          |
|    actor_loss      | -0.776   |
|    critic_loss     | 0.000324 |
|    ent_coef        | 0.000448 |
|    ent_coef_loss   | 5.51     |
|    learning_rate   | 0.00106  |
|    n_updates       | 19260    |
---------------------------------
Eval num_timesteps=3946000, episode_reward=2732.81 +/- 28.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 3946000  |
---------------------------------
Eval num_timesteps=3947000, episode_reward=2747.88 +/- 26.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 3947000  |
| train/             |          |
|    actor_loss      | -0.778   |
|    critic_loss     | 0.000345 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | 7.47     |
|    learning_rate   | 0.00105  |
|    n_updates       | 19270    |
---------------------------------
Eval num_timesteps=3948000, episode_reward=2726.06 +/- 17.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 3948000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    episodes        | 3948     |
|    fps             | 640      |
|    time_elapsed    | 6161     |
|    total_timesteps | 3948000  |
---------------------------------
Eval num_timesteps=3949000, episode_reward=2679.31 +/- 29.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 3949000  |
| train/             |          |
|    actor_loss      | -0.771   |
|    critic_loss     | 0.000335 |
|    ent_coef        | 0.000452 |
|    ent_coef_loss   | 2.44     |
|    learning_rate   | 0.00105  |
|    n_updates       | 19280    |
---------------------------------
Eval num_timesteps=3950000, episode_reward=2655.73 +/- 49.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 3950000  |
---------------------------------
Eval num_timesteps=3951000, episode_reward=2614.33 +/- 16.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 3951000  |
| train/             |          |
|    actor_loss      | -0.764   |
|    critic_loss     | 0.000328 |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | 2.44     |
|    learning_rate   | 0.00105  |
|    n_updates       | 19290    |
---------------------------------
Eval num_timesteps=3952000, episode_reward=2658.99 +/- 23.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 3952000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    episodes        | 3952     |
|    fps             | 640      |
|    time_elapsed    | 6167     |
|    total_timesteps | 3952000  |
---------------------------------
Eval num_timesteps=3953000, episode_reward=2761.88 +/- 25.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3953000  |
| train/             |          |
|    actor_loss      | -0.752   |
|    critic_loss     | 0.000393 |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | 1.81     |
|    learning_rate   | 0.00105  |
|    n_updates       | 19300    |
---------------------------------
Eval num_timesteps=3954000, episode_reward=2716.83 +/- 46.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 3954000  |
---------------------------------
Eval num_timesteps=3955000, episode_reward=2500.31 +/- 38.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 3955000  |
| train/             |          |
|    actor_loss      | -0.77    |
|    critic_loss     | 0.000352 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | 3.37     |
|    learning_rate   | 0.00105  |
|    n_updates       | 19310    |
---------------------------------
Eval num_timesteps=3956000, episode_reward=2501.80 +/- 47.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 3956000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 3956     |
|    fps             | 640      |
|    time_elapsed    | 6173     |
|    total_timesteps | 3956000  |
---------------------------------
Eval num_timesteps=3957000, episode_reward=2508.97 +/- 78.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 3957000  |
| train/             |          |
|    actor_loss      | -0.736   |
|    critic_loss     | 0.000381 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | -1.68    |
|    learning_rate   | 0.00104  |
|    n_updates       | 19320    |
---------------------------------
Eval num_timesteps=3958000, episode_reward=2502.16 +/- 16.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.5e+03  |
| time/              |          |
|    total_timesteps | 3958000  |
---------------------------------
Eval num_timesteps=3959000, episode_reward=2456.31 +/- 53.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.46e+03 |
| time/              |          |
|    total_timesteps | 3959000  |
| train/             |          |
|    actor_loss      | -0.746   |
|    critic_loss     | 0.000353 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | -0.241   |
|    learning_rate   | 0.00104  |
|    n_updates       | 19330    |
---------------------------------
Eval num_timesteps=3960000, episode_reward=2447.80 +/- 59.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 3960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 3960     |
|    fps             | 640      |
|    time_elapsed    | 6180     |
|    total_timesteps | 3960000  |
---------------------------------
Eval num_timesteps=3961000, episode_reward=2632.06 +/- 40.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 3961000  |
| train/             |          |
|    actor_loss      | -0.753   |
|    critic_loss     | 0.000386 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | 2.01     |
|    learning_rate   | 0.00104  |
|    n_updates       | 19340    |
---------------------------------
Eval num_timesteps=3962000, episode_reward=2619.03 +/- 53.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 3962000  |
---------------------------------
Eval num_timesteps=3963000, episode_reward=2451.20 +/- 39.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 3963000  |
| train/             |          |
|    actor_loss      | -0.756   |
|    critic_loss     | 0.000375 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | 2.1      |
|    learning_rate   | 0.00104  |
|    n_updates       | 19350    |
---------------------------------
Eval num_timesteps=3964000, episode_reward=2479.57 +/- 27.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.48e+03 |
| time/              |          |
|    total_timesteps | 3964000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 3964     |
|    fps             | 640      |
|    time_elapsed    | 6186     |
|    total_timesteps | 3964000  |
---------------------------------
Eval num_timesteps=3965000, episode_reward=2627.66 +/- 55.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 3965000  |
| train/             |          |
|    actor_loss      | -0.746   |
|    critic_loss     | 0.000387 |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | 2.22     |
|    learning_rate   | 0.00104  |
|    n_updates       | 19360    |
---------------------------------
Eval num_timesteps=3966000, episode_reward=2630.82 +/- 40.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 3966000  |
---------------------------------
Eval num_timesteps=3967000, episode_reward=2683.30 +/- 51.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 3967000  |
| train/             |          |
|    actor_loss      | -0.763   |
|    critic_loss     | 0.000424 |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | 0.921    |
|    learning_rate   | 0.00103  |
|    n_updates       | 19370    |
---------------------------------
Eval num_timesteps=3968000, episode_reward=2697.68 +/- 27.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3968000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    episodes        | 3968     |
|    fps             | 640      |
|    time_elapsed    | 6192     |
|    total_timesteps | 3968000  |
---------------------------------
Eval num_timesteps=3969000, episode_reward=2697.06 +/- 51.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 3969000  |
---------------------------------
Eval num_timesteps=3970000, episode_reward=2777.91 +/- 37.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 3970000  |
| train/             |          |
|    actor_loss      | -0.769   |
|    critic_loss     | 0.000421 |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | 2.42     |
|    learning_rate   | 0.00103  |
|    n_updates       | 19380    |
---------------------------------
Eval num_timesteps=3971000, episode_reward=2751.89 +/- 54.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 3971000  |
---------------------------------
Eval num_timesteps=3972000, episode_reward=2708.40 +/- 35.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 3972000  |
| train/             |          |
|    actor_loss      | -0.765   |
|    critic_loss     | 0.000397 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | 0.555    |
|    learning_rate   | 0.00103  |
|    n_updates       | 19390    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    episodes        | 3972     |
|    fps             | 640      |
|    time_elapsed    | 6198     |
|    total_timesteps | 3972000  |
---------------------------------
Eval num_timesteps=3973000, episode_reward=2730.15 +/- 25.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 3973000  |
---------------------------------
Eval num_timesteps=3974000, episode_reward=2760.82 +/- 60.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3974000  |
| train/             |          |
|    actor_loss      | -0.773   |
|    critic_loss     | 0.000354 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | 2.16     |
|    learning_rate   | 0.00103  |
|    n_updates       | 19400    |
---------------------------------
Eval num_timesteps=3975000, episode_reward=2796.17 +/- 50.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 3975000  |
---------------------------------
Eval num_timesteps=3976000, episode_reward=2873.03 +/- 38.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 3976000  |
| train/             |          |
|    actor_loss      | -0.776   |
|    critic_loss     | 0.000397 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | -0.102   |
|    learning_rate   | 0.00102  |
|    n_updates       | 19410    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    episodes        | 3976     |
|    fps             | 640      |
|    time_elapsed    | 6204     |
|    total_timesteps | 3976000  |
---------------------------------
Eval num_timesteps=3977000, episode_reward=2866.69 +/- 42.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 3977000  |
---------------------------------
Eval num_timesteps=3978000, episode_reward=2826.27 +/- 28.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 3978000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.000344 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | 3.15     |
|    learning_rate   | 0.00102  |
|    n_updates       | 19420    |
---------------------------------
Eval num_timesteps=3979000, episode_reward=2869.13 +/- 23.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 3979000  |
---------------------------------
Eval num_timesteps=3980000, episode_reward=2786.36 +/- 24.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 3980000  |
| train/             |          |
|    actor_loss      | -0.79    |
|    critic_loss     | 0.0004   |
|    ent_coef        | 0.000461 |
|    ent_coef_loss   | 3.98     |
|    learning_rate   | 0.00102  |
|    n_updates       | 19430    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    episodes        | 3980     |
|    fps             | 640      |
|    time_elapsed    | 6211     |
|    total_timesteps | 3980000  |
---------------------------------
Eval num_timesteps=3981000, episode_reward=2692.77 +/- 57.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 3981000  |
---------------------------------
Eval num_timesteps=3982000, episode_reward=2843.65 +/- 52.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 3982000  |
| train/             |          |
|    actor_loss      | -0.792   |
|    critic_loss     | 0.00032  |
|    ent_coef        | 0.000462 |
|    ent_coef_loss   | 4.33     |
|    learning_rate   | 0.00102  |
|    n_updates       | 19440    |
---------------------------------
Eval num_timesteps=3983000, episode_reward=2259.60 +/- 1223.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 3983000  |
---------------------------------
Eval num_timesteps=3984000, episode_reward=2719.08 +/- 35.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 3984000  |
| train/             |          |
|    actor_loss      | -0.786   |
|    critic_loss     | 0.000351 |
|    ent_coef        | 0.000464 |
|    ent_coef_loss   | 1.95     |
|    learning_rate   | 0.00102  |
|    n_updates       | 19450    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 3984     |
|    fps             | 640      |
|    time_elapsed    | 6217     |
|    total_timesteps | 3984000  |
---------------------------------
Eval num_timesteps=3985000, episode_reward=2720.96 +/- 50.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 3985000  |
---------------------------------
Eval num_timesteps=3986000, episode_reward=2850.36 +/- 61.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 3986000  |
| train/             |          |
|    actor_loss      | -0.779   |
|    critic_loss     | 0.00032  |
|    ent_coef        | 0.000465 |
|    ent_coef_loss   | 4.33     |
|    learning_rate   | 0.00101  |
|    n_updates       | 19460    |
---------------------------------
Eval num_timesteps=3987000, episode_reward=2852.11 +/- 32.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 3987000  |
---------------------------------
Eval num_timesteps=3988000, episode_reward=2221.06 +/- 1221.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 3988000  |
| train/             |          |
|    actor_loss      | -0.783   |
|    critic_loss     | 0.000352 |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | 2.8      |
|    learning_rate   | 0.00101  |
|    n_updates       | 19470    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    episodes        | 3988     |
|    fps             | 640      |
|    time_elapsed    | 6223     |
|    total_timesteps | 3988000  |
---------------------------------
Eval num_timesteps=3989000, episode_reward=2175.97 +/- 1230.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 3989000  |
---------------------------------
Eval num_timesteps=3990000, episode_reward=2759.68 +/- 50.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 3990000  |
| train/             |          |
|    actor_loss      | -0.778   |
|    critic_loss     | 0.000323 |
|    ent_coef        | 0.000467 |
|    ent_coef_loss   | 1.44     |
|    learning_rate   | 0.00101  |
|    n_updates       | 19480    |
---------------------------------
Eval num_timesteps=3991000, episode_reward=2725.70 +/- 57.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 3991000  |
---------------------------------
Eval num_timesteps=3992000, episode_reward=2597.25 +/- 31.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 3992000  |
| train/             |          |
|    actor_loss      | -0.688   |
|    critic_loss     | 0.0029   |
|    ent_coef        | 0.000468 |
|    ent_coef_loss   | 5.98     |
|    learning_rate   | 0.00101  |
|    n_updates       | 19490    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 3992     |
|    fps             | 640      |
|    time_elapsed    | 6229     |
|    total_timesteps | 3992000  |
---------------------------------
Eval num_timesteps=3993000, episode_reward=2563.80 +/- 52.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 3993000  |
---------------------------------
Eval num_timesteps=3994000, episode_reward=1357.40 +/- 18.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.36e+03 |
| time/              |          |
|    total_timesteps | 3994000  |
| train/             |          |
|    actor_loss      | -0.689   |
|    critic_loss     | 0.000994 |
|    ent_coef        | 0.00047  |
|    ent_coef_loss   | 15.5     |
|    learning_rate   | 0.00101  |
|    n_updates       | 19500    |
---------------------------------
Eval num_timesteps=3995000, episode_reward=1376.55 +/- 23.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.38e+03 |
| time/              |          |
|    total_timesteps | 3995000  |
---------------------------------
Eval num_timesteps=3996000, episode_reward=2001.92 +/- 45.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 3996000  |
| train/             |          |
|    actor_loss      | -0.635   |
|    critic_loss     | 0.000536 |
|    ent_coef        | 0.000474 |
|    ent_coef_loss   | 19.9     |
|    learning_rate   | 0.001    |
|    n_updates       | 19510    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    episodes        | 3996     |
|    fps             | 640      |
|    time_elapsed    | 6236     |
|    total_timesteps | 3996000  |
---------------------------------
Eval num_timesteps=3997000, episode_reward=1976.93 +/- 80.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.98e+03 |
| time/              |          |
|    total_timesteps | 3997000  |
---------------------------------
Eval num_timesteps=3998000, episode_reward=1925.46 +/- 66.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.93e+03 |
| time/              |          |
|    total_timesteps | 3998000  |
| train/             |          |
|    actor_loss      | -0.664   |
|    critic_loss     | 0.000613 |
|    ent_coef        | 0.000481 |
|    ent_coef_loss   | 12.8     |
|    learning_rate   | 0.001    |
|    n_updates       | 19520    |
---------------------------------
Eval num_timesteps=3999000, episode_reward=2004.07 +/- 68.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2e+03    |
| time/              |          |
|    total_timesteps | 3999000  |
---------------------------------
Eval num_timesteps=4000000, episode_reward=2137.33 +/- 63.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 4000000  |
| train/             |          |
|    actor_loss      | -0.656   |
|    critic_loss     | 0.000554 |
|    ent_coef        | 0.000486 |
|    ent_coef_loss   | -1.14    |
|    learning_rate   | 0.001    |
|    n_updates       | 19530    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    episodes        | 4000     |
|    fps             | 640      |
|    time_elapsed    | 6242     |
|    total_timesteps | 4000000  |
---------------------------------
Eval num_timesteps=4001000, episode_reward=2071.63 +/- 34.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 4001000  |
---------------------------------
Eval num_timesteps=4002000, episode_reward=2273.72 +/- 40.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 4002000  |
| train/             |          |
|    actor_loss      | -0.689   |
|    critic_loss     | 0.000433 |
|    ent_coef        | 0.000488 |
|    ent_coef_loss   | 5.04     |
|    learning_rate   | 0.000998 |
|    n_updates       | 19540    |
---------------------------------
Eval num_timesteps=4003000, episode_reward=2274.42 +/- 61.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 4003000  |
---------------------------------
Eval num_timesteps=4004000, episode_reward=2156.30 +/- 34.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 4004000  |
| train/             |          |
|    actor_loss      | -0.72    |
|    critic_loss     | 0.000409 |
|    ent_coef        | 0.00049  |
|    ent_coef_loss   | 7.55     |
|    learning_rate   | 0.000996 |
|    n_updates       | 19550    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 4004     |
|    fps             | 640      |
|    time_elapsed    | 6248     |
|    total_timesteps | 4004000  |
---------------------------------
Eval num_timesteps=4005000, episode_reward=2165.04 +/- 36.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 4005000  |
---------------------------------
Eval num_timesteps=4006000, episode_reward=2150.15 +/- 69.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 4006000  |
| train/             |          |
|    actor_loss      | -0.715   |
|    critic_loss     | 0.000393 |
|    ent_coef        | 0.000492 |
|    ent_coef_loss   | 2.95     |
|    learning_rate   | 0.000994 |
|    n_updates       | 19560    |
---------------------------------
Eval num_timesteps=4007000, episode_reward=2222.08 +/- 83.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 4007000  |
---------------------------------
Eval num_timesteps=4008000, episode_reward=2357.89 +/- 23.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 4008000  |
| train/             |          |
|    actor_loss      | -0.697   |
|    critic_loss     | 0.000387 |
|    ent_coef        | 0.000493 |
|    ent_coef_loss   | -2.02    |
|    learning_rate   | 0.000992 |
|    n_updates       | 19570    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    episodes        | 4008     |
|    fps             | 640      |
|    time_elapsed    | 6254     |
|    total_timesteps | 4008000  |
---------------------------------
Eval num_timesteps=4009000, episode_reward=2386.63 +/- 45.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 4009000  |
---------------------------------
Eval num_timesteps=4010000, episode_reward=2569.64 +/- 37.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 4010000  |
| train/             |          |
|    actor_loss      | -0.722   |
|    critic_loss     | 0.000344 |
|    ent_coef        | 0.000494 |
|    ent_coef_loss   | 4.55     |
|    learning_rate   | 0.00099  |
|    n_updates       | 19580    |
---------------------------------
Eval num_timesteps=4011000, episode_reward=2594.98 +/- 44.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 4011000  |
---------------------------------
Eval num_timesteps=4012000, episode_reward=2583.70 +/- 55.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 4012000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 4012     |
|    fps             | 640      |
|    time_elapsed    | 6260     |
|    total_timesteps | 4012000  |
---------------------------------
Eval num_timesteps=4013000, episode_reward=2607.18 +/- 18.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 4013000  |
| train/             |          |
|    actor_loss      | -0.744   |
|    critic_loss     | 0.000363 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | 2.03     |
|    learning_rate   | 0.000988 |
|    n_updates       | 19590    |
---------------------------------
Eval num_timesteps=4014000, episode_reward=2617.69 +/- 27.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 4014000  |
---------------------------------
Eval num_timesteps=4015000, episode_reward=2788.43 +/- 33.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4015000  |
| train/             |          |
|    actor_loss      | -0.728   |
|    critic_loss     | 0.000367 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | -5.68    |
|    learning_rate   | 0.000986 |
|    n_updates       | 19600    |
---------------------------------
Eval num_timesteps=4016000, episode_reward=2781.59 +/- 48.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4016000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 4016     |
|    fps             | 640      |
|    time_elapsed    | 6266     |
|    total_timesteps | 4016000  |
---------------------------------
Eval num_timesteps=4017000, episode_reward=2837.46 +/- 29.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4017000  |
| train/             |          |
|    actor_loss      | -0.756   |
|    critic_loss     | 0.000419 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | 3.15     |
|    learning_rate   | 0.000984 |
|    n_updates       | 19610    |
---------------------------------
Eval num_timesteps=4018000, episode_reward=2833.08 +/- 29.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4018000  |
---------------------------------
Eval num_timesteps=4019000, episode_reward=2794.10 +/- 52.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4019000  |
| train/             |          |
|    actor_loss      | -0.773   |
|    critic_loss     | 0.000426 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | 5.23     |
|    learning_rate   | 0.000982 |
|    n_updates       | 19620    |
---------------------------------
Eval num_timesteps=4020000, episode_reward=2791.28 +/- 27.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4020000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 4020     |
|    fps             | 640      |
|    time_elapsed    | 6272     |
|    total_timesteps | 4020000  |
---------------------------------
Eval num_timesteps=4021000, episode_reward=2783.20 +/- 30.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4021000  |
| train/             |          |
|    actor_loss      | -0.768   |
|    critic_loss     | 0.000412 |
|    ent_coef        | 0.000496 |
|    ent_coef_loss   | 0.204    |
|    learning_rate   | 0.00098  |
|    n_updates       | 19630    |
---------------------------------
Eval num_timesteps=4022000, episode_reward=2777.81 +/- 27.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4022000  |
---------------------------------
Eval num_timesteps=4023000, episode_reward=2739.47 +/- 32.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4023000  |
| train/             |          |
|    actor_loss      | -0.767   |
|    critic_loss     | 0.000443 |
|    ent_coef        | 0.000497 |
|    ent_coef_loss   | 2.61     |
|    learning_rate   | 0.000978 |
|    n_updates       | 19640    |
---------------------------------
Eval num_timesteps=4024000, episode_reward=2783.99 +/- 28.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4024000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    episodes        | 4024     |
|    fps             | 640      |
|    time_elapsed    | 6279     |
|    total_timesteps | 4024000  |
---------------------------------
Eval num_timesteps=4025000, episode_reward=2827.48 +/- 31.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4025000  |
| train/             |          |
|    actor_loss      | -0.765   |
|    critic_loss     | 0.000391 |
|    ent_coef        | 0.000498 |
|    ent_coef_loss   | 1.28     |
|    learning_rate   | 0.000976 |
|    n_updates       | 19650    |
---------------------------------
Eval num_timesteps=4026000, episode_reward=2827.30 +/- 7.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4026000  |
---------------------------------
Eval num_timesteps=4027000, episode_reward=2844.76 +/- 12.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4027000  |
| train/             |          |
|    actor_loss      | -0.772   |
|    critic_loss     | 0.000436 |
|    ent_coef        | 0.000499 |
|    ent_coef_loss   | 1.6      |
|    learning_rate   | 0.000974 |
|    n_updates       | 19660    |
---------------------------------
Eval num_timesteps=4028000, episode_reward=2823.35 +/- 20.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 4028000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    episodes        | 4028     |
|    fps             | 640      |
|    time_elapsed    | 6285     |
|    total_timesteps | 4028000  |
---------------------------------
Eval num_timesteps=4029000, episode_reward=2884.28 +/- 41.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4029000  |
| train/             |          |
|    actor_loss      | -0.771   |
|    critic_loss     | 0.000364 |
|    ent_coef        | 0.000499 |
|    ent_coef_loss   | 0.762    |
|    learning_rate   | 0.000972 |
|    n_updates       | 19670    |
---------------------------------
New best mean reward!
Eval num_timesteps=4030000, episode_reward=2849.86 +/- 44.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4030000  |
---------------------------------
Eval num_timesteps=4031000, episode_reward=2863.99 +/- 44.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4031000  |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.000407 |
|    ent_coef        | 0.0005   |
|    ent_coef_loss   | 0.509    |
|    learning_rate   | 0.00097  |
|    n_updates       | 19680    |
---------------------------------
Eval num_timesteps=4032000, episode_reward=2837.95 +/- 70.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4032000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    episodes        | 4032     |
|    fps             | 640      |
|    time_elapsed    | 6291     |
|    total_timesteps | 4032000  |
---------------------------------
Eval num_timesteps=4033000, episode_reward=2934.49 +/- 54.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4033000  |
| train/             |          |
|    actor_loss      | -0.776   |
|    critic_loss     | 0.000447 |
|    ent_coef        | 0.0005   |
|    ent_coef_loss   | 2.3      |
|    learning_rate   | 0.000967 |
|    n_updates       | 19690    |
---------------------------------
New best mean reward!
Eval num_timesteps=4034000, episode_reward=2930.92 +/- 52.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4034000  |
---------------------------------
Eval num_timesteps=4035000, episode_reward=2848.03 +/- 20.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4035000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.000404 |
|    ent_coef        | 0.000501 |
|    ent_coef_loss   | 4.02     |
|    learning_rate   | 0.000965 |
|    n_updates       | 19700    |
---------------------------------
Eval num_timesteps=4036000, episode_reward=2813.63 +/- 26.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4036000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 4036     |
|    fps             | 640      |
|    time_elapsed    | 6297     |
|    total_timesteps | 4036000  |
---------------------------------
Eval num_timesteps=4037000, episode_reward=2907.62 +/- 30.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4037000  |
| train/             |          |
|    actor_loss      | -0.785   |
|    critic_loss     | 0.000392 |
|    ent_coef        | 0.000502 |
|    ent_coef_loss   | 4.21     |
|    learning_rate   | 0.000963 |
|    n_updates       | 19710    |
---------------------------------
Eval num_timesteps=4038000, episode_reward=2932.27 +/- 36.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4038000  |
---------------------------------
Eval num_timesteps=4039000, episode_reward=2872.48 +/- 34.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4039000  |
| train/             |          |
|    actor_loss      | -0.787   |
|    critic_loss     | 0.000377 |
|    ent_coef        | 0.000504 |
|    ent_coef_loss   | 4.23     |
|    learning_rate   | 0.000961 |
|    n_updates       | 19720    |
---------------------------------
Eval num_timesteps=4040000, episode_reward=2906.79 +/- 18.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4040000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 4040     |
|    fps             | 640      |
|    time_elapsed    | 6303     |
|    total_timesteps | 4040000  |
---------------------------------
Eval num_timesteps=4041000, episode_reward=2960.11 +/- 16.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4041000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.000434 |
|    ent_coef        | 0.000505 |
|    ent_coef_loss   | 2.92     |
|    learning_rate   | 0.000959 |
|    n_updates       | 19730    |
---------------------------------
New best mean reward!
Eval num_timesteps=4042000, episode_reward=2941.20 +/- 71.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4042000  |
---------------------------------
Eval num_timesteps=4043000, episode_reward=2954.37 +/- 29.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4043000  |
| train/             |          |
|    actor_loss      | -0.632   |
|    critic_loss     | 0.000513 |
|    ent_coef        | 0.000508 |
|    ent_coef_loss   | 14.2     |
|    learning_rate   | 0.000957 |
|    n_updates       | 19740    |
---------------------------------
Eval num_timesteps=4044000, episode_reward=2969.18 +/- 38.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4044000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.34e+03 |
| time/              |          |
|    episodes        | 4044     |
|    fps             | 640      |
|    time_elapsed    | 6310     |
|    total_timesteps | 4044000  |
---------------------------------
Eval num_timesteps=4045000, episode_reward=2940.31 +/- 27.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4045000  |
| train/             |          |
|    actor_loss      | -0.723   |
|    critic_loss     | 0.000496 |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | -3.82    |
|    learning_rate   | 0.000955 |
|    n_updates       | 19750    |
---------------------------------
Eval num_timesteps=4046000, episode_reward=2953.25 +/- 36.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4046000  |
---------------------------------
Eval num_timesteps=4047000, episode_reward=2901.58 +/- 23.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4047000  |
| train/             |          |
|    actor_loss      | -0.792   |
|    critic_loss     | 0.000469 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | 3.55     |
|    learning_rate   | 0.000953 |
|    n_updates       | 19760    |
---------------------------------
Eval num_timesteps=4048000, episode_reward=2865.49 +/- 39.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4048000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    episodes        | 4048     |
|    fps             | 640      |
|    time_elapsed    | 6316     |
|    total_timesteps | 4048000  |
---------------------------------
Eval num_timesteps=4049000, episode_reward=2922.86 +/- 26.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4049000  |
| train/             |          |
|    actor_loss      | -0.778   |
|    critic_loss     | 0.000475 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | -2.38    |
|    learning_rate   | 0.000951 |
|    n_updates       | 19770    |
---------------------------------
Eval num_timesteps=4050000, episode_reward=2899.57 +/- 41.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4050000  |
---------------------------------
Eval num_timesteps=4051000, episode_reward=2876.79 +/- 29.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4051000  |
| train/             |          |
|    actor_loss      | -0.776   |
|    critic_loss     | 0.000452 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | -2.74    |
|    learning_rate   | 0.000949 |
|    n_updates       | 19780    |
---------------------------------
Eval num_timesteps=4052000, episode_reward=2832.41 +/- 35.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4052000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    episodes        | 4052     |
|    fps             | 640      |
|    time_elapsed    | 6323     |
|    total_timesteps | 4052000  |
---------------------------------
Eval num_timesteps=4053000, episode_reward=2804.13 +/- 24.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4053000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.000417 |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | -0.534   |
|    learning_rate   | 0.000947 |
|    n_updates       | 19790    |
---------------------------------
Eval num_timesteps=4054000, episode_reward=2821.99 +/- 32.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 4054000  |
---------------------------------
Eval num_timesteps=4055000, episode_reward=2838.58 +/- 30.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4055000  |
---------------------------------
Eval num_timesteps=4056000, episode_reward=2860.46 +/- 15.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4056000  |
| train/             |          |
|    actor_loss      | -0.771   |
|    critic_loss     | 0.000432 |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | -2.05    |
|    learning_rate   | 0.000945 |
|    n_updates       | 19800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    episodes        | 4056     |
|    fps             | 640      |
|    time_elapsed    | 6329     |
|    total_timesteps | 4056000  |
---------------------------------
Eval num_timesteps=4057000, episode_reward=2877.23 +/- 25.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4057000  |
---------------------------------
Eval num_timesteps=4058000, episode_reward=2931.05 +/- 42.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4058000  |
| train/             |          |
|    actor_loss      | -0.783   |
|    critic_loss     | 0.000431 |
|    ent_coef        | 0.00051  |
|    ent_coef_loss   | 1.01     |
|    learning_rate   | 0.000943 |
|    n_updates       | 19810    |
---------------------------------
Eval num_timesteps=4059000, episode_reward=2934.83 +/- 13.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4059000  |
---------------------------------
Eval num_timesteps=4060000, episode_reward=2887.91 +/- 5.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4060000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.000401 |
|    ent_coef        | 0.00051  |
|    ent_coef_loss   | -0.56    |
|    learning_rate   | 0.000941 |
|    n_updates       | 19820    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    episodes        | 4060     |
|    fps             | 640      |
|    time_elapsed    | 6336     |
|    total_timesteps | 4060000  |
---------------------------------
Eval num_timesteps=4061000, episode_reward=2859.12 +/- 31.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4061000  |
---------------------------------
Eval num_timesteps=4062000, episode_reward=2964.65 +/- 13.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4062000  |
| train/             |          |
|    actor_loss      | -0.798   |
|    critic_loss     | 0.000427 |
|    ent_coef        | 0.00051  |
|    ent_coef_loss   | 2.88     |
|    learning_rate   | 0.000939 |
|    n_updates       | 19830    |
---------------------------------
Eval num_timesteps=4063000, episode_reward=2955.99 +/- 18.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4063000  |
---------------------------------
Eval num_timesteps=4064000, episode_reward=2893.23 +/- 39.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4064000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000421 |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | 2.83     |
|    learning_rate   | 0.000937 |
|    n_updates       | 19840    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 4064     |
|    fps             | 640      |
|    time_elapsed    | 6342     |
|    total_timesteps | 4064000  |
---------------------------------
Eval num_timesteps=4065000, episode_reward=2830.98 +/- 9.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4065000  |
---------------------------------
Eval num_timesteps=4066000, episode_reward=2806.45 +/- 28.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4066000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.000407 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | 0.0898   |
|    learning_rate   | 0.000935 |
|    n_updates       | 19850    |
---------------------------------
Eval num_timesteps=4067000, episode_reward=2796.68 +/- 29.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4067000  |
---------------------------------
Eval num_timesteps=4068000, episode_reward=2880.27 +/- 41.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4068000  |
| train/             |          |
|    actor_loss      | -0.785   |
|    critic_loss     | 0.000438 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | 1.07     |
|    learning_rate   | 0.000933 |
|    n_updates       | 19860    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    episodes        | 4068     |
|    fps             | 640      |
|    time_elapsed    | 6348     |
|    total_timesteps | 4068000  |
---------------------------------
Eval num_timesteps=4069000, episode_reward=2867.83 +/- 27.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4069000  |
---------------------------------
Eval num_timesteps=4070000, episode_reward=2684.73 +/- 21.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 4070000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.000447 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | 0.128    |
|    learning_rate   | 0.000931 |
|    n_updates       | 19870    |
---------------------------------
Eval num_timesteps=4071000, episode_reward=2683.68 +/- 49.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 4071000  |
---------------------------------
Eval num_timesteps=4072000, episode_reward=2837.42 +/- 34.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4072000  |
| train/             |          |
|    actor_loss      | -0.776   |
|    critic_loss     | 0.000362 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | -1.28    |
|    learning_rate   | 0.000929 |
|    n_updates       | 19880    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    episodes        | 4072     |
|    fps             | 640      |
|    time_elapsed    | 6354     |
|    total_timesteps | 4072000  |
---------------------------------
Eval num_timesteps=4073000, episode_reward=2800.52 +/- 14.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4073000  |
---------------------------------
Eval num_timesteps=4074000, episode_reward=2719.94 +/- 44.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4074000  |
| train/             |          |
|    actor_loss      | -0.786   |
|    critic_loss     | 0.000404 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | 0.392    |
|    learning_rate   | 0.000927 |
|    n_updates       | 19890    |
---------------------------------
Eval num_timesteps=4075000, episode_reward=2711.87 +/- 36.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4075000  |
---------------------------------
Eval num_timesteps=4076000, episode_reward=2697.21 +/- 12.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4076000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.000397 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | -0.512   |
|    learning_rate   | 0.000924 |
|    n_updates       | 19900    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    episodes        | 4076     |
|    fps             | 640      |
|    time_elapsed    | 6360     |
|    total_timesteps | 4076000  |
---------------------------------
Eval num_timesteps=4077000, episode_reward=2673.19 +/- 43.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 4077000  |
---------------------------------
Eval num_timesteps=4078000, episode_reward=2751.34 +/- 34.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4078000  |
| train/             |          |
|    actor_loss      | -0.783   |
|    critic_loss     | 0.000418 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | -1.82    |
|    learning_rate   | 0.000922 |
|    n_updates       | 19910    |
---------------------------------
Eval num_timesteps=4079000, episode_reward=2711.35 +/- 50.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4079000  |
---------------------------------
Eval num_timesteps=4080000, episode_reward=2741.57 +/- 18.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4080000  |
| train/             |          |
|    actor_loss      | -0.794   |
|    critic_loss     | 0.000429 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | 1.38     |
|    learning_rate   | 0.00092  |
|    n_updates       | 19920    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    episodes        | 4080     |
|    fps             | 640      |
|    time_elapsed    | 6366     |
|    total_timesteps | 4080000  |
---------------------------------
Eval num_timesteps=4081000, episode_reward=2716.56 +/- 12.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4081000  |
---------------------------------
Eval num_timesteps=4082000, episode_reward=2746.49 +/- 11.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4082000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.00043  |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | -0.494   |
|    learning_rate   | 0.000918 |
|    n_updates       | 19930    |
---------------------------------
Eval num_timesteps=4083000, episode_reward=2722.23 +/- 31.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4083000  |
---------------------------------
Eval num_timesteps=4084000, episode_reward=2719.60 +/- 26.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4084000  |
| train/             |          |
|    actor_loss      | -0.775   |
|    critic_loss     | 0.000425 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | -0.277   |
|    learning_rate   | 0.000916 |
|    n_updates       | 19940    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    episodes        | 4084     |
|    fps             | 640      |
|    time_elapsed    | 6373     |
|    total_timesteps | 4084000  |
---------------------------------
Eval num_timesteps=4085000, episode_reward=2710.32 +/- 26.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4085000  |
---------------------------------
Eval num_timesteps=4086000, episode_reward=2667.04 +/- 24.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 4086000  |
| train/             |          |
|    actor_loss      | -0.776   |
|    critic_loss     | 0.000389 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000914 |
|    n_updates       | 19950    |
---------------------------------
Eval num_timesteps=4087000, episode_reward=2694.63 +/- 12.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4087000  |
---------------------------------
Eval num_timesteps=4088000, episode_reward=2740.66 +/- 13.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4088000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.00037  |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | 0.521    |
|    learning_rate   | 0.000912 |
|    n_updates       | 19960    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.41e+03 |
| time/              |          |
|    episodes        | 4088     |
|    fps             | 640      |
|    time_elapsed    | 6379     |
|    total_timesteps | 4088000  |
---------------------------------
Eval num_timesteps=4089000, episode_reward=2704.74 +/- 52.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4089000  |
---------------------------------
Eval num_timesteps=4090000, episode_reward=2679.86 +/- 40.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 4090000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.000409 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | 0.075    |
|    learning_rate   | 0.00091  |
|    n_updates       | 19970    |
---------------------------------
Eval num_timesteps=4091000, episode_reward=2685.82 +/- 38.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4091000  |
---------------------------------
Eval num_timesteps=4092000, episode_reward=2508.87 +/- 37.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.51e+03 |
| time/              |          |
|    total_timesteps | 4092000  |
| train/             |          |
|    actor_loss      | -0.772   |
|    critic_loss     | 0.00042  |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | 0.515    |
|    learning_rate   | 0.000908 |
|    n_updates       | 19980    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.43e+03 |
| time/              |          |
|    episodes        | 4092     |
|    fps             | 640      |
|    time_elapsed    | 6385     |
|    total_timesteps | 4092000  |
---------------------------------
Eval num_timesteps=4093000, episode_reward=2537.12 +/- 39.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 4093000  |
---------------------------------
Eval num_timesteps=4094000, episode_reward=2620.01 +/- 53.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 4094000  |
| train/             |          |
|    actor_loss      | -0.776   |
|    critic_loss     | 0.000381 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | -1.35    |
|    learning_rate   | 0.000906 |
|    n_updates       | 19990    |
---------------------------------
Eval num_timesteps=4095000, episode_reward=2582.89 +/- 19.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 4095000  |
---------------------------------
Eval num_timesteps=4096000, episode_reward=2563.82 +/- 45.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 4096000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    episodes        | 4096     |
|    fps             | 640      |
|    time_elapsed    | 6391     |
|    total_timesteps | 4096000  |
---------------------------------
Eval num_timesteps=4097000, episode_reward=2570.49 +/- 33.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 4097000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.000393 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | -0.374   |
|    learning_rate   | 0.000904 |
|    n_updates       | 20000    |
---------------------------------
Eval num_timesteps=4098000, episode_reward=2568.34 +/- 48.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.57e+03 |
| time/              |          |
|    total_timesteps | 4098000  |
---------------------------------
Eval num_timesteps=4099000, episode_reward=2641.51 +/- 55.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 4099000  |
| train/             |          |
|    actor_loss      | -0.764   |
|    critic_loss     | 0.000376 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | -2.19    |
|    learning_rate   | 0.000902 |
|    n_updates       | 20010    |
---------------------------------
Eval num_timesteps=4100000, episode_reward=2621.67 +/- 51.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.62e+03 |
| time/              |          |
|    total_timesteps | 4100000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    episodes        | 4100     |
|    fps             | 640      |
|    time_elapsed    | 6397     |
|    total_timesteps | 4100000  |
---------------------------------
Eval num_timesteps=4101000, episode_reward=2688.75 +/- 28.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4101000  |
| train/             |          |
|    actor_loss      | -0.769   |
|    critic_loss     | 0.000367 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | -0.296   |
|    learning_rate   | 0.0009   |
|    n_updates       | 20020    |
---------------------------------
Eval num_timesteps=4102000, episode_reward=2674.08 +/- 24.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 4102000  |
---------------------------------
Eval num_timesteps=4103000, episode_reward=2717.27 +/- 34.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4103000  |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.000377 |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | -0.226   |
|    learning_rate   | 0.000898 |
|    n_updates       | 20030    |
---------------------------------
Eval num_timesteps=4104000, episode_reward=2695.52 +/- 21.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4104000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.52e+03 |
| time/              |          |
|    episodes        | 4104     |
|    fps             | 640      |
|    time_elapsed    | 6403     |
|    total_timesteps | 4104000  |
---------------------------------
Eval num_timesteps=4105000, episode_reward=2564.52 +/- 45.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 4105000  |
| train/             |          |
|    actor_loss      | -0.793   |
|    critic_loss     | 0.000405 |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | 2.43     |
|    learning_rate   | 0.000896 |
|    n_updates       | 20040    |
---------------------------------
Eval num_timesteps=4106000, episode_reward=2561.78 +/- 15.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 4106000  |
---------------------------------
Eval num_timesteps=4107000, episode_reward=2695.66 +/- 45.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4107000  |
| train/             |          |
|    actor_loss      | -0.76    |
|    critic_loss     | 0.000335 |
|    ent_coef        | 0.000512 |
|    ent_coef_loss   | -3.23    |
|    learning_rate   | 0.000894 |
|    n_updates       | 20050    |
---------------------------------
Eval num_timesteps=4108000, episode_reward=2686.33 +/- 44.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4108000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    episodes        | 4108     |
|    fps             | 640      |
|    time_elapsed    | 6409     |
|    total_timesteps | 4108000  |
---------------------------------
Eval num_timesteps=4109000, episode_reward=2626.42 +/- 14.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 4109000  |
| train/             |          |
|    actor_loss      | -0.761   |
|    critic_loss     | 0.000351 |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | -4.31    |
|    learning_rate   | 0.000892 |
|    n_updates       | 20060    |
---------------------------------
Eval num_timesteps=4110000, episode_reward=2626.90 +/- 18.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 4110000  |
---------------------------------
Eval num_timesteps=4111000, episode_reward=2646.41 +/- 36.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 4111000  |
| train/             |          |
|    actor_loss      | -0.77    |
|    critic_loss     | 0.00033  |
|    ent_coef        | 0.000509 |
|    ent_coef_loss   | -2.19    |
|    learning_rate   | 0.00089  |
|    n_updates       | 20070    |
---------------------------------
Eval num_timesteps=4112000, episode_reward=2612.43 +/- 38.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 4112000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 4112     |
|    fps             | 640      |
|    time_elapsed    | 6416     |
|    total_timesteps | 4112000  |
---------------------------------
Eval num_timesteps=4113000, episode_reward=2694.66 +/- 19.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4113000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.000369 |
|    ent_coef        | 0.000508 |
|    ent_coef_loss   | -1.21    |
|    learning_rate   | 0.000888 |
|    n_updates       | 20080    |
---------------------------------
Eval num_timesteps=4114000, episode_reward=2657.15 +/- 51.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 4114000  |
---------------------------------
Eval num_timesteps=4115000, episode_reward=2780.58 +/- 46.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4115000  |
| train/             |          |
|    actor_loss      | -0.777   |
|    critic_loss     | 0.000411 |
|    ent_coef        | 0.000508 |
|    ent_coef_loss   | -0.281   |
|    learning_rate   | 0.000886 |
|    n_updates       | 20090    |
---------------------------------
Eval num_timesteps=4116000, episode_reward=2782.96 +/- 39.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4116000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 4116     |
|    fps             | 640      |
|    time_elapsed    | 6422     |
|    total_timesteps | 4116000  |
---------------------------------
Eval num_timesteps=4117000, episode_reward=2760.04 +/- 14.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4117000  |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.000413 |
|    ent_coef        | 0.000507 |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.000884 |
|    n_updates       | 20100    |
---------------------------------
Eval num_timesteps=4118000, episode_reward=2790.43 +/- 42.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4118000  |
---------------------------------
Eval num_timesteps=4119000, episode_reward=2923.06 +/- 21.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4119000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.00041  |
|    ent_coef        | 0.000507 |
|    ent_coef_loss   | -0.596   |
|    learning_rate   | 0.000881 |
|    n_updates       | 20110    |
---------------------------------
Eval num_timesteps=4120000, episode_reward=2935.25 +/- 14.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4120000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 4120     |
|    fps             | 640      |
|    time_elapsed    | 6428     |
|    total_timesteps | 4120000  |
---------------------------------
Eval num_timesteps=4121000, episode_reward=2882.65 +/- 22.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4121000  |
| train/             |          |
|    actor_loss      | -0.804   |
|    critic_loss     | 0.000419 |
|    ent_coef        | 0.000506 |
|    ent_coef_loss   | 2.58     |
|    learning_rate   | 0.000879 |
|    n_updates       | 20120    |
---------------------------------
Eval num_timesteps=4122000, episode_reward=2868.43 +/- 61.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4122000  |
---------------------------------
Eval num_timesteps=4123000, episode_reward=2886.28 +/- 20.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4123000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000407 |
|    ent_coef        | 0.000507 |
|    ent_coef_loss   | 3.05     |
|    learning_rate   | 0.000877 |
|    n_updates       | 20130    |
---------------------------------
Eval num_timesteps=4124000, episode_reward=2893.20 +/- 34.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4124000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 4124     |
|    fps             | 640      |
|    time_elapsed    | 6434     |
|    total_timesteps | 4124000  |
---------------------------------
Eval num_timesteps=4125000, episode_reward=2887.68 +/- 35.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4125000  |
| train/             |          |
|    actor_loss      | -0.567   |
|    critic_loss     | 0.000347 |
|    ent_coef        | 0.000507 |
|    ent_coef_loss   | -14.2    |
|    learning_rate   | 0.000875 |
|    n_updates       | 20140    |
---------------------------------
Eval num_timesteps=4126000, episode_reward=2933.18 +/- 9.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4126000  |
---------------------------------
Eval num_timesteps=4127000, episode_reward=2534.67 +/- 34.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 4127000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000426 |
|    ent_coef        | 0.000504 |
|    ent_coef_loss   | 0.135    |
|    learning_rate   | 0.000873 |
|    n_updates       | 20150    |
---------------------------------
Eval num_timesteps=4128000, episode_reward=2543.61 +/- 52.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 4128000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 4128     |
|    fps             | 640      |
|    time_elapsed    | 6440     |
|    total_timesteps | 4128000  |
---------------------------------
Eval num_timesteps=4129000, episode_reward=2991.68 +/- 26.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4129000  |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.000382 |
|    ent_coef        | 0.000503 |
|    ent_coef_loss   | -1.55    |
|    learning_rate   | 0.000871 |
|    n_updates       | 20160    |
---------------------------------
New best mean reward!
Eval num_timesteps=4130000, episode_reward=2977.23 +/- 22.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4130000  |
---------------------------------
Eval num_timesteps=4131000, episode_reward=2938.47 +/- 19.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4131000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.000444 |
|    ent_coef        | 0.000502 |
|    ent_coef_loss   | 4.82     |
|    learning_rate   | 0.000869 |
|    n_updates       | 20170    |
---------------------------------
Eval num_timesteps=4132000, episode_reward=2941.65 +/- 25.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4132000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 4132     |
|    fps             | 640      |
|    time_elapsed    | 6447     |
|    total_timesteps | 4132000  |
---------------------------------
Eval num_timesteps=4133000, episode_reward=3070.43 +/- 37.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4133000  |
| train/             |          |
|    actor_loss      | -0.8     |
|    critic_loss     | 0.00039  |
|    ent_coef        | 0.000503 |
|    ent_coef_loss   | 4.01     |
|    learning_rate   | 0.000867 |
|    n_updates       | 20180    |
---------------------------------
New best mean reward!
Eval num_timesteps=4134000, episode_reward=3080.63 +/- 34.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4134000  |
---------------------------------
New best mean reward!
Eval num_timesteps=4135000, episode_reward=2996.12 +/- 41.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4135000  |
| train/             |          |
|    actor_loss      | -0.807   |
|    critic_loss     | 0.000441 |
|    ent_coef        | 0.000505 |
|    ent_coef_loss   | 4.36     |
|    learning_rate   | 0.000865 |
|    n_updates       | 20190    |
---------------------------------
Eval num_timesteps=4136000, episode_reward=2995.01 +/- 53.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4136000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 4136     |
|    fps             | 640      |
|    time_elapsed    | 6453     |
|    total_timesteps | 4136000  |
---------------------------------
Eval num_timesteps=4137000, episode_reward=2988.08 +/- 26.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4137000  |
| train/             |          |
|    actor_loss      | -0.804   |
|    critic_loss     | 0.000483 |
|    ent_coef        | 0.000507 |
|    ent_coef_loss   | 4.54     |
|    learning_rate   | 0.000863 |
|    n_updates       | 20200    |
---------------------------------
Eval num_timesteps=4138000, episode_reward=3012.00 +/- 26.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4138000  |
---------------------------------
Eval num_timesteps=4139000, episode_reward=2989.82 +/- 31.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4139000  |
---------------------------------
Eval num_timesteps=4140000, episode_reward=3002.06 +/- 46.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4140000  |
| train/             |          |
|    actor_loss      | -0.808   |
|    critic_loss     | 0.000493 |
|    ent_coef        | 0.000509 |
|    ent_coef_loss   | 6.55     |
|    learning_rate   | 0.000861 |
|    n_updates       | 20210    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 4140     |
|    fps             | 640      |
|    time_elapsed    | 6460     |
|    total_timesteps | 4140000  |
---------------------------------
Eval num_timesteps=4141000, episode_reward=3018.20 +/- 31.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4141000  |
---------------------------------
Eval num_timesteps=4142000, episode_reward=2991.56 +/- 6.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4142000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000429 |
|    ent_coef        | 0.000511 |
|    ent_coef_loss   | 1.59     |
|    learning_rate   | 0.000859 |
|    n_updates       | 20220    |
---------------------------------
Eval num_timesteps=4143000, episode_reward=2987.61 +/- 18.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4143000  |
---------------------------------
Eval num_timesteps=4144000, episode_reward=3070.87 +/- 19.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4144000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.000493 |
|    ent_coef        | 0.000513 |
|    ent_coef_loss   | 3.34     |
|    learning_rate   | 0.000857 |
|    n_updates       | 20230    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 4144     |
|    fps             | 640      |
|    time_elapsed    | 6466     |
|    total_timesteps | 4144000  |
---------------------------------
Eval num_timesteps=4145000, episode_reward=3074.46 +/- 30.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4145000  |
---------------------------------
Eval num_timesteps=4146000, episode_reward=3032.35 +/- 26.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4146000  |
| train/             |          |
|    actor_loss      | -0.804   |
|    critic_loss     | 0.000486 |
|    ent_coef        | 0.000514 |
|    ent_coef_loss   | 2.98     |
|    learning_rate   | 0.000855 |
|    n_updates       | 20240    |
---------------------------------
Eval num_timesteps=4147000, episode_reward=3002.10 +/- 29.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4147000  |
---------------------------------
Eval num_timesteps=4148000, episode_reward=3019.00 +/- 22.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4148000  |
| train/             |          |
|    actor_loss      | -0.809   |
|    critic_loss     | 0.000463 |
|    ent_coef        | 0.000516 |
|    ent_coef_loss   | 3.58     |
|    learning_rate   | 0.000853 |
|    n_updates       | 20250    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 4148     |
|    fps             | 640      |
|    time_elapsed    | 6472     |
|    total_timesteps | 4148000  |
---------------------------------
Eval num_timesteps=4149000, episode_reward=2995.42 +/- 30.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4149000  |
---------------------------------
Eval num_timesteps=4150000, episode_reward=3016.32 +/- 34.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4150000  |
| train/             |          |
|    actor_loss      | -0.8     |
|    critic_loss     | 0.000459 |
|    ent_coef        | 0.000517 |
|    ent_coef_loss   | 1.54     |
|    learning_rate   | 0.000851 |
|    n_updates       | 20260    |
---------------------------------
Eval num_timesteps=4151000, episode_reward=3009.43 +/- 34.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4151000  |
---------------------------------
Eval num_timesteps=4152000, episode_reward=2906.00 +/- 13.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4152000  |
| train/             |          |
|    actor_loss      | -0.793   |
|    critic_loss     | 0.000507 |
|    ent_coef        | 0.000518 |
|    ent_coef_loss   | 0.272    |
|    learning_rate   | 0.000849 |
|    n_updates       | 20270    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 4152     |
|    fps             | 640      |
|    time_elapsed    | 6478     |
|    total_timesteps | 4152000  |
---------------------------------
Eval num_timesteps=4153000, episode_reward=2898.12 +/- 10.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4153000  |
---------------------------------
Eval num_timesteps=4154000, episode_reward=3075.46 +/- 19.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4154000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.000466 |
|    ent_coef        | 0.000519 |
|    ent_coef_loss   | 3.21     |
|    learning_rate   | 0.000847 |
|    n_updates       | 20280    |
---------------------------------
Eval num_timesteps=4155000, episode_reward=3071.47 +/- 30.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4155000  |
---------------------------------
Eval num_timesteps=4156000, episode_reward=3052.78 +/- 21.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4156000  |
| train/             |          |
|    actor_loss      | -0.804   |
|    critic_loss     | 0.000509 |
|    ent_coef        | 0.00052  |
|    ent_coef_loss   | 5.22     |
|    learning_rate   | 0.000845 |
|    n_updates       | 20290    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 4156     |
|    fps             | 640      |
|    time_elapsed    | 6484     |
|    total_timesteps | 4156000  |
---------------------------------
Eval num_timesteps=4157000, episode_reward=3044.65 +/- 14.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4157000  |
---------------------------------
Eval num_timesteps=4158000, episode_reward=3029.74 +/- 20.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4158000  |
| train/             |          |
|    actor_loss      | -0.792   |
|    critic_loss     | 0.00057  |
|    ent_coef        | 0.000522 |
|    ent_coef_loss   | 4.31     |
|    learning_rate   | 0.000843 |
|    n_updates       | 20300    |
---------------------------------
Eval num_timesteps=4159000, episode_reward=2989.78 +/- 31.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4159000  |
---------------------------------
Eval num_timesteps=4160000, episode_reward=3028.54 +/- 24.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4160000  |
| train/             |          |
|    actor_loss      | -0.797   |
|    critic_loss     | 0.000506 |
|    ent_coef        | 0.000524 |
|    ent_coef_loss   | 3.46     |
|    learning_rate   | 0.000841 |
|    n_updates       | 20310    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 4160     |
|    fps             | 640      |
|    time_elapsed    | 6491     |
|    total_timesteps | 4160000  |
---------------------------------
Eval num_timesteps=4161000, episode_reward=3049.69 +/- 28.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4161000  |
---------------------------------
Eval num_timesteps=4162000, episode_reward=3026.14 +/- 65.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4162000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.000513 |
|    ent_coef        | 0.000526 |
|    ent_coef_loss   | 3.95     |
|    learning_rate   | 0.000838 |
|    n_updates       | 20320    |
---------------------------------
Eval num_timesteps=4163000, episode_reward=3085.70 +/- 26.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4163000  |
---------------------------------
New best mean reward!
Eval num_timesteps=4164000, episode_reward=3050.01 +/- 23.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4164000  |
| train/             |          |
|    actor_loss      | -0.671   |
|    critic_loss     | 0.000534 |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | -5.72    |
|    learning_rate   | 0.000836 |
|    n_updates       | 20330    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 4164     |
|    fps             | 640      |
|    time_elapsed    | 6497     |
|    total_timesteps | 4164000  |
---------------------------------
Eval num_timesteps=4165000, episode_reward=3066.17 +/- 39.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4165000  |
---------------------------------
Eval num_timesteps=4166000, episode_reward=2985.66 +/- 14.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4166000  |
| train/             |          |
|    actor_loss      | -0.699   |
|    critic_loss     | 0.000481 |
|    ent_coef        | 0.000526 |
|    ent_coef_loss   | -6.39    |
|    learning_rate   | 0.000834 |
|    n_updates       | 20340    |
---------------------------------
Eval num_timesteps=4167000, episode_reward=2984.31 +/- 21.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4167000  |
---------------------------------
Eval num_timesteps=4168000, episode_reward=3017.97 +/- 44.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4168000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.000493 |
|    ent_coef        | 0.000525 |
|    ent_coef_loss   | 4.02     |
|    learning_rate   | 0.000832 |
|    n_updates       | 20350    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 4168     |
|    fps             | 640      |
|    time_elapsed    | 6503     |
|    total_timesteps | 4168000  |
---------------------------------
Eval num_timesteps=4169000, episode_reward=3022.05 +/- 34.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4169000  |
---------------------------------
Eval num_timesteps=4170000, episode_reward=3026.10 +/- 28.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4170000  |
| train/             |          |
|    actor_loss      | -0.794   |
|    critic_loss     | 0.000493 |
|    ent_coef        | 0.000525 |
|    ent_coef_loss   | 1.29     |
|    learning_rate   | 0.00083  |
|    n_updates       | 20360    |
---------------------------------
Eval num_timesteps=4171000, episode_reward=2988.47 +/- 21.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4171000  |
---------------------------------
Eval num_timesteps=4172000, episode_reward=3008.57 +/- 27.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4172000  |
| train/             |          |
|    actor_loss      | -0.79    |
|    critic_loss     | 0.000417 |
|    ent_coef        | 0.000525 |
|    ent_coef_loss   | -0.484   |
|    learning_rate   | 0.000828 |
|    n_updates       | 20370    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 4172     |
|    fps             | 640      |
|    time_elapsed    | 6509     |
|    total_timesteps | 4172000  |
---------------------------------
Eval num_timesteps=4173000, episode_reward=3016.83 +/- 11.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4173000  |
---------------------------------
Eval num_timesteps=4174000, episode_reward=3039.76 +/- 17.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4174000  |
| train/             |          |
|    actor_loss      | -0.795   |
|    critic_loss     | 0.000508 |
|    ent_coef        | 0.000525 |
|    ent_coef_loss   | 2.56     |
|    learning_rate   | 0.000826 |
|    n_updates       | 20380    |
---------------------------------
Eval num_timesteps=4175000, episode_reward=3038.12 +/- 18.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4175000  |
---------------------------------
Eval num_timesteps=4176000, episode_reward=3032.28 +/- 38.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4176000  |
| train/             |          |
|    actor_loss      | -0.79    |
|    critic_loss     | 0.000514 |
|    ent_coef        | 0.000526 |
|    ent_coef_loss   | 1.29     |
|    learning_rate   | 0.000824 |
|    n_updates       | 20390    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 4176     |
|    fps             | 640      |
|    time_elapsed    | 6515     |
|    total_timesteps | 4176000  |
---------------------------------
Eval num_timesteps=4177000, episode_reward=3051.22 +/- 23.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4177000  |
---------------------------------
Eval num_timesteps=4178000, episode_reward=3087.96 +/- 24.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4178000  |
| train/             |          |
|    actor_loss      | -0.794   |
|    critic_loss     | 0.000499 |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | 1.41     |
|    learning_rate   | 0.000822 |
|    n_updates       | 20400    |
---------------------------------
New best mean reward!
Eval num_timesteps=4179000, episode_reward=3074.62 +/- 32.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4179000  |
---------------------------------
Eval num_timesteps=4180000, episode_reward=3087.59 +/- 24.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4180000  |
| train/             |          |
|    actor_loss      | -0.797   |
|    critic_loss     | 0.000518 |
|    ent_coef        | 0.000528 |
|    ent_coef_loss   | 2.55     |
|    learning_rate   | 0.00082  |
|    n_updates       | 20410    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 4180     |
|    fps             | 640      |
|    time_elapsed    | 6522     |
|    total_timesteps | 4180000  |
---------------------------------
Eval num_timesteps=4181000, episode_reward=3091.97 +/- 23.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4181000  |
---------------------------------
New best mean reward!
Eval num_timesteps=4182000, episode_reward=3096.62 +/- 33.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4182000  |
---------------------------------
New best mean reward!
Eval num_timesteps=4183000, episode_reward=3053.73 +/- 18.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4183000  |
| train/             |          |
|    actor_loss      | -0.797   |
|    critic_loss     | 0.000471 |
|    ent_coef        | 0.000529 |
|    ent_coef_loss   | 1.68     |
|    learning_rate   | 0.000818 |
|    n_updates       | 20420    |
---------------------------------
Eval num_timesteps=4184000, episode_reward=3031.43 +/- 43.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4184000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 4184     |
|    fps             | 640      |
|    time_elapsed    | 6528     |
|    total_timesteps | 4184000  |
---------------------------------
Eval num_timesteps=4185000, episode_reward=3029.98 +/- 26.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4185000  |
| train/             |          |
|    actor_loss      | -0.794   |
|    critic_loss     | 0.00046  |
|    ent_coef        | 0.000529 |
|    ent_coef_loss   | -0.31    |
|    learning_rate   | 0.000816 |
|    n_updates       | 20430    |
---------------------------------
Eval num_timesteps=4186000, episode_reward=3061.04 +/- 15.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4186000  |
---------------------------------
Eval num_timesteps=4187000, episode_reward=3053.39 +/- 25.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4187000  |
| train/             |          |
|    actor_loss      | -0.796   |
|    critic_loss     | 0.000479 |
|    ent_coef        | 0.00053  |
|    ent_coef_loss   | -0.707   |
|    learning_rate   | 0.000814 |
|    n_updates       | 20440    |
---------------------------------
Eval num_timesteps=4188000, episode_reward=3039.87 +/- 26.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4188000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 4188     |
|    fps             | 640      |
|    time_elapsed    | 6536     |
|    total_timesteps | 4188000  |
---------------------------------
Eval num_timesteps=4189000, episode_reward=3003.85 +/- 24.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4189000  |
| train/             |          |
|    actor_loss      | -0.806   |
|    critic_loss     | 0.000467 |
|    ent_coef        | 0.00053  |
|    ent_coef_loss   | 3.73     |
|    learning_rate   | 0.000812 |
|    n_updates       | 20450    |
---------------------------------
Eval num_timesteps=4190000, episode_reward=3026.95 +/- 18.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4190000  |
---------------------------------
Eval num_timesteps=4191000, episode_reward=3075.92 +/- 11.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4191000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.000427 |
|    ent_coef        | 0.000531 |
|    ent_coef_loss   | 1.45     |
|    learning_rate   | 0.00081  |
|    n_updates       | 20460    |
---------------------------------
Eval num_timesteps=4192000, episode_reward=3047.53 +/- 17.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4192000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 4192     |
|    fps             | 640      |
|    time_elapsed    | 6542     |
|    total_timesteps | 4192000  |
---------------------------------
Eval num_timesteps=4193000, episode_reward=3067.92 +/- 12.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4193000  |
| train/             |          |
|    actor_loss      | -0.804   |
|    critic_loss     | 0.000503 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | 2.28     |
|    learning_rate   | 0.000808 |
|    n_updates       | 20470    |
---------------------------------
Eval num_timesteps=4194000, episode_reward=3069.35 +/- 38.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4194000  |
---------------------------------
Eval num_timesteps=4195000, episode_reward=3003.58 +/- 27.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4195000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000415 |
|    ent_coef        | 0.000533 |
|    ent_coef_loss   | 2.37     |
|    learning_rate   | 0.000806 |
|    n_updates       | 20480    |
---------------------------------
Eval num_timesteps=4196000, episode_reward=3003.33 +/- 26.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4196000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.69e+03 |
| time/              |          |
|    episodes        | 4196     |
|    fps             | 640      |
|    time_elapsed    | 6549     |
|    total_timesteps | 4196000  |
---------------------------------
Eval num_timesteps=4197000, episode_reward=3111.04 +/- 17.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 4197000  |
| train/             |          |
|    actor_loss      | -0.81    |
|    critic_loss     | 0.000439 |
|    ent_coef        | 0.000534 |
|    ent_coef_loss   | 2.81     |
|    learning_rate   | 0.000804 |
|    n_updates       | 20490    |
---------------------------------
New best mean reward!
Eval num_timesteps=4198000, episode_reward=3099.02 +/- 31.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4198000  |
---------------------------------
Eval num_timesteps=4199000, episode_reward=2968.14 +/- 26.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4199000  |
| train/             |          |
|    actor_loss      | -0.817   |
|    critic_loss     | 0.000462 |
|    ent_coef        | 0.000535 |
|    ent_coef_loss   | 2.07     |
|    learning_rate   | 0.000802 |
|    n_updates       | 20500    |
---------------------------------
Eval num_timesteps=4200000, episode_reward=2978.18 +/- 20.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4200000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.71e+03 |
| time/              |          |
|    episodes        | 4200     |
|    fps             | 640      |
|    time_elapsed    | 6555     |
|    total_timesteps | 4200000  |
---------------------------------
Eval num_timesteps=4201000, episode_reward=2904.53 +/- 11.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4201000  |
| train/             |          |
|    actor_loss      | -0.802   |
|    critic_loss     | 0.00043  |
|    ent_coef        | 0.000536 |
|    ent_coef_loss   | 0.368    |
|    learning_rate   | 0.0008   |
|    n_updates       | 20510    |
---------------------------------
Eval num_timesteps=4202000, episode_reward=2891.44 +/- 11.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4202000  |
---------------------------------
Eval num_timesteps=4203000, episode_reward=2875.25 +/- 19.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4203000  |
| train/             |          |
|    actor_loss      | -0.799   |
|    critic_loss     | 0.000454 |
|    ent_coef        | 0.000537 |
|    ent_coef_loss   | 2.99     |
|    learning_rate   | 0.000798 |
|    n_updates       | 20520    |
---------------------------------
Eval num_timesteps=4204000, episode_reward=2840.26 +/- 20.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4204000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 4204     |
|    fps             | 640      |
|    time_elapsed    | 6561     |
|    total_timesteps | 4204000  |
---------------------------------
Eval num_timesteps=4205000, episode_reward=2773.20 +/- 24.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4205000  |
| train/             |          |
|    actor_loss      | -0.8     |
|    critic_loss     | 0.00041  |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | -0.375   |
|    learning_rate   | 0.000795 |
|    n_updates       | 20530    |
---------------------------------
Eval num_timesteps=4206000, episode_reward=2771.11 +/- 21.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4206000  |
---------------------------------
Eval num_timesteps=4207000, episode_reward=2830.19 +/- 27.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4207000  |
| train/             |          |
|    actor_loss      | -0.792   |
|    critic_loss     | 0.000431 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | -1.07    |
|    learning_rate   | 0.000793 |
|    n_updates       | 20540    |
---------------------------------
Eval num_timesteps=4208000, episode_reward=2850.99 +/- 20.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4208000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    episodes        | 4208     |
|    fps             | 640      |
|    time_elapsed    | 6568     |
|    total_timesteps | 4208000  |
---------------------------------
Eval num_timesteps=4209000, episode_reward=2770.53 +/- 24.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4209000  |
| train/             |          |
|    actor_loss      | -0.798   |
|    critic_loss     | 0.000421 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | 0.963    |
|    learning_rate   | 0.000791 |
|    n_updates       | 20550    |
---------------------------------
Eval num_timesteps=4210000, episode_reward=2804.78 +/- 18.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4210000  |
---------------------------------
Eval num_timesteps=4211000, episode_reward=2855.75 +/- 12.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4211000  |
| train/             |          |
|    actor_loss      | -0.785   |
|    critic_loss     | 0.000413 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | -1.02    |
|    learning_rate   | 0.000789 |
|    n_updates       | 20560    |
---------------------------------
Eval num_timesteps=4212000, episode_reward=2869.30 +/- 18.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4212000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    episodes        | 4212     |
|    fps             | 640      |
|    time_elapsed    | 6574     |
|    total_timesteps | 4212000  |
---------------------------------
Eval num_timesteps=4213000, episode_reward=2742.20 +/- 14.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4213000  |
| train/             |          |
|    actor_loss      | -0.793   |
|    critic_loss     | 0.000441 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | 0.662    |
|    learning_rate   | 0.000787 |
|    n_updates       | 20570    |
---------------------------------
Eval num_timesteps=4214000, episode_reward=2757.52 +/- 13.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4214000  |
---------------------------------
Eval num_timesteps=4215000, episode_reward=2703.44 +/- 19.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4215000  |
| train/             |          |
|    actor_loss      | -0.785   |
|    critic_loss     | 0.000398 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | 0.105    |
|    learning_rate   | 0.000785 |
|    n_updates       | 20580    |
---------------------------------
Eval num_timesteps=4216000, episode_reward=2711.42 +/- 28.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4216000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    episodes        | 4216     |
|    fps             | 640      |
|    time_elapsed    | 6580     |
|    total_timesteps | 4216000  |
---------------------------------
Eval num_timesteps=4217000, episode_reward=2595.05 +/- 31.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 4217000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.000401 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | -0.0849  |
|    learning_rate   | 0.000783 |
|    n_updates       | 20590    |
---------------------------------
Eval num_timesteps=4218000, episode_reward=2592.19 +/- 17.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.59e+03 |
| time/              |          |
|    total_timesteps | 4218000  |
---------------------------------
Eval num_timesteps=4219000, episode_reward=2780.25 +/- 6.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4219000  |
| train/             |          |
|    actor_loss      | -0.773   |
|    critic_loss     | 0.000384 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | -4.05    |
|    learning_rate   | 0.000781 |
|    n_updates       | 20600    |
---------------------------------
Eval num_timesteps=4220000, episode_reward=2767.34 +/- 52.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4220000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.74e+03 |
| time/              |          |
|    episodes        | 4220     |
|    fps             | 640      |
|    time_elapsed    | 6586     |
|    total_timesteps | 4220000  |
---------------------------------
Eval num_timesteps=4221000, episode_reward=2180.70 +/- 1145.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 4221000  |
| train/             |          |
|    actor_loss      | -0.783   |
|    critic_loss     | 0.00044  |
|    ent_coef        | 0.000537 |
|    ent_coef_loss   | -0.872   |
|    learning_rate   | 0.000779 |
|    n_updates       | 20610    |
---------------------------------
Eval num_timesteps=4222000, episode_reward=2740.55 +/- 30.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4222000  |
---------------------------------
Eval num_timesteps=4223000, episode_reward=2822.52 +/- 7.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 4223000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.000412 |
|    ent_coef        | 0.000536 |
|    ent_coef_loss   | -1.24    |
|    learning_rate   | 0.000777 |
|    n_updates       | 20620    |
---------------------------------
Eval num_timesteps=4224000, episode_reward=2809.89 +/- 19.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4224000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4224     |
|    fps             | 640      |
|    time_elapsed    | 6592     |
|    total_timesteps | 4224000  |
---------------------------------
Eval num_timesteps=4225000, episode_reward=2809.99 +/- 11.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4225000  |
---------------------------------
Eval num_timesteps=4226000, episode_reward=2647.13 +/- 11.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 4226000  |
| train/             |          |
|    actor_loss      | -0.773   |
|    critic_loss     | 0.000426 |
|    ent_coef        | 0.000535 |
|    ent_coef_loss   | -2.37    |
|    learning_rate   | 0.000775 |
|    n_updates       | 20630    |
---------------------------------
Eval num_timesteps=4227000, episode_reward=2663.58 +/- 37.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 4227000  |
---------------------------------
Eval num_timesteps=4228000, episode_reward=2781.05 +/- 33.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4228000  |
| train/             |          |
|    actor_loss      | -0.767   |
|    critic_loss     | 0.00038  |
|    ent_coef        | 0.000534 |
|    ent_coef_loss   | -4.98    |
|    learning_rate   | 0.000773 |
|    n_updates       | 20640    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4228     |
|    fps             | 640      |
|    time_elapsed    | 6598     |
|    total_timesteps | 4228000  |
---------------------------------
Eval num_timesteps=4229000, episode_reward=2776.87 +/- 25.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4229000  |
---------------------------------
Eval num_timesteps=4230000, episode_reward=2829.65 +/- 33.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4230000  |
| train/             |          |
|    actor_loss      | -0.775   |
|    critic_loss     | 0.000419 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | 0.0463   |
|    learning_rate   | 0.000771 |
|    n_updates       | 20650    |
---------------------------------
Eval num_timesteps=4231000, episode_reward=2845.93 +/- 16.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4231000  |
---------------------------------
Eval num_timesteps=4232000, episode_reward=2955.74 +/- 26.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4232000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.000427 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | -0.72    |
|    learning_rate   | 0.000769 |
|    n_updates       | 20660    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4232     |
|    fps             | 640      |
|    time_elapsed    | 6605     |
|    total_timesteps | 4232000  |
---------------------------------
Eval num_timesteps=4233000, episode_reward=2984.09 +/- 16.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4233000  |
---------------------------------
Eval num_timesteps=4234000, episode_reward=3046.65 +/- 30.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4234000  |
| train/             |          |
|    actor_loss      | -0.797   |
|    critic_loss     | 0.000445 |
|    ent_coef        | 0.000531 |
|    ent_coef_loss   | 3.4      |
|    learning_rate   | 0.000767 |
|    n_updates       | 20670    |
---------------------------------
Eval num_timesteps=4235000, episode_reward=3047.46 +/- 3.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4235000  |
---------------------------------
Eval num_timesteps=4236000, episode_reward=3035.64 +/- 34.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4236000  |
| train/             |          |
|    actor_loss      | -0.805   |
|    critic_loss     | 0.000459 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | 2.38     |
|    learning_rate   | 0.000765 |
|    n_updates       | 20680    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4236     |
|    fps             | 640      |
|    time_elapsed    | 6611     |
|    total_timesteps | 4236000  |
---------------------------------
Eval num_timesteps=4237000, episode_reward=3048.66 +/- 40.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4237000  |
---------------------------------
Eval num_timesteps=4238000, episode_reward=3067.30 +/- 29.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4238000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000501 |
|    ent_coef        | 0.000533 |
|    ent_coef_loss   | 3.15     |
|    learning_rate   | 0.000763 |
|    n_updates       | 20690    |
---------------------------------
Eval num_timesteps=4239000, episode_reward=3076.38 +/- 35.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4239000  |
---------------------------------
Eval num_timesteps=4240000, episode_reward=3005.69 +/- 39.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4240000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.000469 |
|    ent_coef        | 0.000535 |
|    ent_coef_loss   | 2.2      |
|    learning_rate   | 0.000761 |
|    n_updates       | 20700    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4240     |
|    fps             | 640      |
|    time_elapsed    | 6617     |
|    total_timesteps | 4240000  |
---------------------------------
Eval num_timesteps=4241000, episode_reward=3020.40 +/- 20.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4241000  |
---------------------------------
Eval num_timesteps=4242000, episode_reward=3063.25 +/- 16.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4242000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000457 |
|    ent_coef        | 0.000536 |
|    ent_coef_loss   | 1.66     |
|    learning_rate   | 0.000759 |
|    n_updates       | 20710    |
---------------------------------
Eval num_timesteps=4243000, episode_reward=3069.63 +/- 40.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4243000  |
---------------------------------
Eval num_timesteps=4244000, episode_reward=2982.57 +/- 55.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4244000  |
| train/             |          |
|    actor_loss      | -0.799   |
|    critic_loss     | 0.0005   |
|    ent_coef        | 0.000537 |
|    ent_coef_loss   | 0.217    |
|    learning_rate   | 0.000757 |
|    n_updates       | 20720    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4244     |
|    fps             | 640      |
|    time_elapsed    | 6623     |
|    total_timesteps | 4244000  |
---------------------------------
Eval num_timesteps=4245000, episode_reward=2975.77 +/- 41.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4245000  |
---------------------------------
Eval num_timesteps=4246000, episode_reward=2974.17 +/- 19.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4246000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000449 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | 1.84     |
|    learning_rate   | 0.000754 |
|    n_updates       | 20730    |
---------------------------------
Eval num_timesteps=4247000, episode_reward=2955.10 +/- 17.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4247000  |
---------------------------------
Eval num_timesteps=4248000, episode_reward=2883.16 +/- 12.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4248000  |
| train/             |          |
|    actor_loss      | -0.795   |
|    critic_loss     | 0.000477 |
|    ent_coef        | 0.000538 |
|    ent_coef_loss   | 2.24     |
|    learning_rate   | 0.000752 |
|    n_updates       | 20740    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4248     |
|    fps             | 640      |
|    time_elapsed    | 6629     |
|    total_timesteps | 4248000  |
---------------------------------
Eval num_timesteps=4249000, episode_reward=2894.83 +/- 29.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4249000  |
---------------------------------
Eval num_timesteps=4250000, episode_reward=2857.53 +/- 21.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4250000  |
| train/             |          |
|    actor_loss      | -0.793   |
|    critic_loss     | 0.000529 |
|    ent_coef        | 0.000539 |
|    ent_coef_loss   | -0.0645  |
|    learning_rate   | 0.00075  |
|    n_updates       | 20750    |
---------------------------------
Eval num_timesteps=4251000, episode_reward=2928.86 +/- 27.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4251000  |
---------------------------------
Eval num_timesteps=4252000, episode_reward=2816.30 +/- 19.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 4252000  |
| train/             |          |
|    actor_loss      | -0.786   |
|    critic_loss     | 0.000478 |
|    ent_coef        | 0.00054  |
|    ent_coef_loss   | 0.886    |
|    learning_rate   | 0.000748 |
|    n_updates       | 20760    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.78e+03 |
| time/              |          |
|    episodes        | 4252     |
|    fps             | 640      |
|    time_elapsed    | 6636     |
|    total_timesteps | 4252000  |
---------------------------------
Eval num_timesteps=4253000, episode_reward=2799.36 +/- 15.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4253000  |
---------------------------------
Eval num_timesteps=4254000, episode_reward=2839.18 +/- 29.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4254000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.000449 |
|    ent_coef        | 0.00054  |
|    ent_coef_loss   | -0.501   |
|    learning_rate   | 0.000746 |
|    n_updates       | 20770    |
---------------------------------
Eval num_timesteps=4255000, episode_reward=2811.12 +/- 17.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4255000  |
---------------------------------
Eval num_timesteps=4256000, episode_reward=2915.85 +/- 43.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4256000  |
| train/             |          |
|    actor_loss      | -0.783   |
|    critic_loss     | 0.000454 |
|    ent_coef        | 0.00054  |
|    ent_coef_loss   | -1.92    |
|    learning_rate   | 0.000744 |
|    n_updates       | 20780    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4256     |
|    fps             | 640      |
|    time_elapsed    | 6642     |
|    total_timesteps | 4256000  |
---------------------------------
Eval num_timesteps=4257000, episode_reward=2891.54 +/- 22.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4257000  |
---------------------------------
Eval num_timesteps=4258000, episode_reward=3085.45 +/- 43.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4258000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.000444 |
|    ent_coef        | 0.000539 |
|    ent_coef_loss   | 0.878    |
|    learning_rate   | 0.000742 |
|    n_updates       | 20790    |
---------------------------------
Eval num_timesteps=4259000, episode_reward=3130.70 +/- 30.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 4259000  |
---------------------------------
New best mean reward!
Eval num_timesteps=4260000, episode_reward=3062.46 +/- 32.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4260000  |
| train/             |          |
|    actor_loss      | -0.773   |
|    critic_loss     | 0.000517 |
|    ent_coef        | 0.00054  |
|    ent_coef_loss   | 0.991    |
|    learning_rate   | 0.00074  |
|    n_updates       | 20800    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4260     |
|    fps             | 640      |
|    time_elapsed    | 6648     |
|    total_timesteps | 4260000  |
---------------------------------
Eval num_timesteps=4261000, episode_reward=3073.77 +/- 21.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4261000  |
---------------------------------
Eval num_timesteps=4262000, episode_reward=3015.88 +/- 51.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4262000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.000513 |
|    ent_coef        | 0.00054  |
|    ent_coef_loss   | 3.03     |
|    learning_rate   | 0.000738 |
|    n_updates       | 20810    |
---------------------------------
Eval num_timesteps=4263000, episode_reward=3043.15 +/- 13.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4263000  |
---------------------------------
Eval num_timesteps=4264000, episode_reward=3063.98 +/- 33.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4264000  |
| train/             |          |
|    actor_loss      | -0.807   |
|    critic_loss     | 0.000487 |
|    ent_coef        | 0.000541 |
|    ent_coef_loss   | 2.4      |
|    learning_rate   | 0.000736 |
|    n_updates       | 20820    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 4264     |
|    fps             | 640      |
|    time_elapsed    | 6654     |
|    total_timesteps | 4264000  |
---------------------------------
Eval num_timesteps=4265000, episode_reward=3079.85 +/- 39.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4265000  |
---------------------------------
Eval num_timesteps=4266000, episode_reward=3070.02 +/- 23.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4266000  |
| train/             |          |
|    actor_loss      | -0.787   |
|    critic_loss     | 0.000569 |
|    ent_coef        | 0.000542 |
|    ent_coef_loss   | 2.39     |
|    learning_rate   | 0.000734 |
|    n_updates       | 20830    |
---------------------------------
Eval num_timesteps=4267000, episode_reward=3070.05 +/- 37.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4267000  |
---------------------------------
Eval num_timesteps=4268000, episode_reward=3072.01 +/- 15.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4268000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 4268     |
|    fps             | 640      |
|    time_elapsed    | 6660     |
|    total_timesteps | 4268000  |
---------------------------------
Eval num_timesteps=4269000, episode_reward=3189.98 +/- 18.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4269000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.000506 |
|    ent_coef        | 0.000544 |
|    ent_coef_loss   | 6.88     |
|    learning_rate   | 0.000732 |
|    n_updates       | 20840    |
---------------------------------
New best mean reward!
Eval num_timesteps=4270000, episode_reward=3186.58 +/- 33.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4270000  |
---------------------------------
Eval num_timesteps=4271000, episode_reward=3208.42 +/- 28.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 4271000  |
| train/             |          |
|    actor_loss      | -0.815   |
|    critic_loss     | 0.000484 |
|    ent_coef        | 0.000547 |
|    ent_coef_loss   | 3.98     |
|    learning_rate   | 0.00073  |
|    n_updates       | 20850    |
---------------------------------
New best mean reward!
Eval num_timesteps=4272000, episode_reward=3234.71 +/- 7.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4272000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 4272     |
|    fps             | 640      |
|    time_elapsed    | 6667     |
|    total_timesteps | 4272000  |
---------------------------------
Eval num_timesteps=4273000, episode_reward=3189.45 +/- 7.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4273000  |
| train/             |          |
|    actor_loss      | -0.802   |
|    critic_loss     | 0.000558 |
|    ent_coef        | 0.000549 |
|    ent_coef_loss   | 2.55     |
|    learning_rate   | 0.000728 |
|    n_updates       | 20860    |
---------------------------------
Eval num_timesteps=4274000, episode_reward=3209.50 +/- 41.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 4274000  |
---------------------------------
Eval num_timesteps=4275000, episode_reward=3138.13 +/- 52.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4275000  |
| train/             |          |
|    actor_loss      | -0.818   |
|    critic_loss     | 0.000532 |
|    ent_coef        | 0.000551 |
|    ent_coef_loss   | 7.23     |
|    learning_rate   | 0.000726 |
|    n_updates       | 20870    |
---------------------------------
Eval num_timesteps=4276000, episode_reward=3154.17 +/- 32.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 4276000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    episodes        | 4276     |
|    fps             | 640      |
|    time_elapsed    | 6673     |
|    total_timesteps | 4276000  |
---------------------------------
Eval num_timesteps=4277000, episode_reward=3037.68 +/- 16.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4277000  |
| train/             |          |
|    actor_loss      | -0.816   |
|    critic_loss     | 0.000523 |
|    ent_coef        | 0.000555 |
|    ent_coef_loss   | 3.89     |
|    learning_rate   | 0.000724 |
|    n_updates       | 20880    |
---------------------------------
Eval num_timesteps=4278000, episode_reward=3029.15 +/- 15.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4278000  |
---------------------------------
Eval num_timesteps=4279000, episode_reward=3095.55 +/- 54.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4279000  |
| train/             |          |
|    actor_loss      | -0.737   |
|    critic_loss     | 0.000476 |
|    ent_coef        | 0.000556 |
|    ent_coef_loss   | -6.97    |
|    learning_rate   | 0.000722 |
|    n_updates       | 20890    |
---------------------------------
Eval num_timesteps=4280000, episode_reward=3125.44 +/- 38.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 4280000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 4280     |
|    fps             | 640      |
|    time_elapsed    | 6680     |
|    total_timesteps | 4280000  |
---------------------------------
Eval num_timesteps=4281000, episode_reward=3086.45 +/- 23.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4281000  |
| train/             |          |
|    actor_loss      | -0.629   |
|    critic_loss     | 0.00035  |
|    ent_coef        | 0.000554 |
|    ent_coef_loss   | -19.7    |
|    learning_rate   | 0.00072  |
|    n_updates       | 20900    |
---------------------------------
Eval num_timesteps=4282000, episode_reward=3081.04 +/- 28.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4282000  |
---------------------------------
Eval num_timesteps=4283000, episode_reward=3165.65 +/- 30.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4283000  |
| train/             |          |
|    actor_loss      | -0.797   |
|    critic_loss     | 0.000494 |
|    ent_coef        | 0.000548 |
|    ent_coef_loss   | 2.49     |
|    learning_rate   | 0.000718 |
|    n_updates       | 20910    |
---------------------------------
Eval num_timesteps=4284000, episode_reward=3186.72 +/- 49.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4284000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.81e+03 |
| time/              |          |
|    episodes        | 4284     |
|    fps             | 640      |
|    time_elapsed    | 6686     |
|    total_timesteps | 4284000  |
---------------------------------
Eval num_timesteps=4285000, episode_reward=3221.70 +/- 15.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4285000  |
| train/             |          |
|    actor_loss      | -0.803   |
|    critic_loss     | 0.000537 |
|    ent_coef        | 0.000546 |
|    ent_coef_loss   | 4.43     |
|    learning_rate   | 0.000716 |
|    n_updates       | 20920    |
---------------------------------
Eval num_timesteps=4286000, episode_reward=3243.36 +/- 37.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4286000  |
---------------------------------
New best mean reward!
Eval num_timesteps=4287000, episode_reward=3175.16 +/- 28.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4287000  |
| train/             |          |
|    actor_loss      | -0.815   |
|    critic_loss     | 0.00057  |
|    ent_coef        | 0.000547 |
|    ent_coef_loss   | 6.84     |
|    learning_rate   | 0.000714 |
|    n_updates       | 20930    |
---------------------------------
Eval num_timesteps=4288000, episode_reward=3190.81 +/- 44.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4288000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 4288     |
|    fps             | 640      |
|    time_elapsed    | 6692     |
|    total_timesteps | 4288000  |
---------------------------------
Eval num_timesteps=4289000, episode_reward=3228.75 +/- 32.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4289000  |
| train/             |          |
|    actor_loss      | -0.802   |
|    critic_loss     | 0.00053  |
|    ent_coef        | 0.00055  |
|    ent_coef_loss   | 2.66     |
|    learning_rate   | 0.000711 |
|    n_updates       | 20940    |
---------------------------------
Eval num_timesteps=4290000, episode_reward=3227.17 +/- 40.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4290000  |
---------------------------------
Eval num_timesteps=4291000, episode_reward=3104.41 +/- 44.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4291000  |
| train/             |          |
|    actor_loss      | -0.802   |
|    critic_loss     | 0.000587 |
|    ent_coef        | 0.000552 |
|    ent_coef_loss   | 1.62     |
|    learning_rate   | 0.000709 |
|    n_updates       | 20950    |
---------------------------------
Eval num_timesteps=4292000, episode_reward=3122.08 +/- 42.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 4292000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.82e+03 |
| time/              |          |
|    episodes        | 4292     |
|    fps             | 640      |
|    time_elapsed    | 6698     |
|    total_timesteps | 4292000  |
---------------------------------
Eval num_timesteps=4293000, episode_reward=3239.21 +/- 64.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4293000  |
| train/             |          |
|    actor_loss      | -0.8     |
|    critic_loss     | 0.000499 |
|    ent_coef        | 0.000553 |
|    ent_coef_loss   | 3.04     |
|    learning_rate   | 0.000707 |
|    n_updates       | 20960    |
---------------------------------
Eval num_timesteps=4294000, episode_reward=3228.25 +/- 27.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4294000  |
---------------------------------
Eval num_timesteps=4295000, episode_reward=3194.91 +/- 45.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4295000  |
| train/             |          |
|    actor_loss      | -0.806   |
|    critic_loss     | 0.000509 |
|    ent_coef        | 0.000554 |
|    ent_coef_loss   | 2.14     |
|    learning_rate   | 0.000705 |
|    n_updates       | 20970    |
---------------------------------
Eval num_timesteps=4296000, episode_reward=3205.50 +/- 26.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 4296000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    episodes        | 4296     |
|    fps             | 640      |
|    time_elapsed    | 6705     |
|    total_timesteps | 4296000  |
---------------------------------
Eval num_timesteps=4297000, episode_reward=3191.74 +/- 23.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4297000  |
| train/             |          |
|    actor_loss      | -0.804   |
|    critic_loss     | 0.000577 |
|    ent_coef        | 0.000556 |
|    ent_coef_loss   | 2.12     |
|    learning_rate   | 0.000703 |
|    n_updates       | 20980    |
---------------------------------
Eval num_timesteps=4298000, episode_reward=3225.19 +/- 35.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4298000  |
---------------------------------
Eval num_timesteps=4299000, episode_reward=3181.43 +/- 16.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4299000  |
| train/             |          |
|    actor_loss      | -0.811   |
|    critic_loss     | 0.000559 |
|    ent_coef        | 0.000557 |
|    ent_coef_loss   | 2.44     |
|    learning_rate   | 0.000701 |
|    n_updates       | 20990    |
---------------------------------
Eval num_timesteps=4300000, episode_reward=3161.35 +/- 38.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 4300000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    episodes        | 4300     |
|    fps             | 640      |
|    time_elapsed    | 6711     |
|    total_timesteps | 4300000  |
---------------------------------
Eval num_timesteps=4301000, episode_reward=3260.87 +/- 21.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4301000  |
| train/             |          |
|    actor_loss      | -0.802   |
|    critic_loss     | 0.000543 |
|    ent_coef        | 0.000558 |
|    ent_coef_loss   | 3.56     |
|    learning_rate   | 0.000699 |
|    n_updates       | 21000    |
---------------------------------
New best mean reward!
Eval num_timesteps=4302000, episode_reward=3234.35 +/- 26.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4302000  |
---------------------------------
Eval num_timesteps=4303000, episode_reward=3185.51 +/- 18.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4303000  |
| train/             |          |
|    actor_loss      | -0.799   |
|    critic_loss     | 0.000542 |
|    ent_coef        | 0.00056  |
|    ent_coef_loss   | 3.09     |
|    learning_rate   | 0.000697 |
|    n_updates       | 21010    |
---------------------------------
Eval num_timesteps=4304000, episode_reward=3188.04 +/- 30.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4304000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.84e+03 |
| time/              |          |
|    episodes        | 4304     |
|    fps             | 640      |
|    time_elapsed    | 6717     |
|    total_timesteps | 4304000  |
---------------------------------
Eval num_timesteps=4305000, episode_reward=3236.93 +/- 58.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4305000  |
| train/             |          |
|    actor_loss      | -0.808   |
|    critic_loss     | 0.000566 |
|    ent_coef        | 0.000562 |
|    ent_coef_loss   | 4.18     |
|    learning_rate   | 0.000695 |
|    n_updates       | 21020    |
---------------------------------
Eval num_timesteps=4306000, episode_reward=3250.21 +/- 35.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4306000  |
---------------------------------
Eval num_timesteps=4307000, episode_reward=3256.26 +/- 44.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4307000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000607 |
|    ent_coef        | 0.000564 |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 0.000693 |
|    n_updates       | 21030    |
---------------------------------
Eval num_timesteps=4308000, episode_reward=3269.30 +/- 51.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 4308000  |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 4308     |
|    fps             | 640      |
|    time_elapsed    | 6724     |
|    total_timesteps | 4308000  |
---------------------------------
Eval num_timesteps=4309000, episode_reward=3202.09 +/- 30.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4309000  |
| train/             |          |
|    actor_loss      | -0.797   |
|    critic_loss     | 0.000531 |
|    ent_coef        | 0.000566 |
|    ent_coef_loss   | 1.59     |
|    learning_rate   | 0.000691 |
|    n_updates       | 21040    |
---------------------------------
Eval num_timesteps=4310000, episode_reward=3205.56 +/- 71.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 4310000  |
---------------------------------
Eval num_timesteps=4311000, episode_reward=3228.98 +/- 40.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4311000  |
---------------------------------
Eval num_timesteps=4312000, episode_reward=3247.40 +/- 34.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4312000  |
| train/             |          |
|    actor_loss      | -0.789   |
|    critic_loss     | 0.000568 |
|    ent_coef        | 0.000567 |
|    ent_coef_loss   | 1.59     |
|    learning_rate   | 0.000689 |
|    n_updates       | 21050    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.85e+03 |
| time/              |          |
|    episodes        | 4312     |
|    fps             | 640      |
|    time_elapsed    | 6730     |
|    total_timesteps | 4312000  |
---------------------------------
Eval num_timesteps=4313000, episode_reward=3198.37 +/- 29.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4313000  |
---------------------------------
Eval num_timesteps=4314000, episode_reward=3212.75 +/- 32.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 4314000  |
| train/             |          |
|    actor_loss      | -0.787   |
|    critic_loss     | 0.000583 |
|    ent_coef        | 0.000568 |
|    ent_coef_loss   | 0.912    |
|    learning_rate   | 0.000687 |
|    n_updates       | 21060    |
---------------------------------
Eval num_timesteps=4315000, episode_reward=3199.13 +/- 41.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4315000  |
---------------------------------
Eval num_timesteps=4316000, episode_reward=3200.49 +/- 27.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4316000  |
| train/             |          |
|    actor_loss      | -0.787   |
|    critic_loss     | 0.000581 |
|    ent_coef        | 0.000568 |
|    ent_coef_loss   | 0.364    |
|    learning_rate   | 0.000685 |
|    n_updates       | 21070    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    episodes        | 4316     |
|    fps             | 640      |
|    time_elapsed    | 6736     |
|    total_timesteps | 4316000  |
---------------------------------
Eval num_timesteps=4317000, episode_reward=3179.41 +/- 41.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4317000  |
---------------------------------
Eval num_timesteps=4318000, episode_reward=3164.74 +/- 55.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 4318000  |
| train/             |          |
|    actor_loss      | -0.778   |
|    critic_loss     | 0.000578 |
|    ent_coef        | 0.000569 |
|    ent_coef_loss   | 1.72     |
|    learning_rate   | 0.000683 |
|    n_updates       | 21080    |
---------------------------------
Eval num_timesteps=4319000, episode_reward=3195.99 +/- 17.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4319000  |
---------------------------------
Eval num_timesteps=4320000, episode_reward=3139.71 +/- 59.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4320000  |
| train/             |          |
|    actor_loss      | -0.786   |
|    critic_loss     | 0.00059  |
|    ent_coef        | 0.00057  |
|    ent_coef_loss   | 1.03     |
|    learning_rate   | 0.000681 |
|    n_updates       | 21090    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 4320     |
|    fps             | 640      |
|    time_elapsed    | 6742     |
|    total_timesteps | 4320000  |
---------------------------------
Eval num_timesteps=4321000, episode_reward=3199.44 +/- 17.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4321000  |
---------------------------------
Eval num_timesteps=4322000, episode_reward=3130.97 +/- 16.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 4322000  |
| train/             |          |
|    actor_loss      | -0.785   |
|    critic_loss     | 0.00054  |
|    ent_coef        | 0.00057  |
|    ent_coef_loss   | -0.306   |
|    learning_rate   | 0.000679 |
|    n_updates       | 21100    |
---------------------------------
Eval num_timesteps=4323000, episode_reward=3136.05 +/- 33.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4323000  |
---------------------------------
Eval num_timesteps=4324000, episode_reward=3144.48 +/- 24.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4324000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.000547 |
|    ent_coef        | 0.00057  |
|    ent_coef_loss   | -0.856   |
|    learning_rate   | 0.000677 |
|    n_updates       | 21110    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 4324     |
|    fps             | 640      |
|    time_elapsed    | 6749     |
|    total_timesteps | 4324000  |
---------------------------------
Eval num_timesteps=4325000, episode_reward=3155.28 +/- 27.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 4325000  |
---------------------------------
Eval num_timesteps=4326000, episode_reward=3216.66 +/- 36.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4326000  |
| train/             |          |
|    actor_loss      | -0.779   |
|    critic_loss     | 0.000531 |
|    ent_coef        | 0.00057  |
|    ent_coef_loss   | -1.51    |
|    learning_rate   | 0.000675 |
|    n_updates       | 21120    |
---------------------------------
Eval num_timesteps=4327000, episode_reward=3171.27 +/- 26.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4327000  |
---------------------------------
Eval num_timesteps=4328000, episode_reward=3169.50 +/- 25.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4328000  |
| train/             |          |
|    actor_loss      | -0.779   |
|    critic_loss     | 0.000542 |
|    ent_coef        | 0.000569 |
|    ent_coef_loss   | 1.27     |
|    learning_rate   | 0.000673 |
|    n_updates       | 21130    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    episodes        | 4328     |
|    fps             | 640      |
|    time_elapsed    | 6755     |
|    total_timesteps | 4328000  |
---------------------------------
Eval num_timesteps=4329000, episode_reward=3225.43 +/- 22.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4329000  |
---------------------------------
Eval num_timesteps=4330000, episode_reward=3193.05 +/- 52.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4330000  |
| train/             |          |
|    actor_loss      | -0.801   |
|    critic_loss     | 0.000519 |
|    ent_coef        | 0.00057  |
|    ent_coef_loss   | 3.17     |
|    learning_rate   | 0.000671 |
|    n_updates       | 21140    |
---------------------------------
Eval num_timesteps=4331000, episode_reward=3227.06 +/- 44.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4331000  |
---------------------------------
Eval num_timesteps=4332000, episode_reward=3248.22 +/- 19.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4332000  |
| train/             |          |
|    actor_loss      | -0.797   |
|    critic_loss     | 0.000536 |
|    ent_coef        | 0.000571 |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 0.000668 |
|    n_updates       | 21150    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    episodes        | 4332     |
|    fps             | 640      |
|    time_elapsed    | 6761     |
|    total_timesteps | 4332000  |
---------------------------------
Eval num_timesteps=4333000, episode_reward=3203.31 +/- 42.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4333000  |
---------------------------------
Eval num_timesteps=4334000, episode_reward=3245.00 +/- 36.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4334000  |
| train/             |          |
|    actor_loss      | -0.79    |
|    critic_loss     | 0.000563 |
|    ent_coef        | 0.000572 |
|    ent_coef_loss   | 1.43     |
|    learning_rate   | 0.000666 |
|    n_updates       | 21160    |
---------------------------------
Eval num_timesteps=4335000, episode_reward=3236.32 +/- 34.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4335000  |
---------------------------------
Eval num_timesteps=4336000, episode_reward=3238.38 +/- 43.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4336000  |
| train/             |          |
|    actor_loss      | -0.787   |
|    critic_loss     | 0.000611 |
|    ent_coef        | 0.000573 |
|    ent_coef_loss   | 2.46     |
|    learning_rate   | 0.000664 |
|    n_updates       | 21170    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    episodes        | 4336     |
|    fps             | 640      |
|    time_elapsed    | 6767     |
|    total_timesteps | 4336000  |
---------------------------------
Eval num_timesteps=4337000, episode_reward=3204.35 +/- 27.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4337000  |
---------------------------------
Eval num_timesteps=4338000, episode_reward=3294.64 +/- 14.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 4338000  |
| train/             |          |
|    actor_loss      | -0.772   |
|    critic_loss     | 0.000699 |
|    ent_coef        | 0.000574 |
|    ent_coef_loss   | -0.172   |
|    learning_rate   | 0.000662 |
|    n_updates       | 21180    |
---------------------------------
New best mean reward!
Eval num_timesteps=4339000, episode_reward=3274.60 +/- 47.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 4339000  |
---------------------------------
Eval num_timesteps=4340000, episode_reward=3145.47 +/- 45.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 4340000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.000654 |
|    ent_coef        | 0.000574 |
|    ent_coef_loss   | 0.99     |
|    learning_rate   | 0.00066  |
|    n_updates       | 21190    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    episodes        | 4340     |
|    fps             | 640      |
|    time_elapsed    | 6774     |
|    total_timesteps | 4340000  |
---------------------------------
Eval num_timesteps=4341000, episode_reward=3190.88 +/- 17.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4341000  |
---------------------------------
Eval num_timesteps=4342000, episode_reward=3185.84 +/- 40.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4342000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.000565 |
|    ent_coef        | 0.000575 |
|    ent_coef_loss   | -0.362   |
|    learning_rate   | 0.000658 |
|    n_updates       | 21200    |
---------------------------------
Eval num_timesteps=4343000, episode_reward=3225.04 +/- 42.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4343000  |
---------------------------------
Eval num_timesteps=4344000, episode_reward=3181.96 +/- 20.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4344000  |
| train/             |          |
|    actor_loss      | -0.779   |
|    critic_loss     | 0.000556 |
|    ent_coef        | 0.000575 |
|    ent_coef_loss   | -1.25    |
|    learning_rate   | 0.000656 |
|    n_updates       | 21210    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.9e+03  |
| time/              |          |
|    episodes        | 4344     |
|    fps             | 640      |
|    time_elapsed    | 6780     |
|    total_timesteps | 4344000  |
---------------------------------
Eval num_timesteps=4345000, episode_reward=3188.05 +/- 27.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4345000  |
---------------------------------
Eval num_timesteps=4346000, episode_reward=3192.62 +/- 23.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4346000  |
| train/             |          |
|    actor_loss      | -0.777   |
|    critic_loss     | 0.000547 |
|    ent_coef        | 0.000574 |
|    ent_coef_loss   | -0.533   |
|    learning_rate   | 0.000654 |
|    n_updates       | 21220    |
---------------------------------
Eval num_timesteps=4347000, episode_reward=3199.13 +/- 32.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4347000  |
---------------------------------
Eval num_timesteps=4348000, episode_reward=3174.08 +/- 61.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4348000  |
| train/             |          |
|    actor_loss      | -0.787   |
|    critic_loss     | 0.00054  |
|    ent_coef        | 0.000574 |
|    ent_coef_loss   | 2.76     |
|    learning_rate   | 0.000652 |
|    n_updates       | 21230    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    episodes        | 4348     |
|    fps             | 640      |
|    time_elapsed    | 6786     |
|    total_timesteps | 4348000  |
---------------------------------
Eval num_timesteps=4349000, episode_reward=3172.72 +/- 48.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4349000  |
---------------------------------
Eval num_timesteps=4350000, episode_reward=3184.50 +/- 13.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4350000  |
| train/             |          |
|    actor_loss      | -0.78    |
|    critic_loss     | 0.00053  |
|    ent_coef        | 0.000575 |
|    ent_coef_loss   | 1.72     |
|    learning_rate   | 0.00065  |
|    n_updates       | 21240    |
---------------------------------
Eval num_timesteps=4351000, episode_reward=3191.39 +/- 40.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.19e+03 |
| time/              |          |
|    total_timesteps | 4351000  |
---------------------------------
Eval num_timesteps=4352000, episode_reward=3151.41 +/- 34.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 4352000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.91e+03 |
| time/              |          |
|    episodes        | 4352     |
|    fps             | 640      |
|    time_elapsed    | 6792     |
|    total_timesteps | 4352000  |
---------------------------------
Eval num_timesteps=4353000, episode_reward=3200.45 +/- 37.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4353000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.000519 |
|    ent_coef        | 0.000576 |
|    ent_coef_loss   | -0.869   |
|    learning_rate   | 0.000648 |
|    n_updates       | 21250    |
---------------------------------
Eval num_timesteps=4354000, episode_reward=3226.54 +/- 37.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4354000  |
---------------------------------
Eval num_timesteps=4355000, episode_reward=3268.99 +/- 31.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 4355000  |
| train/             |          |
|    actor_loss      | -0.777   |
|    critic_loss     | 0.000575 |
|    ent_coef        | 0.000576 |
|    ent_coef_loss   | 2.17     |
|    learning_rate   | 0.000646 |
|    n_updates       | 21260    |
---------------------------------
Eval num_timesteps=4356000, episode_reward=3254.01 +/- 57.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4356000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    episodes        | 4356     |
|    fps             | 640      |
|    time_elapsed    | 6799     |
|    total_timesteps | 4356000  |
---------------------------------
Eval num_timesteps=4357000, episode_reward=3232.27 +/- 58.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4357000  |
| train/             |          |
|    actor_loss      | -0.793   |
|    critic_loss     | 0.000582 |
|    ent_coef        | 0.000577 |
|    ent_coef_loss   | 4.71     |
|    learning_rate   | 0.000644 |
|    n_updates       | 21270    |
---------------------------------
Eval num_timesteps=4358000, episode_reward=3239.93 +/- 39.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4358000  |
---------------------------------
Eval num_timesteps=4359000, episode_reward=3259.97 +/- 31.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4359000  |
| train/             |          |
|    actor_loss      | -0.792   |
|    critic_loss     | 0.000597 |
|    ent_coef        | 0.000579 |
|    ent_coef_loss   | 3.6      |
|    learning_rate   | 0.000642 |
|    n_updates       | 21280    |
---------------------------------
Eval num_timesteps=4360000, episode_reward=3250.12 +/- 17.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4360000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 4360     |
|    fps             | 640      |
|    time_elapsed    | 6805     |
|    total_timesteps | 4360000  |
---------------------------------
Eval num_timesteps=4361000, episode_reward=3223.86 +/- 46.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4361000  |
| train/             |          |
|    actor_loss      | -0.792   |
|    critic_loss     | 0.000622 |
|    ent_coef        | 0.000581 |
|    ent_coef_loss   | 4.78     |
|    learning_rate   | 0.00064  |
|    n_updates       | 21290    |
---------------------------------
Eval num_timesteps=4362000, episode_reward=3248.21 +/- 39.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4362000  |
---------------------------------
Eval num_timesteps=4363000, episode_reward=3252.89 +/- 20.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4363000  |
| train/             |          |
|    actor_loss      | -0.788   |
|    critic_loss     | 0.000593 |
|    ent_coef        | 0.000584 |
|    ent_coef_loss   | 3.64     |
|    learning_rate   | 0.000638 |
|    n_updates       | 21300    |
---------------------------------
Eval num_timesteps=4364000, episode_reward=3258.70 +/- 30.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4364000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 4364     |
|    fps             | 640      |
|    time_elapsed    | 6811     |
|    total_timesteps | 4364000  |
---------------------------------
Eval num_timesteps=4365000, episode_reward=3235.78 +/- 23.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4365000  |
| train/             |          |
|    actor_loss      | -0.776   |
|    critic_loss     | 0.000619 |
|    ent_coef        | 0.000586 |
|    ent_coef_loss   | 2.44     |
|    learning_rate   | 0.000636 |
|    n_updates       | 21310    |
---------------------------------
Eval num_timesteps=4366000, episode_reward=3246.92 +/- 42.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4366000  |
---------------------------------
Eval num_timesteps=4367000, episode_reward=3249.44 +/- 35.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4367000  |
| train/             |          |
|    actor_loss      | -0.792   |
|    critic_loss     | 0.000587 |
|    ent_coef        | 0.000588 |
|    ent_coef_loss   | 2.78     |
|    learning_rate   | 0.000634 |
|    n_updates       | 21320    |
---------------------------------
Eval num_timesteps=4368000, episode_reward=3245.06 +/- 37.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4368000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 4368     |
|    fps             | 640      |
|    time_elapsed    | 6817     |
|    total_timesteps | 4368000  |
---------------------------------
Eval num_timesteps=4369000, episode_reward=3221.17 +/- 19.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4369000  |
| train/             |          |
|    actor_loss      | -0.771   |
|    critic_loss     | 0.000597 |
|    ent_coef        | 0.000589 |
|    ent_coef_loss   | -0.417   |
|    learning_rate   | 0.000632 |
|    n_updates       | 21330    |
---------------------------------
Eval num_timesteps=4370000, episode_reward=3260.56 +/- 32.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4370000  |
---------------------------------
Eval num_timesteps=4371000, episode_reward=3251.31 +/- 30.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4371000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.000536 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | -0.76    |
|    learning_rate   | 0.00063  |
|    n_updates       | 21340    |
---------------------------------
Eval num_timesteps=4372000, episode_reward=3221.24 +/- 14.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4372000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 4372     |
|    fps             | 640      |
|    time_elapsed    | 6823     |
|    total_timesteps | 4372000  |
---------------------------------
Eval num_timesteps=4373000, episode_reward=3259.78 +/- 38.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4373000  |
| train/             |          |
|    actor_loss      | -0.793   |
|    critic_loss     | 0.000585 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | 1.47     |
|    learning_rate   | 0.000628 |
|    n_updates       | 21350    |
---------------------------------
Eval num_timesteps=4374000, episode_reward=3272.27 +/- 16.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 4374000  |
---------------------------------
Eval num_timesteps=4375000, episode_reward=3160.71 +/- 31.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 4375000  |
| train/             |          |
|    actor_loss      | -0.786   |
|    critic_loss     | 0.000622 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | 0.0626   |
|    learning_rate   | 0.000625 |
|    n_updates       | 21360    |
---------------------------------
Eval num_timesteps=4376000, episode_reward=3140.01 +/- 46.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4376000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 4376     |
|    fps             | 640      |
|    time_elapsed    | 6830     |
|    total_timesteps | 4376000  |
---------------------------------
Eval num_timesteps=4377000, episode_reward=3259.77 +/- 17.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4377000  |
| train/             |          |
|    actor_loss      | -0.788   |
|    critic_loss     | 0.00056  |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | 0.214    |
|    learning_rate   | 0.000623 |
|    n_updates       | 21370    |
---------------------------------
Eval num_timesteps=4378000, episode_reward=3261.42 +/- 36.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4378000  |
---------------------------------
Eval num_timesteps=4379000, episode_reward=3053.94 +/- 25.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4379000  |
| train/             |          |
|    actor_loss      | -0.789   |
|    critic_loss     | 0.000577 |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | 3.27     |
|    learning_rate   | 0.000621 |
|    n_updates       | 21380    |
---------------------------------
Eval num_timesteps=4380000, episode_reward=3034.43 +/- 21.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4380000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 4380     |
|    fps             | 640      |
|    time_elapsed    | 6836     |
|    total_timesteps | 4380000  |
---------------------------------
Eval num_timesteps=4381000, episode_reward=3076.14 +/- 35.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4381000  |
| train/             |          |
|    actor_loss      | -0.771   |
|    critic_loss     | 0.000516 |
|    ent_coef        | 0.000592 |
|    ent_coef_loss   | -2.84    |
|    learning_rate   | 0.000619 |
|    n_updates       | 21390    |
---------------------------------
Eval num_timesteps=4382000, episode_reward=3087.09 +/- 14.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4382000  |
---------------------------------
Eval num_timesteps=4383000, episode_reward=3077.37 +/- 29.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4383000  |
| train/             |          |
|    actor_loss      | -0.775   |
|    critic_loss     | 0.000565 |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | -3.5     |
|    learning_rate   | 0.000617 |
|    n_updates       | 21400    |
---------------------------------
Eval num_timesteps=4384000, episode_reward=3064.63 +/- 49.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4384000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 4384     |
|    fps             | 640      |
|    time_elapsed    | 6842     |
|    total_timesteps | 4384000  |
---------------------------------
Eval num_timesteps=4385000, episode_reward=2898.21 +/- 46.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4385000  |
| train/             |          |
|    actor_loss      | -0.769   |
|    critic_loss     | 0.000526 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | -2       |
|    learning_rate   | 0.000615 |
|    n_updates       | 21410    |
---------------------------------
Eval num_timesteps=4386000, episode_reward=2856.74 +/- 27.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4386000  |
---------------------------------
Eval num_timesteps=4387000, episode_reward=3138.47 +/- 21.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4387000  |
| train/             |          |
|    actor_loss      | -0.765   |
|    critic_loss     | 0.000498 |
|    ent_coef        | 0.000588 |
|    ent_coef_loss   | -2.19    |
|    learning_rate   | 0.000613 |
|    n_updates       | 21420    |
---------------------------------
Eval num_timesteps=4388000, episode_reward=3163.32 +/- 25.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 4388000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 4388     |
|    fps             | 640      |
|    time_elapsed    | 6848     |
|    total_timesteps | 4388000  |
---------------------------------
Eval num_timesteps=4389000, episode_reward=3097.54 +/- 18.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4389000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.000532 |
|    ent_coef        | 0.000587 |
|    ent_coef_loss   | -1.45    |
|    learning_rate   | 0.000611 |
|    n_updates       | 21430    |
---------------------------------
Eval num_timesteps=4390000, episode_reward=3110.98 +/- 32.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 4390000  |
---------------------------------
Eval num_timesteps=4391000, episode_reward=3168.41 +/- 15.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4391000  |
| train/             |          |
|    actor_loss      | -0.773   |
|    critic_loss     | 0.000568 |
|    ent_coef        | 0.000586 |
|    ent_coef_loss   | 0.379    |
|    learning_rate   | 0.000609 |
|    n_updates       | 21440    |
---------------------------------
Eval num_timesteps=4392000, episode_reward=3131.50 +/- 31.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 4392000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.95e+03 |
| time/              |          |
|    episodes        | 4392     |
|    fps             | 640      |
|    time_elapsed    | 6854     |
|    total_timesteps | 4392000  |
---------------------------------
Eval num_timesteps=4393000, episode_reward=3120.65 +/- 33.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 4393000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.00054  |
|    ent_coef        | 0.000586 |
|    ent_coef_loss   | 2.48     |
|    learning_rate   | 0.000607 |
|    n_updates       | 21450    |
---------------------------------
Eval num_timesteps=4394000, episode_reward=3120.12 +/- 35.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 4394000  |
---------------------------------
Eval num_timesteps=4395000, episode_reward=3109.33 +/- 27.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 4395000  |
---------------------------------
Eval num_timesteps=4396000, episode_reward=3201.32 +/- 41.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4396000  |
| train/             |          |
|    actor_loss      | -0.785   |
|    critic_loss     | 0.000536 |
|    ent_coef        | 0.000587 |
|    ent_coef_loss   | 1.29     |
|    learning_rate   | 0.000605 |
|    n_updates       | 21460    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 4396     |
|    fps             | 640      |
|    time_elapsed    | 6860     |
|    total_timesteps | 4396000  |
---------------------------------
Eval num_timesteps=4397000, episode_reward=3199.61 +/- 66.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.2e+03  |
| time/              |          |
|    total_timesteps | 4397000  |
---------------------------------
Eval num_timesteps=4398000, episode_reward=3131.36 +/- 24.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 4398000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.000588 |
|    ent_coef        | 0.000588 |
|    ent_coef_loss   | 0.79     |
|    learning_rate   | 0.000603 |
|    n_updates       | 21470    |
---------------------------------
Eval num_timesteps=4399000, episode_reward=3164.73 +/- 37.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.16e+03 |
| time/              |          |
|    total_timesteps | 4399000  |
---------------------------------
Eval num_timesteps=4400000, episode_reward=3311.41 +/- 27.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.31e+03 |
| time/              |          |
|    total_timesteps | 4400000  |
| train/             |          |
|    actor_loss      | -0.777   |
|    critic_loss     | 0.000592 |
|    ent_coef        | 0.000588 |
|    ent_coef_loss   | 0.248    |
|    learning_rate   | 0.000601 |
|    n_updates       | 21480    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 4400     |
|    fps             | 640      |
|    time_elapsed    | 6867     |
|    total_timesteps | 4400000  |
---------------------------------
Eval num_timesteps=4401000, episode_reward=3347.44 +/- 39.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.35e+03 |
| time/              |          |
|    total_timesteps | 4401000  |
---------------------------------
New best mean reward!
Eval num_timesteps=4402000, episode_reward=3223.05 +/- 52.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4402000  |
| train/             |          |
|    actor_loss      | -0.796   |
|    critic_loss     | 0.000621 |
|    ent_coef        | 0.000589 |
|    ent_coef_loss   | 3.24     |
|    learning_rate   | 0.000599 |
|    n_updates       | 21490    |
---------------------------------
Eval num_timesteps=4403000, episode_reward=3215.90 +/- 30.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4403000  |
---------------------------------
Eval num_timesteps=4404000, episode_reward=3254.96 +/- 43.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4404000  |
| train/             |          |
|    actor_loss      | -0.782   |
|    critic_loss     | 0.000543 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | -0.622   |
|    learning_rate   | 0.000597 |
|    n_updates       | 21500    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 4404     |
|    fps             | 640      |
|    time_elapsed    | 6873     |
|    total_timesteps | 4404000  |
---------------------------------
Eval num_timesteps=4405000, episode_reward=3300.93 +/- 17.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.3e+03  |
| time/              |          |
|    total_timesteps | 4405000  |
---------------------------------
Eval num_timesteps=4406000, episode_reward=3249.56 +/- 44.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4406000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.000588 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | 0.341    |
|    learning_rate   | 0.000595 |
|    n_updates       | 21510    |
---------------------------------
Eval num_timesteps=4407000, episode_reward=3245.08 +/- 21.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4407000  |
---------------------------------
Eval num_timesteps=4408000, episode_reward=3288.40 +/- 43.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 4408000  |
| train/             |          |
|    actor_loss      | -0.791   |
|    critic_loss     | 0.000595 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | 0.702    |
|    learning_rate   | 0.000593 |
|    n_updates       | 21520    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 4408     |
|    fps             | 640      |
|    time_elapsed    | 6880     |
|    total_timesteps | 4408000  |
---------------------------------
Eval num_timesteps=4409000, episode_reward=3235.47 +/- 32.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4409000  |
---------------------------------
Eval num_timesteps=4410000, episode_reward=3308.70 +/- 29.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.31e+03 |
| time/              |          |
|    total_timesteps | 4410000  |
| train/             |          |
|    actor_loss      | -0.785   |
|    critic_loss     | 0.000579 |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | 1.34     |
|    learning_rate   | 0.000591 |
|    n_updates       | 21530    |
---------------------------------
Eval num_timesteps=4411000, episode_reward=3269.33 +/- 35.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 4411000  |
---------------------------------
Eval num_timesteps=4412000, episode_reward=3249.55 +/- 49.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4412000  |
| train/             |          |
|    actor_loss      | -0.784   |
|    critic_loss     | 0.000579 |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | 1.62     |
|    learning_rate   | 0.000589 |
|    n_updates       | 21540    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 4412     |
|    fps             | 640      |
|    time_elapsed    | 6886     |
|    total_timesteps | 4412000  |
---------------------------------
Eval num_timesteps=4413000, episode_reward=3283.70 +/- 47.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.28e+03 |
| time/              |          |
|    total_timesteps | 4413000  |
---------------------------------
Eval num_timesteps=4414000, episode_reward=3180.34 +/- 37.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4414000  |
| train/             |          |
|    actor_loss      | -0.787   |
|    critic_loss     | 0.000579 |
|    ent_coef        | 0.000592 |
|    ent_coef_loss   | 0.0764   |
|    learning_rate   | 0.000587 |
|    n_updates       | 21550    |
---------------------------------
Eval num_timesteps=4415000, episode_reward=3217.21 +/- 24.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4415000  |
---------------------------------
Eval num_timesteps=4416000, episode_reward=3338.41 +/- 47.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.34e+03 |
| time/              |          |
|    total_timesteps | 4416000  |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.000595 |
|    ent_coef        | 0.000592 |
|    ent_coef_loss   | -2.1     |
|    learning_rate   | 0.000585 |
|    n_updates       | 21560    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 4416     |
|    fps             | 640      |
|    time_elapsed    | 6892     |
|    total_timesteps | 4416000  |
---------------------------------
Eval num_timesteps=4417000, episode_reward=3359.94 +/- 23.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.36e+03 |
| time/              |          |
|    total_timesteps | 4417000  |
---------------------------------
New best mean reward!
Eval num_timesteps=4418000, episode_reward=3306.92 +/- 47.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.31e+03 |
| time/              |          |
|    total_timesteps | 4418000  |
| train/             |          |
|    actor_loss      | -0.787   |
|    critic_loss     | 0.000647 |
|    ent_coef        | 0.000592 |
|    ent_coef_loss   | 2.28     |
|    learning_rate   | 0.000582 |
|    n_updates       | 21570    |
---------------------------------
Eval num_timesteps=4419000, episode_reward=3305.53 +/- 27.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.31e+03 |
| time/              |          |
|    total_timesteps | 4419000  |
---------------------------------
Eval num_timesteps=4420000, episode_reward=3216.26 +/- 33.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4420000  |
| train/             |          |
|    actor_loss      | -0.783   |
|    critic_loss     | 0.000592 |
|    ent_coef        | 0.000592 |
|    ent_coef_loss   | -1.53    |
|    learning_rate   | 0.00058  |
|    n_updates       | 21580    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.98e+03 |
| time/              |          |
|    episodes        | 4420     |
|    fps             | 640      |
|    time_elapsed    | 6898     |
|    total_timesteps | 4420000  |
---------------------------------
Eval num_timesteps=4421000, episode_reward=3210.77 +/- 13.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 4421000  |
---------------------------------
Eval num_timesteps=4422000, episode_reward=3254.92 +/- 30.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4422000  |
| train/             |          |
|    actor_loss      | -0.779   |
|    critic_loss     | 0.000548 |
|    ent_coef        | 0.000592 |
|    ent_coef_loss   | -1.74    |
|    learning_rate   | 0.000578 |
|    n_updates       | 21590    |
---------------------------------
Eval num_timesteps=4423000, episode_reward=3229.06 +/- 31.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4423000  |
---------------------------------
Eval num_timesteps=4424000, episode_reward=3267.94 +/- 50.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 4424000  |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.000583 |
|    ent_coef        | 0.000591 |
|    ent_coef_loss   | -2.11    |
|    learning_rate   | 0.000576 |
|    n_updates       | 21600    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.98e+03 |
| time/              |          |
|    episodes        | 4424     |
|    fps             | 640      |
|    time_elapsed    | 6905     |
|    total_timesteps | 4424000  |
---------------------------------
Eval num_timesteps=4425000, episode_reward=3254.61 +/- 31.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4425000  |
---------------------------------
Eval num_timesteps=4426000, episode_reward=3273.18 +/- 30.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 4426000  |
| train/             |          |
|    actor_loss      | -0.785   |
|    critic_loss     | 0.000544 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | -0.485   |
|    learning_rate   | 0.000574 |
|    n_updates       | 21610    |
---------------------------------
Eval num_timesteps=4427000, episode_reward=3258.04 +/- 44.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4427000  |
---------------------------------
Eval num_timesteps=4428000, episode_reward=3284.81 +/- 50.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.28e+03 |
| time/              |          |
|    total_timesteps | 4428000  |
| train/             |          |
|    actor_loss      | -0.78    |
|    critic_loss     | 0.000567 |
|    ent_coef        | 0.00059  |
|    ent_coef_loss   | 0.103    |
|    learning_rate   | 0.000572 |
|    n_updates       | 21620    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 4428     |
|    fps             | 640      |
|    time_elapsed    | 6911     |
|    total_timesteps | 4428000  |
---------------------------------
Eval num_timesteps=4429000, episode_reward=3244.71 +/- 62.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4429000  |
---------------------------------
Eval num_timesteps=4430000, episode_reward=3222.51 +/- 44.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4430000  |
| train/             |          |
|    actor_loss      | -0.781   |
|    critic_loss     | 0.000619 |
|    ent_coef        | 0.000589 |
|    ent_coef_loss   | -0.232   |
|    learning_rate   | 0.00057  |
|    n_updates       | 21630    |
---------------------------------
Eval num_timesteps=4431000, episode_reward=3223.62 +/- 12.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4431000  |
---------------------------------
Eval num_timesteps=4432000, episode_reward=3315.63 +/- 31.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.32e+03 |
| time/              |          |
|    total_timesteps | 4432000  |
| train/             |          |
|    actor_loss      | -0.783   |
|    critic_loss     | 0.000543 |
|    ent_coef        | 0.000589 |
|    ent_coef_loss   | 0.356    |
|    learning_rate   | 0.000568 |
|    n_updates       | 21640    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 4432     |
|    fps             | 640      |
|    time_elapsed    | 6917     |
|    total_timesteps | 4432000  |
---------------------------------
Eval num_timesteps=4433000, episode_reward=3342.85 +/- 25.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.34e+03 |
| time/              |          |
|    total_timesteps | 4433000  |
---------------------------------
Eval num_timesteps=4434000, episode_reward=3144.14 +/- 47.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4434000  |
| train/             |          |
|    actor_loss      | -0.795   |
|    critic_loss     | 0.000612 |
|    ent_coef        | 0.000589 |
|    ent_coef_loss   | 0.475    |
|    learning_rate   | 0.000566 |
|    n_updates       | 21650    |
---------------------------------
Eval num_timesteps=4435000, episode_reward=3168.65 +/- 34.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4435000  |
---------------------------------
Eval num_timesteps=4436000, episode_reward=3179.43 +/- 35.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4436000  |
| train/             |          |
|    actor_loss      | -0.776   |
|    critic_loss     | 0.000551 |
|    ent_coef        | 0.000589 |
|    ent_coef_loss   | -0.664   |
|    learning_rate   | 0.000564 |
|    n_updates       | 21660    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 4436     |
|    fps             | 640      |
|    time_elapsed    | 6923     |
|    total_timesteps | 4436000  |
---------------------------------
Eval num_timesteps=4437000, episode_reward=3143.82 +/- 43.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4437000  |
---------------------------------
Eval num_timesteps=4438000, episode_reward=3151.53 +/- 28.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 4438000  |
---------------------------------
Eval num_timesteps=4439000, episode_reward=3103.31 +/- 56.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4439000  |
| train/             |          |
|    actor_loss      | -0.77    |
|    critic_loss     | 0.000579 |
|    ent_coef        | 0.000589 |
|    ent_coef_loss   | -0.959   |
|    learning_rate   | 0.000562 |
|    n_updates       | 21670    |
---------------------------------
Eval num_timesteps=4440000, episode_reward=3102.31 +/- 39.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4440000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 4440     |
|    fps             | 640      |
|    time_elapsed    | 6929     |
|    total_timesteps | 4440000  |
---------------------------------
Eval num_timesteps=4441000, episode_reward=3062.46 +/- 19.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4441000  |
| train/             |          |
|    actor_loss      | -0.767   |
|    critic_loss     | 0.000595 |
|    ent_coef        | 0.000589 |
|    ent_coef_loss   | -1.27    |
|    learning_rate   | 0.00056  |
|    n_updates       | 21680    |
---------------------------------
Eval num_timesteps=4442000, episode_reward=3057.12 +/- 41.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4442000  |
---------------------------------
Eval num_timesteps=4443000, episode_reward=3051.28 +/- 19.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4443000  |
| train/             |          |
|    actor_loss      | -0.762   |
|    critic_loss     | 0.000574 |
|    ent_coef        | 0.000588 |
|    ent_coef_loss   | -1.49    |
|    learning_rate   | 0.000558 |
|    n_updates       | 21690    |
---------------------------------
Eval num_timesteps=4444000, episode_reward=2995.16 +/- 29.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4444000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 4444     |
|    fps             | 640      |
|    time_elapsed    | 6936     |
|    total_timesteps | 4444000  |
---------------------------------
Eval num_timesteps=4445000, episode_reward=3211.48 +/- 44.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 4445000  |
| train/             |          |
|    actor_loss      | -0.756   |
|    critic_loss     | 0.000589 |
|    ent_coef        | 0.000587 |
|    ent_coef_loss   | -2.11    |
|    learning_rate   | 0.000556 |
|    n_updates       | 21700    |
---------------------------------
Eval num_timesteps=4446000, episode_reward=3176.71 +/- 44.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4446000  |
---------------------------------
Eval num_timesteps=4447000, episode_reward=3003.33 +/- 46.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4447000  |
| train/             |          |
|    actor_loss      | -0.774   |
|    critic_loss     | 0.000593 |
|    ent_coef        | 0.000586 |
|    ent_coef_loss   | -0.222   |
|    learning_rate   | 0.000554 |
|    n_updates       | 21710    |
---------------------------------
Eval num_timesteps=4448000, episode_reward=3008.93 +/- 31.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4448000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 4448     |
|    fps             | 640      |
|    time_elapsed    | 6942     |
|    total_timesteps | 4448000  |
---------------------------------
Eval num_timesteps=4449000, episode_reward=3210.41 +/- 46.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.21e+03 |
| time/              |          |
|    total_timesteps | 4449000  |
| train/             |          |
|    actor_loss      | -0.764   |
|    critic_loss     | 0.000601 |
|    ent_coef        | 0.000586 |
|    ent_coef_loss   | -1.55    |
|    learning_rate   | 0.000552 |
|    n_updates       | 21720    |
---------------------------------
Eval num_timesteps=4450000, episode_reward=3166.40 +/- 34.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4450000  |
---------------------------------
Eval num_timesteps=4451000, episode_reward=3005.42 +/- 33.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4451000  |
| train/             |          |
|    actor_loss      | -0.769   |
|    critic_loss     | 0.000624 |
|    ent_coef        | 0.000585 |
|    ent_coef_loss   | -1.52    |
|    learning_rate   | 0.00055  |
|    n_updates       | 21730    |
---------------------------------
Eval num_timesteps=4452000, episode_reward=3040.77 +/- 11.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4452000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4452     |
|    fps             | 640      |
|    time_elapsed    | 6948     |
|    total_timesteps | 4452000  |
---------------------------------
Eval num_timesteps=4453000, episode_reward=3167.35 +/- 19.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4453000  |
| train/             |          |
|    actor_loss      | -0.754   |
|    critic_loss     | 0.000588 |
|    ent_coef        | 0.000584 |
|    ent_coef_loss   | -2.3     |
|    learning_rate   | 0.000548 |
|    n_updates       | 21740    |
---------------------------------
Eval num_timesteps=4454000, episode_reward=3146.23 +/- 78.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.15e+03 |
| time/              |          |
|    total_timesteps | 4454000  |
---------------------------------
Eval num_timesteps=4455000, episode_reward=3271.47 +/- 33.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.27e+03 |
| time/              |          |
|    total_timesteps | 4455000  |
| train/             |          |
|    actor_loss      | -0.766   |
|    critic_loss     | 0.000586 |
|    ent_coef        | 0.000583 |
|    ent_coef_loss   | -0.253   |
|    learning_rate   | 0.000546 |
|    n_updates       | 21750    |
---------------------------------
Eval num_timesteps=4456000, episode_reward=3283.10 +/- 39.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.28e+03 |
| time/              |          |
|    total_timesteps | 4456000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4456     |
|    fps             | 640      |
|    time_elapsed    | 6954     |
|    total_timesteps | 4456000  |
---------------------------------
Eval num_timesteps=4457000, episode_reward=3316.52 +/- 39.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.32e+03 |
| time/              |          |
|    total_timesteps | 4457000  |
| train/             |          |
|    actor_loss      | -0.766   |
|    critic_loss     | 0.000609 |
|    ent_coef        | 0.000582 |
|    ent_coef_loss   | -2.36    |
|    learning_rate   | 0.000544 |
|    n_updates       | 21760    |
---------------------------------
Eval num_timesteps=4458000, episode_reward=3299.61 +/- 33.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.3e+03  |
| time/              |          |
|    total_timesteps | 4458000  |
---------------------------------
Eval num_timesteps=4459000, episode_reward=3244.16 +/- 51.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.24e+03 |
| time/              |          |
|    total_timesteps | 4459000  |
| train/             |          |
|    actor_loss      | -0.765   |
|    critic_loss     | 0.000585 |
|    ent_coef        | 0.000581 |
|    ent_coef_loss   | -1.77    |
|    learning_rate   | 0.000542 |
|    n_updates       | 21770    |
---------------------------------
Eval num_timesteps=4460000, episode_reward=3259.69 +/- 38.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4460000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4460     |
|    fps             | 640      |
|    time_elapsed    | 6960     |
|    total_timesteps | 4460000  |
---------------------------------
Eval num_timesteps=4461000, episode_reward=3288.45 +/- 37.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 4461000  |
| train/             |          |
|    actor_loss      | -0.767   |
|    critic_loss     | 0.000646 |
|    ent_coef        | 0.00058  |
|    ent_coef_loss   | -0.747   |
|    learning_rate   | 0.000539 |
|    n_updates       | 21780    |
---------------------------------
Eval num_timesteps=4462000, episode_reward=3286.06 +/- 58.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.29e+03 |
| time/              |          |
|    total_timesteps | 4462000  |
---------------------------------
Eval num_timesteps=4463000, episode_reward=3120.54 +/- 33.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 4463000  |
| train/             |          |
|    actor_loss      | -0.763   |
|    critic_loss     | 0.000617 |
|    ent_coef        | 0.000579 |
|    ent_coef_loss   | -2.1     |
|    learning_rate   | 0.000537 |
|    n_updates       | 21790    |
---------------------------------
Eval num_timesteps=4464000, episode_reward=3126.34 +/- 39.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 4464000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4464     |
|    fps             | 640      |
|    time_elapsed    | 6966     |
|    total_timesteps | 4464000  |
---------------------------------
Eval num_timesteps=4465000, episode_reward=3337.73 +/- 12.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.34e+03 |
| time/              |          |
|    total_timesteps | 4465000  |
| train/             |          |
|    actor_loss      | -0.761   |
|    critic_loss     | 0.000622 |
|    ent_coef        | 0.000578 |
|    ent_coef_loss   | -0.968   |
|    learning_rate   | 0.000535 |
|    n_updates       | 21800    |
---------------------------------
Eval num_timesteps=4466000, episode_reward=3326.86 +/- 27.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.33e+03 |
| time/              |          |
|    total_timesteps | 4466000  |
---------------------------------
Eval num_timesteps=4467000, episode_reward=3215.94 +/- 38.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.22e+03 |
| time/              |          |
|    total_timesteps | 4467000  |
| train/             |          |
|    actor_loss      | -0.779   |
|    critic_loss     | 0.000672 |
|    ent_coef        | 0.000578 |
|    ent_coef_loss   | 2.12     |
|    learning_rate   | 0.000533 |
|    n_updates       | 21810    |
---------------------------------
Eval num_timesteps=4468000, episode_reward=3226.42 +/- 26.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.23e+03 |
| time/              |          |
|    total_timesteps | 4468000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4468     |
|    fps             | 640      |
|    time_elapsed    | 6973     |
|    total_timesteps | 4468000  |
---------------------------------
Eval num_timesteps=4469000, episode_reward=3302.43 +/- 20.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.3e+03  |
| time/              |          |
|    total_timesteps | 4469000  |
| train/             |          |
|    actor_loss      | -0.766   |
|    critic_loss     | 0.000566 |
|    ent_coef        | 0.000578 |
|    ent_coef_loss   | -2.55    |
|    learning_rate   | 0.000531 |
|    n_updates       | 21820    |
---------------------------------
Eval num_timesteps=4470000, episode_reward=3281.10 +/- 26.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.28e+03 |
| time/              |          |
|    total_timesteps | 4470000  |
---------------------------------
Eval num_timesteps=4471000, episode_reward=3144.81 +/- 17.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4471000  |
| train/             |          |
|    actor_loss      | -0.771   |
|    critic_loss     | 0.000633 |
|    ent_coef        | 0.000577 |
|    ent_coef_loss   | -0.684   |
|    learning_rate   | 0.000529 |
|    n_updates       | 21830    |
---------------------------------
Eval num_timesteps=4472000, episode_reward=3178.42 +/- 22.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4472000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4472     |
|    fps             | 640      |
|    time_elapsed    | 6979     |
|    total_timesteps | 4472000  |
---------------------------------
Eval num_timesteps=4473000, episode_reward=3174.05 +/- 65.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4473000  |
| train/             |          |
|    actor_loss      | -0.746   |
|    critic_loss     | 0.000676 |
|    ent_coef        | 0.000576 |
|    ent_coef_loss   | -4.72    |
|    learning_rate   | 0.000527 |
|    n_updates       | 21840    |
---------------------------------
Eval num_timesteps=4474000, episode_reward=3183.53 +/- 23.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.18e+03 |
| time/              |          |
|    total_timesteps | 4474000  |
---------------------------------
Eval num_timesteps=4475000, episode_reward=3247.83 +/- 32.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.25e+03 |
| time/              |          |
|    total_timesteps | 4475000  |
| train/             |          |
|    actor_loss      | -0.77    |
|    critic_loss     | 0.000604 |
|    ent_coef        | 0.000574 |
|    ent_coef_loss   | -0.922   |
|    learning_rate   | 0.000525 |
|    n_updates       | 21850    |
---------------------------------
Eval num_timesteps=4476000, episode_reward=3257.61 +/- 21.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.26e+03 |
| time/              |          |
|    total_timesteps | 4476000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4476     |
|    fps             | 640      |
|    time_elapsed    | 6985     |
|    total_timesteps | 4476000  |
---------------------------------
Eval num_timesteps=4477000, episode_reward=3086.68 +/- 8.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4477000  |
| train/             |          |
|    actor_loss      | -0.775   |
|    critic_loss     | 0.000622 |
|    ent_coef        | 0.000573 |
|    ent_coef_loss   | -0.436   |
|    learning_rate   | 0.000523 |
|    n_updates       | 21860    |
---------------------------------
Eval num_timesteps=4478000, episode_reward=3090.12 +/- 21.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4478000  |
---------------------------------
Eval num_timesteps=4479000, episode_reward=3073.25 +/- 28.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4479000  |
| train/             |          |
|    actor_loss      | -0.757   |
|    critic_loss     | 0.00061  |
|    ent_coef        | 0.000573 |
|    ent_coef_loss   | -3.53    |
|    learning_rate   | 0.000521 |
|    n_updates       | 21870    |
---------------------------------
Eval num_timesteps=4480000, episode_reward=3113.85 +/- 36.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 4480000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4480     |
|    fps             | 640      |
|    time_elapsed    | 6991     |
|    total_timesteps | 4480000  |
---------------------------------
Eval num_timesteps=4481000, episode_reward=3060.69 +/- 27.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4481000  |
---------------------------------
Eval num_timesteps=4482000, episode_reward=2928.49 +/- 43.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4482000  |
| train/             |          |
|    actor_loss      | -0.759   |
|    critic_loss     | 0.000613 |
|    ent_coef        | 0.000571 |
|    ent_coef_loss   | -3.5     |
|    learning_rate   | 0.000519 |
|    n_updates       | 21880    |
---------------------------------
Eval num_timesteps=4483000, episode_reward=2924.61 +/- 35.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4483000  |
---------------------------------
Eval num_timesteps=4484000, episode_reward=2982.49 +/- 24.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4484000  |
| train/             |          |
|    actor_loss      | -0.749   |
|    critic_loss     | 0.00059  |
|    ent_coef        | 0.000569 |
|    ent_coef_loss   | -3.92    |
|    learning_rate   | 0.000517 |
|    n_updates       | 21890    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.02e+03 |
| time/              |          |
|    episodes        | 4484     |
|    fps             | 640      |
|    time_elapsed    | 6997     |
|    total_timesteps | 4484000  |
---------------------------------
Eval num_timesteps=4485000, episode_reward=3029.23 +/- 21.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4485000  |
---------------------------------
Eval num_timesteps=4486000, episode_reward=3000.08 +/- 24.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4486000  |
| train/             |          |
|    actor_loss      | -0.735   |
|    critic_loss     | 0.000611 |
|    ent_coef        | 0.000567 |
|    ent_coef_loss   | -5.69    |
|    learning_rate   | 0.000515 |
|    n_updates       | 21900    |
---------------------------------
Eval num_timesteps=4487000, episode_reward=2998.35 +/- 26.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4487000  |
---------------------------------
Eval num_timesteps=4488000, episode_reward=3034.79 +/- 38.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4488000  |
| train/             |          |
|    actor_loss      | -0.743   |
|    critic_loss     | 0.000605 |
|    ent_coef        | 0.000564 |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.000513 |
|    n_updates       | 21910    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.02e+03 |
| time/              |          |
|    episodes        | 4488     |
|    fps             | 640      |
|    time_elapsed    | 7004     |
|    total_timesteps | 4488000  |
---------------------------------
Eval num_timesteps=4489000, episode_reward=3018.63 +/- 31.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4489000  |
---------------------------------
Eval num_timesteps=4490000, episode_reward=2930.72 +/- 42.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4490000  |
| train/             |          |
|    actor_loss      | -0.753   |
|    critic_loss     | 0.000641 |
|    ent_coef        | 0.000562 |
|    ent_coef_loss   | -1.56    |
|    learning_rate   | 0.000511 |
|    n_updates       | 21920    |
---------------------------------
Eval num_timesteps=4491000, episode_reward=2933.14 +/- 35.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4491000  |
---------------------------------
Eval num_timesteps=4492000, episode_reward=3124.09 +/- 34.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 4492000  |
| train/             |          |
|    actor_loss      | -0.745   |
|    critic_loss     | 0.000555 |
|    ent_coef        | 0.000561 |
|    ent_coef_loss   | -2.44    |
|    learning_rate   | 0.000509 |
|    n_updates       | 21930    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4492     |
|    fps             | 640      |
|    time_elapsed    | 7010     |
|    total_timesteps | 4492000  |
---------------------------------
Eval num_timesteps=4493000, episode_reward=3103.88 +/- 24.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4493000  |
---------------------------------
Eval num_timesteps=4494000, episode_reward=2897.97 +/- 32.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4494000  |
| train/             |          |
|    actor_loss      | -0.749   |
|    critic_loss     | 0.000652 |
|    ent_coef        | 0.000559 |
|    ent_coef_loss   | -0.996   |
|    learning_rate   | 0.000507 |
|    n_updates       | 21940    |
---------------------------------
Eval num_timesteps=4495000, episode_reward=2888.14 +/- 25.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4495000  |
---------------------------------
Eval num_timesteps=4496000, episode_reward=3020.13 +/- 51.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4496000  |
| train/             |          |
|    actor_loss      | -0.736   |
|    critic_loss     | 0.000568 |
|    ent_coef        | 0.000558 |
|    ent_coef_loss   | -1.69    |
|    learning_rate   | 0.000505 |
|    n_updates       | 21950    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3.01e+03 |
| time/              |          |
|    episodes        | 4496     |
|    fps             | 640      |
|    time_elapsed    | 7016     |
|    total_timesteps | 4496000  |
---------------------------------
Eval num_timesteps=4497000, episode_reward=3029.62 +/- 20.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4497000  |
---------------------------------
Eval num_timesteps=4498000, episode_reward=2790.36 +/- 39.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4498000  |
| train/             |          |
|    actor_loss      | -0.755   |
|    critic_loss     | 0.000596 |
|    ent_coef        | 0.000558 |
|    ent_coef_loss   | 0.241    |
|    learning_rate   | 0.000503 |
|    n_updates       | 21960    |
---------------------------------
Eval num_timesteps=4499000, episode_reward=2812.02 +/- 12.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4499000  |
---------------------------------
Eval num_timesteps=4500000, episode_reward=2989.96 +/- 17.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4500000  |
| train/             |          |
|    actor_loss      | -0.729   |
|    critic_loss     | 0.000555 |
|    ent_coef        | 0.000557 |
|    ent_coef_loss   | -4.26    |
|    learning_rate   | 0.000501 |
|    n_updates       | 21970    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 3e+03    |
| time/              |          |
|    episodes        | 4500     |
|    fps             | 640      |
|    time_elapsed    | 7023     |
|    total_timesteps | 4500000  |
---------------------------------
Eval num_timesteps=4501000, episode_reward=2957.07 +/- 28.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4501000  |
---------------------------------
Eval num_timesteps=4502000, episode_reward=2978.45 +/- 31.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4502000  |
| train/             |          |
|    actor_loss      | -0.746   |
|    critic_loss     | 0.000565 |
|    ent_coef        | 0.000556 |
|    ent_coef_loss   | -1.28    |
|    learning_rate   | 0.000498 |
|    n_updates       | 21980    |
---------------------------------
Eval num_timesteps=4503000, episode_reward=2954.73 +/- 23.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4503000  |
---------------------------------
Eval num_timesteps=4504000, episode_reward=2863.79 +/- 44.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4504000  |
| train/             |          |
|    actor_loss      | -0.752   |
|    critic_loss     | 0.000587 |
|    ent_coef        | 0.000554 |
|    ent_coef_loss   | -1.54    |
|    learning_rate   | 0.000496 |
|    n_updates       | 21990    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 4504     |
|    fps             | 640      |
|    time_elapsed    | 7030     |
|    total_timesteps | 4504000  |
---------------------------------
Eval num_timesteps=4505000, episode_reward=2843.53 +/- 40.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4505000  |
---------------------------------
Eval num_timesteps=4506000, episode_reward=3088.29 +/- 38.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4506000  |
| train/             |          |
|    actor_loss      | -0.736   |
|    critic_loss     | 0.000548 |
|    ent_coef        | 0.000553 |
|    ent_coef_loss   | -3.39    |
|    learning_rate   | 0.000494 |
|    n_updates       | 22000    |
---------------------------------
Eval num_timesteps=4507000, episode_reward=3056.61 +/- 44.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4507000  |
---------------------------------
Eval num_timesteps=4508000, episode_reward=2788.24 +/- 31.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4508000  |
| train/             |          |
|    actor_loss      | -0.749   |
|    critic_loss     | 0.000553 |
|    ent_coef        | 0.000552 |
|    ent_coef_loss   | -1.96    |
|    learning_rate   | 0.000492 |
|    n_updates       | 22010    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.99e+03 |
| time/              |          |
|    episodes        | 4508     |
|    fps             | 640      |
|    time_elapsed    | 7036     |
|    total_timesteps | 4508000  |
---------------------------------
Eval num_timesteps=4509000, episode_reward=2782.70 +/- 41.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4509000  |
---------------------------------
Eval num_timesteps=4510000, episode_reward=2892.21 +/- 24.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4510000  |
| train/             |          |
|    actor_loss      | -0.713   |
|    critic_loss     | 0.000534 |
|    ent_coef        | 0.00055  |
|    ent_coef_loss   | -4.14    |
|    learning_rate   | 0.00049  |
|    n_updates       | 22020    |
---------------------------------
Eval num_timesteps=4511000, episode_reward=2929.12 +/- 22.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4511000  |
---------------------------------
Eval num_timesteps=4512000, episode_reward=2922.33 +/- 24.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4512000  |
| train/             |          |
|    actor_loss      | -0.73    |
|    critic_loss     | 0.000563 |
|    ent_coef        | 0.000549 |
|    ent_coef_loss   | -2.05    |
|    learning_rate   | 0.000488 |
|    n_updates       | 22030    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.98e+03 |
| time/              |          |
|    episodes        | 4512     |
|    fps             | 640      |
|    time_elapsed    | 7042     |
|    total_timesteps | 4512000  |
---------------------------------
Eval num_timesteps=4513000, episode_reward=2923.72 +/- 8.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4513000  |
---------------------------------
Eval num_timesteps=4514000, episode_reward=2899.47 +/- 43.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4514000  |
| train/             |          |
|    actor_loss      | -0.735   |
|    critic_loss     | 0.000581 |
|    ent_coef        | 0.000547 |
|    ent_coef_loss   | -2.14    |
|    learning_rate   | 0.000486 |
|    n_updates       | 22040    |
---------------------------------
Eval num_timesteps=4515000, episode_reward=2925.91 +/- 22.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4515000  |
---------------------------------
Eval num_timesteps=4516000, episode_reward=2892.34 +/- 35.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4516000  |
| train/             |          |
|    actor_loss      | -0.739   |
|    critic_loss     | 0.000561 |
|    ent_coef        | 0.000546 |
|    ent_coef_loss   | -1.28    |
|    learning_rate   | 0.000484 |
|    n_updates       | 22050    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.97e+03 |
| time/              |          |
|    episodes        | 4516     |
|    fps             | 640      |
|    time_elapsed    | 7049     |
|    total_timesteps | 4516000  |
---------------------------------
Eval num_timesteps=4517000, episode_reward=2925.24 +/- 26.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4517000  |
---------------------------------
Eval num_timesteps=4518000, episode_reward=2782.42 +/- 46.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4518000  |
| train/             |          |
|    actor_loss      | -0.737   |
|    critic_loss     | 0.000591 |
|    ent_coef        | 0.000545 |
|    ent_coef_loss   | -1.54    |
|    learning_rate   | 0.000482 |
|    n_updates       | 22060    |
---------------------------------
Eval num_timesteps=4519000, episode_reward=2771.62 +/- 18.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4519000  |
---------------------------------
Eval num_timesteps=4520000, episode_reward=2941.63 +/- 16.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4520000  |
| train/             |          |
|    actor_loss      | -0.717   |
|    critic_loss     | 0.000516 |
|    ent_coef        | 0.000544 |
|    ent_coef_loss   | -2.75    |
|    learning_rate   | 0.00048  |
|    n_updates       | 22070    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.96e+03 |
| time/              |          |
|    episodes        | 4520     |
|    fps             | 640      |
|    time_elapsed    | 7055     |
|    total_timesteps | 4520000  |
---------------------------------
Eval num_timesteps=4521000, episode_reward=2967.05 +/- 17.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4521000  |
---------------------------------
Eval num_timesteps=4522000, episode_reward=2694.96 +/- 30.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4522000  |
| train/             |          |
|    actor_loss      | -0.734   |
|    critic_loss     | 0.000545 |
|    ent_coef        | 0.000543 |
|    ent_coef_loss   | -2.56    |
|    learning_rate   | 0.000478 |
|    n_updates       | 22080    |
---------------------------------
Eval num_timesteps=4523000, episode_reward=2707.86 +/- 48.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4523000  |
---------------------------------
Eval num_timesteps=4524000, episode_reward=2706.02 +/- 36.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4524000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.95e+03 |
| time/              |          |
|    episodes        | 4524     |
|    fps             | 640      |
|    time_elapsed    | 7061     |
|    total_timesteps | 4524000  |
---------------------------------
Eval num_timesteps=4525000, episode_reward=2876.27 +/- 44.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4525000  |
| train/             |          |
|    actor_loss      | -0.707   |
|    critic_loss     | 0.000527 |
|    ent_coef        | 0.000541 |
|    ent_coef_loss   | -5.47    |
|    learning_rate   | 0.000476 |
|    n_updates       | 22090    |
---------------------------------
Eval num_timesteps=4526000, episode_reward=2875.95 +/- 41.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4526000  |
---------------------------------
Eval num_timesteps=4527000, episode_reward=2900.51 +/- 20.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4527000  |
| train/             |          |
|    actor_loss      | -0.724   |
|    critic_loss     | 0.000591 |
|    ent_coef        | 0.000539 |
|    ent_coef_loss   | -2.1     |
|    learning_rate   | 0.000474 |
|    n_updates       | 22100    |
---------------------------------
Eval num_timesteps=4528000, episode_reward=2831.41 +/- 32.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4528000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.94e+03 |
| time/              |          |
|    episodes        | 4528     |
|    fps             | 640      |
|    time_elapsed    | 7067     |
|    total_timesteps | 4528000  |
---------------------------------
Eval num_timesteps=4529000, episode_reward=2968.95 +/- 43.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4529000  |
| train/             |          |
|    actor_loss      | -0.727   |
|    critic_loss     | 0.000572 |
|    ent_coef        | 0.000537 |
|    ent_coef_loss   | 0.117    |
|    learning_rate   | 0.000472 |
|    n_updates       | 22110    |
---------------------------------
Eval num_timesteps=4530000, episode_reward=2950.85 +/- 21.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4530000  |
---------------------------------
Eval num_timesteps=4531000, episode_reward=2906.86 +/- 14.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4531000  |
| train/             |          |
|    actor_loss      | -0.734   |
|    critic_loss     | 0.000564 |
|    ent_coef        | 0.000537 |
|    ent_coef_loss   | -0.283   |
|    learning_rate   | 0.00047  |
|    n_updates       | 22120    |
---------------------------------
Eval num_timesteps=4532000, episode_reward=2879.21 +/- 54.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4532000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 4532     |
|    fps             | 640      |
|    time_elapsed    | 7074     |
|    total_timesteps | 4532000  |
---------------------------------
Eval num_timesteps=4533000, episode_reward=2878.22 +/- 24.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4533000  |
| train/             |          |
|    actor_loss      | -0.729   |
|    critic_loss     | 0.000585 |
|    ent_coef        | 0.000537 |
|    ent_coef_loss   | -1.17    |
|    learning_rate   | 0.000468 |
|    n_updates       | 22130    |
---------------------------------
Eval num_timesteps=4534000, episode_reward=2876.44 +/- 35.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4534000  |
---------------------------------
Eval num_timesteps=4535000, episode_reward=3134.67 +/- 43.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 4535000  |
| train/             |          |
|    actor_loss      | -0.73    |
|    critic_loss     | 0.000553 |
|    ent_coef        | 0.000536 |
|    ent_coef_loss   | -0.428   |
|    learning_rate   | 0.000466 |
|    n_updates       | 22140    |
---------------------------------
Eval num_timesteps=4536000, episode_reward=3142.53 +/- 28.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.14e+03 |
| time/              |          |
|    total_timesteps | 4536000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 4536     |
|    fps             | 640      |
|    time_elapsed    | 7080     |
|    total_timesteps | 4536000  |
---------------------------------
Eval num_timesteps=4537000, episode_reward=3068.51 +/- 22.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4537000  |
| train/             |          |
|    actor_loss      | -0.758   |
|    critic_loss     | 0.00059  |
|    ent_coef        | 0.000536 |
|    ent_coef_loss   | 3.02     |
|    learning_rate   | 0.000464 |
|    n_updates       | 22150    |
---------------------------------
Eval num_timesteps=4538000, episode_reward=3067.68 +/- 48.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4538000  |
---------------------------------
Eval num_timesteps=4539000, episode_reward=3026.98 +/- 48.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4539000  |
| train/             |          |
|    actor_loss      | -0.742   |
|    critic_loss     | 0.000563 |
|    ent_coef        | 0.000537 |
|    ent_coef_loss   | -0.78    |
|    learning_rate   | 0.000462 |
|    n_updates       | 22160    |
---------------------------------
Eval num_timesteps=4540000, episode_reward=3007.24 +/- 13.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4540000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.92e+03 |
| time/              |          |
|    episodes        | 4540     |
|    fps             | 640      |
|    time_elapsed    | 7086     |
|    total_timesteps | 4540000  |
---------------------------------
Eval num_timesteps=4541000, episode_reward=2989.92 +/- 20.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4541000  |
| train/             |          |
|    actor_loss      | -0.741   |
|    critic_loss     | 0.000578 |
|    ent_coef        | 0.000537 |
|    ent_coef_loss   | 0.763    |
|    learning_rate   | 0.00046  |
|    n_updates       | 22170    |
---------------------------------
Eval num_timesteps=4542000, episode_reward=3024.01 +/- 24.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4542000  |
---------------------------------
Eval num_timesteps=4543000, episode_reward=2994.64 +/- 38.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4543000  |
| train/             |          |
|    actor_loss      | -0.742   |
|    critic_loss     | 0.000579 |
|    ent_coef        | 0.000537 |
|    ent_coef_loss   | -0.314   |
|    learning_rate   | 0.000458 |
|    n_updates       | 22180    |
---------------------------------
Eval num_timesteps=4544000, episode_reward=2976.30 +/- 46.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4544000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.93e+03 |
| time/              |          |
|    episodes        | 4544     |
|    fps             | 640      |
|    time_elapsed    | 7094     |
|    total_timesteps | 4544000  |
---------------------------------
Eval num_timesteps=4545000, episode_reward=3098.01 +/- 19.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4545000  |
| train/             |          |
|    actor_loss      | -0.626   |
|    critic_loss     | 0.000578 |
|    ent_coef        | 0.000536 |
|    ent_coef_loss   | -10.9    |
|    learning_rate   | 0.000455 |
|    n_updates       | 22190    |
---------------------------------
Eval num_timesteps=4546000, episode_reward=3086.87 +/- 48.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4546000  |
---------------------------------
Eval num_timesteps=4547000, episode_reward=2899.12 +/- 25.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4547000  |
| train/             |          |
|    actor_loss      | -0.628   |
|    critic_loss     | 0.000475 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | -14.5    |
|    learning_rate   | 0.000453 |
|    n_updates       | 22200    |
---------------------------------
Eval num_timesteps=4548000, episode_reward=2913.48 +/- 65.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4548000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    episodes        | 4548     |
|    fps             | 640      |
|    time_elapsed    | 7102     |
|    total_timesteps | 4548000  |
---------------------------------
Eval num_timesteps=4549000, episode_reward=2808.36 +/- 30.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4549000  |
| train/             |          |
|    actor_loss      | -0.722   |
|    critic_loss     | 0.00058  |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.000451 |
|    n_updates       | 22210    |
---------------------------------
Eval num_timesteps=4550000, episode_reward=2753.46 +/- 21.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4550000  |
---------------------------------
Eval num_timesteps=4551000, episode_reward=2892.31 +/- 30.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4551000  |
| train/             |          |
|    actor_loss      | -0.707   |
|    critic_loss     | 0.00052  |
|    ent_coef        | 0.000524 |
|    ent_coef_loss   | -3.05    |
|    learning_rate   | 0.000449 |
|    n_updates       | 22220    |
---------------------------------
Eval num_timesteps=4552000, episode_reward=2837.28 +/- 30.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4552000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    episodes        | 4552     |
|    fps             | 640      |
|    time_elapsed    | 7109     |
|    total_timesteps | 4552000  |
---------------------------------
Eval num_timesteps=4553000, episode_reward=3015.25 +/- 38.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4553000  |
| train/             |          |
|    actor_loss      | -0.731   |
|    critic_loss     | 0.000553 |
|    ent_coef        | 0.000523 |
|    ent_coef_loss   | 1.09     |
|    learning_rate   | 0.000447 |
|    n_updates       | 22230    |
---------------------------------
Eval num_timesteps=4554000, episode_reward=2997.87 +/- 51.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4554000  |
---------------------------------
Eval num_timesteps=4555000, episode_reward=2976.63 +/- 39.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4555000  |
| train/             |          |
|    actor_loss      | -0.738   |
|    critic_loss     | 0.000592 |
|    ent_coef        | 0.000522 |
|    ent_coef_loss   | 2.51     |
|    learning_rate   | 0.000445 |
|    n_updates       | 22240    |
---------------------------------
Eval num_timesteps=4556000, episode_reward=2983.68 +/- 33.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4556000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.89e+03 |
| time/              |          |
|    episodes        | 4556     |
|    fps             | 640      |
|    time_elapsed    | 7117     |
|    total_timesteps | 4556000  |
---------------------------------
Eval num_timesteps=4557000, episode_reward=2906.83 +/- 41.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4557000  |
| train/             |          |
|    actor_loss      | -0.737   |
|    critic_loss     | 0.000555 |
|    ent_coef        | 0.000523 |
|    ent_coef_loss   | 1.57     |
|    learning_rate   | 0.000443 |
|    n_updates       | 22250    |
---------------------------------
Eval num_timesteps=4558000, episode_reward=2926.76 +/- 41.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4558000  |
---------------------------------
Eval num_timesteps=4559000, episode_reward=2228.88 +/- 1317.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 4559000  |
| train/             |          |
|    actor_loss      | -0.724   |
|    critic_loss     | 0.000609 |
|    ent_coef        | 0.000524 |
|    ent_coef_loss   | 0.773    |
|    learning_rate   | 0.000441 |
|    n_updates       | 22260    |
---------------------------------
Eval num_timesteps=4560000, episode_reward=2901.97 +/- 24.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4560000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 4560     |
|    fps             | 639      |
|    time_elapsed    | 7125     |
|    total_timesteps | 4560000  |
---------------------------------
Eval num_timesteps=4561000, episode_reward=2997.52 +/- 45.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4561000  |
| train/             |          |
|    actor_loss      | -0.725   |
|    critic_loss     | 0.000619 |
|    ent_coef        | 0.000524 |
|    ent_coef_loss   | 0.701    |
|    learning_rate   | 0.000439 |
|    n_updates       | 22270    |
---------------------------------
Eval num_timesteps=4562000, episode_reward=3021.59 +/- 12.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4562000  |
---------------------------------
Eval num_timesteps=4563000, episode_reward=2886.11 +/- 41.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4563000  |
| train/             |          |
|    actor_loss      | -0.732   |
|    critic_loss     | 0.000581 |
|    ent_coef        | 0.000525 |
|    ent_coef_loss   | 1.35     |
|    learning_rate   | 0.000437 |
|    n_updates       | 22280    |
---------------------------------
Eval num_timesteps=4564000, episode_reward=2901.39 +/- 18.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4564000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.88e+03 |
| time/              |          |
|    episodes        | 4564     |
|    fps             | 639      |
|    time_elapsed    | 7132     |
|    total_timesteps | 4564000  |
---------------------------------
Eval num_timesteps=4565000, episode_reward=3125.40 +/- 54.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.13e+03 |
| time/              |          |
|    total_timesteps | 4565000  |
| train/             |          |
|    actor_loss      | -0.713   |
|    critic_loss     | 0.000534 |
|    ent_coef        | 0.000525 |
|    ent_coef_loss   | 0.526    |
|    learning_rate   | 0.000435 |
|    n_updates       | 22290    |
---------------------------------
Eval num_timesteps=4566000, episode_reward=3110.90 +/- 51.97
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 4566000  |
---------------------------------
Eval num_timesteps=4567000, episode_reward=2445.24 +/- 1391.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 4567000  |
---------------------------------
Eval num_timesteps=4568000, episode_reward=3076.25 +/- 53.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4568000  |
| train/             |          |
|    actor_loss      | -0.741   |
|    critic_loss     | 0.000599 |
|    ent_coef        | 0.000526 |
|    ent_coef_loss   | 3.53     |
|    learning_rate   | 0.000433 |
|    n_updates       | 22300    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 4568     |
|    fps             | 639      |
|    time_elapsed    | 7138     |
|    total_timesteps | 4568000  |
---------------------------------
Eval num_timesteps=4569000, episode_reward=2393.11 +/- 1393.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 4569000  |
---------------------------------
Eval num_timesteps=4570000, episode_reward=3054.35 +/- 57.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4570000  |
| train/             |          |
|    actor_loss      | -0.718   |
|    critic_loss     | 0.000613 |
|    ent_coef        | 0.000527 |
|    ent_coef_loss   | 1.4      |
|    learning_rate   | 0.000431 |
|    n_updates       | 22310    |
---------------------------------
Eval num_timesteps=4571000, episode_reward=3074.58 +/- 37.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4571000  |
---------------------------------
Eval num_timesteps=4572000, episode_reward=3082.02 +/- 19.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4572000  |
| train/             |          |
|    actor_loss      | -0.733   |
|    critic_loss     | 0.000608 |
|    ent_coef        | 0.000528 |
|    ent_coef_loss   | 2.96     |
|    learning_rate   | 0.000429 |
|    n_updates       | 22320    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 4572     |
|    fps             | 639      |
|    time_elapsed    | 7145     |
|    total_timesteps | 4572000  |
---------------------------------
Eval num_timesteps=4573000, episode_reward=3114.35 +/- 29.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 4573000  |
---------------------------------
Eval num_timesteps=4574000, episode_reward=2413.23 +/- 1411.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 4574000  |
| train/             |          |
|    actor_loss      | -0.743   |
|    critic_loss     | 0.000568 |
|    ent_coef        | 0.000529 |
|    ent_coef_loss   | 3.15     |
|    learning_rate   | 0.000427 |
|    n_updates       | 22330    |
---------------------------------
Eval num_timesteps=4575000, episode_reward=3118.45 +/- 19.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.12e+03 |
| time/              |          |
|    total_timesteps | 4575000  |
---------------------------------
Eval num_timesteps=4576000, episode_reward=2465.12 +/- 1345.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.47e+03 |
| time/              |          |
|    total_timesteps | 4576000  |
| train/             |          |
|    actor_loss      | -0.738   |
|    critic_loss     | 0.000571 |
|    ent_coef        | 0.000531 |
|    ent_coef_loss   | 1.38     |
|    learning_rate   | 0.000425 |
|    n_updates       | 22340    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.87e+03 |
| time/              |          |
|    episodes        | 4576     |
|    fps             | 639      |
|    time_elapsed    | 7151     |
|    total_timesteps | 4576000  |
---------------------------------
Eval num_timesteps=4577000, episode_reward=3087.76 +/- 34.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4577000  |
---------------------------------
Eval num_timesteps=4578000, episode_reward=3078.51 +/- 32.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4578000  |
| train/             |          |
|    actor_loss      | -0.73    |
|    critic_loss     | 0.000665 |
|    ent_coef        | 0.000531 |
|    ent_coef_loss   | -0.122   |
|    learning_rate   | 0.000423 |
|    n_updates       | 22350    |
---------------------------------
Eval num_timesteps=4579000, episode_reward=3109.54 +/- 28.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 4579000  |
---------------------------------
Eval num_timesteps=4580000, episode_reward=3065.43 +/- 25.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4580000  |
| train/             |          |
|    actor_loss      | -0.734   |
|    critic_loss     | 0.000584 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | 2.16     |
|    learning_rate   | 0.000421 |
|    n_updates       | 22360    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.86e+03 |
| time/              |          |
|    episodes        | 4580     |
|    fps             | 639      |
|    time_elapsed    | 7157     |
|    total_timesteps | 4580000  |
---------------------------------
Eval num_timesteps=4581000, episode_reward=3068.12 +/- 43.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4581000  |
---------------------------------
Eval num_timesteps=4582000, episode_reward=3172.74 +/- 23.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.17e+03 |
| time/              |          |
|    total_timesteps | 4582000  |
| train/             |          |
|    actor_loss      | -0.732   |
|    critic_loss     | 0.000605 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | 0.0202   |
|    learning_rate   | 0.000419 |
|    n_updates       | 22370    |
---------------------------------
Eval num_timesteps=4583000, episode_reward=2400.86 +/- 1406.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 4583000  |
---------------------------------
Eval num_timesteps=4584000, episode_reward=3108.39 +/- 40.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 4584000  |
| train/             |          |
|    actor_loss      | -0.618   |
|    critic_loss     | 0.000916 |
|    ent_coef        | 0.000533 |
|    ent_coef_loss   | 0.458    |
|    learning_rate   | 0.000417 |
|    n_updates       | 22380    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.83e+03 |
| time/              |          |
|    episodes        | 4584     |
|    fps             | 639      |
|    time_elapsed    | 7164     |
|    total_timesteps | 4584000  |
---------------------------------
Eval num_timesteps=4585000, episode_reward=2404.90 +/- 1420.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 4585000  |
---------------------------------
Eval num_timesteps=4586000, episode_reward=2901.43 +/- 28.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4586000  |
| train/             |          |
|    actor_loss      | -0.471   |
|    critic_loss     | 0.000706 |
|    ent_coef        | 0.000532 |
|    ent_coef_loss   | -9.88    |
|    learning_rate   | 0.000415 |
|    n_updates       | 22390    |
---------------------------------
Eval num_timesteps=4587000, episode_reward=2923.13 +/- 12.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4587000  |
---------------------------------
Eval num_timesteps=4588000, episode_reward=2129.97 +/- 1157.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.13e+03 |
| time/              |          |
|    total_timesteps | 4588000  |
| train/             |          |
|    actor_loss      | -0.598   |
|    critic_loss     | 0.000702 |
|    ent_coef        | 0.000529 |
|    ent_coef_loss   | -14.4    |
|    learning_rate   | 0.000412 |
|    n_updates       | 22400    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.8e+03  |
| time/              |          |
|    episodes        | 4588     |
|    fps             | 639      |
|    time_elapsed    | 7171     |
|    total_timesteps | 4588000  |
---------------------------------
Eval num_timesteps=4589000, episode_reward=2068.12 +/- 1257.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 4589000  |
---------------------------------
Eval num_timesteps=4590000, episode_reward=2856.49 +/- 19.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4590000  |
| train/             |          |
|    actor_loss      | -0.699   |
|    critic_loss     | 0.000642 |
|    ent_coef        | 0.000525 |
|    ent_coef_loss   | -5.04    |
|    learning_rate   | 0.00041  |
|    n_updates       | 22410    |
---------------------------------
Eval num_timesteps=4591000, episode_reward=2892.48 +/- 18.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4591000  |
---------------------------------
Eval num_timesteps=4592000, episode_reward=874.70 +/- 1621.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 875      |
| time/              |          |
|    total_timesteps | 4592000  |
| train/             |          |
|    actor_loss      | -0.557   |
|    critic_loss     | 0.000536 |
|    ent_coef        | 0.000521 |
|    ent_coef_loss   | -19      |
|    learning_rate   | 0.000408 |
|    n_updates       | 22420    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.77e+03 |
| time/              |          |
|    episodes        | 4592     |
|    fps             | 639      |
|    time_elapsed    | 7178     |
|    total_timesteps | 4592000  |
---------------------------------
Eval num_timesteps=4593000, episode_reward=935.87 +/- 1581.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 936      |
| time/              |          |
|    total_timesteps | 4593000  |
---------------------------------
Eval num_timesteps=4594000, episode_reward=2052.47 +/- 1099.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.05e+03 |
| time/              |          |
|    total_timesteps | 4594000  |
| train/             |          |
|    actor_loss      | -0.625   |
|    critic_loss     | 0.000509 |
|    ent_coef        | 0.000515 |
|    ent_coef_loss   | -13.5    |
|    learning_rate   | 0.000406 |
|    n_updates       | 22430    |
---------------------------------
Eval num_timesteps=4595000, episode_reward=2548.01 +/- 63.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.55e+03 |
| time/              |          |
|    total_timesteps | 4595000  |
---------------------------------
Eval num_timesteps=4596000, episode_reward=1880.18 +/- 1167.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.88e+03 |
| time/              |          |
|    total_timesteps | 4596000  |
| train/             |          |
|    actor_loss      | -0.662   |
|    critic_loss     | 0.000493 |
|    ent_coef        | 0.000509 |
|    ent_coef_loss   | -9.14    |
|    learning_rate   | 0.000404 |
|    n_updates       | 22440    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.76e+03 |
| time/              |          |
|    episodes        | 4596     |
|    fps             | 639      |
|    time_elapsed    | 7186     |
|    total_timesteps | 4596000  |
---------------------------------
Eval num_timesteps=4597000, episode_reward=2434.12 +/- 18.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 4597000  |
---------------------------------
Eval num_timesteps=4598000, episode_reward=2163.21 +/- 1155.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.16e+03 |
| time/              |          |
|    total_timesteps | 4598000  |
| train/             |          |
|    actor_loss      | -0.66    |
|    critic_loss     | 0.000463 |
|    ent_coef        | 0.000505 |
|    ent_coef_loss   | -6.98    |
|    learning_rate   | 0.000402 |
|    n_updates       | 22450    |
---------------------------------
Eval num_timesteps=4599000, episode_reward=2232.89 +/- 1187.04
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 4599000  |
---------------------------------
Eval num_timesteps=4600000, episode_reward=2785.09 +/- 27.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4600000  |
| train/             |          |
|    actor_loss      | -0.489   |
|    critic_loss     | 0.000308 |
|    ent_coef        | 0.000501 |
|    ent_coef_loss   | -29.4    |
|    learning_rate   | 0.0004   |
|    n_updates       | 22460    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.72e+03 |
| time/              |          |
|    episodes        | 4600     |
|    fps             | 639      |
|    time_elapsed    | 7193     |
|    total_timesteps | 4600000  |
---------------------------------
Eval num_timesteps=4601000, episode_reward=1555.72 +/- 1534.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.56e+03 |
| time/              |          |
|    total_timesteps | 4601000  |
---------------------------------
Eval num_timesteps=4602000, episode_reward=2534.33 +/- 22.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.53e+03 |
| time/              |          |
|    total_timesteps | 4602000  |
| train/             |          |
|    actor_loss      | -0.687   |
|    critic_loss     | 0.000589 |
|    ent_coef        | 0.000495 |
|    ent_coef_loss   | -3.48    |
|    learning_rate   | 0.000398 |
|    n_updates       | 22470    |
---------------------------------
Eval num_timesteps=4603000, episode_reward=2537.80 +/- 46.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.54e+03 |
| time/              |          |
|    total_timesteps | 4603000  |
---------------------------------
Eval num_timesteps=4604000, episode_reward=2153.19 +/- 1137.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.15e+03 |
| time/              |          |
|    total_timesteps | 4604000  |
| train/             |          |
|    actor_loss      | -0.421   |
|    critic_loss     | 0.000953 |
|    ent_coef        | 0.000492 |
|    ent_coef_loss   | -0.244   |
|    learning_rate   | 0.000396 |
|    n_updates       | 22480    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 4604     |
|    fps             | 639      |
|    time_elapsed    | 7199     |
|    total_timesteps | 4604000  |
---------------------------------
Eval num_timesteps=4605000, episode_reward=2756.89 +/- 56.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4605000  |
---------------------------------
Eval num_timesteps=4606000, episode_reward=2979.60 +/- 45.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4606000  |
| train/             |          |
|    actor_loss      | -0.684   |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000491 |
|    ent_coef_loss   | -4.72    |
|    learning_rate   | 0.000394 |
|    n_updates       | 22490    |
---------------------------------
Eval num_timesteps=4607000, episode_reward=2864.26 +/- 193.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4607000  |
---------------------------------
Eval num_timesteps=4608000, episode_reward=2973.66 +/- 37.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4608000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 4608     |
|    fps             | 639      |
|    time_elapsed    | 7205     |
|    total_timesteps | 4608000  |
---------------------------------
Eval num_timesteps=4609000, episode_reward=2806.29 +/- 35.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4609000  |
| train/             |          |
|    actor_loss      | -0.489   |
|    critic_loss     | 0.000518 |
|    ent_coef        | 0.000488 |
|    ent_coef_loss   | -26.2    |
|    learning_rate   | 0.000392 |
|    n_updates       | 22500    |
---------------------------------
Eval num_timesteps=4610000, episode_reward=2811.25 +/- 40.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4610000  |
---------------------------------
Eval num_timesteps=4611000, episode_reward=2746.77 +/- 21.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4611000  |
| train/             |          |
|    actor_loss      | -0.45    |
|    critic_loss     | 0.00068  |
|    ent_coef        | 0.000483 |
|    ent_coef_loss   | -19.1    |
|    learning_rate   | 0.00039  |
|    n_updates       | 22510    |
---------------------------------
Eval num_timesteps=4612000, episode_reward=2760.22 +/- 19.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4612000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 4612     |
|    fps             | 639      |
|    time_elapsed    | 7211     |
|    total_timesteps | 4612000  |
---------------------------------
Eval num_timesteps=4613000, episode_reward=2672.84 +/- 35.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 4613000  |
| train/             |          |
|    actor_loss      | -0.675   |
|    critic_loss     | 0.000793 |
|    ent_coef        | 0.000479 |
|    ent_coef_loss   | -7.23    |
|    learning_rate   | 0.000388 |
|    n_updates       | 22520    |
---------------------------------
Eval num_timesteps=4614000, episode_reward=2699.38 +/- 12.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4614000  |
---------------------------------
Eval num_timesteps=4615000, episode_reward=2579.53 +/- 32.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 4615000  |
| train/             |          |
|    actor_loss      | -0.644   |
|    critic_loss     | 0.000572 |
|    ent_coef        | 0.000475 |
|    ent_coef_loss   | -8.16    |
|    learning_rate   | 0.000386 |
|    n_updates       | 22530    |
---------------------------------
Eval num_timesteps=4616000, episode_reward=2558.70 +/- 37.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.56e+03 |
| time/              |          |
|    total_timesteps | 4616000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 4616     |
|    fps             | 639      |
|    time_elapsed    | 7218     |
|    total_timesteps | 4616000  |
---------------------------------
Eval num_timesteps=4617000, episode_reward=2713.66 +/- 18.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4617000  |
| train/             |          |
|    actor_loss      | -0.479   |
|    critic_loss     | 0.000407 |
|    ent_coef        | 0.000472 |
|    ent_coef_loss   | -30.1    |
|    learning_rate   | 0.000384 |
|    n_updates       | 22540    |
---------------------------------
Eval num_timesteps=4618000, episode_reward=2216.97 +/- 1078.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 4618000  |
---------------------------------
Eval num_timesteps=4619000, episode_reward=2760.98 +/- 28.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4619000  |
| train/             |          |
|    actor_loss      | -0.66    |
|    critic_loss     | 0.000561 |
|    ent_coef        | 0.000468 |
|    ent_coef_loss   | -5.97    |
|    learning_rate   | 0.000382 |
|    n_updates       | 22550    |
---------------------------------
Eval num_timesteps=4620000, episode_reward=2771.20 +/- 24.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4620000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 4620     |
|    fps             | 639      |
|    time_elapsed    | 7225     |
|    total_timesteps | 4620000  |
---------------------------------
Eval num_timesteps=4621000, episode_reward=2754.30 +/- 49.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4621000  |
| train/             |          |
|    actor_loss      | -0.66    |
|    critic_loss     | 0.000486 |
|    ent_coef        | 0.000465 |
|    ent_coef_loss   | -3.98    |
|    learning_rate   | 0.00038  |
|    n_updates       | 22560    |
---------------------------------
Eval num_timesteps=4622000, episode_reward=2750.87 +/- 34.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4622000  |
---------------------------------
Eval num_timesteps=4623000, episode_reward=2873.07 +/- 41.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4623000  |
| train/             |          |
|    actor_loss      | -0.673   |
|    critic_loss     | 0.000463 |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | -4.77    |
|    learning_rate   | 0.000378 |
|    n_updates       | 22570    |
---------------------------------
Eval num_timesteps=4624000, episode_reward=2838.97 +/- 15.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4624000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 4624     |
|    fps             | 639      |
|    time_elapsed    | 7231     |
|    total_timesteps | 4624000  |
---------------------------------
Eval num_timesteps=4625000, episode_reward=2894.77 +/- 16.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4625000  |
| train/             |          |
|    actor_loss      | -0.682   |
|    critic_loss     | 0.000454 |
|    ent_coef        | 0.000462 |
|    ent_coef_loss   | -1.46    |
|    learning_rate   | 0.000376 |
|    n_updates       | 22580    |
---------------------------------
Eval num_timesteps=4626000, episode_reward=2907.37 +/- 22.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4626000  |
---------------------------------
Eval num_timesteps=4627000, episode_reward=2846.89 +/- 37.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4627000  |
| train/             |          |
|    actor_loss      | -0.681   |
|    critic_loss     | 0.000469 |
|    ent_coef        | 0.000461 |
|    ent_coef_loss   | -1.99    |
|    learning_rate   | 0.000374 |
|    n_updates       | 22590    |
---------------------------------
Eval num_timesteps=4628000, episode_reward=2249.02 +/- 1178.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.25e+03 |
| time/              |          |
|    total_timesteps | 4628000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    episodes        | 4628     |
|    fps             | 639      |
|    time_elapsed    | 7237     |
|    total_timesteps | 4628000  |
---------------------------------
Eval num_timesteps=4629000, episode_reward=2841.06 +/- 28.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4629000  |
| train/             |          |
|    actor_loss      | -0.685   |
|    critic_loss     | 0.000464 |
|    ent_coef        | 0.000461 |
|    ent_coef_loss   | -1.87    |
|    learning_rate   | 0.000372 |
|    n_updates       | 22600    |
---------------------------------
Eval num_timesteps=4630000, episode_reward=2826.54 +/- 23.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4630000  |
---------------------------------
Eval num_timesteps=4631000, episode_reward=3040.05 +/- 21.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4631000  |
| train/             |          |
|    actor_loss      | -0.681   |
|    critic_loss     | 0.000466 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | -0.521   |
|    learning_rate   | 0.000369 |
|    n_updates       | 22610    |
---------------------------------
Eval num_timesteps=4632000, episode_reward=3074.00 +/- 48.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.07e+03 |
| time/              |          |
|    total_timesteps | 4632000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    episodes        | 4632     |
|    fps             | 639      |
|    time_elapsed    | 7244     |
|    total_timesteps | 4632000  |
---------------------------------
Eval num_timesteps=4633000, episode_reward=3022.03 +/- 37.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4633000  |
| train/             |          |
|    actor_loss      | -0.706   |
|    critic_loss     | 0.000494 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | 2.93     |
|    learning_rate   | 0.000367 |
|    n_updates       | 22620    |
---------------------------------
Eval num_timesteps=4634000, episode_reward=3029.83 +/- 36.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4634000  |
---------------------------------
Eval num_timesteps=4635000, episode_reward=2925.81 +/- 33.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4635000  |
| train/             |          |
|    actor_loss      | -0.697   |
|    critic_loss     | 0.000501 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | 2.01     |
|    learning_rate   | 0.000365 |
|    n_updates       | 22630    |
---------------------------------
Eval num_timesteps=4636000, episode_reward=2874.88 +/- 48.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4636000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 4636     |
|    fps             | 639      |
|    time_elapsed    | 7250     |
|    total_timesteps | 4636000  |
---------------------------------
Eval num_timesteps=4637000, episode_reward=2908.82 +/- 43.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4637000  |
| train/             |          |
|    actor_loss      | -0.688   |
|    critic_loss     | 0.000483 |
|    ent_coef        | 0.000461 |
|    ent_coef_loss   | -0.362   |
|    learning_rate   | 0.000363 |
|    n_updates       | 22640    |
---------------------------------
Eval num_timesteps=4638000, episode_reward=2903.39 +/- 59.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4638000  |
---------------------------------
Eval num_timesteps=4639000, episode_reward=2944.03 +/- 47.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4639000  |
| train/             |          |
|    actor_loss      | -0.514   |
|    critic_loss     | 0.000992 |
|    ent_coef        | 0.000461 |
|    ent_coef_loss   | -3.56    |
|    learning_rate   | 0.000361 |
|    n_updates       | 22650    |
---------------------------------
Eval num_timesteps=4640000, episode_reward=2933.84 +/- 34.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4640000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 4640     |
|    fps             | 639      |
|    time_elapsed    | 7256     |
|    total_timesteps | 4640000  |
---------------------------------
Eval num_timesteps=4641000, episode_reward=3003.99 +/- 48.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4641000  |
| train/             |          |
|    actor_loss      | -0.625   |
|    critic_loss     | 0.000906 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | -1.6     |
|    learning_rate   | 0.000359 |
|    n_updates       | 22660    |
---------------------------------
Eval num_timesteps=4642000, episode_reward=2376.23 +/- 1239.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 4642000  |
---------------------------------
Eval num_timesteps=4643000, episode_reward=2430.18 +/- 1275.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 4643000  |
| train/             |          |
|    actor_loss      | -0.501   |
|    critic_loss     | 0.000427 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | -23.5    |
|    learning_rate   | 0.000357 |
|    n_updates       | 22670    |
---------------------------------
Eval num_timesteps=4644000, episode_reward=3075.95 +/- 10.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4644000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.46e+03 |
| time/              |          |
|    episodes        | 4644     |
|    fps             | 639      |
|    time_elapsed    | 7263     |
|    total_timesteps | 4644000  |
---------------------------------
Eval num_timesteps=4645000, episode_reward=2400.29 +/- 1269.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.4e+03  |
| time/              |          |
|    total_timesteps | 4645000  |
| train/             |          |
|    actor_loss      | -0.697   |
|    critic_loss     | 0.000628 |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | -1.16    |
|    learning_rate   | 0.000355 |
|    n_updates       | 22680    |
---------------------------------
Eval num_timesteps=4646000, episode_reward=3023.88 +/- 52.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4646000  |
---------------------------------
Eval num_timesteps=4647000, episode_reward=2983.47 +/- 23.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4647000  |
| train/             |          |
|    actor_loss      | -0.702   |
|    critic_loss     | 0.000555 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | 2.65     |
|    learning_rate   | 0.000353 |
|    n_updates       | 22690    |
---------------------------------
Eval num_timesteps=4648000, episode_reward=2961.51 +/- 18.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4648000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 4648     |
|    fps             | 639      |
|    time_elapsed    | 7269     |
|    total_timesteps | 4648000  |
---------------------------------
Eval num_timesteps=4649000, episode_reward=2972.90 +/- 13.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4649000  |
| train/             |          |
|    actor_loss      | -0.699   |
|    critic_loss     | 0.000531 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | 1.83     |
|    learning_rate   | 0.000351 |
|    n_updates       | 22700    |
---------------------------------
Eval num_timesteps=4650000, episode_reward=3009.83 +/- 36.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4650000  |
---------------------------------
Eval num_timesteps=4651000, episode_reward=2363.55 +/- 1244.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 4651000  |
---------------------------------
Eval num_timesteps=4652000, episode_reward=3018.31 +/- 39.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4652000  |
| train/             |          |
|    actor_loss      | -0.696   |
|    critic_loss     | 0.000521 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | 1.5      |
|    learning_rate   | 0.000349 |
|    n_updates       | 22710    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 4652     |
|    fps             | 639      |
|    time_elapsed    | 7275     |
|    total_timesteps | 4652000  |
---------------------------------
Eval num_timesteps=4653000, episode_reward=3017.63 +/- 41.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4653000  |
---------------------------------
Eval num_timesteps=4654000, episode_reward=2375.51 +/- 1261.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 4654000  |
| train/             |          |
|    actor_loss      | -0.7     |
|    critic_loss     | 0.000476 |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | 1.29     |
|    learning_rate   | 0.000347 |
|    n_updates       | 22720    |
---------------------------------
Eval num_timesteps=4655000, episode_reward=2385.41 +/- 1266.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 4655000  |
---------------------------------
Eval num_timesteps=4656000, episode_reward=2935.76 +/- 51.58
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4656000  |
| train/             |          |
|    actor_loss      | -0.703   |
|    critic_loss     | 0.000481 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | 3.17     |
|    learning_rate   | 0.000345 |
|    n_updates       | 22730    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 4656     |
|    fps             | 639      |
|    time_elapsed    | 7281     |
|    total_timesteps | 4656000  |
---------------------------------
Eval num_timesteps=4657000, episode_reward=2312.64 +/- 1215.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 4657000  |
---------------------------------
Eval num_timesteps=4658000, episode_reward=2275.82 +/- 1205.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 4658000  |
| train/             |          |
|    actor_loss      | -0.685   |
|    critic_loss     | 0.000491 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | 1.05     |
|    learning_rate   | 0.000343 |
|    n_updates       | 22740    |
---------------------------------
Eval num_timesteps=4659000, episode_reward=2859.22 +/- 34.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4659000  |
---------------------------------
Eval num_timesteps=4660000, episode_reward=2930.34 +/- 29.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4660000  |
| train/             |          |
|    actor_loss      | -0.699   |
|    critic_loss     | 0.000466 |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | 0.996    |
|    learning_rate   | 0.000341 |
|    n_updates       | 22750    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 4660     |
|    fps             | 639      |
|    time_elapsed    | 7288     |
|    total_timesteps | 4660000  |
---------------------------------
Eval num_timesteps=4661000, episode_reward=2877.25 +/- 23.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4661000  |
---------------------------------
Eval num_timesteps=4662000, episode_reward=2928.39 +/- 18.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4662000  |
| train/             |          |
|    actor_loss      | -0.688   |
|    critic_loss     | 0.000508 |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | 1.78     |
|    learning_rate   | 0.000339 |
|    n_updates       | 22760    |
---------------------------------
Eval num_timesteps=4663000, episode_reward=2923.67 +/- 46.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4663000  |
---------------------------------
Eval num_timesteps=4664000, episode_reward=2955.84 +/- 25.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4664000  |
| train/             |          |
|    actor_loss      | -0.691   |
|    critic_loss     | 0.00049  |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | 3.67     |
|    learning_rate   | 0.000337 |
|    n_updates       | 22770    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 4664     |
|    fps             | 639      |
|    time_elapsed    | 7294     |
|    total_timesteps | 4664000  |
---------------------------------
Eval num_timesteps=4665000, episode_reward=1684.47 +/- 1533.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.68e+03 |
| time/              |          |
|    total_timesteps | 4665000  |
---------------------------------
Eval num_timesteps=4666000, episode_reward=3079.12 +/- 23.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.08e+03 |
| time/              |          |
|    total_timesteps | 4666000  |
| train/             |          |
|    actor_loss      | -0.699   |
|    critic_loss     | 0.000526 |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | 4.21     |
|    learning_rate   | 0.000335 |
|    n_updates       | 22780    |
---------------------------------
Eval num_timesteps=4667000, episode_reward=3091.17 +/- 29.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.09e+03 |
| time/              |          |
|    total_timesteps | 4667000  |
---------------------------------
Eval num_timesteps=4668000, episode_reward=2933.43 +/- 22.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4668000  |
| train/             |          |
|    actor_loss      | -0.712   |
|    critic_loss     | 0.000479 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | 4.15     |
|    learning_rate   | 0.000333 |
|    n_updates       | 22790    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.49e+03 |
| time/              |          |
|    episodes        | 4668     |
|    fps             | 639      |
|    time_elapsed    | 7300     |
|    total_timesteps | 4668000  |
---------------------------------
Eval num_timesteps=4669000, episode_reward=2925.67 +/- 42.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4669000  |
---------------------------------
Eval num_timesteps=4670000, episode_reward=2854.73 +/- 91.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4670000  |
| train/             |          |
|    actor_loss      | -0.69    |
|    critic_loss     | 0.000482 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | 2.83     |
|    learning_rate   | 0.000331 |
|    n_updates       | 22800    |
---------------------------------
Eval num_timesteps=4671000, episode_reward=2942.36 +/- 32.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4671000  |
---------------------------------
Eval num_timesteps=4672000, episode_reward=2762.77 +/- 50.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4672000  |
| train/             |          |
|    actor_loss      | -0.693   |
|    critic_loss     | 0.000526 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | 2.8      |
|    learning_rate   | 0.000329 |
|    n_updates       | 22810    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    episodes        | 4672     |
|    fps             | 639      |
|    time_elapsed    | 7307     |
|    total_timesteps | 4672000  |
---------------------------------
Eval num_timesteps=4673000, episode_reward=2211.72 +/- 1166.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 4673000  |
---------------------------------
Eval num_timesteps=4674000, episode_reward=2875.01 +/- 16.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4674000  |
| train/             |          |
|    actor_loss      | -0.69    |
|    critic_loss     | 0.000501 |
|    ent_coef        | 0.000461 |
|    ent_coef_loss   | 3.27     |
|    learning_rate   | 0.000326 |
|    n_updates       | 22820    |
---------------------------------
Eval num_timesteps=4675000, episode_reward=2851.37 +/- 23.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4675000  |
---------------------------------
Eval num_timesteps=4676000, episode_reward=2875.56 +/- 29.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4676000  |
| train/             |          |
|    actor_loss      | -0.691   |
|    critic_loss     | 0.000485 |
|    ent_coef        | 0.000462 |
|    ent_coef_loss   | 3.08     |
|    learning_rate   | 0.000324 |
|    n_updates       | 22830    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    episodes        | 4676     |
|    fps             | 639      |
|    time_elapsed    | 7313     |
|    total_timesteps | 4676000  |
---------------------------------
Eval num_timesteps=4677000, episode_reward=2901.23 +/- 39.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4677000  |
---------------------------------
Eval num_timesteps=4678000, episode_reward=2880.71 +/- 31.70
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4678000  |
| train/             |          |
|    actor_loss      | -0.688   |
|    critic_loss     | 0.000456 |
|    ent_coef        | 0.000462 |
|    ent_coef_loss   | 2.12     |
|    learning_rate   | 0.000322 |
|    n_updates       | 22840    |
---------------------------------
Eval num_timesteps=4679000, episode_reward=2835.73 +/- 36.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4679000  |
---------------------------------
Eval num_timesteps=4680000, episode_reward=2895.63 +/- 18.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4680000  |
| train/             |          |
|    actor_loss      | -0.682   |
|    critic_loss     | 0.000464 |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | 2.04     |
|    learning_rate   | 0.00032  |
|    n_updates       | 22850    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    episodes        | 4680     |
|    fps             | 639      |
|    time_elapsed    | 7319     |
|    total_timesteps | 4680000  |
---------------------------------
Eval num_timesteps=4681000, episode_reward=2871.49 +/- 46.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4681000  |
---------------------------------
Eval num_timesteps=4682000, episode_reward=2974.77 +/- 24.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4682000  |
| train/             |          |
|    actor_loss      | -0.693   |
|    critic_loss     | 0.00046  |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | 2.46     |
|    learning_rate   | 0.000318 |
|    n_updates       | 22860    |
---------------------------------
Eval num_timesteps=4683000, episode_reward=2958.36 +/- 23.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4683000  |
---------------------------------
Eval num_timesteps=4684000, episode_reward=2991.64 +/- 36.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4684000  |
| train/             |          |
|    actor_loss      | -0.701   |
|    critic_loss     | 0.000527 |
|    ent_coef        | 0.000464 |
|    ent_coef_loss   | 4.16     |
|    learning_rate   | 0.000316 |
|    n_updates       | 22870    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    episodes        | 4684     |
|    fps             | 639      |
|    time_elapsed    | 7325     |
|    total_timesteps | 4684000  |
---------------------------------
Eval num_timesteps=4685000, episode_reward=2991.74 +/- 30.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4685000  |
---------------------------------
Eval num_timesteps=4686000, episode_reward=2980.87 +/- 24.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4686000  |
| train/             |          |
|    actor_loss      | -0.691   |
|    critic_loss     | 0.000492 |
|    ent_coef        | 0.000464 |
|    ent_coef_loss   | 2.22     |
|    learning_rate   | 0.000314 |
|    n_updates       | 22880    |
---------------------------------
Eval num_timesteps=4687000, episode_reward=2967.32 +/- 35.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4687000  |
---------------------------------
Eval num_timesteps=4688000, episode_reward=1621.40 +/- 1441.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.62e+03 |
| time/              |          |
|    total_timesteps | 4688000  |
| train/             |          |
|    actor_loss      | -0.691   |
|    critic_loss     | 0.000537 |
|    ent_coef        | 0.000465 |
|    ent_coef_loss   | 2.49     |
|    learning_rate   | 0.000312 |
|    n_updates       | 22890    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.53e+03 |
| time/              |          |
|    episodes        | 4688     |
|    fps             | 639      |
|    time_elapsed    | 7332     |
|    total_timesteps | 4688000  |
---------------------------------
Eval num_timesteps=4689000, episode_reward=2805.26 +/- 32.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4689000  |
---------------------------------
Eval num_timesteps=4690000, episode_reward=2267.14 +/- 1207.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.27e+03 |
| time/              |          |
|    total_timesteps | 4690000  |
| train/             |          |
|    actor_loss      | -0.676   |
|    critic_loss     | 0.000474 |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | 0.134    |
|    learning_rate   | 0.00031  |
|    n_updates       | 22900    |
---------------------------------
Eval num_timesteps=4691000, episode_reward=2838.31 +/- 44.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4691000  |
---------------------------------
Eval num_timesteps=4692000, episode_reward=3036.88 +/- 30.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4692000  |
| train/             |          |
|    actor_loss      | -0.686   |
|    critic_loss     | 0.000482 |
|    ent_coef        | 0.000466 |
|    ent_coef_loss   | 2.41     |
|    learning_rate   | 0.000308 |
|    n_updates       | 22910    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 4692     |
|    fps             | 639      |
|    time_elapsed    | 7338     |
|    total_timesteps | 4692000  |
---------------------------------
Eval num_timesteps=4693000, episode_reward=3042.57 +/- 26.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4693000  |
---------------------------------
Eval num_timesteps=4694000, episode_reward=1764.88 +/- 1562.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.76e+03 |
| time/              |          |
|    total_timesteps | 4694000  |
---------------------------------
Eval num_timesteps=4695000, episode_reward=3045.63 +/- 11.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4695000  |
| train/             |          |
|    actor_loss      | -0.492   |
|    critic_loss     | 0.000496 |
|    ent_coef        | 0.000465 |
|    ent_coef_loss   | -21.9    |
|    learning_rate   | 0.000306 |
|    n_updates       | 22920    |
---------------------------------
Eval num_timesteps=4696000, episode_reward=2392.25 +/- 1282.07
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 4696000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    episodes        | 4696     |
|    fps             | 639      |
|    time_elapsed    | 7344     |
|    total_timesteps | 4696000  |
---------------------------------
Eval num_timesteps=4697000, episode_reward=2851.43 +/- 49.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4697000  |
| train/             |          |
|    actor_loss      | -0.699   |
|    critic_loss     | 0.000642 |
|    ent_coef        | 0.000464 |
|    ent_coef_loss   | 3.75     |
|    learning_rate   | 0.000304 |
|    n_updates       | 22930    |
---------------------------------
Eval num_timesteps=4698000, episode_reward=2232.08 +/- 1187.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 4698000  |
---------------------------------
Eval num_timesteps=4699000, episode_reward=2834.09 +/- 13.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4699000  |
| train/             |          |
|    actor_loss      | -0.676   |
|    critic_loss     | 0.000485 |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | 1.42     |
|    learning_rate   | 0.000302 |
|    n_updates       | 22940    |
---------------------------------
Eval num_timesteps=4700000, episode_reward=2824.88 +/- 36.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 4700000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 4700     |
|    fps             | 639      |
|    time_elapsed    | 7351     |
|    total_timesteps | 4700000  |
---------------------------------
Eval num_timesteps=4701000, episode_reward=2695.02 +/- 29.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4701000  |
| train/             |          |
|    actor_loss      | -0.693   |
|    critic_loss     | 0.00051  |
|    ent_coef        | 0.000463 |
|    ent_coef_loss   | 0.575    |
|    learning_rate   | 0.0003   |
|    n_updates       | 22950    |
---------------------------------
Eval num_timesteps=4702000, episode_reward=2703.61 +/- 43.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4702000  |
---------------------------------
Eval num_timesteps=4703000, episode_reward=2816.06 +/- 43.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 4703000  |
| train/             |          |
|    actor_loss      | -0.483   |
|    critic_loss     | 0.000347 |
|    ent_coef        | 0.000462 |
|    ent_coef_loss   | -28.2    |
|    learning_rate   | 0.000298 |
|    n_updates       | 22960    |
---------------------------------
Eval num_timesteps=4704000, episode_reward=2833.08 +/- 20.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4704000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.59e+03 |
| time/              |          |
|    episodes        | 4704     |
|    fps             | 639      |
|    time_elapsed    | 7357     |
|    total_timesteps | 4704000  |
---------------------------------
Eval num_timesteps=4705000, episode_reward=2916.07 +/- 41.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4705000  |
| train/             |          |
|    actor_loss      | -0.677   |
|    critic_loss     | 0.000575 |
|    ent_coef        | 0.00046  |
|    ent_coef_loss   | 0.603    |
|    learning_rate   | 0.000296 |
|    n_updates       | 22970    |
---------------------------------
Eval num_timesteps=4706000, episode_reward=2305.00 +/- 1235.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 4706000  |
---------------------------------
Eval num_timesteps=4707000, episode_reward=2900.62 +/- 45.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4707000  |
| train/             |          |
|    actor_loss      | -0.677   |
|    critic_loss     | 0.000553 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | 1.48     |
|    learning_rate   | 0.000294 |
|    n_updates       | 22980    |
---------------------------------
Eval num_timesteps=4708000, episode_reward=1660.65 +/- 1502.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 4708000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 4708     |
|    fps             | 639      |
|    time_elapsed    | 7363     |
|    total_timesteps | 4708000  |
---------------------------------
Eval num_timesteps=4709000, episode_reward=2919.58 +/- 37.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4709000  |
| train/             |          |
|    actor_loss      | -0.679   |
|    critic_loss     | 0.000458 |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | 0.818    |
|    learning_rate   | 0.000292 |
|    n_updates       | 22990    |
---------------------------------
Eval num_timesteps=4710000, episode_reward=2334.00 +/- 1246.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 4710000  |
---------------------------------
Eval num_timesteps=4711000, episode_reward=2800.23 +/- 23.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4711000  |
| train/             |          |
|    actor_loss      | -0.683   |
|    critic_loss     | 0.000488 |
|    ent_coef        | 0.000458 |
|    ent_coef_loss   | 2.15     |
|    learning_rate   | 0.00029  |
|    n_updates       | 23000    |
---------------------------------
Eval num_timesteps=4712000, episode_reward=2861.14 +/- 18.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4712000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 4712     |
|    fps             | 639      |
|    time_elapsed    | 7369     |
|    total_timesteps | 4712000  |
---------------------------------
Eval num_timesteps=4713000, episode_reward=2864.02 +/- 38.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4713000  |
| train/             |          |
|    actor_loss      | -0.676   |
|    critic_loss     | 0.000516 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 0.000288 |
|    n_updates       | 23010    |
---------------------------------
Eval num_timesteps=4714000, episode_reward=2257.44 +/- 1219.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 4714000  |
---------------------------------
Eval num_timesteps=4715000, episode_reward=2955.40 +/- 38.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4715000  |
| train/             |          |
|    actor_loss      | -0.585   |
|    critic_loss     | 0.000432 |
|    ent_coef        | 0.000459 |
|    ent_coef_loss   | -12      |
|    learning_rate   | 0.000286 |
|    n_updates       | 23020    |
---------------------------------
Eval num_timesteps=4716000, episode_reward=2968.63 +/- 23.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4716000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 4716     |
|    fps             | 639      |
|    time_elapsed    | 7376     |
|    total_timesteps | 4716000  |
---------------------------------
Eval num_timesteps=4717000, episode_reward=2978.94 +/- 20.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4717000  |
| train/             |          |
|    actor_loss      | -0.59    |
|    critic_loss     | 0.000454 |
|    ent_coef        | 0.000457 |
|    ent_coef_loss   | -11.5    |
|    learning_rate   | 0.000283 |
|    n_updates       | 23030    |
---------------------------------
Eval num_timesteps=4718000, episode_reward=3001.07 +/- 21.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4718000  |
---------------------------------
Eval num_timesteps=4719000, episode_reward=1655.96 +/- 1509.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.66e+03 |
| time/              |          |
|    total_timesteps | 4719000  |
| train/             |          |
|    actor_loss      | -0.68    |
|    critic_loss     | 0.000514 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | 3.51     |
|    learning_rate   | 0.000281 |
|    n_updates       | 23040    |
---------------------------------
Eval num_timesteps=4720000, episode_reward=2942.61 +/- 17.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4720000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.68e+03 |
| time/              |          |
|    episodes        | 4720     |
|    fps             | 639      |
|    time_elapsed    | 7382     |
|    total_timesteps | 4720000  |
---------------------------------
Eval num_timesteps=4721000, episode_reward=2891.66 +/- 24.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4721000  |
| train/             |          |
|    actor_loss      | -0.684   |
|    critic_loss     | 0.000487 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | 2.98     |
|    learning_rate   | 0.000279 |
|    n_updates       | 23050    |
---------------------------------
Eval num_timesteps=4722000, episode_reward=2286.61 +/- 1240.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 4722000  |
---------------------------------
Eval num_timesteps=4723000, episode_reward=2822.50 +/- 14.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 4723000  |
| train/             |          |
|    actor_loss      | -0.686   |
|    critic_loss     | 0.000422 |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | 2.43     |
|    learning_rate   | 0.000277 |
|    n_updates       | 23060    |
---------------------------------
Eval num_timesteps=4724000, episode_reward=2260.67 +/- 1216.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.26e+03 |
| time/              |          |
|    total_timesteps | 4724000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 4724     |
|    fps             | 639      |
|    time_elapsed    | 7389     |
|    total_timesteps | 4724000  |
---------------------------------
Eval num_timesteps=4725000, episode_reward=2232.32 +/- 1206.57
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 4725000  |
| train/             |          |
|    actor_loss      | -0.468   |
|    critic_loss     | 0.00044  |
|    ent_coef        | 0.000456 |
|    ent_coef_loss   | -27.9    |
|    learning_rate   | 0.000275 |
|    n_updates       | 23070    |
---------------------------------
Eval num_timesteps=4726000, episode_reward=2876.07 +/- 28.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4726000  |
---------------------------------
Eval num_timesteps=4727000, episode_reward=2993.12 +/- 33.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4727000  |
| train/             |          |
|    actor_loss      | -0.682   |
|    critic_loss     | 0.000549 |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | 2.11     |
|    learning_rate   | 0.000273 |
|    n_updates       | 23080    |
---------------------------------
Eval num_timesteps=4728000, episode_reward=2336.45 +/- 1257.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 4728000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 4728     |
|    fps             | 639      |
|    time_elapsed    | 7395     |
|    total_timesteps | 4728000  |
---------------------------------
Eval num_timesteps=4729000, episode_reward=2948.58 +/- 18.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4729000  |
| train/             |          |
|    actor_loss      | -0.679   |
|    critic_loss     | 0.000496 |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | 2.38     |
|    learning_rate   | 0.000271 |
|    n_updates       | 23090    |
---------------------------------
Eval num_timesteps=4730000, episode_reward=2927.27 +/- 34.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4730000  |
---------------------------------
Eval num_timesteps=4731000, episode_reward=2286.61 +/- 1227.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.29e+03 |
| time/              |          |
|    total_timesteps | 4731000  |
| train/             |          |
|    actor_loss      | -0.672   |
|    critic_loss     | 0.000483 |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | 0.748    |
|    learning_rate   | 0.000269 |
|    n_updates       | 23100    |
---------------------------------
Eval num_timesteps=4732000, episode_reward=2898.89 +/- 38.28
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4732000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 4732     |
|    fps             | 639      |
|    time_elapsed    | 7401     |
|    total_timesteps | 4732000  |
---------------------------------
Eval num_timesteps=4733000, episode_reward=2934.39 +/- 28.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4733000  |
| train/             |          |
|    actor_loss      | -0.675   |
|    critic_loss     | 0.00048  |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | 1.42     |
|    learning_rate   | 0.000267 |
|    n_updates       | 23110    |
---------------------------------
Eval num_timesteps=4734000, episode_reward=2935.45 +/- 40.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4734000  |
---------------------------------
Eval num_timesteps=4735000, episode_reward=2966.35 +/- 18.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4735000  |
| train/             |          |
|    actor_loss      | -0.687   |
|    critic_loss     | 0.000512 |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | 2.3      |
|    learning_rate   | 0.000265 |
|    n_updates       | 23120    |
---------------------------------
Eval num_timesteps=4736000, episode_reward=2926.04 +/- 25.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4736000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.66e+03 |
| time/              |          |
|    episodes        | 4736     |
|    fps             | 639      |
|    time_elapsed    | 7408     |
|    total_timesteps | 4736000  |
---------------------------------
Eval num_timesteps=4737000, episode_reward=2935.43 +/- 35.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4737000  |
---------------------------------
Eval num_timesteps=4738000, episode_reward=2986.15 +/- 19.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4738000  |
| train/             |          |
|    actor_loss      | -0.662   |
|    critic_loss     | 0.000458 |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | -0.655   |
|    learning_rate   | 0.000263 |
|    n_updates       | 23130    |
---------------------------------
Eval num_timesteps=4739000, episode_reward=2994.02 +/- 39.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4739000  |
---------------------------------
Eval num_timesteps=4740000, episode_reward=2929.90 +/- 24.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4740000  |
| train/             |          |
|    actor_loss      | -0.681   |
|    critic_loss     | 0.000495 |
|    ent_coef        | 0.000453 |
|    ent_coef_loss   | 2.77     |
|    learning_rate   | 0.000261 |
|    n_updates       | 23140    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 4740     |
|    fps             | 639      |
|    time_elapsed    | 7414     |
|    total_timesteps | 4740000  |
---------------------------------
Eval num_timesteps=4741000, episode_reward=2945.85 +/- 23.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4741000  |
---------------------------------
Eval num_timesteps=4742000, episode_reward=3000.02 +/- 19.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4742000  |
| train/             |          |
|    actor_loss      | -0.677   |
|    critic_loss     | 0.000473 |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | 2.93     |
|    learning_rate   | 0.000259 |
|    n_updates       | 23150    |
---------------------------------
Eval num_timesteps=4743000, episode_reward=1728.17 +/- 1551.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.73e+03 |
| time/              |          |
|    total_timesteps | 4743000  |
---------------------------------
Eval num_timesteps=4744000, episode_reward=2980.10 +/- 19.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4744000  |
| train/             |          |
|    actor_loss      | -0.682   |
|    critic_loss     | 0.000483 |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | 2.78     |
|    learning_rate   | 0.000257 |
|    n_updates       | 23160    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.73e+03 |
| time/              |          |
|    episodes        | 4744     |
|    fps             | 639      |
|    time_elapsed    | 7420     |
|    total_timesteps | 4744000  |
---------------------------------
Eval num_timesteps=4745000, episode_reward=2997.23 +/- 10.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4745000  |
---------------------------------
Eval num_timesteps=4746000, episode_reward=3023.91 +/- 55.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4746000  |
| train/             |          |
|    actor_loss      | -0.683   |
|    critic_loss     | 0.000506 |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | 4.37     |
|    learning_rate   | 0.000255 |
|    n_updates       | 23170    |
---------------------------------
Eval num_timesteps=4747000, episode_reward=2408.00 +/- 1287.56
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.41e+03 |
| time/              |          |
|    total_timesteps | 4747000  |
---------------------------------
Eval num_timesteps=4748000, episode_reward=2433.36 +/- 1303.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.43e+03 |
| time/              |          |
|    total_timesteps | 4748000  |
| train/             |          |
|    actor_loss      | -0.636   |
|    critic_loss     | 0.00056  |
|    ent_coef        | 0.000455 |
|    ent_coef_loss   | -1.86    |
|    learning_rate   | 0.000253 |
|    n_updates       | 23180    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.7e+03  |
| time/              |          |
|    episodes        | 4748     |
|    fps             | 639      |
|    time_elapsed    | 7426     |
|    total_timesteps | 4748000  |
---------------------------------
Eval num_timesteps=4749000, episode_reward=3111.51 +/- 21.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.11e+03 |
| time/              |          |
|    total_timesteps | 4749000  |
---------------------------------
Eval num_timesteps=4750000, episode_reward=2388.52 +/- 1299.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.39e+03 |
| time/              |          |
|    total_timesteps | 4750000  |
| train/             |          |
|    actor_loss      | -0.342   |
|    critic_loss     | 0.000152 |
|    ent_coef        | 0.000454 |
|    ent_coef_loss   | -48.8    |
|    learning_rate   | 0.000251 |
|    n_updates       | 23190    |
---------------------------------
Eval num_timesteps=4751000, episode_reward=3097.07 +/- 31.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.1e+03  |
| time/              |          |
|    total_timesteps | 4751000  |
---------------------------------
Eval num_timesteps=4752000, episode_reward=3051.60 +/- 23.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4752000  |
| train/             |          |
|    actor_loss      | -0.681   |
|    critic_loss     | 0.00061  |
|    ent_coef        | 0.000451 |
|    ent_coef_loss   | 4.34     |
|    learning_rate   | 0.000249 |
|    n_updates       | 23200    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 4752     |
|    fps             | 639      |
|    time_elapsed    | 7433     |
|    total_timesteps | 4752000  |
---------------------------------
Eval num_timesteps=4753000, episode_reward=3056.53 +/- 23.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4753000  |
---------------------------------
Eval num_timesteps=4754000, episode_reward=2927.17 +/- 24.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4754000  |
| train/             |          |
|    actor_loss      | -0.68    |
|    critic_loss     | 0.000498 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | 4.08     |
|    learning_rate   | 0.000247 |
|    n_updates       | 23210    |
---------------------------------
Eval num_timesteps=4755000, episode_reward=2298.61 +/- 1257.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 4755000  |
---------------------------------
Eval num_timesteps=4756000, episode_reward=2893.56 +/- 31.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4756000  |
| train/             |          |
|    actor_loss      | -0.675   |
|    critic_loss     | 0.000539 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | 2.09     |
|    learning_rate   | 0.000245 |
|    n_updates       | 23220    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 4756     |
|    fps             | 639      |
|    time_elapsed    | 7439     |
|    total_timesteps | 4756000  |
---------------------------------
Eval num_timesteps=4757000, episode_reward=2893.78 +/- 31.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4757000  |
---------------------------------
Eval num_timesteps=4758000, episode_reward=2297.30 +/- 1274.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.3e+03  |
| time/              |          |
|    total_timesteps | 4758000  |
| train/             |          |
|    actor_loss      | -0.664   |
|    critic_loss     | 0.000539 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | 1.92     |
|    learning_rate   | 0.000242 |
|    n_updates       | 23230    |
---------------------------------
Eval num_timesteps=4759000, episode_reward=2919.06 +/- 22.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4759000  |
---------------------------------
Eval num_timesteps=4760000, episode_reward=2988.38 +/- 42.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4760000  |
| train/             |          |
|    actor_loss      | -0.669   |
|    critic_loss     | 0.000449 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | 1.6      |
|    learning_rate   | 0.00024  |
|    n_updates       | 23240    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 4760     |
|    fps             | 639      |
|    time_elapsed    | 7445     |
|    total_timesteps | 4760000  |
---------------------------------
Eval num_timesteps=4761000, episode_reward=2988.66 +/- 24.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4761000  |
---------------------------------
Eval num_timesteps=4762000, episode_reward=3008.22 +/- 21.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4762000  |
| train/             |          |
|    actor_loss      | -0.683   |
|    critic_loss     | 0.000518 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | 4.59     |
|    learning_rate   | 0.000238 |
|    n_updates       | 23250    |
---------------------------------
Eval num_timesteps=4763000, episode_reward=2324.00 +/- 1284.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 4763000  |
---------------------------------
Eval num_timesteps=4764000, episode_reward=2305.81 +/- 1276.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 4764000  |
| train/             |          |
|    actor_loss      | -0.669   |
|    critic_loss     | 0.00047  |
|    ent_coef        | 0.000451 |
|    ent_coef_loss   | 1.85     |
|    learning_rate   | 0.000236 |
|    n_updates       | 23260    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.67e+03 |
| time/              |          |
|    episodes        | 4764     |
|    fps             | 639      |
|    time_elapsed    | 7452     |
|    total_timesteps | 4764000  |
---------------------------------
Eval num_timesteps=4765000, episode_reward=2279.83 +/- 1252.79
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 4765000  |
---------------------------------
Eval num_timesteps=4766000, episode_reward=1038.62 +/- 1549.72
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 1.04e+03 |
| time/              |          |
|    total_timesteps | 4766000  |
| train/             |          |
|    actor_loss      | -0.528   |
|    critic_loss     | 0.000527 |
|    ent_coef        | 0.000451 |
|    ent_coef_loss   | -16      |
|    learning_rate   | 0.000234 |
|    n_updates       | 23270    |
---------------------------------
Eval num_timesteps=4767000, episode_reward=2965.46 +/- 16.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4767000  |
---------------------------------
Eval num_timesteps=4768000, episode_reward=2379.23 +/- 1303.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 4768000  |
| train/             |          |
|    actor_loss      | -0.605   |
|    critic_loss     | 0.000485 |
|    ent_coef        | 0.00045  |
|    ent_coef_loss   | -6       |
|    learning_rate   | 0.000232 |
|    n_updates       | 23280    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 4768     |
|    fps             | 639      |
|    time_elapsed    | 7458     |
|    total_timesteps | 4768000  |
---------------------------------
Eval num_timesteps=4769000, episode_reward=2357.94 +/- 1293.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 4769000  |
---------------------------------
Eval num_timesteps=4770000, episode_reward=3012.97 +/- 30.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4770000  |
| train/             |          |
|    actor_loss      | -0.676   |
|    critic_loss     | 0.000552 |
|    ent_coef        | 0.000449 |
|    ent_coef_loss   | 4.5      |
|    learning_rate   | 0.00023  |
|    n_updates       | 23290    |
---------------------------------
Eval num_timesteps=4771000, episode_reward=2961.38 +/- 45.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4771000  |
---------------------------------
Eval num_timesteps=4772000, episode_reward=2985.81 +/- 32.33
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4772000  |
| train/             |          |
|    actor_loss      | -0.478   |
|    critic_loss     | 0.000421 |
|    ent_coef        | 0.000449 |
|    ent_coef_loss   | -24.9    |
|    learning_rate   | 0.000228 |
|    n_updates       | 23300    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 4772     |
|    fps             | 639      |
|    time_elapsed    | 7464     |
|    total_timesteps | 4772000  |
---------------------------------
Eval num_timesteps=4773000, episode_reward=2307.24 +/- 1261.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.31e+03 |
| time/              |          |
|    total_timesteps | 4773000  |
---------------------------------
Eval num_timesteps=4774000, episode_reward=2971.91 +/- 29.34
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4774000  |
| train/             |          |
|    actor_loss      | -0.46    |
|    critic_loss     | 0.000418 |
|    ent_coef        | 0.000447 |
|    ent_coef_loss   | -24.7    |
|    learning_rate   | 0.000226 |
|    n_updates       | 23310    |
---------------------------------
Eval num_timesteps=4775000, episode_reward=2959.55 +/- 29.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4775000  |
---------------------------------
Eval num_timesteps=4776000, episode_reward=2377.74 +/- 1260.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.38e+03 |
| time/              |          |
|    total_timesteps | 4776000  |
| train/             |          |
|    actor_loss      | -0.667   |
|    critic_loss     | 0.000583 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 2.83     |
|    learning_rate   | 0.000224 |
|    n_updates       | 23320    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 4776     |
|    fps             | 639      |
|    time_elapsed    | 7471     |
|    total_timesteps | 4776000  |
---------------------------------
Eval num_timesteps=4777000, episode_reward=3023.47 +/- 54.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4777000  |
---------------------------------
Eval num_timesteps=4778000, episode_reward=2448.87 +/- 1311.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.45e+03 |
| time/              |          |
|    total_timesteps | 4778000  |
| train/             |          |
|    actor_loss      | -0.673   |
|    critic_loss     | 0.000543 |
|    ent_coef        | 0.000444 |
|    ent_coef_loss   | 3.24     |
|    learning_rate   | 0.000222 |
|    n_updates       | 23330    |
---------------------------------
Eval num_timesteps=4779000, episode_reward=2439.67 +/- 1298.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 4779000  |
---------------------------------
Eval num_timesteps=4780000, episode_reward=2437.18 +/- 1323.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.44e+03 |
| time/              |          |
|    total_timesteps | 4780000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 4780     |
|    fps             | 639      |
|    time_elapsed    | 7477     |
|    total_timesteps | 4780000  |
---------------------------------
Eval num_timesteps=4781000, episode_reward=2978.33 +/- 16.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4781000  |
| train/             |          |
|    actor_loss      | -0.672   |
|    critic_loss     | 0.000495 |
|    ent_coef        | 0.000444 |
|    ent_coef_loss   | 3.39     |
|    learning_rate   | 0.00022  |
|    n_updates       | 23340    |
---------------------------------
Eval num_timesteps=4782000, episode_reward=2364.18 +/- 1272.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.36e+03 |
| time/              |          |
|    total_timesteps | 4782000  |
---------------------------------
Eval num_timesteps=4783000, episode_reward=2322.34 +/- 1247.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 4783000  |
| train/             |          |
|    actor_loss      | -0.673   |
|    critic_loss     | 0.000538 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 3.53     |
|    learning_rate   | 0.000218 |
|    n_updates       | 23350    |
---------------------------------
Eval num_timesteps=4784000, episode_reward=2954.50 +/- 31.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4784000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 4784     |
|    fps             | 639      |
|    time_elapsed    | 7483     |
|    total_timesteps | 4784000  |
---------------------------------
Eval num_timesteps=4785000, episode_reward=2336.79 +/- 1271.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.34e+03 |
| time/              |          |
|    total_timesteps | 4785000  |
| train/             |          |
|    actor_loss      | -0.665   |
|    critic_loss     | 0.000556 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 2.99     |
|    learning_rate   | 0.000216 |
|    n_updates       | 23360    |
---------------------------------
Eval num_timesteps=4786000, episode_reward=2926.02 +/- 35.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4786000  |
---------------------------------
Eval num_timesteps=4787000, episode_reward=2316.33 +/- 1239.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.32e+03 |
| time/              |          |
|    total_timesteps | 4787000  |
| train/             |          |
|    actor_loss      | -0.662   |
|    critic_loss     | 0.000489 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 2.49     |
|    learning_rate   | 0.000214 |
|    n_updates       | 23370    |
---------------------------------
Eval num_timesteps=4788000, episode_reward=2942.34 +/- 11.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4788000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 4788     |
|    fps             | 639      |
|    time_elapsed    | 7490     |
|    total_timesteps | 4788000  |
---------------------------------
Eval num_timesteps=4789000, episode_reward=3041.78 +/- 19.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4789000  |
| train/             |          |
|    actor_loss      | -0.666   |
|    critic_loss     | 0.000513 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 2.93     |
|    learning_rate   | 0.000212 |
|    n_updates       | 23380    |
---------------------------------
Eval num_timesteps=4790000, episode_reward=2990.64 +/- 23.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.99e+03 |
| time/              |          |
|    total_timesteps | 4790000  |
---------------------------------
Eval num_timesteps=4791000, episode_reward=3019.32 +/- 25.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.02e+03 |
| time/              |          |
|    total_timesteps | 4791000  |
| train/             |          |
|    actor_loss      | -0.459   |
|    critic_loss     | 0.000567 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | -22.2    |
|    learning_rate   | 0.00021  |
|    n_updates       | 23390    |
---------------------------------
Eval num_timesteps=4792000, episode_reward=3011.17 +/- 45.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4792000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 4792     |
|    fps             | 639      |
|    time_elapsed    | 7496     |
|    total_timesteps | 4792000  |
---------------------------------
Eval num_timesteps=4793000, episode_reward=3048.26 +/- 23.37
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.05e+03 |
| time/              |          |
|    total_timesteps | 4793000  |
| train/             |          |
|    actor_loss      | -0.666   |
|    critic_loss     | 0.000791 |
|    ent_coef        | 0.000445 |
|    ent_coef_loss   | 4.39     |
|    learning_rate   | 0.000208 |
|    n_updates       | 23400    |
---------------------------------
Eval num_timesteps=4794000, episode_reward=3059.03 +/- 28.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.06e+03 |
| time/              |          |
|    total_timesteps | 4794000  |
---------------------------------
Eval num_timesteps=4795000, episode_reward=2963.56 +/- 48.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4795000  |
| train/             |          |
|    actor_loss      | -0.588   |
|    critic_loss     | 0.000943 |
|    ent_coef        | 0.000444 |
|    ent_coef_loss   | -3.76    |
|    learning_rate   | 0.000206 |
|    n_updates       | 23410    |
---------------------------------
Eval num_timesteps=4796000, episode_reward=2980.94 +/- 46.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4796000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 4796     |
|    fps             | 639      |
|    time_elapsed    | 7502     |
|    total_timesteps | 4796000  |
---------------------------------
Eval num_timesteps=4797000, episode_reward=2931.56 +/- 24.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4797000  |
| train/             |          |
|    actor_loss      | -0.527   |
|    critic_loss     | 0.000523 |
|    ent_coef        | 0.000444 |
|    ent_coef_loss   | -17.7    |
|    learning_rate   | 0.000204 |
|    n_updates       | 23420    |
---------------------------------
Eval num_timesteps=4798000, episode_reward=2968.11 +/- 33.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4798000  |
---------------------------------
Eval num_timesteps=4799000, episode_reward=2892.89 +/- 20.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4799000  |
| train/             |          |
|    actor_loss      | -0.66    |
|    critic_loss     | 0.000544 |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | 1.68     |
|    learning_rate   | 0.000202 |
|    n_updates       | 23430    |
---------------------------------
Eval num_timesteps=4800000, episode_reward=2909.93 +/- 14.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4800000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.55e+03 |
| time/              |          |
|    episodes        | 4800     |
|    fps             | 639      |
|    time_elapsed    | 7509     |
|    total_timesteps | 4800000  |
---------------------------------
Eval num_timesteps=4801000, episode_reward=2206.02 +/- 1232.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.21e+03 |
| time/              |          |
|    total_timesteps | 4801000  |
| train/             |          |
|    actor_loss      | -0.649   |
|    critic_loss     | 0.000499 |
|    ent_coef        | 0.000442 |
|    ent_coef_loss   | 2.42     |
|    learning_rate   | 0.000199 |
|    n_updates       | 23440    |
---------------------------------
Eval num_timesteps=4802000, episode_reward=2816.71 +/- 31.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 4802000  |
---------------------------------
Eval num_timesteps=4803000, episode_reward=2840.32 +/- 46.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4803000  |
| train/             |          |
|    actor_loss      | -0.645   |
|    critic_loss     | 0.000485 |
|    ent_coef        | 0.000442 |
|    ent_coef_loss   | 0.621    |
|    learning_rate   | 0.000197 |
|    n_updates       | 23450    |
---------------------------------
Eval num_timesteps=4804000, episode_reward=2793.92 +/- 32.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4804000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 4804     |
|    fps             | 639      |
|    time_elapsed    | 7515     |
|    total_timesteps | 4804000  |
---------------------------------
Eval num_timesteps=4805000, episode_reward=2747.29 +/- 33.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4805000  |
| train/             |          |
|    actor_loss      | -0.652   |
|    critic_loss     | 0.000508 |
|    ent_coef        | 0.000442 |
|    ent_coef_loss   | 1.91     |
|    learning_rate   | 0.000195 |
|    n_updates       | 23460    |
---------------------------------
Eval num_timesteps=4806000, episode_reward=2183.33 +/- 1175.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 4806000  |
---------------------------------
Eval num_timesteps=4807000, episode_reward=2868.50 +/- 25.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4807000  |
| train/             |          |
|    actor_loss      | -0.645   |
|    critic_loss     | 0.000442 |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | 0.293    |
|    learning_rate   | 0.000193 |
|    n_updates       | 23470    |
---------------------------------
Eval num_timesteps=4808000, episode_reward=2893.68 +/- 64.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4808000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.58e+03 |
| time/              |          |
|    episodes        | 4808     |
|    fps             | 639      |
|    time_elapsed    | 7522     |
|    total_timesteps | 4808000  |
---------------------------------
Eval num_timesteps=4809000, episode_reward=2914.49 +/- 26.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4809000  |
| train/             |          |
|    actor_loss      | -0.648   |
|    critic_loss     | 0.000496 |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | 2.23     |
|    learning_rate   | 0.000191 |
|    n_updates       | 23480    |
---------------------------------
Eval num_timesteps=4810000, episode_reward=2898.67 +/- 25.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4810000  |
---------------------------------
Eval num_timesteps=4811000, episode_reward=2165.05 +/- 1214.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 4811000  |
| train/             |          |
|    actor_loss      | -0.642   |
|    critic_loss     | 0.000474 |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | 0.639    |
|    learning_rate   | 0.000189 |
|    n_updates       | 23490    |
---------------------------------
Eval num_timesteps=4812000, episode_reward=2804.47 +/- 21.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4812000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 4812     |
|    fps             | 638      |
|    time_elapsed    | 7530     |
|    total_timesteps | 4812000  |
---------------------------------
Eval num_timesteps=4813000, episode_reward=2234.16 +/- 1245.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.23e+03 |
| time/              |          |
|    total_timesteps | 4813000  |
| train/             |          |
|    actor_loss      | -0.647   |
|    critic_loss     | 0.000453 |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | 0.81     |
|    learning_rate   | 0.000187 |
|    n_updates       | 23500    |
---------------------------------
Eval num_timesteps=4814000, episode_reward=2203.71 +/- 1232.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 4814000  |
---------------------------------
Eval num_timesteps=4815000, episode_reward=2909.72 +/- 10.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4815000  |
| train/             |          |
|    actor_loss      | -0.648   |
|    critic_loss     | 0.00047  |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | 0.688    |
|    learning_rate   | 0.000185 |
|    n_updates       | 23510    |
---------------------------------
Eval num_timesteps=4816000, episode_reward=2934.39 +/- 22.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4816000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.6e+03  |
| time/              |          |
|    episodes        | 4816     |
|    fps             | 638      |
|    time_elapsed    | 7540     |
|    total_timesteps | 4816000  |
---------------------------------
Eval num_timesteps=4817000, episode_reward=2279.34 +/- 1267.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.28e+03 |
| time/              |          |
|    total_timesteps | 4817000  |
| train/             |          |
|    actor_loss      | -0.653   |
|    critic_loss     | 0.000444 |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | 1.29     |
|    learning_rate   | 0.000183 |
|    n_updates       | 23520    |
---------------------------------
Eval num_timesteps=4818000, episode_reward=2910.99 +/- 37.82
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4818000  |
---------------------------------
Eval num_timesteps=4819000, episode_reward=2891.39 +/- 16.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4819000  |
| train/             |          |
|    actor_loss      | -0.429   |
|    critic_loss     | 0.00119  |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | -8.07    |
|    learning_rate   | 0.000181 |
|    n_updates       | 23530    |
---------------------------------
Eval num_timesteps=4820000, episode_reward=2914.13 +/- 26.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4820000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 4820     |
|    fps             | 638      |
|    time_elapsed    | 7547     |
|    total_timesteps | 4820000  |
---------------------------------
Eval num_timesteps=4821000, episode_reward=2968.60 +/- 29.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4821000  |
| train/             |          |
|    actor_loss      | -0.617   |
|    critic_loss     | 0.00116  |
|    ent_coef        | 0.000443 |
|    ent_coef_loss   | 1.09     |
|    learning_rate   | 0.000179 |
|    n_updates       | 23540    |
---------------------------------
Eval num_timesteps=4822000, episode_reward=2976.45 +/- 17.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4822000  |
---------------------------------
Eval num_timesteps=4823000, episode_reward=3013.82 +/- 50.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4823000  |
---------------------------------
Eval num_timesteps=4824000, episode_reward=3004.01 +/- 45.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4824000  |
| train/             |          |
|    actor_loss      | -0.224   |
|    critic_loss     | 0.000189 |
|    ent_coef        | 0.000442 |
|    ent_coef_loss   | -16.4    |
|    learning_rate   | 0.000177 |
|    n_updates       | 23550    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    episodes        | 4824     |
|    fps             | 638      |
|    time_elapsed    | 7553     |
|    total_timesteps | 4824000  |
---------------------------------
Eval num_timesteps=4825000, episode_reward=3028.30 +/- 30.03
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.03e+03 |
| time/              |          |
|    total_timesteps | 4825000  |
---------------------------------
Eval num_timesteps=4826000, episode_reward=2952.12 +/- 47.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4826000  |
| train/             |          |
|    actor_loss      | -0.655   |
|    critic_loss     | 0.000842 |
|    ent_coef        | 0.000442 |
|    ent_coef_loss   | 2.38     |
|    learning_rate   | 0.000175 |
|    n_updates       | 23560    |
---------------------------------
Eval num_timesteps=4827000, episode_reward=2330.84 +/- 1349.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.33e+03 |
| time/              |          |
|    total_timesteps | 4827000  |
---------------------------------
Eval num_timesteps=4828000, episode_reward=2912.30 +/- 33.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4828000  |
| train/             |          |
|    actor_loss      | -0.638   |
|    critic_loss     | 0.000579 |
|    ent_coef        | 0.000441 |
|    ent_coef_loss   | -0.104   |
|    learning_rate   | 0.000173 |
|    n_updates       | 23570    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.54e+03 |
| time/              |          |
|    episodes        | 4828     |
|    fps             | 638      |
|    time_elapsed    | 7560     |
|    total_timesteps | 4828000  |
---------------------------------
Eval num_timesteps=4829000, episode_reward=2910.37 +/- 16.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4829000  |
---------------------------------
Eval num_timesteps=4830000, episode_reward=2802.51 +/- 16.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4830000  |
| train/             |          |
|    actor_loss      | -0.646   |
|    critic_loss     | 0.000532 |
|    ent_coef        | 0.000441 |
|    ent_coef_loss   | 0.124    |
|    learning_rate   | 0.000171 |
|    n_updates       | 23580    |
---------------------------------
Eval num_timesteps=4831000, episode_reward=2814.59 +/- 13.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4831000  |
---------------------------------
Eval num_timesteps=4832000, episode_reward=2217.62 +/- 1199.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.22e+03 |
| time/              |          |
|    total_timesteps | 4832000  |
| train/             |          |
|    actor_loss      | -0.584   |
|    critic_loss     | 0.000985 |
|    ent_coef        | 0.000441 |
|    ent_coef_loss   | -2.55    |
|    learning_rate   | 0.000169 |
|    n_updates       | 23590    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    episodes        | 4832     |
|    fps             | 638      |
|    time_elapsed    | 7566     |
|    total_timesteps | 4832000  |
---------------------------------
Eval num_timesteps=4833000, episode_reward=2801.91 +/- 22.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4833000  |
---------------------------------
Eval num_timesteps=4834000, episode_reward=2806.22 +/- 33.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4834000  |
| train/             |          |
|    actor_loss      | -0.399   |
|    critic_loss     | 0.00123  |
|    ent_coef        | 0.000441 |
|    ent_coef_loss   | -11.4    |
|    learning_rate   | 0.000167 |
|    n_updates       | 23600    |
---------------------------------
Eval num_timesteps=4835000, episode_reward=2183.27 +/- 1282.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.18e+03 |
| time/              |          |
|    total_timesteps | 4835000  |
---------------------------------
Eval num_timesteps=4836000, episode_reward=2869.30 +/- 38.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4836000  |
| train/             |          |
|    actor_loss      | -0.454   |
|    critic_loss     | 0.00123  |
|    ent_coef        | 0.00044  |
|    ent_coef_loss   | -11.6    |
|    learning_rate   | 0.000165 |
|    n_updates       | 23610    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.47e+03 |
| time/              |          |
|    episodes        | 4836     |
|    fps             | 638      |
|    time_elapsed    | 7573     |
|    total_timesteps | 4836000  |
---------------------------------
Eval num_timesteps=4837000, episode_reward=2839.70 +/- 30.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4837000  |
---------------------------------
Eval num_timesteps=4838000, episode_reward=2926.10 +/- 46.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4838000  |
| train/             |          |
|    actor_loss      | -0.543   |
|    critic_loss     | 0.00117  |
|    ent_coef        | 0.00044  |
|    ent_coef_loss   | -4.99    |
|    learning_rate   | 0.000163 |
|    n_updates       | 23620    |
---------------------------------
Eval num_timesteps=4839000, episode_reward=2894.26 +/- 20.96
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4839000  |
---------------------------------
Eval num_timesteps=4840000, episode_reward=2882.00 +/- 46.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4840000  |
| train/             |          |
|    actor_loss      | -0.416   |
|    critic_loss     | 0.0012   |
|    ent_coef        | 0.000439 |
|    ent_coef_loss   | -10.4    |
|    learning_rate   | 0.000161 |
|    n_updates       | 23630    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    episodes        | 4840     |
|    fps             | 638      |
|    time_elapsed    | 7579     |
|    total_timesteps | 4840000  |
---------------------------------
Eval num_timesteps=4841000, episode_reward=2880.35 +/- 39.19
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4841000  |
---------------------------------
Eval num_timesteps=4842000, episode_reward=2828.59 +/- 29.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4842000  |
| train/             |          |
|    actor_loss      | -0.489   |
|    critic_loss     | 0.00119  |
|    ent_coef        | 0.000438 |
|    ent_coef_loss   | -6.19    |
|    learning_rate   | 0.000159 |
|    n_updates       | 23640    |
---------------------------------
Eval num_timesteps=4843000, episode_reward=2845.23 +/- 34.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4843000  |
---------------------------------
Eval num_timesteps=4844000, episode_reward=2197.06 +/- 1266.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.2e+03  |
| time/              |          |
|    total_timesteps | 4844000  |
| train/             |          |
|    actor_loss      | -0.624   |
|    critic_loss     | 0.000665 |
|    ent_coef        | 0.000438 |
|    ent_coef_loss   | -0.12    |
|    learning_rate   | 0.000156 |
|    n_updates       | 23650    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 4844     |
|    fps             | 638      |
|    time_elapsed    | 7585     |
|    total_timesteps | 4844000  |
---------------------------------
Eval num_timesteps=4845000, episode_reward=2192.73 +/- 1263.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.19e+03 |
| time/              |          |
|    total_timesteps | 4845000  |
---------------------------------
Eval num_timesteps=4846000, episode_reward=2173.09 +/- 1255.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.17e+03 |
| time/              |          |
|    total_timesteps | 4846000  |
| train/             |          |
|    actor_loss      | -0.638   |
|    critic_loss     | 0.000527 |
|    ent_coef        | 0.000438 |
|    ent_coef_loss   | 0.655    |
|    learning_rate   | 0.000154 |
|    n_updates       | 23660    |
---------------------------------
Eval num_timesteps=4847000, episode_reward=2826.57 +/- 23.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4847000  |
---------------------------------
Eval num_timesteps=4848000, episode_reward=2694.08 +/- 38.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4848000  |
| train/             |          |
|    actor_loss      | -0.626   |
|    critic_loss     | 0.00044  |
|    ent_coef        | 0.000438 |
|    ent_coef_loss   | -0.579   |
|    learning_rate   | 0.000152 |
|    n_updates       | 23670    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    episodes        | 4848     |
|    fps             | 638      |
|    time_elapsed    | 7592     |
|    total_timesteps | 4848000  |
---------------------------------
Eval num_timesteps=4849000, episode_reward=2689.83 +/- 18.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4849000  |
---------------------------------
Eval num_timesteps=4850000, episode_reward=2650.78 +/- 59.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 4850000  |
| train/             |          |
|    actor_loss      | -0.621   |
|    critic_loss     | 0.000398 |
|    ent_coef        | 0.000438 |
|    ent_coef_loss   | -3.22    |
|    learning_rate   | 0.00015  |
|    n_updates       | 23680    |
---------------------------------
Eval num_timesteps=4851000, episode_reward=2652.27 +/- 28.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 4851000  |
---------------------------------
Eval num_timesteps=4852000, episode_reward=2600.35 +/- 25.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 4852000  |
| train/             |          |
|    actor_loss      | -0.612   |
|    critic_loss     | 0.000421 |
|    ent_coef        | 0.000437 |
|    ent_coef_loss   | -3.75    |
|    learning_rate   | 0.000148 |
|    n_updates       | 23690    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.44e+03 |
| time/              |          |
|    episodes        | 4852     |
|    fps             | 638      |
|    time_elapsed    | 7598     |
|    total_timesteps | 4852000  |
---------------------------------
Eval num_timesteps=4853000, episode_reward=2583.60 +/- 29.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.58e+03 |
| time/              |          |
|    total_timesteps | 4853000  |
---------------------------------
Eval num_timesteps=4854000, episode_reward=2598.81 +/- 29.69
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.6e+03  |
| time/              |          |
|    total_timesteps | 4854000  |
| train/             |          |
|    actor_loss      | -0.61    |
|    critic_loss     | 0.000426 |
|    ent_coef        | 0.000437 |
|    ent_coef_loss   | -2.68    |
|    learning_rate   | 0.000146 |
|    n_updates       | 23700    |
---------------------------------
Eval num_timesteps=4855000, episode_reward=2020.47 +/- 1175.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.02e+03 |
| time/              |          |
|    total_timesteps | 4855000  |
---------------------------------
Eval num_timesteps=4856000, episode_reward=2627.22 +/- 26.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 4856000  |
| train/             |          |
|    actor_loss      | -0.444   |
|    critic_loss     | 0.00109  |
|    ent_coef        | 0.000437 |
|    ent_coef_loss   | -12.8    |
|    learning_rate   | 0.000144 |
|    n_updates       | 23710    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    episodes        | 4856     |
|    fps             | 638      |
|    time_elapsed    | 7604     |
|    total_timesteps | 4856000  |
---------------------------------
Eval num_timesteps=4857000, episode_reward=2660.81 +/- 53.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 4857000  |
---------------------------------
Eval num_timesteps=4858000, episode_reward=2718.82 +/- 27.48
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4858000  |
| train/             |          |
|    actor_loss      | -0.554   |
|    critic_loss     | 0.000916 |
|    ent_coef        | 0.000436 |
|    ent_coef_loss   | -4.01    |
|    learning_rate   | 0.000142 |
|    n_updates       | 23720    |
---------------------------------
Eval num_timesteps=4859000, episode_reward=2666.20 +/- 29.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 4859000  |
---------------------------------
Eval num_timesteps=4860000, episode_reward=2783.91 +/- 37.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4860000  |
| train/             |          |
|    actor_loss      | -0.63    |
|    critic_loss     | 0.000464 |
|    ent_coef        | 0.000436 |
|    ent_coef_loss   | 0.119    |
|    learning_rate   | 0.00014  |
|    n_updates       | 23730    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 4860     |
|    fps             | 638      |
|    time_elapsed    | 7610     |
|    total_timesteps | 4860000  |
---------------------------------
Eval num_timesteps=4861000, episode_reward=2797.39 +/- 44.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4861000  |
---------------------------------
Eval num_timesteps=4862000, episode_reward=2735.34 +/- 31.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4862000  |
| train/             |          |
|    actor_loss      | -0.632   |
|    critic_loss     | 0.00049  |
|    ent_coef        | 0.000436 |
|    ent_coef_loss   | 1.03     |
|    learning_rate   | 0.000138 |
|    n_updates       | 23740    |
---------------------------------
Eval num_timesteps=4863000, episode_reward=2728.73 +/- 43.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 4863000  |
---------------------------------
Eval num_timesteps=4864000, episode_reward=2760.60 +/- 30.35
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4864000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.35e+03 |
| time/              |          |
|    episodes        | 4864     |
|    fps             | 638      |
|    time_elapsed    | 7617     |
|    total_timesteps | 4864000  |
---------------------------------
Eval num_timesteps=4865000, episode_reward=2708.32 +/- 35.36
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4865000  |
| train/             |          |
|    actor_loss      | -0.444   |
|    critic_loss     | 0.000383 |
|    ent_coef        | 0.000435 |
|    ent_coef_loss   | -29      |
|    learning_rate   | 0.000136 |
|    n_updates       | 23750    |
---------------------------------
Eval num_timesteps=4866000, episode_reward=2691.48 +/- 42.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4866000  |
---------------------------------
Eval num_timesteps=4867000, episode_reward=2678.95 +/- 34.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 4867000  |
| train/             |          |
|    actor_loss      | -0.602   |
|    critic_loss     | 0.00048  |
|    ent_coef        | 0.000435 |
|    ent_coef_loss   | -1.67    |
|    learning_rate   | 0.000134 |
|    n_updates       | 23760    |
---------------------------------
Eval num_timesteps=4868000, episode_reward=2666.06 +/- 18.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.67e+03 |
| time/              |          |
|    total_timesteps | 4868000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 4868     |
|    fps             | 638      |
|    time_elapsed    | 7623     |
|    total_timesteps | 4868000  |
---------------------------------
Eval num_timesteps=4869000, episode_reward=2688.26 +/- 27.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4869000  |
| train/             |          |
|    actor_loss      | -0.61    |
|    critic_loss     | 0.000446 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | -2.18    |
|    learning_rate   | 0.000132 |
|    n_updates       | 23770    |
---------------------------------
Eval num_timesteps=4870000, episode_reward=2074.80 +/- 1197.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 4870000  |
---------------------------------
Eval num_timesteps=4871000, episode_reward=2677.91 +/- 45.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 4871000  |
| train/             |          |
|    actor_loss      | -0.618   |
|    critic_loss     | 0.000406 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | -1.18    |
|    learning_rate   | 0.00013  |
|    n_updates       | 23780    |
---------------------------------
Eval num_timesteps=4872000, episode_reward=2689.23 +/- 38.76
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4872000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 4872     |
|    fps             | 638      |
|    time_elapsed    | 7629     |
|    total_timesteps | 4872000  |
---------------------------------
Eval num_timesteps=4873000, episode_reward=2648.16 +/- 28.26
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 4873000  |
| train/             |          |
|    actor_loss      | -0.611   |
|    critic_loss     | 0.000412 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | -0.113   |
|    learning_rate   | 0.000128 |
|    n_updates       | 23790    |
---------------------------------
Eval num_timesteps=4874000, episode_reward=2609.74 +/- 19.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.61e+03 |
| time/              |          |
|    total_timesteps | 4874000  |
---------------------------------
Eval num_timesteps=4875000, episode_reward=2628.80 +/- 31.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.63e+03 |
| time/              |          |
|    total_timesteps | 4875000  |
| train/             |          |
|    actor_loss      | -0.604   |
|    critic_loss     | 0.000373 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | -2.37    |
|    learning_rate   | 0.000126 |
|    n_updates       | 23800    |
---------------------------------
Eval num_timesteps=4876000, episode_reward=2651.36 +/- 30.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.65e+03 |
| time/              |          |
|    total_timesteps | 4876000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.42e+03 |
| time/              |          |
|    episodes        | 4876     |
|    fps             | 638      |
|    time_elapsed    | 7636     |
|    total_timesteps | 4876000  |
---------------------------------
Eval num_timesteps=4877000, episode_reward=2644.73 +/- 36.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.64e+03 |
| time/              |          |
|    total_timesteps | 4877000  |
| train/             |          |
|    actor_loss      | -0.61    |
|    critic_loss     | 0.000409 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -0.424   |
|    learning_rate   | 0.000124 |
|    n_updates       | 23810    |
---------------------------------
Eval num_timesteps=4878000, episode_reward=2692.01 +/- 48.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.69e+03 |
| time/              |          |
|    total_timesteps | 4878000  |
---------------------------------
Eval num_timesteps=4879000, episode_reward=2683.00 +/- 53.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 4879000  |
| train/             |          |
|    actor_loss      | -0.61    |
|    critic_loss     | 0.000391 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.121    |
|    learning_rate   | 0.000122 |
|    n_updates       | 23820    |
---------------------------------
Eval num_timesteps=4880000, episode_reward=2658.60 +/- 14.62
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.66e+03 |
| time/              |          |
|    total_timesteps | 4880000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    episodes        | 4880     |
|    fps             | 638      |
|    time_elapsed    | 7642     |
|    total_timesteps | 4880000  |
---------------------------------
Eval num_timesteps=4881000, episode_reward=2073.40 +/- 1196.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.07e+03 |
| time/              |          |
|    total_timesteps | 4881000  |
| train/             |          |
|    actor_loss      | -0.615   |
|    critic_loss     | 0.000399 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.228    |
|    learning_rate   | 0.00012  |
|    n_updates       | 23830    |
---------------------------------
Eval num_timesteps=4882000, episode_reward=2715.86 +/- 49.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4882000  |
---------------------------------
Eval num_timesteps=4883000, episode_reward=2705.65 +/- 50.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4883000  |
| train/             |          |
|    actor_loss      | -0.613   |
|    critic_loss     | 0.000422 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -0.235   |
|    learning_rate   | 0.000118 |
|    n_updates       | 23840    |
---------------------------------
Eval num_timesteps=4884000, episode_reward=2706.28 +/- 25.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4884000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 4884     |
|    fps             | 638      |
|    time_elapsed    | 7648     |
|    total_timesteps | 4884000  |
---------------------------------
Eval num_timesteps=4885000, episode_reward=2716.80 +/- 50.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4885000  |
| train/             |          |
|    actor_loss      | -0.613   |
|    critic_loss     | 0.000396 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.191    |
|    learning_rate   | 0.000116 |
|    n_updates       | 23850    |
---------------------------------
Eval num_timesteps=4886000, episode_reward=2707.73 +/- 57.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.71e+03 |
| time/              |          |
|    total_timesteps | 4886000  |
---------------------------------
Eval num_timesteps=4887000, episode_reward=2720.50 +/- 24.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4887000  |
| train/             |          |
|    actor_loss      | -0.614   |
|    critic_loss     | 0.000404 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.864    |
|    learning_rate   | 0.000113 |
|    n_updates       | 23860    |
---------------------------------
Eval num_timesteps=4888000, episode_reward=2725.55 +/- 47.17
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.73e+03 |
| time/              |          |
|    total_timesteps | 4888000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.36e+03 |
| time/              |          |
|    episodes        | 4888     |
|    fps             | 638      |
|    time_elapsed    | 7655     |
|    total_timesteps | 4888000  |
---------------------------------
Eval num_timesteps=4889000, episode_reward=2680.58 +/- 39.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.68e+03 |
| time/              |          |
|    total_timesteps | 4889000  |
| train/             |          |
|    actor_loss      | -0.402   |
|    critic_loss     | 0.00101  |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -16.3    |
|    learning_rate   | 0.000111 |
|    n_updates       | 23870    |
---------------------------------
Eval num_timesteps=4890000, episode_reward=2697.27 +/- 39.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4890000  |
---------------------------------
Eval num_timesteps=4891000, episode_reward=2776.95 +/- 58.60
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4891000  |
| train/             |          |
|    actor_loss      | -0.6     |
|    critic_loss     | 0.000691 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.64     |
|    learning_rate   | 0.000109 |
|    n_updates       | 23880    |
---------------------------------
Eval num_timesteps=4892000, episode_reward=2775.02 +/- 72.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4892000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    episodes        | 4892     |
|    fps             | 638      |
|    time_elapsed    | 7661     |
|    total_timesteps | 4892000  |
---------------------------------
Eval num_timesteps=4893000, episode_reward=2799.74 +/- 39.59
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4893000  |
| train/             |          |
|    actor_loss      | -0.62    |
|    critic_loss     | 0.00047  |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.919    |
|    learning_rate   | 0.000107 |
|    n_updates       | 23890    |
---------------------------------
Eval num_timesteps=4894000, episode_reward=2830.48 +/- 46.16
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4894000  |
---------------------------------
Eval num_timesteps=4895000, episode_reward=2741.61 +/- 51.92
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4895000  |
| train/             |          |
|    actor_loss      | -0.618   |
|    critic_loss     | 0.00044  |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.251    |
|    learning_rate   | 0.000105 |
|    n_updates       | 23900    |
---------------------------------
Eval num_timesteps=4896000, episode_reward=2788.26 +/- 36.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4896000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    episodes        | 4896     |
|    fps             | 638      |
|    time_elapsed    | 7667     |
|    total_timesteps | 4896000  |
---------------------------------
Eval num_timesteps=4897000, episode_reward=2769.22 +/- 17.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4897000  |
| train/             |          |
|    actor_loss      | -0.61    |
|    critic_loss     | 0.000403 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.219    |
|    learning_rate   | 0.000103 |
|    n_updates       | 23910    |
---------------------------------
Eval num_timesteps=4898000, episode_reward=2761.13 +/- 37.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4898000  |
---------------------------------
Eval num_timesteps=4899000, episode_reward=2761.22 +/- 28.63
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4899000  |
| train/             |          |
|    actor_loss      | -0.605   |
|    critic_loss     | 0.000419 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -0.499   |
|    learning_rate   | 0.000101 |
|    n_updates       | 23920    |
---------------------------------
Eval num_timesteps=4900000, episode_reward=2783.63 +/- 35.98
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4900000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 4900     |
|    fps             | 638      |
|    time_elapsed    | 7673     |
|    total_timesteps | 4900000  |
---------------------------------
Eval num_timesteps=4901000, episode_reward=2760.53 +/- 32.78
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4901000  |
| train/             |          |
|    actor_loss      | -0.602   |
|    critic_loss     | 0.000426 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -1.47    |
|    learning_rate   | 9.91e-05 |
|    n_updates       | 23930    |
---------------------------------
Eval num_timesteps=4902000, episode_reward=2779.71 +/- 34.87
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4902000  |
---------------------------------
Eval num_timesteps=4903000, episode_reward=2767.82 +/- 53.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4903000  |
| train/             |          |
|    actor_loss      | -0.615   |
|    critic_loss     | 0.000385 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.723    |
|    learning_rate   | 9.71e-05 |
|    n_updates       | 23940    |
---------------------------------
Eval num_timesteps=4904000, episode_reward=2769.80 +/- 36.12
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4904000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.39e+03 |
| time/              |          |
|    episodes        | 4904     |
|    fps             | 638      |
|    time_elapsed    | 7680     |
|    total_timesteps | 4904000  |
---------------------------------
Eval num_timesteps=4905000, episode_reward=2753.02 +/- 33.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4905000  |
| train/             |          |
|    actor_loss      | -0.599   |
|    critic_loss     | 0.000419 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -0.203   |
|    learning_rate   | 9.5e-05  |
|    n_updates       | 23950    |
---------------------------------
Eval num_timesteps=4906000, episode_reward=2783.73 +/- 49.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4906000  |
---------------------------------
Eval num_timesteps=4907000, episode_reward=2715.85 +/- 22.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4907000  |
---------------------------------
Eval num_timesteps=4908000, episode_reward=2744.93 +/- 44.24
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4908000  |
| train/             |          |
|    actor_loss      | -0.602   |
|    critic_loss     | 0.000374 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -0.909   |
|    learning_rate   | 9.3e-05  |
|    n_updates       | 23960    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    episodes        | 4908     |
|    fps             | 638      |
|    time_elapsed    | 7686     |
|    total_timesteps | 4908000  |
---------------------------------
Eval num_timesteps=4909000, episode_reward=2722.73 +/- 41.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4909000  |
---------------------------------
Eval num_timesteps=4910000, episode_reward=2697.82 +/- 39.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4910000  |
| train/             |          |
|    actor_loss      | -0.596   |
|    critic_loss     | 0.000398 |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | -1.12    |
|    learning_rate   | 9.09e-05 |
|    n_updates       | 23970    |
---------------------------------
Eval num_timesteps=4911000, episode_reward=2696.33 +/- 52.90
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.7e+03  |
| time/              |          |
|    total_timesteps | 4911000  |
---------------------------------
Eval num_timesteps=4912000, episode_reward=2723.71 +/- 33.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.72e+03 |
| time/              |          |
|    total_timesteps | 4912000  |
| train/             |          |
|    actor_loss      | -0.611   |
|    critic_loss     | 0.000405 |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | 0.0823   |
|    learning_rate   | 8.89e-05 |
|    n_updates       | 23980    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.38e+03 |
| time/              |          |
|    episodes        | 4912     |
|    fps             | 638      |
|    time_elapsed    | 7692     |
|    total_timesteps | 4912000  |
---------------------------------
Eval num_timesteps=4913000, episode_reward=2760.89 +/- 48.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4913000  |
---------------------------------
Eval num_timesteps=4914000, episode_reward=2753.73 +/- 48.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.75e+03 |
| time/              |          |
|    total_timesteps | 4914000  |
| train/             |          |
|    actor_loss      | -0.603   |
|    critic_loss     | 0.000432 |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | 0.694    |
|    learning_rate   | 8.68e-05 |
|    n_updates       | 23990    |
---------------------------------
Eval num_timesteps=4915000, episode_reward=2765.85 +/- 27.15
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.77e+03 |
| time/              |          |
|    total_timesteps | 4915000  |
---------------------------------
Eval num_timesteps=4916000, episode_reward=2141.49 +/- 1219.31
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.14e+03 |
| time/              |          |
|    total_timesteps | 4916000  |
| train/             |          |
|    actor_loss      | -0.607   |
|    critic_loss     | 0.000392 |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | 1.13     |
|    learning_rate   | 8.48e-05 |
|    n_updates       | 24000    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.37e+03 |
| time/              |          |
|    episodes        | 4916     |
|    fps             | 638      |
|    time_elapsed    | 7699     |
|    total_timesteps | 4916000  |
---------------------------------
Eval num_timesteps=4917000, episode_reward=2787.22 +/- 44.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.79e+03 |
| time/              |          |
|    total_timesteps | 4917000  |
---------------------------------
Eval num_timesteps=4918000, episode_reward=2795.87 +/- 32.13
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4918000  |
| train/             |          |
|    actor_loss      | -0.611   |
|    critic_loss     | 0.000406 |
|    ent_coef        | 0.000432 |
|    ent_coef_loss   | 0.733    |
|    learning_rate   | 8.28e-05 |
|    n_updates       | 24010    |
---------------------------------
Eval num_timesteps=4919000, episode_reward=2825.86 +/- 41.32
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4919000  |
---------------------------------
Eval num_timesteps=4920000, episode_reward=2737.45 +/- 40.49
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.74e+03 |
| time/              |          |
|    total_timesteps | 4920000  |
| train/             |          |
|    actor_loss      | -0.599   |
|    critic_loss     | 0.000437 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.2      |
|    learning_rate   | 8.07e-05 |
|    n_updates       | 24020    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.4e+03  |
| time/              |          |
|    episodes        | 4920     |
|    fps             | 638      |
|    time_elapsed    | 7705     |
|    total_timesteps | 4920000  |
---------------------------------
Eval num_timesteps=4921000, episode_reward=2778.01 +/- 10.42
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4921000  |
---------------------------------
Eval num_timesteps=4922000, episode_reward=2778.34 +/- 30.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4922000  |
| train/             |          |
|    actor_loss      | -0.604   |
|    critic_loss     | 0.000395 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.667    |
|    learning_rate   | 7.87e-05 |
|    n_updates       | 24030    |
---------------------------------
Eval num_timesteps=4923000, episode_reward=2808.64 +/- 52.64
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4923000  |
---------------------------------
Eval num_timesteps=4924000, episode_reward=2821.91 +/- 41.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.82e+03 |
| time/              |          |
|    total_timesteps | 4924000  |
| train/             |          |
|    actor_loss      | -0.607   |
|    critic_loss     | 0.000409 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.62     |
|    learning_rate   | 7.66e-05 |
|    n_updates       | 24040    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 4924     |
|    fps             | 638      |
|    time_elapsed    | 7711     |
|    total_timesteps | 4924000  |
---------------------------------
Eval num_timesteps=4925000, episode_reward=2781.82 +/- 19.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4925000  |
---------------------------------
Eval num_timesteps=4926000, episode_reward=2840.58 +/- 60.66
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4926000  |
| train/             |          |
|    actor_loss      | -0.607   |
|    critic_loss     | 0.000392 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.43     |
|    learning_rate   | 7.46e-05 |
|    n_updates       | 24050    |
---------------------------------
Eval num_timesteps=4927000, episode_reward=2809.63 +/- 47.23
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.81e+03 |
| time/              |          |
|    total_timesteps | 4927000  |
---------------------------------
Eval num_timesteps=4928000, episode_reward=2783.65 +/- 40.09
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.78e+03 |
| time/              |          |
|    total_timesteps | 4928000  |
| train/             |          |
|    actor_loss      | -0.61    |
|    critic_loss     | 0.000413 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.44     |
|    learning_rate   | 7.25e-05 |
|    n_updates       | 24060    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.45e+03 |
| time/              |          |
|    episodes        | 4928     |
|    fps             | 638      |
|    time_elapsed    | 7718     |
|    total_timesteps | 4928000  |
---------------------------------
Eval num_timesteps=4929000, episode_reward=2797.04 +/- 23.51
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.8e+03  |
| time/              |          |
|    total_timesteps | 4929000  |
---------------------------------
Eval num_timesteps=4930000, episode_reward=2832.88 +/- 36.88
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.83e+03 |
| time/              |          |
|    total_timesteps | 4930000  |
| train/             |          |
|    actor_loss      | -0.608   |
|    critic_loss     | 0.000416 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.17     |
|    learning_rate   | 7.05e-05 |
|    n_updates       | 24070    |
---------------------------------
Eval num_timesteps=4931000, episode_reward=2762.87 +/- 52.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.76e+03 |
| time/              |          |
|    total_timesteps | 4931000  |
---------------------------------
Eval num_timesteps=4932000, episode_reward=2853.70 +/- 30.85
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4932000  |
| train/             |          |
|    actor_loss      | -0.605   |
|    critic_loss     | 0.000412 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.22     |
|    learning_rate   | 6.84e-05 |
|    n_updates       | 24080    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.48e+03 |
| time/              |          |
|    episodes        | 4932     |
|    fps             | 638      |
|    time_elapsed    | 7724     |
|    total_timesteps | 4932000  |
---------------------------------
Eval num_timesteps=4933000, episode_reward=2838.77 +/- 32.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.84e+03 |
| time/              |          |
|    total_timesteps | 4933000  |
---------------------------------
Eval num_timesteps=4934000, episode_reward=2863.53 +/- 33.77
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4934000  |
| train/             |          |
|    actor_loss      | -0.6     |
|    critic_loss     | 0.000387 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.468    |
|    learning_rate   | 6.64e-05 |
|    n_updates       | 24090    |
---------------------------------
Eval num_timesteps=4935000, episode_reward=2856.33 +/- 24.83
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4935000  |
---------------------------------
Eval num_timesteps=4936000, episode_reward=2910.79 +/- 35.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4936000  |
| train/             |          |
|    actor_loss      | -0.603   |
|    critic_loss     | 0.000426 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.991    |
|    learning_rate   | 6.43e-05 |
|    n_updates       | 24100    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.5e+03  |
| time/              |          |
|    episodes        | 4936     |
|    fps             | 638      |
|    time_elapsed    | 7730     |
|    total_timesteps | 4936000  |
---------------------------------
Eval num_timesteps=4937000, episode_reward=2877.04 +/- 14.21
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4937000  |
---------------------------------
Eval num_timesteps=4938000, episode_reward=2908.01 +/- 44.11
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4938000  |
| train/             |          |
|    actor_loss      | -0.606   |
|    critic_loss     | 0.000409 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.25     |
|    learning_rate   | 6.23e-05 |
|    n_updates       | 24110    |
---------------------------------
Eval num_timesteps=4939000, episode_reward=2899.17 +/- 30.99
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4939000  |
---------------------------------
Eval num_timesteps=4940000, episode_reward=2860.48 +/- 33.86
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4940000  |
| train/             |          |
|    actor_loss      | -0.611   |
|    critic_loss     | 0.000406 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.96     |
|    learning_rate   | 6.02e-05 |
|    n_updates       | 24120    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 4940     |
|    fps             | 638      |
|    time_elapsed    | 7736     |
|    total_timesteps | 4940000  |
---------------------------------
Eval num_timesteps=4941000, episode_reward=2869.16 +/- 41.94
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4941000  |
---------------------------------
Eval num_timesteps=4942000, episode_reward=2898.71 +/- 36.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4942000  |
| train/             |          |
|    actor_loss      | -0.612   |
|    critic_loss     | 0.000423 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 2.31     |
|    learning_rate   | 5.82e-05 |
|    n_updates       | 24130    |
---------------------------------
Eval num_timesteps=4943000, episode_reward=2875.51 +/- 43.65
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4943000  |
---------------------------------
Eval num_timesteps=4944000, episode_reward=2857.63 +/- 56.27
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4944000  |
| train/             |          |
|    actor_loss      | -0.606   |
|    critic_loss     | 0.000402 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 2.32     |
|    learning_rate   | 5.61e-05 |
|    n_updates       | 24140    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.56e+03 |
| time/              |          |
|    episodes        | 4944     |
|    fps             | 638      |
|    time_elapsed    | 7743     |
|    total_timesteps | 4944000  |
---------------------------------
Eval num_timesteps=4945000, episode_reward=2875.90 +/- 48.80
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4945000  |
---------------------------------
Eval num_timesteps=4946000, episode_reward=2873.43 +/- 46.05
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4946000  |
| train/             |          |
|    actor_loss      | -0.62    |
|    critic_loss     | 0.000438 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 3.44     |
|    learning_rate   | 5.41e-05 |
|    n_updates       | 24150    |
---------------------------------
Eval num_timesteps=4947000, episode_reward=2868.71 +/- 35.71
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4947000  |
---------------------------------
Eval num_timesteps=4948000, episode_reward=2883.25 +/- 27.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4948000  |
| train/             |          |
|    actor_loss      | -0.611   |
|    critic_loss     | 0.000451 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 3.22     |
|    learning_rate   | 5.2e-05  |
|    n_updates       | 24160    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 4948     |
|    fps             | 638      |
|    time_elapsed    | 7749     |
|    total_timesteps | 4948000  |
---------------------------------
Eval num_timesteps=4949000, episode_reward=2244.06 +/- 1267.01
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.24e+03 |
| time/              |          |
|    total_timesteps | 4949000  |
---------------------------------
Eval num_timesteps=4950000, episode_reward=2878.53 +/- 52.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4950000  |
---------------------------------
Eval num_timesteps=4951000, episode_reward=2887.00 +/- 28.46
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4951000  |
| train/             |          |
|    actor_loss      | -0.605   |
|    critic_loss     | 0.00045  |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 2.73     |
|    learning_rate   | 5e-05    |
|    n_updates       | 24170    |
---------------------------------
Eval num_timesteps=4952000, episode_reward=2903.28 +/- 42.38
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4952000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.57e+03 |
| time/              |          |
|    episodes        | 4952     |
|    fps             | 638      |
|    time_elapsed    | 7755     |
|    total_timesteps | 4952000  |
---------------------------------
Eval num_timesteps=4953000, episode_reward=2915.72 +/- 25.43
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4953000  |
| train/             |          |
|    actor_loss      | -0.603   |
|    critic_loss     | 0.00047  |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.96     |
|    learning_rate   | 4.79e-05 |
|    n_updates       | 24180    |
---------------------------------
Eval num_timesteps=4954000, episode_reward=2906.47 +/- 56.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4954000  |
---------------------------------
Eval num_timesteps=4955000, episode_reward=2865.08 +/- 52.84
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4955000  |
| train/             |          |
|    actor_loss      | -0.601   |
|    critic_loss     | 0.000444 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.36     |
|    learning_rate   | 4.59e-05 |
|    n_updates       | 24190    |
---------------------------------
Eval num_timesteps=4956000, episode_reward=2889.97 +/- 55.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4956000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 4956     |
|    fps             | 638      |
|    time_elapsed    | 7762     |
|    total_timesteps | 4956000  |
---------------------------------
Eval num_timesteps=4957000, episode_reward=2905.30 +/- 29.54
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.91e+03 |
| time/              |          |
|    total_timesteps | 4957000  |
| train/             |          |
|    actor_loss      | -0.606   |
|    critic_loss     | 0.000446 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | 2.13     |
|    learning_rate   | 4.38e-05 |
|    n_updates       | 24200    |
---------------------------------
Eval num_timesteps=4958000, episode_reward=2853.19 +/- 53.68
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.85e+03 |
| time/              |          |
|    total_timesteps | 4958000  |
---------------------------------
Eval num_timesteps=4959000, episode_reward=2880.76 +/- 38.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.88e+03 |
| time/              |          |
|    total_timesteps | 4959000  |
| train/             |          |
|    actor_loss      | -0.607   |
|    critic_loss     | 0.00043  |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | 2.22     |
|    learning_rate   | 4.18e-05 |
|    n_updates       | 24210    |
---------------------------------
Eval num_timesteps=4960000, episode_reward=2894.61 +/- 33.91
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4960000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 4960     |
|    fps             | 638      |
|    time_elapsed    | 7768     |
|    total_timesteps | 4960000  |
---------------------------------
Eval num_timesteps=4961000, episode_reward=2861.91 +/- 54.67
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.86e+03 |
| time/              |          |
|    total_timesteps | 4961000  |
| train/             |          |
|    actor_loss      | -0.601   |
|    critic_loss     | 0.000412 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | 0.729    |
|    learning_rate   | 3.97e-05 |
|    n_updates       | 24220    |
---------------------------------
Eval num_timesteps=4962000, episode_reward=2893.34 +/- 65.30
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4962000  |
---------------------------------
Eval num_timesteps=4963000, episode_reward=2901.37 +/- 79.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4963000  |
| train/             |          |
|    actor_loss      | -0.435   |
|    critic_loss     | 0.000602 |
|    ent_coef        | 0.000434 |
|    ent_coef_loss   | -24      |
|    learning_rate   | 3.77e-05 |
|    n_updates       | 24230    |
---------------------------------
Eval num_timesteps=4964000, episode_reward=2871.30 +/- 67.74
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.87e+03 |
| time/              |          |
|    total_timesteps | 4964000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 4964     |
|    fps             | 638      |
|    time_elapsed    | 7774     |
|    total_timesteps | 4964000  |
---------------------------------
Eval num_timesteps=4965000, episode_reward=2960.43 +/- 27.18
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4965000  |
| train/             |          |
|    actor_loss      | -0.605   |
|    critic_loss     | 0.000434 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.544    |
|    learning_rate   | 3.56e-05 |
|    n_updates       | 24240    |
---------------------------------
Eval num_timesteps=4966000, episode_reward=2898.50 +/- 44.25
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4966000  |
---------------------------------
Eval num_timesteps=4967000, episode_reward=2922.73 +/- 43.00
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4967000  |
| train/             |          |
|    actor_loss      | -0.607   |
|    critic_loss     | 0.000439 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.37     |
|    learning_rate   | 3.36e-05 |
|    n_updates       | 24250    |
---------------------------------
Eval num_timesteps=4968000, episode_reward=2923.57 +/- 9.20
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4968000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 4968     |
|    fps             | 638      |
|    time_elapsed    | 7780     |
|    total_timesteps | 4968000  |
---------------------------------
Eval num_timesteps=4969000, episode_reward=2973.58 +/- 67.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4969000  |
| train/             |          |
|    actor_loss      | -0.601   |
|    critic_loss     | 0.00049  |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 2.54     |
|    learning_rate   | 3.16e-05 |
|    n_updates       | 24260    |
---------------------------------
Eval num_timesteps=4970000, episode_reward=2943.06 +/- 39.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4970000  |
---------------------------------
Eval num_timesteps=4971000, episode_reward=2886.21 +/- 24.55
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.89e+03 |
| time/              |          |
|    total_timesteps | 4971000  |
| train/             |          |
|    actor_loss      | -0.605   |
|    critic_loss     | 0.000412 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.908    |
|    learning_rate   | 2.95e-05 |
|    n_updates       | 24270    |
---------------------------------
Eval num_timesteps=4972000, episode_reward=2943.18 +/- 57.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4972000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.62e+03 |
| time/              |          |
|    episodes        | 4972     |
|    fps             | 638      |
|    time_elapsed    | 7787     |
|    total_timesteps | 4972000  |
---------------------------------
Eval num_timesteps=4973000, episode_reward=2933.80 +/- 36.02
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4973000  |
| train/             |          |
|    actor_loss      | -0.606   |
|    critic_loss     | 0.000437 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 2.84     |
|    learning_rate   | 2.75e-05 |
|    n_updates       | 24280    |
---------------------------------
Eval num_timesteps=4974000, episode_reward=2958.24 +/- 16.44
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4974000  |
---------------------------------
Eval num_timesteps=4975000, episode_reward=2966.42 +/- 78.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4975000  |
| train/             |          |
|    actor_loss      | -0.603   |
|    critic_loss     | 0.00045  |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 2.74     |
|    learning_rate   | 2.54e-05 |
|    n_updates       | 24290    |
---------------------------------
Eval num_timesteps=4976000, episode_reward=2920.62 +/- 34.40
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4976000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 4976     |
|    fps             | 638      |
|    time_elapsed    | 7793     |
|    total_timesteps | 4976000  |
---------------------------------
Eval num_timesteps=4977000, episode_reward=3043.51 +/- 69.14
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.04e+03 |
| time/              |          |
|    total_timesteps | 4977000  |
| train/             |          |
|    actor_loss      | -0.604   |
|    critic_loss     | 0.000419 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.69     |
|    learning_rate   | 2.34e-05 |
|    n_updates       | 24300    |
---------------------------------
Eval num_timesteps=4978000, episode_reward=2950.68 +/- 47.06
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4978000  |
---------------------------------
Eval num_timesteps=4979000, episode_reward=2951.91 +/- 32.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4979000  |
| train/             |          |
|    actor_loss      | -0.605   |
|    critic_loss     | 0.000447 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.08     |
|    learning_rate   | 2.13e-05 |
|    n_updates       | 24310    |
---------------------------------
Eval num_timesteps=4980000, episode_reward=2970.53 +/- 85.53
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4980000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.63e+03 |
| time/              |          |
|    episodes        | 4980     |
|    fps             | 638      |
|    time_elapsed    | 7799     |
|    total_timesteps | 4980000  |
---------------------------------
Eval num_timesteps=4981000, episode_reward=2921.95 +/- 53.50
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4981000  |
| train/             |          |
|    actor_loss      | -0.6     |
|    critic_loss     | 0.000414 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.68     |
|    learning_rate   | 1.93e-05 |
|    n_updates       | 24320    |
---------------------------------
Eval num_timesteps=4982000, episode_reward=2895.21 +/- 57.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4982000  |
---------------------------------
Eval num_timesteps=4983000, episode_reward=2927.06 +/- 49.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4983000  |
| train/             |          |
|    actor_loss      | -0.403   |
|    critic_loss     | 0.000894 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | -19.4    |
|    learning_rate   | 1.72e-05 |
|    n_updates       | 24330    |
---------------------------------
Eval num_timesteps=4984000, episode_reward=2959.56 +/- 65.93
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.96e+03 |
| time/              |          |
|    total_timesteps | 4984000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.61e+03 |
| time/              |          |
|    episodes        | 4984     |
|    fps             | 638      |
|    time_elapsed    | 7806     |
|    total_timesteps | 4984000  |
---------------------------------
Eval num_timesteps=4985000, episode_reward=2949.44 +/- 22.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.95e+03 |
| time/              |          |
|    total_timesteps | 4985000  |
| train/             |          |
|    actor_loss      | -0.606   |
|    critic_loss     | 0.000459 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 3.3      |
|    learning_rate   | 1.52e-05 |
|    n_updates       | 24340    |
---------------------------------
Eval num_timesteps=4986000, episode_reward=2941.57 +/- 32.73
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4986000  |
---------------------------------
Eval num_timesteps=4987000, episode_reward=2901.05 +/- 51.22
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4987000  |
| train/             |          |
|    actor_loss      | -0.595   |
|    critic_loss     | 0.000441 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.11     |
|    learning_rate   | 1.31e-05 |
|    n_updates       | 24350    |
---------------------------------
Eval num_timesteps=4988000, episode_reward=2970.88 +/- 47.75
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4988000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 4988     |
|    fps             | 638      |
|    time_elapsed    | 7812     |
|    total_timesteps | 4988000  |
---------------------------------
Eval num_timesteps=4989000, episode_reward=2928.33 +/- 20.52
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4989000  |
| train/             |          |
|    actor_loss      | -0.592   |
|    critic_loss     | 0.000443 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 0.877    |
|    learning_rate   | 1.11e-05 |
|    n_updates       | 24360    |
---------------------------------
Eval num_timesteps=4990000, episode_reward=2972.28 +/- 69.39
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 4990000  |
---------------------------------
Eval num_timesteps=4991000, episode_reward=2926.73 +/- 31.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4991000  |
| train/             |          |
|    actor_loss      | -0.601   |
|    critic_loss     | 0.000446 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.8      |
|    learning_rate   | 9.02e-06 |
|    n_updates       | 24370    |
---------------------------------
Eval num_timesteps=4992000, episode_reward=3011.13 +/- 59.95
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4992000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.64e+03 |
| time/              |          |
|    episodes        | 4992     |
|    fps             | 638      |
|    time_elapsed    | 7818     |
|    total_timesteps | 4992000  |
---------------------------------
Eval num_timesteps=4993000, episode_reward=3008.38 +/- 58.41
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3.01e+03 |
| time/              |          |
|    total_timesteps | 4993000  |
---------------------------------
Eval num_timesteps=4994000, episode_reward=2929.01 +/- 52.08
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.93e+03 |
| time/              |          |
|    total_timesteps | 4994000  |
| train/             |          |
|    actor_loss      | -0.599   |
|    critic_loss     | 0.00041  |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 2.27     |
|    learning_rate   | 6.98e-06 |
|    n_updates       | 24380    |
---------------------------------
Eval num_timesteps=4995000, episode_reward=2944.43 +/- 62.81
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 4995000  |
---------------------------------
Eval num_timesteps=4996000, episode_reward=2898.32 +/- 77.10
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.9e+03  |
| time/              |          |
|    total_timesteps | 4996000  |
| train/             |          |
|    actor_loss      | -0.608   |
|    critic_loss     | 0.000449 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 3.09     |
|    learning_rate   | 4.93e-06 |
|    n_updates       | 24390    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 4996     |
|    fps             | 638      |
|    time_elapsed    | 7824     |
|    total_timesteps | 4996000  |
---------------------------------
Eval num_timesteps=4997000, episode_reward=2919.70 +/- 67.61
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.92e+03 |
| time/              |          |
|    total_timesteps | 4997000  |
---------------------------------
Eval num_timesteps=4998000, episode_reward=2997.05 +/- 80.47
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 3e+03    |
| time/              |          |
|    total_timesteps | 4998000  |
| train/             |          |
|    actor_loss      | -0.597   |
|    critic_loss     | 0.000409 |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.1      |
|    learning_rate   | 2.88e-06 |
|    n_updates       | 24400    |
---------------------------------
Eval num_timesteps=4999000, episode_reward=2978.96 +/- 74.29
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.98e+03 |
| time/              |          |
|    total_timesteps | 4999000  |
---------------------------------
Eval num_timesteps=5000000, episode_reward=2970.86 +/- 72.45
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.97e+03 |
| time/              |          |
|    total_timesteps | 5000000  |
| train/             |          |
|    actor_loss      | -0.599   |
|    critic_loss     | 0.00045  |
|    ent_coef        | 0.000433 |
|    ent_coef_loss   | 1.75     |
|    learning_rate   | 8.32e-07 |
|    n_updates       | 24410    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 1e+03    |
|    ep_rew_mean     | 2.65e+03 |
| time/              |          |
|    episodes        | 5000     |
|    fps             | 638      |
|    time_elapsed    | 7831     |
|    total_timesteps | 5000000  |
---------------------------------
Eval num_timesteps=5001000, episode_reward=2935.33 +/- 18.89
Episode length: 1000.00 +/- 0.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 1e+03    |
|    mean_reward     | 2.94e+03 |
| time/              |          |
|    total_timesteps | 5001000  |
---------------------------------
