{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/miniforge3/envs/irl/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Hopper-v4\n",
    "env_id = 'Hopper-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "sac_model = SAC(\"MlpPolicy\", env, verbose=10)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1e5, save_path=f'../logs/{env_id}')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'./logs/{env_id}/best_model',\n",
    "                             log_path=f'./logs/{env_id}/results', eval_freq=10000)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.3     |\n",
      "|    ep_rew_mean     | 15       |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 134      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 223      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.38    |\n",
      "|    critic_loss     | 0.387    |\n",
      "|    ent_coef        | 0.964    |\n",
      "|    ent_coef_loss   | -0.183   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 122      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | 16.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 134      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 419      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.1     |\n",
      "|    critic_loss     | 0.399    |\n",
      "|    ent_coef        | 0.909    |\n",
      "|    ent_coef_loss   | -0.479   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 318      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.3     |\n",
      "|    ep_rew_mean     | 25.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 758      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.3    |\n",
      "|    critic_loss     | 0.782    |\n",
      "|    ent_coef        | 0.822    |\n",
      "|    ent_coef_loss   | -0.969   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 657      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 31.4     |\n",
      "|    ep_rew_mean     | 42.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 1254     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.8    |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.712    |\n",
      "|    ent_coef_loss   | -1.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1153     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 42.9     |\n",
      "|    ep_rew_mean     | 71.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2143     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.4    |\n",
      "|    critic_loss     | 2.89     |\n",
      "|    ent_coef        | 0.557    |\n",
      "|    ent_coef_loss   | -2.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2042     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50       |\n",
      "|    ep_rew_mean     | 90.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 2997     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.4    |\n",
      "|    critic_loss     | 5.99     |\n",
      "|    ent_coef        | 0.443    |\n",
      "|    ent_coef_loss   | -3       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2896     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 55.3     |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    episodes        | 70       |\n",
      "|    fps             | 138      |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 3874     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.4    |\n",
      "|    critic_loss     | 4.79     |\n",
      "|    ent_coef        | 0.349    |\n",
      "|    ent_coef_loss   | -3.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3773     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | 118      |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 138      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 4803     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.8    |\n",
      "|    critic_loss     | 3.5      |\n",
      "|    ent_coef        | 0.271    |\n",
      "|    ent_coef_loss   | -4.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4702     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.5     |\n",
      "|    ep_rew_mean     | 131      |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 41       |\n",
      "|    total_timesteps | 5808     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.6    |\n",
      "|    critic_loss     | 8.96     |\n",
      "|    ent_coef        | 0.209    |\n",
      "|    ent_coef_loss   | -4.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5707     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 68.1     |\n",
      "|    ep_rew_mean     | 141      |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 6812     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69.3    |\n",
      "|    critic_loss     | 3.86     |\n",
      "|    ent_coef        | 0.161    |\n",
      "|    ent_coef_loss   | -3.79    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6711     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.6     |\n",
      "|    ep_rew_mean     | 162      |\n",
      "| time/              |          |\n",
      "|    episodes        | 110      |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 7779     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.5    |\n",
      "|    critic_loss     | 5.43     |\n",
      "|    ent_coef        | 0.128    |\n",
      "|    ent_coef_loss   | -4.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7678     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.2     |\n",
      "|    ep_rew_mean     | 183      |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 62       |\n",
      "|    total_timesteps | 8744     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -82.8    |\n",
      "|    critic_loss     | 5.2      |\n",
      "|    ent_coef        | 0.104    |\n",
      "|    ent_coef_loss   | -2.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8643     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90       |\n",
      "|    ep_rew_mean     | 202      |\n",
      "| time/              |          |\n",
      "|    episodes        | 130      |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 9757     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -91.7    |\n",
      "|    critic_loss     | 9.12     |\n",
      "|    ent_coef        | 0.0867   |\n",
      "|    ent_coef_loss   | -1.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9656     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=261.10 +/- 5.52\n",
      "Episode length: 108.40 +/- 1.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 108      |\n",
      "|    mean_reward     | 261      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -89.8    |\n",
      "|    critic_loss     | 5.17     |\n",
      "|    ent_coef        | 0.0828   |\n",
      "|    ent_coef_loss   | -2.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.6     |\n",
      "|    ep_rew_mean     | 218      |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 137      |\n",
      "|    time_elapsed    | 78       |\n",
      "|    total_timesteps | 10816    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -92.6    |\n",
      "|    critic_loss     | 3.11     |\n",
      "|    ent_coef        | 0.0716   |\n",
      "|    ent_coef_loss   | -0.773   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10715    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.6     |\n",
      "|    ep_rew_mean     | 225      |\n",
      "| time/              |          |\n",
      "|    episodes        | 150      |\n",
      "|    fps             | 135      |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 11900    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -101     |\n",
      "|    critic_loss     | 3.35     |\n",
      "|    ent_coef        | 0.0655   |\n",
      "|    ent_coef_loss   | -0.988   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11799    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.6     |\n",
      "|    ep_rew_mean     | 232      |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 135      |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 12955    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -100     |\n",
      "|    critic_loss     | 3.09     |\n",
      "|    ent_coef        | 0.0585   |\n",
      "|    ent_coef_loss   | -0.657   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12854    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 238      |\n",
      "| time/              |          |\n",
      "|    episodes        | 170      |\n",
      "|    fps             | 135      |\n",
      "|    time_elapsed    | 103      |\n",
      "|    total_timesteps | 14042    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -99.5    |\n",
      "|    critic_loss     | 2.96     |\n",
      "|    ent_coef        | 0.0577   |\n",
      "|    ent_coef_loss   | -0.182   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13941    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 103      |\n",
      "|    ep_rew_mean     | 243      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 110      |\n",
      "|    total_timesteps | 15099    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -106     |\n",
      "|    critic_loss     | 2.86     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | -0.438   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14998    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 103      |\n",
      "|    ep_rew_mean     | 241      |\n",
      "| time/              |          |\n",
      "|    episodes        | 190      |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 16065    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -107     |\n",
      "|    critic_loss     | 3.17     |\n",
      "|    ent_coef        | 0.0571   |\n",
      "|    ent_coef_loss   | -0.311   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15964    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 236      |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 137      |\n",
      "|    time_elapsed    | 123      |\n",
      "|    total_timesteps | 16966    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -104     |\n",
      "|    critic_loss     | 3.78     |\n",
      "|    ent_coef        | 0.062    |\n",
      "|    ent_coef_loss   | -0.448   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16865    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 236      |\n",
      "| time/              |          |\n",
      "|    episodes        | 210      |\n",
      "|    fps             | 138      |\n",
      "|    time_elapsed    | 129      |\n",
      "|    total_timesteps | 17968    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -106     |\n",
      "|    critic_loss     | 4.08     |\n",
      "|    ent_coef        | 0.0594   |\n",
      "|    ent_coef_loss   | 0.049    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17867    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | 243      |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 137      |\n",
      "|    total_timesteps | 19157    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -104     |\n",
      "|    critic_loss     | 2.98     |\n",
      "|    ent_coef        | 0.0544   |\n",
      "|    ent_coef_loss   | -0.0609  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19056    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=324.69 +/- 5.29\n",
      "Episode length: 128.00 +/- 1.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 128      |\n",
      "|    mean_reward     | 325      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 3.92     |\n",
      "|    ent_coef        | 0.0543   |\n",
      "|    ent_coef_loss   | -0.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | 250      |\n",
      "| time/              |          |\n",
      "|    episodes        | 230      |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 149      |\n",
      "|    total_timesteps | 20455    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -105     |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.0549   |\n",
      "|    ent_coef_loss   | 0.769    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20354    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 253      |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 165      |\n",
      "|    total_timesteps | 21607    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -110     |\n",
      "|    critic_loss     | 2.91     |\n",
      "|    ent_coef        | 0.0648   |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21506    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | 256      |\n",
      "| time/              |          |\n",
      "|    episodes        | 250      |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 22795    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -113     |\n",
      "|    critic_loss     | 2.84     |\n",
      "|    ent_coef        | 0.0709   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22694    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | 259      |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 184      |\n",
      "|    total_timesteps | 23951    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -115     |\n",
      "|    critic_loss     | 3.74     |\n",
      "|    ent_coef        | 0.0696   |\n",
      "|    ent_coef_loss   | -0.158   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23850    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 112      |\n",
      "|    ep_rew_mean     | 264      |\n",
      "| time/              |          |\n",
      "|    episodes        | 270      |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 195      |\n",
      "|    total_timesteps | 25246    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -109     |\n",
      "|    critic_loss     | 3.7      |\n",
      "|    ent_coef        | 0.0605   |\n",
      "|    ent_coef_loss   | -0.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25145    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | 268      |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 128      |\n",
      "|    time_elapsed    | 206      |\n",
      "|    total_timesteps | 26453    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -119     |\n",
      "|    critic_loss     | 3.46     |\n",
      "|    ent_coef        | 0.0657   |\n",
      "|    ent_coef_loss   | 0.0838   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26352    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 115      |\n",
      "|    ep_rew_mean     | 273      |\n",
      "| time/              |          |\n",
      "|    episodes        | 290      |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 27577    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -117     |\n",
      "|    critic_loss     | 5        |\n",
      "|    ent_coef        | 0.0724   |\n",
      "|    ent_coef_loss   | -0.561   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27476    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | 287      |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 229      |\n",
      "|    total_timesteps | 29149    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -125     |\n",
      "|    critic_loss     | 58.1     |\n",
      "|    ent_coef        | 0.0787   |\n",
      "|    ent_coef_loss   | 0.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29048    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=384.74 +/- 16.40\n",
      "Episode length: 198.60 +/- 14.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 199      |\n",
      "|    mean_reward     | 385      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -128     |\n",
      "|    critic_loss     | 8.92     |\n",
      "|    ent_coef        | 0.0758   |\n",
      "|    ent_coef_loss   | -0.292   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | 301      |\n",
      "| time/              |          |\n",
      "|    episodes        | 310      |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 245      |\n",
      "|    total_timesteps | 30977    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -127     |\n",
      "|    critic_loss     | 5.66     |\n",
      "|    ent_coef        | 0.0775   |\n",
      "|    ent_coef_loss   | 0.0602   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30876    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 136      |\n",
      "|    ep_rew_mean     | 310      |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 125      |\n",
      "|    time_elapsed    | 260      |\n",
      "|    total_timesteps | 32793    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -127     |\n",
      "|    critic_loss     | 5.45     |\n",
      "|    ent_coef        | 0.0923   |\n",
      "|    ent_coef_loss   | 0.0039   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32692    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 140      |\n",
      "|    ep_rew_mean     | 312      |\n",
      "| time/              |          |\n",
      "|    episodes        | 330      |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 272      |\n",
      "|    total_timesteps | 34445    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -132     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0901   |\n",
      "|    ent_coef_loss   | -0.511   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34344    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | 317      |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 281      |\n",
      "|    total_timesteps | 35817    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -131     |\n",
      "|    critic_loss     | 7.58     |\n",
      "|    ent_coef        | 0.0879   |\n",
      "|    ent_coef_loss   | -0.496   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35716    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 145      |\n",
      "|    ep_rew_mean     | 322      |\n",
      "| time/              |          |\n",
      "|    episodes        | 350      |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 37276    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -139     |\n",
      "|    critic_loss     | 7.12     |\n",
      "|    ent_coef        | 0.0818   |\n",
      "|    ent_coef_loss   | 0.201    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 37175    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 149      |\n",
      "|    ep_rew_mean     | 338      |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 128      |\n",
      "|    time_elapsed    | 302      |\n",
      "|    total_timesteps | 38864    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0779   |\n",
      "|    ent_coef_loss   | -0.113   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38763    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 145      |\n",
      "|    ep_rew_mean     | 328      |\n",
      "| time/              |          |\n",
      "|    episodes        | 370      |\n",
      "|    fps             | 128      |\n",
      "|    time_elapsed    | 308      |\n",
      "|    total_timesteps | 39780    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 5.56     |\n",
      "|    ent_coef        | 0.0753   |\n",
      "|    ent_coef_loss   | -0.112   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39679    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=331.02 +/- 1.62\n",
      "Episode length: 128.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 129      |\n",
      "|    mean_reward     | 331      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.0749   |\n",
      "|    ent_coef_loss   | 0.354    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 147      |\n",
      "|    ep_rew_mean     | 334      |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 317      |\n",
      "|    total_timesteps | 41144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -139     |\n",
      "|    critic_loss     | 8.86     |\n",
      "|    ent_coef        | 0.081    |\n",
      "|    ent_coef_loss   | 0.228    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 41043    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 153      |\n",
      "|    ep_rew_mean     | 355      |\n",
      "| time/              |          |\n",
      "|    episodes        | 390      |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 328      |\n",
      "|    total_timesteps | 42915    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 7.13     |\n",
      "|    ent_coef        | 0.0943   |\n",
      "|    ent_coef_loss   | -0.0864  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42814    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 163      |\n",
      "|    ep_rew_mean     | 394      |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 132      |\n",
      "|    time_elapsed    | 344      |\n",
      "|    total_timesteps | 45464    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -150     |\n",
      "|    critic_loss     | 8.48     |\n",
      "|    ent_coef        | 0.101    |\n",
      "|    ent_coef_loss   | 0.462    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 45363    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 163      |\n",
      "|    ep_rew_mean     | 412      |\n",
      "| time/              |          |\n",
      "|    episodes        | 410      |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 358      |\n",
      "|    total_timesteps | 47276    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -154     |\n",
      "|    critic_loss     | 7.25     |\n",
      "|    ent_coef        | 0.104    |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 47175    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 164      |\n",
      "|    ep_rew_mean     | 434      |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 374      |\n",
      "|    total_timesteps | 49231    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -153     |\n",
      "|    critic_loss     | 7.22     |\n",
      "|    ent_coef        | 0.104    |\n",
      "|    ent_coef_loss   | -0.165   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49130    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=693.45 +/- 357.19\n",
      "Episode length: 236.80 +/- 94.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 237      |\n",
      "|    mean_reward     | 693      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -156     |\n",
      "|    critic_loss     | 18.8     |\n",
      "|    ent_coef        | 0.109    |\n",
      "|    ent_coef_loss   | -0.131   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 171      |\n",
      "|    ep_rew_mean     | 468      |\n",
      "| time/              |          |\n",
      "|    episodes        | 430      |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 396      |\n",
      "|    total_timesteps | 51505    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 8.45     |\n",
      "|    ent_coef        | 0.107    |\n",
      "|    ent_coef_loss   | 0.197    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 51404    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 178      |\n",
      "|    ep_rew_mean     | 503      |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 413      |\n",
      "|    total_timesteps | 53643    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -154     |\n",
      "|    critic_loss     | 7.16     |\n",
      "|    ent_coef        | 0.109    |\n",
      "|    ent_coef_loss   | 0.108    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 53542    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 186      |\n",
      "|    ep_rew_mean     | 542      |\n",
      "| time/              |          |\n",
      "|    episodes        | 450      |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 433      |\n",
      "|    total_timesteps | 55911    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 6.63     |\n",
      "|    ent_coef        | 0.109    |\n",
      "|    ent_coef_loss   | -0.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 55810    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 193      |\n",
      "|    ep_rew_mean     | 570      |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 128      |\n",
      "|    time_elapsed    | 451      |\n",
      "|    total_timesteps | 58160    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 7.46     |\n",
      "|    ent_coef        | 0.111    |\n",
      "|    ent_coef_loss   | -0.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 58059    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=812.27 +/- 22.65\n",
      "Episode length: 249.20 +/- 6.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 249      |\n",
      "|    mean_reward     | 812      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -175     |\n",
      "|    critic_loss     | 7.33     |\n",
      "|    ent_coef        | 0.109    |\n",
      "|    ent_coef_loss   | -0.296   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 208      |\n",
      "|    ep_rew_mean     | 628      |\n",
      "| time/              |          |\n",
      "|    episodes        | 470      |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 474      |\n",
      "|    total_timesteps | 60616    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -173     |\n",
      "|    critic_loss     | 8.7      |\n",
      "|    ent_coef        | 0.108    |\n",
      "|    ent_coef_loss   | -0.482   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 60515    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | 668      |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 497      |\n",
      "|    total_timesteps | 63028    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -177     |\n",
      "|    critic_loss     | 99.2     |\n",
      "|    ent_coef        | 0.104    |\n",
      "|    ent_coef_loss   | 0.676    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 62927    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 232      |\n",
      "|    ep_rew_mean     | 716      |\n",
      "| time/              |          |\n",
      "|    episodes        | 490      |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 522      |\n",
      "|    total_timesteps | 66144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 8.27     |\n",
      "|    ent_coef        | 0.104    |\n",
      "|    ent_coef_loss   | -0.0168  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 66043    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 237      |\n",
      "|    ep_rew_mean     | 741      |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 548      |\n",
      "|    total_timesteps | 69210    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -186     |\n",
      "|    critic_loss     | 93.1     |\n",
      "|    ent_coef        | 0.0994   |\n",
      "|    ent_coef_loss   | -0.868   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69109    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=1178.25 +/- 218.49\n",
      "Episode length: 375.40 +/- 64.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 375      |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -186     |\n",
      "|    critic_loss     | 8.64     |\n",
      "|    ent_coef        | 0.0997   |\n",
      "|    ent_coef_loss   | 0.106    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 252      |\n",
      "|    ep_rew_mean     | 793      |\n",
      "| time/              |          |\n",
      "|    episodes        | 510      |\n",
      "|    fps             | 125      |\n",
      "|    time_elapsed    | 579      |\n",
      "|    total_timesteps | 72503    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -196     |\n",
      "|    critic_loss     | 8.02     |\n",
      "|    ent_coef        | 0.0977   |\n",
      "|    ent_coef_loss   | 0.254    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 72402    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 262      |\n",
      "|    ep_rew_mean     | 829      |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 124      |\n",
      "|    time_elapsed    | 603      |\n",
      "|    total_timesteps | 75460    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -188     |\n",
      "|    critic_loss     | 6.35     |\n",
      "|    ent_coef        | 0.0975   |\n",
      "|    ent_coef_loss   | -0.0961  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 75359    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 268      |\n",
      "|    ep_rew_mean     | 852      |\n",
      "| time/              |          |\n",
      "|    episodes        | 530      |\n",
      "|    fps             | 124      |\n",
      "|    time_elapsed    | 628      |\n",
      "|    total_timesteps | 78345    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -189     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    ent_coef        | 0.0943   |\n",
      "|    ent_coef_loss   | 0.416    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 78244    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=1011.03 +/- 38.14\n",
      "Episode length: 321.80 +/- 12.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 322      |\n",
      "|    mean_reward     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.0939   |\n",
      "|    ent_coef_loss   | 0.0782   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 280      |\n",
      "|    ep_rew_mean     | 886      |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 124      |\n",
      "|    time_elapsed    | 656      |\n",
      "|    total_timesteps | 81594    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -195     |\n",
      "|    critic_loss     | 13.3     |\n",
      "|    ent_coef        | 0.0916   |\n",
      "|    ent_coef_loss   | -0.456   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 81493    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 288      |\n",
      "|    ep_rew_mean     | 908      |\n",
      "| time/              |          |\n",
      "|    episodes        | 550      |\n",
      "|    fps             | 124      |\n",
      "|    time_elapsed    | 682      |\n",
      "|    total_timesteps | 84666    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -200     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0887   |\n",
      "|    ent_coef_loss   | -0.0254  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 84565    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 299      |\n",
      "|    ep_rew_mean     | 946      |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 123      |\n",
      "|    time_elapsed    | 712      |\n",
      "|    total_timesteps | 88069    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -201     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0877   |\n",
      "|    ent_coef_loss   | -0.0212  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 87968    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=1235.47 +/- 40.23\n",
      "Episode length: 386.40 +/- 10.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 386      |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -194     |\n",
      "|    critic_loss     | 6.79     |\n",
      "|    ent_coef        | 0.0836   |\n",
      "|    ent_coef_loss   | 0.436    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 309      |\n",
      "|    ep_rew_mean     | 977      |\n",
      "| time/              |          |\n",
      "|    episodes        | 570      |\n",
      "|    fps             | 123      |\n",
      "|    time_elapsed    | 740      |\n",
      "|    total_timesteps | 91494    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -196     |\n",
      "|    critic_loss     | 6.7      |\n",
      "|    ent_coef        | 0.0825   |\n",
      "|    ent_coef_loss   | -0.351   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 91393    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 318      |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 580      |\n",
      "|    fps             | 123      |\n",
      "|    time_elapsed    | 769      |\n",
      "|    total_timesteps | 94798    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -199     |\n",
      "|    critic_loss     | 7.55     |\n",
      "|    ent_coef        | 0.0835   |\n",
      "|    ent_coef_loss   | 0.494    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 94697    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 328      |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 590      |\n",
      "|    fps             | 122      |\n",
      "|    time_elapsed    | 805      |\n",
      "|    total_timesteps | 98952    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -204     |\n",
      "|    critic_loss     | 21.6     |\n",
      "|    ent_coef        | 0.0806   |\n",
      "|    ent_coef_loss   | -0.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 98851    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=1247.21 +/- 239.04\n",
      "Episode length: 402.40 +/- 72.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 402      |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -202     |\n",
      "|    critic_loss     | 14.4     |\n",
      "|    ent_coef        | 0.0811   |\n",
      "|    ent_coef_loss   | -0.0663  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 337      |\n",
      "|    ep_rew_mean     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 600      |\n",
      "|    fps             | 122      |\n",
      "|    time_elapsed    | 841      |\n",
      "|    total_timesteps | 102894   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 5.93     |\n",
      "|    ent_coef        | 0.0805   |\n",
      "|    ent_coef_loss   | -0.453   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 102793   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 339      |\n",
      "|    ep_rew_mean     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 610      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 873      |\n",
      "|    total_timesteps | 106449   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -207     |\n",
      "|    critic_loss     | 9.26     |\n",
      "|    ent_coef        | 0.0791   |\n",
      "|    ent_coef_loss   | -0.233   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 106348   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=1211.81 +/- 667.79\n",
      "Episode length: 414.60 +/- 215.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 415      |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -205     |\n",
      "|    critic_loss     | 4.65     |\n",
      "|    ent_coef        | 0.0763   |\n",
      "|    ent_coef_loss   | 0.288    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 346      |\n",
      "|    ep_rew_mean     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 620      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 903      |\n",
      "|    total_timesteps | 110081   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -206     |\n",
      "|    critic_loss     | 6.18     |\n",
      "|    ent_coef        | 0.0764   |\n",
      "|    ent_coef_loss   | 0.275    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109980   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 370      |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 630      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 947      |\n",
      "|    total_timesteps | 115370   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -213     |\n",
      "|    critic_loss     | 5.49     |\n",
      "|    ent_coef        | 0.0718   |\n",
      "|    ent_coef_loss   | -0.681   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 115269   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 382      |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 640      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 983      |\n",
      "|    total_timesteps | 119765   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 4.49     |\n",
      "|    ent_coef        | 0.073    |\n",
      "|    ent_coef_loss   | 0.347    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 119664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=962.94 +/- 69.59\n",
      "Episode length: 296.40 +/- 17.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 296      |\n",
      "|    mean_reward     | 963      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -213     |\n",
      "|    critic_loss     | 5.99     |\n",
      "|    ent_coef        | 0.0743   |\n",
      "|    ent_coef_loss   | -0.804   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 119899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 391      |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 650      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 1017     |\n",
      "|    total_timesteps | 123765   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -216     |\n",
      "|    critic_loss     | 4.08     |\n",
      "|    ent_coef        | 0.0715   |\n",
      "|    ent_coef_loss   | -0.502   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 123664   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 390      |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 660      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 1046     |\n",
      "|    total_timesteps | 127092   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -216     |\n",
      "|    critic_loss     | 7.11     |\n",
      "|    ent_coef        | 0.0735   |\n",
      "|    ent_coef_loss   | 0.0364   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 126991   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=1015.45 +/- 66.35\n",
      "Episode length: 313.20 +/- 16.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 313      |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -213     |\n",
      "|    critic_loss     | 7.46     |\n",
      "|    ent_coef        | 0.0707   |\n",
      "|    ent_coef_loss   | 0.0102   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 129899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 390      |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 670      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 1076     |\n",
      "|    total_timesteps | 130473   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 7.99     |\n",
      "|    ent_coef        | 0.0711   |\n",
      "|    ent_coef_loss   | -0.923   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 130372   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 387      |\n",
      "|    ep_rew_mean     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 680      |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 1102     |\n",
      "|    total_timesteps | 133478   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -215     |\n",
      "|    critic_loss     | 4.9      |\n",
      "|    ent_coef        | 0.0716   |\n",
      "|    ent_coef_loss   | -0.0237  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 133377   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 379      |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 690      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1131     |\n",
      "|    total_timesteps | 136814   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -222     |\n",
      "|    critic_loss     | 3.84     |\n",
      "|    ent_coef        | 0.0704   |\n",
      "|    ent_coef_loss   | -0.193   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 136713   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=844.43 +/- 10.62\n",
      "Episode length: 256.60 +/- 2.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 257      |\n",
      "|    mean_reward     | 844      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -224     |\n",
      "|    critic_loss     | 4.39     |\n",
      "|    ent_coef        | 0.0689   |\n",
      "|    ent_coef_loss   | -0.399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 139899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 376      |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 700      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1163     |\n",
      "|    total_timesteps | 140474   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    ent_coef        | 0.0684   |\n",
      "|    ent_coef_loss   | -0.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 140373   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 375      |\n",
      "|    ep_rew_mean     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 710      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1193     |\n",
      "|    total_timesteps | 143969   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 4.18     |\n",
      "|    ent_coef        | 0.0685   |\n",
      "|    ent_coef_loss   | 0.153    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 143868   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 371      |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 720      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1219     |\n",
      "|    total_timesteps | 147140   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -223     |\n",
      "|    critic_loss     | 60.6     |\n",
      "|    ent_coef        | 0.0667   |\n",
      "|    ent_coef_loss   | 0.139    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 147039   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 346      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 730      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1243     |\n",
      "|    total_timesteps | 149961   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 5.71     |\n",
      "|    ent_coef        | 0.0667   |\n",
      "|    ent_coef_loss   | -0.695   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149860   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=709.44 +/- 26.58\n",
      "Episode length: 221.00 +/- 6.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 221      |\n",
      "|    mean_reward     | 709      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -213     |\n",
      "|    critic_loss     | 7.68     |\n",
      "|    ent_coef        | 0.0666   |\n",
      "|    ent_coef_loss   | -0.0401  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 327      |\n",
      "|    ep_rew_mean     | 1.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 740      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1266     |\n",
      "|    total_timesteps | 152496   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -222     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    ent_coef        | 0.0661   |\n",
      "|    ent_coef_loss   | 1.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 152395   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 314      |\n",
      "|    ep_rew_mean     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 750      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1287     |\n",
      "|    total_timesteps | 155118   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -222     |\n",
      "|    critic_loss     | 2.74     |\n",
      "|    ent_coef        | 0.065    |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 155017   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 307      |\n",
      "|    ep_rew_mean     | 1.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 760      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1310     |\n",
      "|    total_timesteps | 157764   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -224     |\n",
      "|    critic_loss     | 3.57     |\n",
      "|    ent_coef        | 0.0646   |\n",
      "|    ent_coef_loss   | 0.0795   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 157663   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=1628.92 +/- 369.30\n",
      "Episode length: 495.00 +/- 111.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 495      |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 9.07     |\n",
      "|    ent_coef        | 0.0635   |\n",
      "|    ent_coef_loss   | -0.274   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 302      |\n",
      "|    ep_rew_mean     | 994      |\n",
      "| time/              |          |\n",
      "|    episodes        | 770      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1336     |\n",
      "|    total_timesteps | 160628   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 4.3      |\n",
      "|    ent_coef        | 0.0618   |\n",
      "|    ent_coef_loss   | -0.453   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 160527   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 300      |\n",
      "|    ep_rew_mean     | 990      |\n",
      "| time/              |          |\n",
      "|    episodes        | 780      |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 1361     |\n",
      "|    total_timesteps | 163504   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 3.78     |\n",
      "|    ent_coef        | 0.0611   |\n",
      "|    ent_coef_loss   | -0.0536  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 163403   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 299      |\n",
      "|    ep_rew_mean     | 984      |\n",
      "| time/              |          |\n",
      "|    episodes        | 790      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 1389     |\n",
      "|    total_timesteps | 166665   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 8.12     |\n",
      "|    ent_coef        | 0.0621   |\n",
      "|    ent_coef_loss   | -0.0944  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 166564   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=1890.57 +/- 445.97\n",
      "Episode length: 562.40 +/- 132.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 562      |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 4.19     |\n",
      "|    ent_coef        | 0.0583   |\n",
      "|    ent_coef_loss   | -0.597   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 301      |\n",
      "|    ep_rew_mean     | 993      |\n",
      "| time/              |          |\n",
      "|    episodes        | 800      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 1423     |\n",
      "|    total_timesteps | 170584   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 3.87     |\n",
      "|    ent_coef        | 0.0586   |\n",
      "|    ent_coef_loss   | 0.488    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 170483   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 308      |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 810      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 1462     |\n",
      "|    total_timesteps | 174745   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 3.82     |\n",
      "|    ent_coef        | 0.0601   |\n",
      "|    ent_coef_loss   | 0.331    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 174644   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 319      |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 820      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 1499     |\n",
      "|    total_timesteps | 179035   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 4.69     |\n",
      "|    ent_coef        | 0.0588   |\n",
      "|    ent_coef_loss   | 0.174    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 178934   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=1321.28 +/- 184.50\n",
      "Episode length: 394.40 +/- 53.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 394      |\n",
      "|    mean_reward     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -230     |\n",
      "|    critic_loss     | 7.48     |\n",
      "|    ent_coef        | 0.057    |\n",
      "|    ent_coef_loss   | -0.611   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 349      |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 830      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 1548     |\n",
      "|    total_timesteps | 184847   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -222     |\n",
      "|    critic_loss     | 2.54     |\n",
      "|    ent_coef        | 0.0579   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 184746   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 370      |\n",
      "|    ep_rew_mean     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 840      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 1587     |\n",
      "|    total_timesteps | 189495   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.0562   |\n",
      "|    ent_coef_loss   | -0.328   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189394   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=1792.31 +/- 110.75\n",
      "Episode length: 540.60 +/- 31.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 541      |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -224     |\n",
      "|    critic_loss     | 2.76     |\n",
      "|    ent_coef        | 0.0567   |\n",
      "|    ent_coef_loss   | 0.573    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 404      |\n",
      "|    ep_rew_mean     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 850      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 1638     |\n",
      "|    total_timesteps | 195553   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -232     |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.0552   |\n",
      "|    ent_coef_loss   | -0.386   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 195452   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=2910.38 +/- 286.22\n",
      "Episode length: 880.40 +/- 99.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 880      |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 6.3      |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | 0.414    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 199899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 463      |\n",
      "|    ep_rew_mean     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 860      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 1713     |\n",
      "|    total_timesteps | 204058   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 3.87     |\n",
      "|    ent_coef        | 0.0551   |\n",
      "|    ent_coef_loss   | 0.492    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 203957   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=1916.15 +/- 163.60\n",
      "Episode length: 576.60 +/- 47.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 577      |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -234     |\n",
      "|    critic_loss     | 4.49     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.248   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 209899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 514      |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 870      |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 1784     |\n",
      "|    total_timesteps | 212022   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -239     |\n",
      "|    critic_loss     | 6.31     |\n",
      "|    ent_coef        | 0.0529   |\n",
      "|    ent_coef_loss   | 0.425    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 211921   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 550      |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 880      |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 1837     |\n",
      "|    total_timesteps | 218538   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -239     |\n",
      "|    critic_loss     | 3.88     |\n",
      "|    ent_coef        | 0.0515   |\n",
      "|    ent_coef_loss   | -0.404   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 218437   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=2302.23 +/- 406.54\n",
      "Episode length: 676.40 +/- 116.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 676      |\n",
      "|    mean_reward     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 3.31     |\n",
      "|    ent_coef        | 0.052    |\n",
      "|    ent_coef_loss   | -0.589   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 586      |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 890      |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 1896     |\n",
      "|    total_timesteps | 225271   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -248     |\n",
      "|    critic_loss     | 3.59     |\n",
      "|    ent_coef        | 0.0504   |\n",
      "|    ent_coef_loss   | 0.298    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 225170   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=3246.35 +/- 9.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -248     |\n",
      "|    critic_loss     | 2.68     |\n",
      "|    ent_coef        | 0.0506   |\n",
      "|    ent_coef_loss   | -0.353   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 229899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 601      |\n",
      "|    ep_rew_mean     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 900      |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 1941     |\n",
      "|    total_timesteps | 230678   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -240     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0499   |\n",
      "|    ent_coef_loss   | 0.438    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 230577   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 639      |\n",
      "|    ep_rew_mean     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 910      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 2003     |\n",
      "|    total_timesteps | 238655   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -245     |\n",
      "|    critic_loss     | 4.83     |\n",
      "|    ent_coef        | 0.0499   |\n",
      "|    ent_coef_loss   | 0.261    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 238554   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=823.80 +/- 22.07\n",
      "Episode length: 272.40 +/- 5.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 272      |\n",
      "|    mean_reward     | 824      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -242     |\n",
      "|    critic_loss     | 7.79     |\n",
      "|    ent_coef        | 0.0501   |\n",
      "|    ent_coef_loss   | -0.0959  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 239899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 678      |\n",
      "|    ep_rew_mean     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 920      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 2071     |\n",
      "|    total_timesteps | 246794   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -243     |\n",
      "|    critic_loss     | 3.39     |\n",
      "|    ent_coef        | 0.0482   |\n",
      "|    ent_coef_loss   | -0.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 246693   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=3225.79 +/- 16.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 2.58     |\n",
      "|    ent_coef        | 0.0491   |\n",
      "|    ent_coef_loss   | 0.405    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 249899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 930      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 2151     |\n",
      "|    total_timesteps | 256163   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 2.34     |\n",
      "|    ent_coef        | 0.0489   |\n",
      "|    ent_coef_loss   | 0.586    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 256062   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=3117.46 +/- 2.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -257     |\n",
      "|    critic_loss     | 4.42     |\n",
      "|    ent_coef        | 0.0475   |\n",
      "|    ent_coef_loss   | -0.282   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 940      |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 2236     |\n",
      "|    total_timesteps | 266163   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 3.1      |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | -0.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 266062   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=3177.99 +/- 4.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -256     |\n",
      "|    critic_loss     | 2.26     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -0.671   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 950      |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2318     |\n",
      "|    total_timesteps | 275575   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 2.77     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.631    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 275474   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=3174.80 +/- 8.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -258     |\n",
      "|    critic_loss     | 4.19     |\n",
      "|    ent_coef        | 0.0451   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 279899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 960      |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2393     |\n",
      "|    total_timesteps | 284466   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -252     |\n",
      "|    critic_loss     | 5.09     |\n",
      "|    ent_coef        | 0.0441   |\n",
      "|    ent_coef_loss   | 0.671    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 284365   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=2811.03 +/- 740.85\n",
      "Episode length: 879.00 +/- 242.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 879      |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 3.75     |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | -0.334   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 289899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 970      |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2465     |\n",
      "|    total_timesteps | 292803   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -267     |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | -0.406   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 292702   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 980      |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2515     |\n",
      "|    total_timesteps | 298887   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0425   |\n",
      "|    ent_coef_loss   | -0.983   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 298786   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=2403.49 +/- 1041.73\n",
      "Episode length: 736.60 +/- 323.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 737      |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -270     |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | -0.446   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 990      |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2565     |\n",
      "|    total_timesteps | 304643   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 4.88     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | 0.374    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 304542   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=3101.40 +/- 5.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -267     |\n",
      "|    critic_loss     | 2.61     |\n",
      "|    ent_coef        | 0.0435   |\n",
      "|    ent_coef_loss   | -0.499   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1000     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2644     |\n",
      "|    total_timesteps | 313825   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -266     |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -0.249   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 313724   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=3152.31 +/- 32.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -259     |\n",
      "|    critic_loss     | 3.3      |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | 0.723    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1010     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2702     |\n",
      "|    total_timesteps | 320695   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -263     |\n",
      "|    critic_loss     | 53.3     |\n",
      "|    ent_coef        | 0.0427   |\n",
      "|    ent_coef_loss   | -0.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 320594   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1020     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2759     |\n",
      "|    total_timesteps | 327480   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -254     |\n",
      "|    critic_loss     | 2.66     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | 0.502    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 327379   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=2550.89 +/- 629.64\n",
      "Episode length: 748.20 +/- 184.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 748      |\n",
      "|    mean_reward     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -266     |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | -0.599   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 329899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 786      |\n",
      "|    ep_rew_mean     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1030     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2821     |\n",
      "|    total_timesteps | 334802   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | 0.0251   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 334701   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=3228.39 +/- 7.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -267     |\n",
      "|    critic_loss     | 3.06     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 0.5      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 339899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 768      |\n",
      "|    ep_rew_mean     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1040     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2891     |\n",
      "|    total_timesteps | 342934   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -267     |\n",
      "|    critic_loss     | 3.39     |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 342833   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=2926.02 +/- 470.13\n",
      "Episode length: 881.20 +/- 153.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 881      |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -265     |\n",
      "|    critic_loss     | 2.13     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | -0.663   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1050     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 2963     |\n",
      "|    total_timesteps | 351632   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 3.04     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | 0.418    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 351531   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 753      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1060     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3031     |\n",
      "|    total_timesteps | 359780   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -267     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | -0.702   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359679   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=3260.03 +/- 13.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -263     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | -0.649   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 754      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1070     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3104     |\n",
      "|    total_timesteps | 368164   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -271     |\n",
      "|    critic_loss     | 2.2      |\n",
      "|    ent_coef        | 0.0391   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 368063   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=3275.34 +/- 18.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -273     |\n",
      "|    critic_loss     | 4.01     |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | 0.411    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1080     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3171     |\n",
      "|    total_timesteps | 375929   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 2.02     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.514    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 375828   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=1414.38 +/- 199.29\n",
      "Episode length: 417.00 +/- 57.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 417      |\n",
      "|    mean_reward     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 2.27     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.208    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 379899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1090     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3239     |\n",
      "|    total_timesteps | 383997   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -277     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 383896   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=3243.92 +/- 9.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -269     |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | 0.436    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 389899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1100     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3318     |\n",
      "|    total_timesteps | 393388   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 2.53     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.626   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 393287   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1110     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3363     |\n",
      "|    total_timesteps | 398741   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -275     |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -0.477   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 398640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1088.71 +/- 27.61\n",
      "Episode length: 329.60 +/- 9.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 330      |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -275     |\n",
      "|    critic_loss     | 2.62     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | -0.323   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1120     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3436     |\n",
      "|    total_timesteps | 406918   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 2.43     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 406817   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=3256.62 +/- 8.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -277     |\n",
      "|    critic_loss     | 2.57     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.203    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 409899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1130     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3511     |\n",
      "|    total_timesteps | 415528   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 9.95     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.0604  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 415427   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=3005.53 +/- 233.60\n",
      "Episode length: 902.80 +/- 82.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 903      |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 2.84     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.694   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 419899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1140     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3587     |\n",
      "|    total_timesteps | 423818   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -282     |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.884   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 423717   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=2203.41 +/- 895.72\n",
      "Episode length: 663.00 +/- 279.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 663      |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 429899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1150     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3648     |\n",
      "|    total_timesteps | 431251   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -275     |\n",
      "|    critic_loss     | 2.43     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | 0.838    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 431150   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 789      |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1160     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3697     |\n",
      "|    total_timesteps | 438701   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.724    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 438600   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=1841.94 +/- 199.47\n",
      "Episode length: 550.00 +/- 58.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 550      |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -0.124   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 782      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1170     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3760     |\n",
      "|    total_timesteps | 446373   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.146    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 446272   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=3279.73 +/- 9.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.252    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 449899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1180     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3839     |\n",
      "|    total_timesteps | 455391   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 1        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 455290   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=3276.53 +/- 5.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -0.365   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 459899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1190     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 3917     |\n",
      "|    total_timesteps | 464693   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 4.61     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 464592   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=3250.73 +/- 9.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -0.302   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 469899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1200     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 4000     |\n",
      "|    total_timesteps | 474569   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -281     |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.447    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 474468   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=3250.96 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.432    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 479899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1210     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 4083     |\n",
      "|    total_timesteps | 484569   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -281     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 484468   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=3248.39 +/- 2.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 0.919    |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | 0.314    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 489899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1220     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 4164     |\n",
      "|    total_timesteps | 494297   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -283     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 0.175    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 494196   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=3213.70 +/- 14.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 1.59     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | 0.267    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1230     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 4246     |\n",
      "|    total_timesteps | 504297   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 2.3      |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | -0.803   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 504196   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=3256.72 +/- 19.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -0.702   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 509899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1240     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 4324     |\n",
      "|    total_timesteps | 513347   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -285     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 513246   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=1821.30 +/- 785.94\n",
      "Episode length: 548.20 +/- 244.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 548      |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 0.438    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 519899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 905      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1250     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 4393     |\n",
      "|    total_timesteps | 521710   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | -0.0682  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 521609   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=1689.04 +/- 792.18\n",
      "Episode length: 508.20 +/- 246.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 508      |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -283     |\n",
      "|    critic_loss     | 1.6      |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 529899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1260     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 4469     |\n",
      "|    total_timesteps | 530994   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 4.1      |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | -0.0712  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 530893   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 927      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1270     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 4535     |\n",
      "|    total_timesteps | 539038   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 538937   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=3271.86 +/- 61.56\n",
      "Episode length: 984.20 +/- 31.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 984      |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | 0.537    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 539899   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 932       |\n",
      "|    ep_rew_mean     | 3.04e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1280      |\n",
      "|    fps             | 118       |\n",
      "|    time_elapsed    | 4615      |\n",
      "|    total_timesteps | 548586    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -287      |\n",
      "|    critic_loss     | 1.91      |\n",
      "|    ent_coef        | 0.0299    |\n",
      "|    ent_coef_loss   | -0.000447 |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 548485    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=3244.62 +/- 7.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | 0.0526   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 918      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1290     |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 4684     |\n",
      "|    total_timesteps | 556528   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | 0.417    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 556427   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=3260.48 +/- 3.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | 0.0399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 559899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1300     |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 4753     |\n",
      "|    total_timesteps | 565717   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -0.963   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 565616   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=3259.52 +/- 5.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -292     |\n",
      "|    critic_loss     | 0.965    |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 569899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1310     |\n",
      "|    fps             | 119      |\n",
      "|    time_elapsed    | 4804     |\n",
      "|    total_timesteps | 574347   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 1.59     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | 0.194    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 574246   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=2560.26 +/- 884.88\n",
      "Episode length: 775.80 +/- 276.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 776      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | 0.164    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 579899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1320     |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 4853     |\n",
      "|    total_timesteps | 582523   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | -0.569   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 582422   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=2508.28 +/- 935.75\n",
      "Episode length: 762.20 +/- 291.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 762      |\n",
      "|    mean_reward     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 3.61     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | -0.716   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 589899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1330     |\n",
      "|    fps             | 120      |\n",
      "|    time_elapsed    | 4907     |\n",
      "|    total_timesteps | 591870   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 2.98     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 591769   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1340     |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 4952     |\n",
      "|    total_timesteps | 599566   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | -0.609   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 599465   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=2006.14 +/- 1015.52\n",
      "Episode length: 611.80 +/- 317.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 612      |\n",
      "|    mean_reward     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -292     |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | -0.131   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 599899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1350     |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 5009     |\n",
      "|    total_timesteps | 609072   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 608971   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=3254.61 +/- 16.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | -0.159   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1360     |\n",
      "|    fps             | 122      |\n",
      "|    time_elapsed    | 5063     |\n",
      "|    total_timesteps | 617887   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | 0.481    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 617786   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=2136.25 +/- 932.52\n",
      "Episode length: 643.40 +/- 292.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 643      |\n",
      "|    mean_reward     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1370     |\n",
      "|    fps             | 122      |\n",
      "|    time_elapsed    | 5115     |\n",
      "|    total_timesteps | 626582   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 626481   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=3249.45 +/- 9.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -292     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | 0.512    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 629899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1380     |\n",
      "|    fps             | 122      |\n",
      "|    time_elapsed    | 5176     |\n",
      "|    total_timesteps | 636582   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 636481   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=2951.05 +/- 509.36\n",
      "Episode length: 932.40 +/- 135.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 932      |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -292     |\n",
      "|    critic_loss     | 3.03     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 0.939    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 639899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1390     |\n",
      "|    fps             | 123      |\n",
      "|    time_elapsed    | 5232     |\n",
      "|    total_timesteps | 645818   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | 0.139    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 645717   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=3255.49 +/- 8.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 2.54     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 0.287    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 649899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 898      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1400     |\n",
      "|    fps             | 123      |\n",
      "|    time_elapsed    | 5291     |\n",
      "|    total_timesteps | 655526   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | 0.277    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 655425   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=3302.10 +/- 5.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 0.73     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 659899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1410     |\n",
      "|    fps             | 124      |\n",
      "|    time_elapsed    | 5349     |\n",
      "|    total_timesteps | 664742   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 0.749    |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | -0.455   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 664641   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=3273.37 +/- 15.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 0.802    |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | 0.166    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1420     |\n",
      "|    fps             | 124      |\n",
      "|    time_elapsed    | 5403     |\n",
      "|    total_timesteps | 673676   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 4.62     |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | 0.296    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 673575   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=3270.64 +/- 6.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | -0.673   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 679899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1430     |\n",
      "|    fps             | 125      |\n",
      "|    time_elapsed    | 5459     |\n",
      "|    total_timesteps | 682893   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | 0.366    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 682792   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=3292.65 +/- 7.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | 0.717    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 689899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 927      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1440     |\n",
      "|    fps             | 125      |\n",
      "|    time_elapsed    | 5515     |\n",
      "|    total_timesteps | 692240   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | -0.195   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 692139   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=3279.18 +/- 34.65\n",
      "Episode length: 989.40 +/- 21.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 989      |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -0.803   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1450     |\n",
      "|    fps             | 125      |\n",
      "|    time_elapsed    | 5569     |\n",
      "|    total_timesteps | 701107   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 701006   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=3274.93 +/- 6.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 0.671    |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -0.338   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 709899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 932      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1460     |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 5631     |\n",
      "|    total_timesteps | 711107   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 0.717    |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -0.685   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 711006   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=3269.05 +/- 14.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 719899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 944      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1470     |\n",
      "|    fps             | 126      |\n",
      "|    time_elapsed    | 5689     |\n",
      "|    total_timesteps | 720943   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 720842   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 918      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1480     |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 5731     |\n",
      "|    total_timesteps | 728345   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 2        |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | 0.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 728244   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=3267.39 +/- 3.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 0.797    |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -0.538   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 729899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1490     |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 5789     |\n",
      "|    total_timesteps | 738345   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 0.827    |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -0.683   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 738244   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=3265.77 +/- 9.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 0.507    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 739899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1500     |\n",
      "|    fps             | 127      |\n",
      "|    time_elapsed    | 5840     |\n",
      "|    total_timesteps | 747224   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0241   |\n",
      "|    ent_coef_loss   | -0.443   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 747123   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=3260.02 +/- 9.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -0.271   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 749899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1510     |\n",
      "|    fps             | 128      |\n",
      "|    time_elapsed    | 5898     |\n",
      "|    total_timesteps | 757083   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 0.701    |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -0.627   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 756982   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=3258.77 +/- 13.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 7.8      |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | 0.784    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 759899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 930      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1520     |\n",
      "|    fps             | 128      |\n",
      "|    time_elapsed    | 5954     |\n",
      "|    total_timesteps | 766689   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0231   |\n",
      "|    ent_coef_loss   | 0.0521   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 766588   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=3241.20 +/- 3.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 0.978    |\n",
      "|    ent_coef        | 0.0236   |\n",
      "|    ent_coef_loss   | 1.45     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 769899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1530     |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 6006     |\n",
      "|    total_timesteps | 775660   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 3.69     |\n",
      "|    ent_coef        | 0.0239   |\n",
      "|    ent_coef_loss   | 0.285    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 775559   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=1215.62 +/- 181.46\n",
      "Episode length: 360.80 +/- 51.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 361      |\n",
      "|    mean_reward     | 1.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 2.1      |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 0.293    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 779899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1540     |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 6051     |\n",
      "|    total_timesteps | 783471   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 0.935    |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 783370   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=1440.22 +/- 174.70\n",
      "Episode length: 426.20 +/- 51.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 426      |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 0.655    |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 0.765    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 789899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1550     |\n",
      "|    fps             | 129      |\n",
      "|    time_elapsed    | 6100     |\n",
      "|    total_timesteps | 791918   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -0.329   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 791817   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=3252.60 +/- 2.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0239   |\n",
      "|    ent_coef_loss   | -0.963   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1560     |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 6154     |\n",
      "|    total_timesteps | 801324   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 0.933    |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | -0.172   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 801223   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1570     |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 6200     |\n",
      "|    total_timesteps | 809012   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | 0.898    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 808911   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=2215.49 +/- 382.31\n",
      "Episode length: 650.60 +/- 111.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 651      |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.0226   |\n",
      "|    ent_coef_loss   | 1.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 809899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1580     |\n",
      "|    fps             | 130      |\n",
      "|    time_elapsed    | 6245     |\n",
      "|    total_timesteps | 816890   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 0.721    |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -0.494   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 816789   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=3259.73 +/- 9.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 0.702    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 819899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1590     |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 6302     |\n",
      "|    total_timesteps | 826763   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 0.659    |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 826662   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=2207.02 +/- 922.96\n",
      "Episode length: 657.00 +/- 280.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 657      |\n",
      "|    mean_reward     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 2        |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | 0.761    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 829899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1600     |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 6339     |\n",
      "|    total_timesteps | 833176   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | -0.759   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 833075   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=3273.38 +/- 23.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | 0.188    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 839899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1610     |\n",
      "|    fps             | 131      |\n",
      "|    time_elapsed    | 6388     |\n",
      "|    total_timesteps | 841610   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 0.0993   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 841509   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=3254.54 +/- 4.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 0.934    |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | -0.812   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 849899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1620     |\n",
      "|    fps             | 132      |\n",
      "|    time_elapsed    | 6439     |\n",
      "|    total_timesteps | 850273   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | -0.0142  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 850172   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1630     |\n",
      "|    fps             | 132      |\n",
      "|    time_elapsed    | 6481     |\n",
      "|    total_timesteps | 857664   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 2.92     |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -0.849   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 857563   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=3299.37 +/- 11.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 0.931    |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | -0.554   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 859899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1640     |\n",
      "|    fps             | 132      |\n",
      "|    time_elapsed    | 6531     |\n",
      "|    total_timesteps | 866192   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 0.614    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 866091   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=2869.33 +/- 543.74\n",
      "Episode length: 860.80 +/- 171.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 861      |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | 0.223    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 869899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1650     |\n",
      "|    fps             | 132      |\n",
      "|    time_elapsed    | 6581     |\n",
      "|    total_timesteps | 874869   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 1.49     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 874768   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=3242.54 +/- 13.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 0.84     |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -0.149   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1660     |\n",
      "|    fps             | 133      |\n",
      "|    time_elapsed    | 6622     |\n",
      "|    total_timesteps | 881831   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 2.33     |\n",
      "|    ent_coef        | 0.0218   |\n",
      "|    ent_coef_loss   | -0.314   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 881730   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=3279.11 +/- 3.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 0.692    |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 0.0309   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 889899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1670     |\n",
      "|    fps             | 133      |\n",
      "|    time_elapsed    | 6679     |\n",
      "|    total_timesteps | 891612   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0218   |\n",
      "|    ent_coef_loss   | 0.979    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 891511   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=3277.81 +/- 14.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | -0.422   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1680     |\n",
      "|    fps             | 133      |\n",
      "|    time_elapsed    | 6730     |\n",
      "|    total_timesteps | 900481   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 0.701    |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -0.697   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 900380   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1690     |\n",
      "|    fps             | 134      |\n",
      "|    time_elapsed    | 6781     |\n",
      "|    total_timesteps | 909362   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 0.481    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 909261   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=2938.44 +/- 667.34\n",
      "Episode length: 893.80 +/- 212.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 894      |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 0.541    |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | -0.697   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 909899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1700     |\n",
      "|    fps             | 134      |\n",
      "|    time_elapsed    | 6840     |\n",
      "|    total_timesteps | 919362   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0226   |\n",
      "|    ent_coef_loss   | 0.616    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 919261   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=3263.98 +/- 11.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 0.703    |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 0.103    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 919899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1710     |\n",
      "|    fps             | 134      |\n",
      "|    time_elapsed    | 6893     |\n",
      "|    total_timesteps | 928485   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 0.585    |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 928384   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=3287.21 +/- 8.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 0.981    |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | -0.925   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 929899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1720     |\n",
      "|    fps             | 134      |\n",
      "|    time_elapsed    | 6940     |\n",
      "|    total_timesteps | 936672   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 0.969    |\n",
      "|    ent_coef        | 0.0226   |\n",
      "|    ent_coef_loss   | -0.506   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 936571   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=3280.47 +/- 4.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 4.99     |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | 0.056    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 939899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1730     |\n",
      "|    fps             | 135      |\n",
      "|    time_elapsed    | 6993     |\n",
      "|    total_timesteps | 945783   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 0.713    |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -0.207   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 945682   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=3290.45 +/- 8.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 25.3     |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -0.386   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 949899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1740     |\n",
      "|    fps             | 135      |\n",
      "|    time_elapsed    | 7043     |\n",
      "|    total_timesteps | 954313   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 0.931    |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | 0.585    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 954212   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=1773.54 +/- 758.69\n",
      "Episode length: 531.20 +/- 239.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 531      |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1750     |\n",
      "|    fps             | 135      |\n",
      "|    time_elapsed    | 7092     |\n",
      "|    total_timesteps | 962909   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | -0.436   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 962808   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=3316.90 +/- 20.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 0.443    |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 969899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1760     |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 7147     |\n",
      "|    total_timesteps | 972246   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 0.987    |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | -0.321   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 972145   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=2793.92 +/- 620.00\n",
      "Episode length: 840.20 +/- 200.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 840      |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 0.996    |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 1.97     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 979899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1770     |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 7204     |\n",
      "|    total_timesteps | 981861   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | -0.194   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 981760   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=2778.99 +/- 552.08\n",
      "Episode length: 827.80 +/- 174.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 828      |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | -0.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 989899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1780     |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 7258     |\n",
      "|    total_timesteps | 991262   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.863   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 991161   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=2898.65 +/- 783.79\n",
      "Episode length: 878.20 +/- 243.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 878      |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | -0.679   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1790     |\n",
      "|    fps             | 136      |\n",
      "|    time_elapsed    | 7313     |\n",
      "|    total_timesteps | 1000808  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | 0.495    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1000707  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=2540.12 +/- 745.10\n",
      "Episode length: 754.40 +/- 228.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 754      |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 0.848    |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | -0.188   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 908      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1800     |\n",
      "|    fps             | 137      |\n",
      "|    time_elapsed    | 7368     |\n",
      "|    total_timesteps | 1010212  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 2.31     |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | 1.67     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1010111  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 915      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1810     |\n",
      "|    fps             | 137      |\n",
      "|    time_elapsed    | 7424     |\n",
      "|    total_timesteps | 1019968  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | 0.688    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019867  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=3263.03 +/- 3.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 0.662    |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | 0.395    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 927      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1820     |\n",
      "|    fps             | 137      |\n",
      "|    time_elapsed    | 7478     |\n",
      "|    total_timesteps | 1029417  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 131      |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029316  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=3273.87 +/- 3.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 0.795    |\n",
      "|    ent_coef        | 0.0188   |\n",
      "|    ent_coef_loss   | -0.564   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 930      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1830     |\n",
      "|    fps             | 137      |\n",
      "|    time_elapsed    | 7533     |\n",
      "|    total_timesteps | 1038782  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0186   |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1038681  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=2612.11 +/- 677.63\n",
      "Episode length: 776.80 +/- 210.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 777      |\n",
      "|    mean_reward     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 0.529    |\n",
      "|    ent_coef        | 0.0184   |\n",
      "|    ent_coef_loss   | -0.153   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 930      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1840     |\n",
      "|    fps             | 138      |\n",
      "|    time_elapsed    | 7582     |\n",
      "|    total_timesteps | 1047318  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 0.691    |\n",
      "|    ent_coef        | 0.0176   |\n",
      "|    ent_coef_loss   | 0.717    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1047217  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=3309.77 +/- 6.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | 0.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 939      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1850     |\n",
      "|    fps             | 138      |\n",
      "|    time_elapsed    | 7637     |\n",
      "|    total_timesteps | 1056827  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.522    |\n",
      "|    ent_coef        | 0.0176   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1056726  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=3319.89 +/- 20.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 0.496    |\n",
      "|    ent_coef        | 0.0176   |\n",
      "|    ent_coef_loss   | 0.317    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1059899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1860     |\n",
      "|    fps             | 138      |\n",
      "|    time_elapsed    | 7676     |\n",
      "|    total_timesteps | 1063526  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 0.609    |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | 0.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1063425  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=3139.08 +/- 311.58\n",
      "Episode length: 947.40 +/- 105.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 947      |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.327    |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | -0.00749 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1870     |\n",
      "|    fps             | 138      |\n",
      "|    time_elapsed    | 7731     |\n",
      "|    total_timesteps | 1072944  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.519    |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | -0.816   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1072843  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=3286.39 +/- 14.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.544    |\n",
      "|    ent_coef        | 0.0172   |\n",
      "|    ent_coef_loss   | 2.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1880     |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 7786     |\n",
      "|    total_timesteps | 1082464  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.483    |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | 0.212    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1082363  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=3298.99 +/- 9.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.599    |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | -0.0877  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1890     |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 7840     |\n",
      "|    total_timesteps | 1091669  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 0.936    |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | 1.67     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1091568  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=3293.39 +/- 9.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.442    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | -0.271   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1900     |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 7895     |\n",
      "|    total_timesteps | 1101270  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.946    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | 0.0965   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1101169  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1910     |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 7943     |\n",
      "|    total_timesteps | 1109578  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.536    |\n",
      "|    ent_coef        | 0.0171   |\n",
      "|    ent_coef_loss   | 0.344    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1109477  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=2103.81 +/- 1016.46\n",
      "Episode length: 626.00 +/- 306.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 626      |\n",
      "|    mean_reward     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.513    |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -0.103   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1920     |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 7989     |\n",
      "|    total_timesteps | 1117590  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.509    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | 0.402    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1117489  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=3301.82 +/- 6.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.348    |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -0.178   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1930     |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 8037     |\n",
      "|    total_timesteps | 1125869  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.424    |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -0.213   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1125768  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=1940.45 +/- 741.92\n",
      "Episode length: 574.60 +/- 218.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 575      |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.546    |\n",
      "|    ent_coef        | 0.0169   |\n",
      "|    ent_coef_loss   | 0.00871  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1940     |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 8069     |\n",
      "|    total_timesteps | 1131266  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.556    |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -0.637   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1131165  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 788      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1950     |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 8094     |\n",
      "|    total_timesteps | 1135677  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.299    |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | 0.772    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1135576  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 723      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1960     |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 8094     |\n",
      "|    total_timesteps | 1135800  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.567    |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | 1.31     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1135699  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 653      |\n",
      "|    ep_rew_mean     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1970     |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 8108     |\n",
      "|    total_timesteps | 1138250  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.698    |\n",
      "|    ent_coef        | 0.0218   |\n",
      "|    ent_coef_loss   | 0.717    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1138149  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=3297.62 +/- 8.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 615      |\n",
      "|    ep_rew_mean     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1980     |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 8142     |\n",
      "|    total_timesteps | 1143946  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.493    |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | 0.536    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1143845  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=3290.70 +/- 5.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 10       |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 594      |\n",
      "|    ep_rew_mean     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1990     |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 8183     |\n",
      "|    total_timesteps | 1151022  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.531    |\n",
      "|    ent_coef        | 0.0183   |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1150921  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 579      |\n",
      "|    ep_rew_mean     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2000     |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 8229     |\n",
      "|    total_timesteps | 1159143  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.338    |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1159042  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=3294.41 +/- 4.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.519    |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | 0.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 596      |\n",
      "|    ep_rew_mean     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2010     |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 8287     |\n",
      "|    total_timesteps | 1169143  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -0.606   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1169042  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=3313.73 +/- 2.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.421    |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | 0.336    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 592      |\n",
      "|    ep_rew_mean     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2020     |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 8332     |\n",
      "|    total_timesteps | 1176803  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 0.786    |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | -0.879   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1176702  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=3308.67 +/- 9.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.353    |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -1.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 598      |\n",
      "|    ep_rew_mean     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2030     |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 8383     |\n",
      "|    total_timesteps | 1185637  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.602    |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1185536  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=2487.83 +/- 821.36\n",
      "Episode length: 735.80 +/- 250.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 736      |\n",
      "|    mean_reward     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.577    |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | -0.0139  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 606      |\n",
      "|    ep_rew_mean     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2040     |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 8420     |\n",
      "|    total_timesteps | 1191885  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.325    |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -0.785   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1191784  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 620      |\n",
      "|    ep_rew_mean     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2050     |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 8453     |\n",
      "|    total_timesteps | 1197708  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 0.51     |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 0.285    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1197607  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=3321.23 +/- 11.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.396    |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1199899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 691      |\n",
      "|    ep_rew_mean     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2060     |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 8494     |\n",
      "|    total_timesteps | 1204857  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 2.1      |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | 0.237    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1204756  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 717      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2070     |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 8524     |\n",
      "|    total_timesteps | 1209936  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.537    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | -0.791   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1209835  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=2828.42 +/- 655.20\n",
      "Episode length: 835.60 +/- 198.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 836      |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.55     |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2080     |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8557     |\n",
      "|    total_timesteps | 1215131  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.657    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | -0.157   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1215030  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=2400.96 +/- 627.03\n",
      "Episode length: 710.20 +/- 192.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 710      |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.765    |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | -0.272   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 709      |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2090     |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8596     |\n",
      "|    total_timesteps | 1221882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.397    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | -0.845   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1221781  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 679      |\n",
      "|    ep_rew_mean     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2100     |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8625     |\n",
      "|    total_timesteps | 1227068  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -0.142   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1226967  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=1624.29 +/- 384.19\n",
      "Episode length: 472.80 +/- 110.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 473      |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.487    |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -0.0176  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 638      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2110     |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8659     |\n",
      "|    total_timesteps | 1232948  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | -0.102   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1232847  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 610      |\n",
      "|    ep_rew_mean     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2120     |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8687     |\n",
      "|    total_timesteps | 1237840  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.445    |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | -0.795   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1237739  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=3269.82 +/- 3.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.48     |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | 0.358    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 570      |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2130     |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8716     |\n",
      "|    total_timesteps | 1242649  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 0.948    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1242548  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 566      |\n",
      "|    ep_rew_mean     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2140     |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8749     |\n",
      "|    total_timesteps | 1248507  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.447    |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -0.544   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1248406  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=1496.91 +/- 223.13\n",
      "Episode length: 445.80 +/- 65.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 446      |\n",
      "|    mean_reward     | 1.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.449    |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | -0.0366  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 547      |\n",
      "|    ep_rew_mean     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2150     |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8772     |\n",
      "|    total_timesteps | 1252425  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.183    |\n",
      "|    ent_coef        | 0.0158   |\n",
      "|    ent_coef_loss   | 0.148    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1252324  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 523      |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2160     |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 8799     |\n",
      "|    total_timesteps | 1257127  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -0.168   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1257026  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=2626.44 +/- 597.84\n",
      "Episode length: 782.80 +/- 190.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 783      |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 5.38     |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 1.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 544      |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2170     |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 8841     |\n",
      "|    total_timesteps | 1264350  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | 0.0973   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1264249  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=3228.02 +/- 98.01\n",
      "Episode length: 978.80 +/- 42.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 979      |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.551    |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | -0.348   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 580      |\n",
      "|    ep_rew_mean     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2180     |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 8892     |\n",
      "|    total_timesteps | 1273165  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.809    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 0.121    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1273064  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=2766.39 +/- 577.84\n",
      "Episode length: 815.20 +/- 174.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 815      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.907    |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -0.507   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 608      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2190     |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 8946     |\n",
      "|    total_timesteps | 1282679  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.378    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -0.456   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1282578  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=3003.47 +/- 463.69\n",
      "Episode length: 896.80 +/- 147.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 897      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.442    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 642      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2200     |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 8996     |\n",
      "|    total_timesteps | 1291250  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.632    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -1.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1291149  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 658      |\n",
      "|    ep_rew_mean     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2210     |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 9038     |\n",
      "|    total_timesteps | 1298711  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.506    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1298610  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=3290.04 +/- 4.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 8.79     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 687      |\n",
      "|    ep_rew_mean     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2220     |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 9085     |\n",
      "|    total_timesteps | 1306517  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.653    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.86     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1306416  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=3273.44 +/- 2.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.703    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | 0.837    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2230     |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 9140     |\n",
      "|    total_timesteps | 1316031  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.358    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1315930  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=3306.58 +/- 8.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.854    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -0.638   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2240     |\n",
      "|    fps             | 144      |\n",
      "|    time_elapsed    | 9188     |\n",
      "|    total_timesteps | 1324298  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.695    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -0.869   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1324197  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=1735.51 +/- 191.68\n",
      "Episode length: 507.60 +/- 54.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 508      |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.529    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.828    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2250     |\n",
      "|    fps             | 144      |\n",
      "|    time_elapsed    | 9234     |\n",
      "|    total_timesteps | 1332297  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.424    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -0.737   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1332196  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=3289.56 +/- 13.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.372    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.776    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2260     |\n",
      "|    fps             | 144      |\n",
      "|    time_elapsed    | 9280     |\n",
      "|    total_timesteps | 1340238  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.607    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.0552  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1340137  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2270     |\n",
      "|    fps             | 144      |\n",
      "|    time_elapsed    | 9321     |\n",
      "|    total_timesteps | 1347465  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.297    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.905   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1347364  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=3303.16 +/- 3.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.52     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2280     |\n",
      "|    fps             | 144      |\n",
      "|    time_elapsed    | 9370     |\n",
      "|    total_timesteps | 1355929  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -0.174   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1355828  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=3313.25 +/- 8.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.316    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | -0.534   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2290     |\n",
      "|    fps             | 144      |\n",
      "|    time_elapsed    | 9420     |\n",
      "|    total_timesteps | 1364492  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.64     |\n",
      "|    ent_coef        | 0.0158   |\n",
      "|    ent_coef_loss   | 0.855    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1364391  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=1226.45 +/- 135.48\n",
      "Episode length: 355.60 +/- 35.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 356      |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.526    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | 0.741    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2300     |\n",
      "|    fps             | 144      |\n",
      "|    time_elapsed    | 9465     |\n",
      "|    total_timesteps | 1372274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.548    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.757    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1372173  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2310     |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 9499     |\n",
      "|    total_timesteps | 1378311  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.601    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 0.728    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1378210  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=3329.59 +/- 24.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.762    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1379899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2320     |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 9530     |\n",
      "|    total_timesteps | 1383529  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.644    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1383428  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=3061.64 +/- 499.57\n",
      "Episode length: 920.40 +/- 159.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 920      |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.368    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.58     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 755      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2330     |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 9577     |\n",
      "|    total_timesteps | 1391554  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.584    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 0.642    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1391453  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 743      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2340     |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 9617     |\n",
      "|    total_timesteps | 1398560  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1398459  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=3304.99 +/- 2.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 2.43     |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | -0.653   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2350     |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 9675     |\n",
      "|    total_timesteps | 1408137  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.314    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 0.301    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1408036  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=3321.80 +/- 7.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.548    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 0.212    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 773      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2360     |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 9729     |\n",
      "|    total_timesteps | 1417554  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.284    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.647   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1417453  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=3321.39 +/- 15.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 0.365    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2370     |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 9782     |\n",
      "|    total_timesteps | 1426691  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.471    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | -0.846   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1426590  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=1331.49 +/- 223.43\n",
      "Episode length: 387.40 +/- 57.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 387      |\n",
      "|    mean_reward     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 788      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2380     |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 9828     |\n",
      "|    total_timesteps | 1434709  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.292    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -0.152   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1434608  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=3313.89 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.377    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | -0.249   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2390     |\n",
      "|    fps             | 146      |\n",
      "|    time_elapsed    | 9884     |\n",
      "|    total_timesteps | 1444363  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.655    |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | -2.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1444262  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=3284.08 +/- 7.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.702    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.482    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2400     |\n",
      "|    fps             | 146      |\n",
      "|    time_elapsed    | 9942     |\n",
      "|    total_timesteps | 1454363  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.256    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -1.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1454262  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=2921.10 +/- 738.02\n",
      "Episode length: 884.00 +/- 232.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 884      |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 0.739    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2410     |\n",
      "|    fps             | 146      |\n",
      "|    time_elapsed    | 9989     |\n",
      "|    total_timesteps | 1462475  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.553    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.679    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1462374  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=2513.87 +/- 637.47\n",
      "Episode length: 734.20 +/- 192.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 734      |\n",
      "|    mean_reward     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.401    |\n",
      "|    ent_coef        | 0.0158   |\n",
      "|    ent_coef_loss   | -0.543   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2420     |\n",
      "|    fps             | 146      |\n",
      "|    time_elapsed    | 10034    |\n",
      "|    total_timesteps | 1470333  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.659    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | -0.0556  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1470232  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2430     |\n",
      "|    fps             | 146      |\n",
      "|    time_elapsed    | 10081    |\n",
      "|    total_timesteps | 1478676  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.354    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.438    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1478575  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=3337.17 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.558   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1479899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2440     |\n",
      "|    fps             | 146      |\n",
      "|    time_elapsed    | 10129    |\n",
      "|    total_timesteps | 1486937  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.391    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -0.878   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1486836  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=3339.52 +/- 8.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.289    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | 0.126    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1489899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2450     |\n",
      "|    fps             | 146      |\n",
      "|    time_elapsed    | 10171    |\n",
      "|    total_timesteps | 1493989  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.494    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 0.491    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1493888  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2460     |\n",
      "|    fps             | 146      |\n",
      "|    time_elapsed    | 10201    |\n",
      "|    total_timesteps | 1499042  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.268    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.462    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1498941  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=3273.52 +/- 2.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.542    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 2.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2470     |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 10257    |\n",
      "|    total_timesteps | 1508615  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.577    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.543    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1508514  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=3293.02 +/- 5.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.701    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2480     |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 10312    |\n",
      "|    total_timesteps | 1517682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.535    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1517581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=3287.23 +/- 8.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.764    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.00392  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2490     |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 10363    |\n",
      "|    total_timesteps | 1526442  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.565    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1526341  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=3227.65 +/- 178.74\n",
      "Episode length: 967.40 +/- 65.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 967      |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.433    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -0.0904  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2500     |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 10419    |\n",
      "|    total_timesteps | 1536163  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1536062  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=3268.53 +/- 3.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.801    |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -0.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2510     |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 10467    |\n",
      "|    total_timesteps | 1544358  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.599    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 2.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1544257  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=3316.76 +/- 7.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | -0.839   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2520     |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 10512    |\n",
      "|    total_timesteps | 1551956  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.713    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 0.383    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1551855  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2530     |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 10556    |\n",
      "|    total_timesteps | 1559738  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.666    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.739   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559637  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=3299.04 +/- 6.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.443    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.275    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2540     |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 10614    |\n",
      "|    total_timesteps | 1569738  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.497    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 1.97     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1569637  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=3301.69 +/- 2.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.32     |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -1.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2550     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 10669    |\n",
      "|    total_timesteps | 1579384  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 3.92     |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 0.941    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579283  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=3307.98 +/- 7.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.57     |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -0.476   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2560     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 10720    |\n",
      "|    total_timesteps | 1588189  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.407    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 0.823    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1588088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=3295.80 +/- 6.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.883    |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2570     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 10778    |\n",
      "|    total_timesteps | 1598189  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.582    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.104    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1598088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=3330.95 +/- 4.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.342    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2580     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 10826    |\n",
      "|    total_timesteps | 1606293  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.718    |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 0.636    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1606192  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=3324.01 +/- 6.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.7      |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 1.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2590     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 10870    |\n",
      "|    total_timesteps | 1613920  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.465    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -0.673   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1613819  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=2975.82 +/- 471.99\n",
      "Episode length: 877.40 +/- 152.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 877      |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.444    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | 0.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2600     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 10916    |\n",
      "|    total_timesteps | 1621872  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.694    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1621771  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2610     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 10959    |\n",
      "|    total_timesteps | 1629434  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.489    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 0.126    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1629333  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=2589.27 +/- 899.28\n",
      "Episode length: 769.40 +/- 282.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 769      |\n",
      "|    mean_reward     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.643    |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 2.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2620     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 11006    |\n",
      "|    total_timesteps | 1637521  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.329    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1637420  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=3306.05 +/- 2.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.578    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.541    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2630     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 11053    |\n",
      "|    total_timesteps | 1645634  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.641    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -0.551   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1645533  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=3261.27 +/- 188.68\n",
      "Episode length: 965.00 +/- 70.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 965      |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.731    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -0.534   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 852      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2640     |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 11107    |\n",
      "|    total_timesteps | 1654895  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.337    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.125   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1654794  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=3328.36 +/- 1.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.539    |\n",
      "|    ent_coef        | 0.0158   |\n",
      "|    ent_coef_loss   | -0.323   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2650     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11163    |\n",
      "|    total_timesteps | 1664530  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.603    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1664429  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=3319.41 +/- 2.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.653    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.208   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2660     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11212    |\n",
      "|    total_timesteps | 1673067  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.389    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -0.987   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1672966  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=3325.61 +/- 4.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.325    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2670     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11269    |\n",
      "|    total_timesteps | 1682900  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.374    |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -0.659   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1682799  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=3352.98 +/- 7.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.483    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1689899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2680     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11323    |\n",
      "|    total_timesteps | 1692314  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.318    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -1.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1692213  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=3319.99 +/- 3.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.605    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 1        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2690     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11371    |\n",
      "|    total_timesteps | 1700118  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.375    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1700017  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 852      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2700     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11413    |\n",
      "|    total_timesteps | 1707115  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.338    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 0.237    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1707014  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=3281.85 +/- 6.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 2.49     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2710     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11468    |\n",
      "|    total_timesteps | 1716526  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.419    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -0.843   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1716425  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=3289.32 +/- 4.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.724    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2720     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11514    |\n",
      "|    total_timesteps | 1724575  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.841    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | -1.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1724474  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=3312.11 +/- 6.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.989    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 0.454    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2730     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11555    |\n",
      "|    total_timesteps | 1731486  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.591    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -0.485   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1731385  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=3329.41 +/- 5.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -0.593   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2740     |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 11604    |\n",
      "|    total_timesteps | 1740000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2750     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 11650    |\n",
      "|    total_timesteps | 1748028  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.52     |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -0.0172  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1747927  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=3298.37 +/- 1.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.564    |\n",
      "|    ent_coef        | 0.0155   |\n",
      "|    ent_coef_loss   | 0.262    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2760     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 11706    |\n",
      "|    total_timesteps | 1757741  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.617    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | -0.906   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1757640  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=3334.66 +/- 10.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.423    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -0.632   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2770     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 11761    |\n",
      "|    total_timesteps | 1767279  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.554    |\n",
      "|    ent_coef        | 0.0149   |\n",
      "|    ent_coef_loss   | -0.611   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1767178  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=3339.34 +/- 4.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.574    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 1.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2780     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 11817    |\n",
      "|    total_timesteps | 1776869  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 30       |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 2.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1776768  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=3333.73 +/- 6.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.833    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | 0.485    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2790     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 11868    |\n",
      "|    total_timesteps | 1785706  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | -0.524   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1785605  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=3347.09 +/- 6.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.837    |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 1.83     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2800     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 11920    |\n",
      "|    total_timesteps | 1794720  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.702    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -0.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1794619  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=3339.48 +/- 2.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.751    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -0.787   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2810     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 11974    |\n",
      "|    total_timesteps | 1804106  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.794    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.907   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1804005  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=3339.67 +/- 5.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.533    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 0.322    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2820     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 12032    |\n",
      "|    total_timesteps | 1814019  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.668    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | 0.322    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1813918  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=2692.82 +/- 321.39\n",
      "Episode length: 778.00 +/- 92.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 778      |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.632    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2830     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 12086    |\n",
      "|    total_timesteps | 1823381  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.502    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -0.158   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1823280  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=2380.83 +/- 840.72\n",
      "Episode length: 698.80 +/- 259.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 699      |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.453    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.973    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 930      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2840     |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 12142    |\n",
      "|    total_timesteps | 1833046  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.448    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -2.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1832945  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=3309.86 +/- 8.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 938      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2850     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12193    |\n",
      "|    total_timesteps | 1841839  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.613    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 0.894    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1841738  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1850000, episode_reward=3191.45 +/- 351.49\n",
      "Episode length: 943.20 +/- 113.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 943      |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.441    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.627    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 941      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2860     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12251    |\n",
      "|    total_timesteps | 1851794  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1851693  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 927      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2870     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12297    |\n",
      "|    total_timesteps | 1859992  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.525    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1859891  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=3338.21 +/- 4.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.622    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 905      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2880     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12340    |\n",
      "|    total_timesteps | 1867403  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.33     |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.818    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1867302  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=3342.05 +/- 4.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.328    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2890     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12388    |\n",
      "|    total_timesteps | 1875607  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.973    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 1.02     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1875506  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=3358.40 +/- 10.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.267    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | -0.742   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1879899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2900     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12444    |\n",
      "|    total_timesteps | 1885146  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.937    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -0.236   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1885045  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=2541.48 +/- 977.59\n",
      "Episode length: 754.20 +/- 301.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 754      |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.768    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 2.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2910     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12496    |\n",
      "|    total_timesteps | 1894080  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.386    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.221   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1893979  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=3351.25 +/- 3.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.572    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2920     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12540    |\n",
      "|    total_timesteps | 1901715  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.566    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | 0.0768   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1901614  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2930     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12584    |\n",
      "|    total_timesteps | 1909365  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.786    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1909264  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=3339.34 +/- 5.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.317    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -0.719   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2940     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12633    |\n",
      "|    total_timesteps | 1917715  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.439    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 0.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1917614  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=2949.02 +/- 785.35\n",
      "Episode length: 878.20 +/- 243.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 878      |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.281    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | 0.0876   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2950     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12677    |\n",
      "|    total_timesteps | 1925290  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.404    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.117    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1925189  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=1399.00 +/- 97.78\n",
      "Episode length: 396.40 +/- 27.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 396      |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.348    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | 0.172    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2960     |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 12710    |\n",
      "|    total_timesteps | 1931013  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.417    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1930912  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2970     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 12752    |\n",
      "|    total_timesteps | 1938422  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.451    |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | -0.558   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1938321  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=2908.38 +/- 845.23\n",
      "Episode length: 870.20 +/- 259.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 870      |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.747    |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 0.655    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2980     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 12805    |\n",
      "|    total_timesteps | 1947659  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.379    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 0.481    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1947558  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=2978.60 +/- 712.69\n",
      "Episode length: 889.40 +/- 221.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 889      |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.26     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -2.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2990     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 12853    |\n",
      "|    total_timesteps | 1955913  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.37     |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 0.589    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1955812  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=3355.95 +/- 4.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.603    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -1.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3000     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 12903    |\n",
      "|    total_timesteps | 1964682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.361    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | -0.796   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1964581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=3359.53 +/- 10.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.474    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1969899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 806      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3010     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 12962    |\n",
      "|    total_timesteps | 1974682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.602    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | 0.989    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1974581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=3356.80 +/- 8.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.542    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -0.321   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3020     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 13015    |\n",
      "|    total_timesteps | 1983754  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.952    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | -0.381   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1983653  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=3349.74 +/- 8.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.402    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | -0.254   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3030     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 13063    |\n",
      "|    total_timesteps | 1991977  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1991876  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3040     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 13100    |\n",
      "|    total_timesteps | 1998494  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.549    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 1.45     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1998393  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=3342.23 +/- 1.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.712    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.0951  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3050     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 13149    |\n",
      "|    total_timesteps | 2006823  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.777    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | 0.531    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2006722  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=3347.18 +/- 2.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 0.969    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3060     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 13205    |\n",
      "|    total_timesteps | 2016440  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.355    |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 0.463    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2016339  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=3342.24 +/- 4.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.883    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -0.287   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3070     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 13257    |\n",
      "|    total_timesteps | 2025479  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | -0.671   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2025378  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=2894.33 +/- 564.94\n",
      "Episode length: 846.20 +/- 173.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 846      |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.519    |\n",
      "|    ent_coef        | 0.0135   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3080     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 13309    |\n",
      "|    total_timesteps | 2034488  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.734    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -0.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2034387  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=3328.71 +/- 3.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.316    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 0.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3090     |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 13361    |\n",
      "|    total_timesteps | 2043317  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.485    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -0.0452  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2043216  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=3351.93 +/- 3.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.621    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 0.929    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3100     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13418    |\n",
      "|    total_timesteps | 2053286  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.367    |\n",
      "|    ent_coef        | 0.0133   |\n",
      "|    ent_coef_loss   | 0.683    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2053185  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=3333.00 +/- 7.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.403    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3110     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13476    |\n",
      "|    total_timesteps | 2063286  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.487    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 0.683    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2063185  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=2550.56 +/- 667.38\n",
      "Episode length: 733.60 +/- 195.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 734      |\n",
      "|    mean_reward     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 2.3      |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -0.961   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3120     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13525    |\n",
      "|    total_timesteps | 2071777  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.6      |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | 0.309    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2071676  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=3346.09 +/- 2.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 0.422    |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3130     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13578    |\n",
      "|    total_timesteps | 2080905  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.406    |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 1.73     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2080804  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3140     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13629    |\n",
      "|    total_timesteps | 2089874  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.57     |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -0.928   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2089773  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=3209.36 +/- 314.77\n",
      "Episode length: 947.20 +/- 105.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 947      |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.753    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -0.855   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3150     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13681    |\n",
      "|    total_timesteps | 2098852  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.491    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -0.228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2098751  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=3336.04 +/- 9.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.373    |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | -0.878   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 905      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3160     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13728    |\n",
      "|    total_timesteps | 2106944  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | -0.558   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2106843  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=3337.18 +/- 7.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.364    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 0.0085   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3170     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13786    |\n",
      "|    total_timesteps | 2116829  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.306    |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -0.957   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2116728  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=3328.48 +/- 2.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.805    |\n",
      "|    ent_coef        | 0.0138   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3180     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13830    |\n",
      "|    total_timesteps | 2124355  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.567    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 4.35     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2124254  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=3340.99 +/- 5.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.307    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | 0.0625   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3190     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13880    |\n",
      "|    total_timesteps | 2132878  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.569    |\n",
      "|    ent_coef        | 0.0147   |\n",
      "|    ent_coef_loss   | -0.598   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2132777  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=3332.08 +/- 3.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.326    |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | 0.904    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3200     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13935    |\n",
      "|    total_timesteps | 2142434  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.596    |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2142333  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=3309.75 +/- 0.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.364    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 2.13     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3210     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 13991    |\n",
      "|    total_timesteps | 2152157  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.334    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -0.282   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2152056  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=3345.61 +/- 7.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.446    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | 0.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 904      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3220     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 14049    |\n",
      "|    total_timesteps | 2162157  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.463    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 2.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2162056  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=3338.41 +/- 4.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.391    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 2.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3230     |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 14104    |\n",
      "|    total_timesteps | 2171635  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.393    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2171534  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3240     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14144    |\n",
      "|    total_timesteps | 2178688  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.42     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2178587  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=1148.67 +/- 972.46\n",
      "Episode length: 337.00 +/- 273.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 337      |\n",
      "|    mean_reward     | 1.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.801    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 2.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3250     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14199    |\n",
      "|    total_timesteps | 2188266  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.426    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 0.343    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2188165  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=3343.25 +/- 6.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.241    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.375    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3260     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14250    |\n",
      "|    total_timesteps | 2197006  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.289    |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | 0.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2196905  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=3328.02 +/- 2.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.368    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 0.647    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3270     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14297    |\n",
      "|    total_timesteps | 2204858  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.311    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2204757  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=2940.26 +/- 741.94\n",
      "Episode length: 883.40 +/- 233.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 883      |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 3.09     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3280     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14345    |\n",
      "|    total_timesteps | 2213175  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.371    |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 0.192    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2213074  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=3332.75 +/- 6.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.26     |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | -0.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3290     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14395    |\n",
      "|    total_timesteps | 2221850  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.278    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | 0.468    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2221749  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=3324.63 +/- 131.08\n",
      "Episode length: 975.60 +/- 48.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 976      |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.265    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 0.339    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3300     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14444    |\n",
      "|    total_timesteps | 2230337  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.251    |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | -0.804   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2230236  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3310     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14493    |\n",
      "|    total_timesteps | 2238846  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.243    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 0.432    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2238745  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=3355.08 +/- 2.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.528    |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | 0.456    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3320     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14541    |\n",
      "|    total_timesteps | 2247166  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.206    |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2247065  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=3295.10 +/- 78.41\n",
      "Episode length: 981.20 +/- 37.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 981      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.25     |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | -0.187   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3330     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14589    |\n",
      "|    total_timesteps | 2255460  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2255359  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=3331.83 +/- 2.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | 0.191    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3340     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14639    |\n",
      "|    total_timesteps | 2264138  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.196    |\n",
      "|    ent_coef        | 0.0112   |\n",
      "|    ent_coef_loss   | 0.324    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2264037  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=3350.78 +/- 7.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.349    |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3350     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14693    |\n",
      "|    total_timesteps | 2273356  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.225    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | -0.752   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2273255  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=2991.19 +/- 715.19\n",
      "Episode length: 889.00 +/- 222.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 889      |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.393    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | -0.287   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3360     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14747    |\n",
      "|    total_timesteps | 2282770  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.319    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -0.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2282669  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=3026.51 +/- 664.52\n",
      "Episode length: 895.60 +/- 208.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 896      |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.279    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 1.74     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3370     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14799    |\n",
      "|    total_timesteps | 2291798  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.161    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 0.409    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2291697  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=3346.74 +/- 2.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.343    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3380     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14850    |\n",
      "|    total_timesteps | 2300584  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.469    |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | -0.762   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2300483  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3390     |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 14901    |\n",
      "|    total_timesteps | 2309281  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.329    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -0.683   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2309180  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310000, episode_reward=3118.28 +/- 432.82\n",
      "Episode length: 915.60 +/- 138.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 916      |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 3.68     |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -2.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3400     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 14947    |\n",
      "|    total_timesteps | 2317196  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.452    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -0.778   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2317095  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=2996.57 +/- 475.61\n",
      "Episode length: 882.80 +/- 155.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 883      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.366    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3410     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 14994    |\n",
      "|    total_timesteps | 2325202  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.143    |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | 0.541    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2325101  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=3349.67 +/- 5.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.197    |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | 0.314    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3420     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15041    |\n",
      "|    total_timesteps | 2333367  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.28     |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | 0.187    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2333266  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=1396.30 +/- 39.56\n",
      "Episode length: 396.60 +/- 10.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 397      |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.374    |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3430     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15089    |\n",
      "|    total_timesteps | 2341762  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.362    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | -0.477   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2341661  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3440     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15135    |\n",
      "|    total_timesteps | 2349838  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.265    |\n",
      "|    ent_coef        | 0.0112   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2349737  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=2463.46 +/- 757.48\n",
      "Episode length: 716.60 +/- 239.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 717      |\n",
      "|    mean_reward     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.621    |\n",
      "|    ent_coef        | 0.0111   |\n",
      "|    ent_coef_loss   | -0.253   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3450     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15183    |\n",
      "|    total_timesteps | 2358129  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0112   |\n",
      "|    ent_coef_loss   | 0.656    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2358028  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=2855.62 +/- 613.55\n",
      "Episode length: 837.80 +/- 198.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 838      |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.364    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3460     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15225    |\n",
      "|    total_timesteps | 2365481  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.244    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -0.0767  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2365380  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=3293.58 +/- 113.91\n",
      "Episode length: 962.80 +/- 49.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 963      |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.276    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3470     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15278    |\n",
      "|    total_timesteps | 2374393  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.311    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -0.131   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2374292  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=3365.55 +/- 2.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.313    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2379899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3480     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15327    |\n",
      "|    total_timesteps | 2382794  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.235    |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 0.992    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2382693  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3490     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15362    |\n",
      "|    total_timesteps | 2388619  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.313    |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | -0.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2388518  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=3367.51 +/- 3.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.2      |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -0.503   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2389899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 788      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3500     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15405    |\n",
      "|    total_timesteps | 2395987  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.218    |\n",
      "|    ent_coef        | 0.0112   |\n",
      "|    ent_coef_loss   | -0.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2395886  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=3351.91 +/- 4.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.28     |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | -2.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3510     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15456    |\n",
      "|    total_timesteps | 2404578  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.537    |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 0.253    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2404477  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=3373.17 +/- 11.77\n",
      "Episode length: 990.00 +/- 20.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 990      |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.335    |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | -0.524   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3520     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15507    |\n",
      "|    total_timesteps | 2413207  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.238    |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | -0.242   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2413106  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=3365.47 +/- 3.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.191    |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | -0.362   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3530     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15563    |\n",
      "|    total_timesteps | 2422805  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.253    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -0.0166  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2422704  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=3380.30 +/- 3.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.244    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -2.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2429899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3540     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15617    |\n",
      "|    total_timesteps | 2432310  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.373    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | 0.0397   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2432209  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3550     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15658    |\n",
      "|    total_timesteps | 2439527  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | 2.91     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2439426  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=3369.81 +/- 8.68\n",
      "Episode length: 991.00 +/- 18.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 991      |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.338    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -0.878   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3560     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15713    |\n",
      "|    total_timesteps | 2449001  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.492    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -0.613   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2448900  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2450000, episode_reward=3193.30 +/- 332.36\n",
      "Episode length: 908.80 +/- 107.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 909      |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.324    |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 0.182    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 840      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3570     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15766    |\n",
      "|    total_timesteps | 2458443  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.199    |\n",
      "|    ent_coef        | 0.0112   |\n",
      "|    ent_coef_loss   | 0.685    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2458342  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=3401.72 +/- 11.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.262    |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 1.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2459899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3580     |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 15820    |\n",
      "|    total_timesteps | 2467626  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 0.248    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | 0.433    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2467525  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=3429.01 +/- 21.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.445    |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2469899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3590     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 15878    |\n",
      "|    total_timesteps | 2477626  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 0.217    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 0.169    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2477525  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=3441.43 +/- 14.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.197    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -0.738   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2479899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3600     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 15932    |\n",
      "|    total_timesteps | 2486961  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.741    |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | 0.105    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2486860  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=3481.83 +/- 4.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.362    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -0.139   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2489899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3610     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 15981    |\n",
      "|    total_timesteps | 2495219  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 0.361    |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -0.172   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2495118  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=3470.97 +/- 15.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 2.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3620     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16032    |\n",
      "|    total_timesteps | 2503906  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.371    |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | 0.064    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2503805  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=3399.71 +/- 2.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.45     |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | -0.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3630     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16084    |\n",
      "|    total_timesteps | 2513072  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.194    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | 0.294    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2512971  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=3411.83 +/- 2.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.232    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | 0.428    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3640     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16139    |\n",
      "|    total_timesteps | 2522586  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.23     |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | 0.346    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2522485  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=3467.90 +/- 3.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 0.438    |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | -0.522   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3650     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16195    |\n",
      "|    total_timesteps | 2532368  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 0.208    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2532267  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=3482.32 +/- 8.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 0.369    |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.534    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2539899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3660     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16247    |\n",
      "|    total_timesteps | 2541504  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 1.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2541403  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=3510.59 +/- 8.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.545    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2549899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 927      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3670     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16303    |\n",
      "|    total_timesteps | 2551134  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 0.207    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | -0.826   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2551033  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=2986.70 +/- 640.55\n",
      "Episode length: 817.00 +/- 175.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 817      |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.71     |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | -0.437   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3680     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16357    |\n",
      "|    total_timesteps | 2560414  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 0.231    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -0.893   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2560313  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=3495.73 +/- 5.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 0.382    |\n",
      "|    ent_coef        | 0.00999  |\n",
      "|    ent_coef_loss   | 0.385    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 924      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3690     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16413    |\n",
      "|    total_timesteps | 2570059  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 0.332    |\n",
      "|    ent_coef        | 0.00996  |\n",
      "|    ent_coef_loss   | 1.13     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2569958  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 924      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3700     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16466    |\n",
      "|    total_timesteps | 2579394  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.125    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -0.892   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2579293  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=3494.66 +/- 2.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.155    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -0.682   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3710     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16515    |\n",
      "|    total_timesteps | 2587970  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.143    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -2.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2587869  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=3159.83 +/- 621.74\n",
      "Episode length: 901.60 +/- 196.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 902      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 936      |\n",
      "|    ep_rew_mean     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3720     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16570    |\n",
      "|    total_timesteps | 2597478  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.261    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -0.459   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2597377  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=3495.07 +/- 2.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.616    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -0.996   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 931      |\n",
      "|    ep_rew_mean     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3730     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16622    |\n",
      "|    total_timesteps | 2606182  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 0.202    |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2606081  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=3521.92 +/- 3.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | -0.795   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3740     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16671    |\n",
      "|    total_timesteps | 2614618  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.521    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2614517  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3750     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16701    |\n",
      "|    total_timesteps | 2619693  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.177    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -0.499   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2619592  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=3568.95 +/- 1.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.841    |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | -0.0332  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2619899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3760     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16759    |\n",
      "|    total_timesteps | 2629446  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | 3.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2629345  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2630000, episode_reward=2220.96 +/- 1064.84\n",
      "Episode length: 623.40 +/- 307.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 623      |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.311    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | 0.328    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3770     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16802    |\n",
      "|    total_timesteps | 2636796  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2636695  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=3524.36 +/- 3.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.616    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3780     |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 16847    |\n",
      "|    total_timesteps | 2644517  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 2.2      |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2644416  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=3540.01 +/- 11.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 0.456    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | 2.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3790     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 16903    |\n",
      "|    total_timesteps | 2654274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.366    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2654173  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=3465.61 +/- 157.64\n",
      "Episode length: 946.60 +/- 59.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 947      |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.531    |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | 0.0221   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3800     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 16952    |\n",
      "|    total_timesteps | 2662811  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.993    |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 0.491    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2662710  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=3516.98 +/- 2.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -2.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3810     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17001    |\n",
      "|    total_timesteps | 2671243  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.913    |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -1.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2671142  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=3515.47 +/- 9.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.228    |\n",
      "|    ent_coef        | 0.00987  |\n",
      "|    ent_coef_loss   | 2.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3820     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17059    |\n",
      "|    total_timesteps | 2681243  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.252    |\n",
      "|    ent_coef        | 0.00995  |\n",
      "|    ent_coef_loss   | 2.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2681142  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=3560.38 +/- 39.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.237    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -2.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3830     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17113    |\n",
      "|    total_timesteps | 2690462  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.31     |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2690361  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3840     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17156    |\n",
      "|    total_timesteps | 2698034  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.39     |\n",
      "|    ent_coef        | 0.0121   |\n",
      "|    ent_coef_loss   | -0.509   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2697933  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=927.41 +/- 213.74\n",
      "Episode length: 262.80 +/- 42.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 263      |\n",
      "|    mean_reward     | 927      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.258    |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | -0.632   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3850     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17204    |\n",
      "|    total_timesteps | 2706579  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.326    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2706478  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=3552.57 +/- 1.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.421    |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 0.713    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3860     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17228    |\n",
      "|    total_timesteps | 2710454  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2710353  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3870     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17281    |\n",
      "|    total_timesteps | 2719722  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.191    |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | -0.308   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719621  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=3514.87 +/- 6.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.363    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -0.227   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3880     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17324    |\n",
      "|    total_timesteps | 2727152  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.305    |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -2.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2727051  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=3532.39 +/- 17.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.355    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3890     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17382    |\n",
      "|    total_timesteps | 2737018  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.342    |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | -0.425   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2736917  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=3376.42 +/- 430.56\n",
      "Episode length: 932.80 +/- 134.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 933      |\n",
      "|    mean_reward     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.571    |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 0.313    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3900     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17426    |\n",
      "|    total_timesteps | 2744601  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.353    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -2.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2744500  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2750000, episode_reward=3574.50 +/- 17.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2749899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3910     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17480    |\n",
      "|    total_timesteps | 2753643  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.706    |\n",
      "|    ent_coef        | 0.0137   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2753542  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=3555.84 +/- 5.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.395    |\n",
      "|    ent_coef        | 0.0131   |\n",
      "|    ent_coef_loss   | 0.252    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3920     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17535    |\n",
      "|    total_timesteps | 2763132  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.585    |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | 1.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2763031  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=3558.20 +/- 8.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.323    |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3930     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17592    |\n",
      "|    total_timesteps | 2773114  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.381    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | 1.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2773013  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=1838.59 +/- 499.75\n",
      "Episode length: 478.40 +/- 129.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 478      |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.441    |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | -0.833   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3940     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17649    |\n",
      "|    total_timesteps | 2782988  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.528    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | 2.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2782887  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=3545.21 +/- 3.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.395    |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -0.399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3950     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17704    |\n",
      "|    total_timesteps | 2792441  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.28     |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 0.573    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2792340  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=3544.72 +/- 27.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | -0.968   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 918      |\n",
      "|    ep_rew_mean     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3960     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17761    |\n",
      "|    total_timesteps | 2802280  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.324    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | -0.396   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2802179  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3970     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17798    |\n",
      "|    total_timesteps | 2808703  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.568    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -0.163   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2808602  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=3572.21 +/- 20.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.267    |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | 0.443    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3980     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17851    |\n",
      "|    total_timesteps | 2817864  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.676    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -0.898   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2817763  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2820000, episode_reward=3324.09 +/- 534.03\n",
      "Episode length: 921.20 +/- 157.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 921      |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.242    |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | -0.698   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3990     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17896    |\n",
      "|    total_timesteps | 2825753  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.54     |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | 0.137    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2825652  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2830000, episode_reward=3215.94 +/- 741.73\n",
      "Episode length: 891.60 +/- 216.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 892      |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.605    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | -0.268   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4000     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17946    |\n",
      "|    total_timesteps | 2834347  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.502    |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | -0.978   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2834246  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2840000, episode_reward=3557.19 +/- 14.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.473    |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | 0.251    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4010     |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 17996    |\n",
      "|    total_timesteps | 2842890  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.431    |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2842789  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=3554.16 +/- 7.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.394    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4020     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18050    |\n",
      "|    total_timesteps | 2852173  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.259    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2852072  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=3583.00 +/- 13.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.31     |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2859899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4030     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18099    |\n",
      "|    total_timesteps | 2860683  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.449    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2860582  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4040     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18150    |\n",
      "|    total_timesteps | 2869553  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.365    |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | 0.555    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869452  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=3095.71 +/- 685.61\n",
      "Episode length: 843.00 +/- 197.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 843      |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.283    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | -0.416   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4050     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18205    |\n",
      "|    total_timesteps | 2879092  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 3.59     |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | -0.179   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2878991  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=2900.22 +/- 591.86\n",
      "Episode length: 796.80 +/- 175.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 797      |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4060     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18246    |\n",
      "|    total_timesteps | 2886133  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.664    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -0.796   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2886032  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=3543.99 +/- 9.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.439    |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | -0.391   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4070     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18288    |\n",
      "|    total_timesteps | 2893227  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.837    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | 0.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2893126  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=3574.74 +/- 12.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.399    |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | -0.485   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4080     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18337    |\n",
      "|    total_timesteps | 2901638  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.732    |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2901537  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4090     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18377    |\n",
      "|    total_timesteps | 2908737  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 0.372    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2908636  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2910000, episode_reward=3351.39 +/- 436.30\n",
      "Episode length: 899.80 +/- 130.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 900      |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.417    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 0.00709  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4100     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18424    |\n",
      "|    total_timesteps | 2916843  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.615    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -2.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2916742  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=3559.37 +/- 4.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.229    |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4110     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18483    |\n",
      "|    total_timesteps | 2926807  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.758    |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | 0.573    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2926706  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=3568.20 +/- 5.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.307    |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4120     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18543    |\n",
      "|    total_timesteps | 2936807  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.344    |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | 0.207    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2936706  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=3265.42 +/- 662.38\n",
      "Episode length: 905.00 +/- 190.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 905      |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.615    |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | 0.803    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4130     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18597    |\n",
      "|    total_timesteps | 2946208  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.799    |\n",
      "|    ent_coef        | 0.0119   |\n",
      "|    ent_coef_loss   | -0.257   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2946107  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2950000, episode_reward=3568.51 +/- 5.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.35     |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | -0.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4140     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18654    |\n",
      "|    total_timesteps | 2956011  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.347    |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | -0.763   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2955910  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2960000, episode_reward=3597.88 +/- 21.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.255    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | 1.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2959899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4150     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18710    |\n",
      "|    total_timesteps | 2965724  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.388    |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2965623  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=3580.31 +/- 14.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.402    |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4160     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18765    |\n",
      "|    total_timesteps | 2974571  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.453    |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | 0.376    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2974470  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=3603.05 +/- 14.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.586    |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | -0.361   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2979899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4170     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18821    |\n",
      "|    total_timesteps | 2984242  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.745    |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | -0.137   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2984141  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=3557.40 +/- 19.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.48     |\n",
      "|    ent_coef        | 0.0136   |\n",
      "|    ent_coef_loss   | 0.473    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4180     |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 18876    |\n",
      "|    total_timesteps | 2993689  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.268    |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | 0.537    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2993588  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=3575.16 +/- 1.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.3      |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999899  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/miniforge3/envs/irl/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path 'logs/expert/Hopper-v4-sac' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "sac_model.learn(total_timesteps=3e6, log_interval=10, callback=callback)\n",
    "sac_model.save(f\"logs/expert/{env_id}-sac/model\")\n",
    "sac_model.save_replay_buffer(f\"logs/expert/{env_id}-sac/buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.6     |\n",
      "|    ep_rew_mean     | -21.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 826      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.9    |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.805    |\n",
      "|    ent_coef_loss   | -2.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 725      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.1     |\n",
      "|    ep_rew_mean     | -32      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 1722     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -27.5    |\n",
      "|    critic_loss     | 3.14     |\n",
      "|    ent_coef        | 0.618    |\n",
      "|    ent_coef_loss   | -5.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1621     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 117      |\n",
      "|    ep_rew_mean     | -47.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 3516     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.3    |\n",
      "|    critic_loss     | 5.71     |\n",
      "|    ent_coef        | 0.37     |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3415     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 123      |\n",
      "|    ep_rew_mean     | -54.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 4921     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.4    |\n",
      "|    critic_loss     | 7.58     |\n",
      "|    ent_coef        | 0.25     |\n",
      "|    ent_coef_loss   | -14      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4820     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 147      |\n",
      "|    ep_rew_mean     | -69.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 7328     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.4    |\n",
      "|    critic_loss     | 5.84     |\n",
      "|    ent_coef        | 0.13     |\n",
      "|    ent_coef_loss   | -17.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7227     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-101.91 +/- 228.29\n",
      "Episode length: 265.60 +/- 367.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 266      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.3    |\n",
      "|    critic_loss     | 4.88     |\n",
      "|    ent_coef        | 0.0635   |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 178      |\n",
      "|    ep_rew_mean     | -79.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 10651    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.5    |\n",
      "|    critic_loss     | 4.44     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 212      |\n",
      "|    ep_rew_mean     | -81.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 70       |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 14841    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.5    |\n",
      "|    critic_loss     | 3.67     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -5.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14740    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 232      |\n",
      "|    ep_rew_mean     | -68.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 18595    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.7    |\n",
      "|    critic_loss     | 4.16     |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18494    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=165.59 +/- 169.34\n",
      "Episode length: 645.20 +/- 435.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 645      |\n",
      "|    mean_reward     | 166      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.8    |\n",
      "|    critic_loss     | 3.69     |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -0.0756  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 259      |\n",
      "|    ep_rew_mean     | -53.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 23320    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.7    |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | -0.949   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23219    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 270      |\n",
      "|    ep_rew_mean     | -44.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 27007    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.9    |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0186   |\n",
      "|    ent_coef_loss   | 0.529    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26906    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=50.57 +/- 31.69\n",
      "Episode length: 314.20 +/- 351.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 314      |\n",
      "|    mean_reward     | 50.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 2.97     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 325      |\n",
      "|    ep_rew_mean     | -30.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 110      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 33304    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.8    |\n",
      "|    critic_loss     | 2.79     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.0852  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33203    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 353      |\n",
      "|    ep_rew_mean     | -14.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 232      |\n",
      "|    total_timesteps | 37030    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.1    |\n",
      "|    critic_loss     | 2.54     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -0.472   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36929    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=228.85 +/- 192.62\n",
      "Episode length: 616.80 +/- 469.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 617      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.1    |\n",
      "|    critic_loss     | 3.17     |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | 0.456    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 370      |\n",
      "|    ep_rew_mean     | 4.77     |\n",
      "| time/              |          |\n",
      "|    episodes        | 130      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 40474    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.2    |\n",
      "|    critic_loss     | 2.94     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.291   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40373    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 421      |\n",
      "|    ep_rew_mean     | 35       |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 46986    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.8    |\n",
      "|    critic_loss     | 2.65     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 0.333    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46885    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=399.54 +/- 281.83\n",
      "Episode length: 657.20 +/- 419.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 657      |\n",
      "|    mean_reward     | 400      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.9    |\n",
      "|    critic_loss     | 2.17     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 0.671    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 449      |\n",
      "|    ep_rew_mean     | 73.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 150      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 327      |\n",
      "|    total_timesteps | 52225    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.2    |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | 0.472    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 52124    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 480      |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 367      |\n",
      "|    total_timesteps | 58607    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | 0.958    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 58506    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=620.12 +/- 53.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 620      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -25.9    |\n",
      "|    critic_loss     | 2.43     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 0.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 510      |\n",
      "|    ep_rew_mean     | 176      |\n",
      "| time/              |          |\n",
      "|    episodes        | 170      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 413      |\n",
      "|    total_timesteps | 65874    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.7    |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 0.345    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 65773    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=534.76 +/- 295.33\n",
      "Episode length: 802.20 +/- 395.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 802      |\n",
      "|    mean_reward     | 535      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -31.9    |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 520      |\n",
      "|    ep_rew_mean     | 205      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 443      |\n",
      "|    total_timesteps | 70580    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -32.7    |\n",
      "|    critic_loss     | 2.14     |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | -0.0312  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 70479    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 557      |\n",
      "|    ep_rew_mean     | 244      |\n",
      "| time/              |          |\n",
      "|    episodes        | 190      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 495      |\n",
      "|    total_timesteps | 79063    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -36.8    |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 0.0181   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 78962    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=332.01 +/- 365.91\n",
      "Episode length: 452.20 +/- 447.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 452      |\n",
      "|    mean_reward     | 332      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.3    |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -0.211   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 577      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 530      |\n",
      "|    total_timesteps | 84675    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39      |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 84574    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=837.32 +/- 43.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 837      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -40.2    |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -0.702   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 595      |\n",
      "|    ep_rew_mean     | 328      |\n",
      "| time/              |          |\n",
      "|    episodes        | 210      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 581      |\n",
      "|    total_timesteps | 92845    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.6    |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | 0.0889   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 92744    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=860.84 +/- 40.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 861      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -43.6    |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -0.254   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 637      |\n",
      "|    ep_rew_mean     | 373      |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 631      |\n",
      "|    total_timesteps | 100734   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -43.7    |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | 0.882    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 100633   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=837.21 +/- 50.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 837      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.5    |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | 0.325    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 703      |\n",
      "|    ep_rew_mean     | 442      |\n",
      "| time/              |          |\n",
      "|    episodes        | 230      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 694      |\n",
      "|    total_timesteps | 110734   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.4    |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 110633   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 698      |\n",
      "|    ep_rew_mean     | 468      |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 730      |\n",
      "|    total_timesteps | 116809   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -49.1    |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -0.933   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 116708   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=814.57 +/- 64.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 815      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -50.1    |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | -0.712   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 119899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 737      |\n",
      "|    ep_rew_mean     | 516      |\n",
      "| time/              |          |\n",
      "|    episodes        | 250      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 788      |\n",
      "|    total_timesteps | 125933   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -51.4    |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | -0.434   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 125832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=665.83 +/- 328.72\n",
      "Episode length: 804.40 +/- 391.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 666      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -52.6    |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | -0.402   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 129899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 748      |\n",
      "|    ep_rew_mean     | 532      |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 835      |\n",
      "|    total_timesteps | 133424   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.5    |\n",
      "|    critic_loss     | 0.818    |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | -1.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 133323   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=888.92 +/- 23.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 889      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.1    |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | -0.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 139899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 574      |\n",
      "| time/              |          |\n",
      "|    episodes        | 270      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 897      |\n",
      "|    total_timesteps | 143424   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -55.1    |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | -0.106   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 143323   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=677.28 +/- 281.50\n",
      "Episode length: 843.80 +/- 312.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 844      |\n",
      "|    mean_reward     | 677      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.3    |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 0.196    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 598      |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 943      |\n",
      "|    total_timesteps | 150598   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -56.5    |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 0.286    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 150497   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 612      |\n",
      "| time/              |          |\n",
      "|    episodes        | 290      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 985      |\n",
      "|    total_timesteps | 157797   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.5    |\n",
      "|    critic_loss     | 0.918    |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -0.0536  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 157696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=549.01 +/- 381.43\n",
      "Episode length: 633.40 +/- 449.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 633      |\n",
      "|    mean_reward     | 549      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.3    |\n",
      "|    critic_loss     | 0.955    |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | -0.982   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 628      |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 1028     |\n",
      "|    total_timesteps | 164986   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.3    |\n",
      "|    critic_loss     | 0.96     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 164885   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=748.32 +/- 365.98\n",
      "Episode length: 804.00 +/- 392.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 748      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.8    |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 626      |\n",
      "| time/              |          |\n",
      "|    episodes        | 310      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 1073     |\n",
      "|    total_timesteps | 172290   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.9    |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | 0.747    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 172189   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 629      |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 1116     |\n",
      "|    total_timesteps | 179426   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.4    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | 0.432    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179325   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=814.03 +/- 227.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 814      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -60.5    |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | 0.823    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 620      |\n",
      "| time/              |          |\n",
      "|    episodes        | 330      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 1172     |\n",
      "|    total_timesteps | 188608   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.7    |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | -0.396   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 188507   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=775.93 +/- 179.36\n",
      "Episode length: 877.80 +/- 244.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 878      |\n",
      "|    mean_reward     | 776      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.7    |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 655      |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1233     |\n",
      "|    total_timesteps | 198608   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.6    |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -0.748   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 198507   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=790.56 +/- 294.37\n",
      "Episode length: 833.20 +/- 333.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 833      |\n",
      "|    mean_reward     | 791      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.7    |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | 0.738    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 199899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 813      |\n",
      "|    ep_rew_mean     | 654      |\n",
      "| time/              |          |\n",
      "|    episodes        | 350      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1286     |\n",
      "|    total_timesteps | 207252   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.7    |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | 0.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 207151   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=909.23 +/- 58.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 909      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -60.9    |\n",
      "|    critic_loss     | 0.997    |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | -0.476   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 209899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 663      |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1330     |\n",
      "|    total_timesteps | 214361   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -62.9    |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.945    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 214260   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=635.21 +/- 282.71\n",
      "Episode length: 818.20 +/- 363.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 818      |\n",
      "|    mean_reward     | 635      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.3    |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | -0.245   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 651      |\n",
      "| time/              |          |\n",
      "|    episodes        | 370      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1385     |\n",
      "|    total_timesteps | 223487   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.6    |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 0.166    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 223386   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=947.12 +/- 65.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 947      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.3    |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.623    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 229899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 659      |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1430     |\n",
      "|    total_timesteps | 230803   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.9    |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | -0.154   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 230702   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 667      |\n",
      "| time/              |          |\n",
      "|    episodes        | 390      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1475     |\n",
      "|    total_timesteps | 238265   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -66.8    |\n",
      "|    critic_loss     | 0.697    |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 0.221    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 238164   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=1036.11 +/- 154.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.3    |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 0.688    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 239899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 667      |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1514     |\n",
      "|    total_timesteps | 244586   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.2    |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | -0.293   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 244485   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=673.75 +/- 269.70\n",
      "Episode length: 619.80 +/- 328.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 620      |\n",
      "|    mean_reward     | 674      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.6    |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 249899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 679      |\n",
      "| time/              |          |\n",
      "|    episodes        | 410      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1559     |\n",
      "|    total_timesteps | 252087   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.3    |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 251986   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=932.41 +/- 372.94\n",
      "Episode length: 841.60 +/- 316.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 842      |\n",
      "|    mean_reward     | 932      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -67.6    |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 0.066    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 694      |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1609     |\n",
      "|    total_timesteps | 260351   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.6    |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 0.608    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 260250   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 702      |\n",
      "| time/              |          |\n",
      "|    episodes        | 430      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1664     |\n",
      "|    total_timesteps | 269583   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -67.8    |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -0.987   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269482   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=836.19 +/- 422.40\n",
      "Episode length: 826.60 +/- 346.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 836      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -66.8    |\n",
      "|    critic_loss     | 2.07     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -0.522   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 718      |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1719     |\n",
      "|    total_timesteps | 278641   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -68.4    |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.939   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 278540   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=1046.75 +/- 364.28\n",
      "Episode length: 868.80 +/- 262.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 869      |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69      |\n",
      "|    critic_loss     | 2.25     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 0.347    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 279899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 730      |\n",
      "| time/              |          |\n",
      "|    episodes        | 450      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1761     |\n",
      "|    total_timesteps | 285618   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69      |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | 0.0859   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 285517   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=1242.62 +/- 257.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -70.9    |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 0.305    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 289899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 757      |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1810     |\n",
      "|    total_timesteps | 293533   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -71      |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -0.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 293432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=624.84 +/- 396.95\n",
      "Episode length: 516.00 +/- 410.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 516      |\n",
      "|    mean_reward     | 625      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -71.7    |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.771    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 781      |\n",
      "| time/              |          |\n",
      "|    episodes        | 470      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1864     |\n",
      "|    total_timesteps | 302563   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -72.4    |\n",
      "|    critic_loss     | 2.07     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 0.0852   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 302462   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=936.97 +/- 235.97\n",
      "Episode length: 561.20 +/- 222.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 561      |\n",
      "|    mean_reward     | 937      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -73.6    |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | 0.414    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 805      |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1915     |\n",
      "|    total_timesteps | 311082   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -75.6    |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | 0.565    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 310981   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 834      |\n",
      "| time/              |          |\n",
      "|    episodes        | 490      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1953     |\n",
      "|    total_timesteps | 317467   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -75.3    |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 317366   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1045.82 +/- 570.19\n",
      "Episode length: 804.40 +/- 391.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -74.2    |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 881      |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2004     |\n",
      "|    total_timesteps | 325941   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.2    |\n",
      "|    critic_loss     | 3.16     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 0.421    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 325840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=1530.58 +/- 668.05\n",
      "Episode length: 851.60 +/- 296.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 852      |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -77.8    |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | 0.162    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 329899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 922      |\n",
      "| time/              |          |\n",
      "|    episodes        | 510      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2052     |\n",
      "|    total_timesteps | 333897   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.1    |\n",
      "|    critic_loss     | 3.54     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 0.174    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 333796   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=1559.38 +/- 439.21\n",
      "Episode length: 916.40 +/- 167.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 916      |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -81.2    |\n",
      "|    critic_loss     | 3.06     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | 0.373    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 339899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 983      |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2105     |\n",
      "|    total_timesteps | 342676   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.1    |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 342575   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 530      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2146     |\n",
      "|    total_timesteps | 349617   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84.4    |\n",
      "|    critic_loss     | 2.7      |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | -0.192   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349516   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=1771.06 +/- 206.85\n",
      "Episode length: 937.20 +/- 78.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 937      |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.4    |\n",
      "|    critic_loss     | 4.9      |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 0.72     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2197     |\n",
      "|    total_timesteps | 358081   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -85.2    |\n",
      "|    critic_loss     | 2.93     |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | 0.384    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 357980   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=1594.24 +/- 487.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -88.2    |\n",
      "|    critic_loss     | 2.91     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -0.666   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 550      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2243     |\n",
      "|    total_timesteps | 365638   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -91.8    |\n",
      "|    critic_loss     | 4.47     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | 0.231    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 365537   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=1092.81 +/- 708.19\n",
      "Episode length: 768.80 +/- 375.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 769      |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -89.7    |\n",
      "|    critic_loss     | 2.46     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -0.429   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2295     |\n",
      "|    total_timesteps | 374258   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -93.3    |\n",
      "|    critic_loss     | 3.56     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 374157   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=1237.38 +/- 740.44\n",
      "Episode length: 783.60 +/- 370.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 784      |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.2    |\n",
      "|    critic_loss     | 3.42     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | -0.596   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 379899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 570      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2343     |\n",
      "|    total_timesteps | 382099   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -96.7    |\n",
      "|    critic_loss     | 3.54     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | -0.755   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 381998   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=1802.85 +/- 479.53\n",
      "Episode length: 992.00 +/- 16.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 992      |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -99.1    |\n",
      "|    critic_loss     | 4.29     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 389899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 580      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2397     |\n",
      "|    total_timesteps | 391034   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -99.3    |\n",
      "|    critic_loss     | 4.98     |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 390933   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 590      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2446     |\n",
      "|    total_timesteps | 399338   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -108     |\n",
      "|    critic_loss     | 5        |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -0.0224  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399237   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1716.29 +/- 566.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -102     |\n",
      "|    critic_loss     | 6.73     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | 1.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 600      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2499     |\n",
      "|    total_timesteps | 408071   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -106     |\n",
      "|    critic_loss     | 3.53     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.207    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 407970   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=1520.36 +/- 779.14\n",
      "Episode length: 751.40 +/- 305.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 751      |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -103     |\n",
      "|    critic_loss     | 4.78     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.0133   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 409899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 610      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2559     |\n",
      "|    total_timesteps | 418071   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 4.97     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 1.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 417970   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=1300.18 +/- 613.63\n",
      "Episode length: 645.80 +/- 312.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 646      |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -101     |\n",
      "|    critic_loss     | 9.44     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.0378   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 419899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 620      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2612     |\n",
      "|    total_timesteps | 426905   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -114     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 426804   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=1995.84 +/- 619.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -108     |\n",
      "|    critic_loss     | 4.41     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.834   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 429899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 630      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2669     |\n",
      "|    total_timesteps | 436287   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -108     |\n",
      "|    critic_loss     | 3.79     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.588   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 436186   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=1173.41 +/- 333.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -118     |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.248   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 640      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2723     |\n",
      "|    total_timesteps | 445324   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -117     |\n",
      "|    critic_loss     | 4.13     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 445223   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=2375.30 +/- 264.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -121     |\n",
      "|    critic_loss     | 5.52     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.671    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 449899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 650      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2784     |\n",
      "|    total_timesteps | 455324   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -116     |\n",
      "|    critic_loss     | 4.81     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -0.243   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 455223   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=1063.87 +/- 555.42\n",
      "Episode length: 748.20 +/- 362.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 748      |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -118     |\n",
      "|    critic_loss     | 3.32     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | 0.847    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 459899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 660      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2833     |\n",
      "|    total_timesteps | 463450   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -124     |\n",
      "|    critic_loss     | 5.34     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.308    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 463349   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=2542.20 +/- 88.50\n",
      "Episode length: 976.80 +/- 46.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 977      |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -124     |\n",
      "|    critic_loss     | 5.44     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 469899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 670      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2874     |\n",
      "|    total_timesteps | 470178   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -128     |\n",
      "|    critic_loss     | 5.46     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -0.561   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 470077   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 680      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2922     |\n",
      "|    total_timesteps | 478375   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -131     |\n",
      "|    critic_loss     | 4.44     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | 0.892    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 478274   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=1679.48 +/- 261.93\n",
      "Episode length: 814.00 +/- 228.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 814      |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -124     |\n",
      "|    critic_loss     | 15.2     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.402    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 479899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 690      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2979     |\n",
      "|    total_timesteps | 487906   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -126     |\n",
      "|    critic_loss     | 4        |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.473   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 487805   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=2702.34 +/- 71.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -130     |\n",
      "|    critic_loss     | 8.61     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 2.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 489899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 700      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3021     |\n",
      "|    total_timesteps | 494891   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -131     |\n",
      "|    critic_loss     | 5.16     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.946    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 494790   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=2618.93 +/- 83.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -128     |\n",
      "|    critic_loss     | 7.42     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.584    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 710      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3081     |\n",
      "|    total_timesteps | 504805   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -133     |\n",
      "|    critic_loss     | 5.42     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.0202  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 504704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=1582.10 +/- 505.26\n",
      "Episode length: 978.00 +/- 44.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 978      |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -139     |\n",
      "|    critic_loss     | 6.84     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.0775  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 509899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 720      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3132     |\n",
      "|    total_timesteps | 513255   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 8.62     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.443    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 513154   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=2532.45 +/- 71.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -136     |\n",
      "|    critic_loss     | 4.08     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 519899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 730      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3192     |\n",
      "|    total_timesteps | 523255   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 6.74     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.148   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 523154   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=1868.61 +/- 924.27\n",
      "Episode length: 954.60 +/- 90.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 955      |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 4.54     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 0.837    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 529899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 740      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3246     |\n",
      "|    total_timesteps | 532142   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 9.09     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 0.897    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 532041   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=2759.01 +/- 271.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 5.29     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | 0.464    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 539899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 750      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3302     |\n",
      "|    total_timesteps | 541351   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 6.3      |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 541250   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 760      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3349     |\n",
      "|    total_timesteps | 549359   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 5.97     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549258   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=2413.57 +/- 571.40\n",
      "Episode length: 901.80 +/- 196.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 902      |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -141     |\n",
      "|    critic_loss     | 5.01     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 770      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3402     |\n",
      "|    total_timesteps | 558183   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -146     |\n",
      "|    critic_loss     | 9.56     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.529    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 558082   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=1924.18 +/- 833.76\n",
      "Episode length: 979.60 +/- 40.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 980      |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.935    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 559899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 780      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3457     |\n",
      "|    total_timesteps | 567244   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 4.87     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.307   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 567143   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=2491.58 +/- 937.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -145     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.529   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 569899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 790      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3499     |\n",
      "|    total_timesteps | 574115   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -152     |\n",
      "|    critic_loss     | 4.94     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 574014   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=2347.43 +/- 682.41\n",
      "Episode length: 911.00 +/- 178.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 911      |\n",
      "|    mean_reward     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 8.19     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.973    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 579899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 800      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3559     |\n",
      "|    total_timesteps | 584115   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -152     |\n",
      "|    critic_loss     | 7.12     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.158   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 584014   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=1747.08 +/- 775.52\n",
      "Episode length: 722.40 +/- 342.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 722      |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -151     |\n",
      "|    critic_loss     | 8.52     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.755    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 589899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 810      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3603     |\n",
      "|    total_timesteps | 591374   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -150     |\n",
      "|    critic_loss     | 5.36     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.0292  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 591273   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=2019.67 +/- 1183.46\n",
      "Episode length: 701.60 +/- 396.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 702      |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -155     |\n",
      "|    critic_loss     | 11.6     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.616    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 599899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 820      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3657     |\n",
      "|    total_timesteps | 600438   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -154     |\n",
      "|    critic_loss     | 5.6      |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -1.55    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 600337   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 830      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3713     |\n",
      "|    total_timesteps | 609812   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -164     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.793    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609711   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=2449.39 +/- 649.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -158     |\n",
      "|    critic_loss     | 9.67     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 840      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3771     |\n",
      "|    total_timesteps | 619536   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619435   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=3250.31 +/- 61.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.944   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 850      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3824     |\n",
      "|    total_timesteps | 628239   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -157     |\n",
      "|    critic_loss     | 5.12     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 628138   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=3031.97 +/- 82.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -163     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.671    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 629899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 860      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3871     |\n",
      "|    total_timesteps | 635912   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 8.51     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.861   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 635811   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=2363.25 +/- 1145.92\n",
      "Episode length: 820.40 +/- 359.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -155     |\n",
      "|    critic_loss     | 7.58     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.031   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 639899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 870      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3927     |\n",
      "|    total_timesteps | 645166   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 11       |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 645065   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=2334.76 +/- 883.28\n",
      "Episode length: 769.60 +/- 298.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 770      |\n",
      "|    mean_reward     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 7.64     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.267    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 649899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 880      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3987     |\n",
      "|    total_timesteps | 655166   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -164     |\n",
      "|    critic_loss     | 7.29     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.175   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 655065   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=2637.44 +/- 741.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 7.08     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 0.773    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 659899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 890      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4032     |\n",
      "|    total_timesteps | 662644   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -165     |\n",
      "|    critic_loss     | 6.92     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.0746   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 662543   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=2050.90 +/- 1149.03\n",
      "Episode length: 638.40 +/- 363.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 638      |\n",
      "|    mean_reward     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -173     |\n",
      "|    critic_loss     | 7.82     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.656   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 900      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4079     |\n",
      "|    total_timesteps | 670450   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 6.98     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -0.468   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 670349   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 910      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4129     |\n",
      "|    total_timesteps | 678962   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 9.2      |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.716   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 678861   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=2766.69 +/- 671.56\n",
      "Episode length: 853.80 +/- 214.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 854      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 8.07     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.303   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 679899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 920      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4175     |\n",
      "|    total_timesteps | 686680   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 8.54     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 686579   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=2903.51 +/- 725.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -169     |\n",
      "|    critic_loss     | 8.01     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.0953  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 689899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 930      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4235     |\n",
      "|    total_timesteps | 696680   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 8.19     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 696579   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=2757.14 +/- 1078.33\n",
      "Episode length: 840.40 +/- 319.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 840      |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -173     |\n",
      "|    critic_loss     | 6.43     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.0634  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 940      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4291     |\n",
      "|    total_timesteps | 706007   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 9.95     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 705906   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=2723.99 +/- 1277.46\n",
      "Episode length: 812.60 +/- 374.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 813      |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.637    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 709899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 950      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4344     |\n",
      "|    total_timesteps | 714807   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -175     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.704   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 714706   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=3372.93 +/- 120.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 7.25     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -1.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 719899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 960      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4392     |\n",
      "|    total_timesteps | 722719   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 7.88     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.223    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 722618   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=2674.37 +/- 1185.47\n",
      "Episode length: 822.80 +/- 354.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 823      |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -179     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.395   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 729899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 970      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4452     |\n",
      "|    total_timesteps | 732719   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -183     |\n",
      "|    critic_loss     | 8.96     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.244   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 732618   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=3298.05 +/- 226.87\n",
      "Episode length: 970.80 +/- 58.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 971      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -186     |\n",
      "|    critic_loss     | 9.7      |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 1.61     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 739899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 980      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4507     |\n",
      "|    total_timesteps | 741807   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 5.74     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.521   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 741706   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=2640.41 +/- 734.19\n",
      "Episode length: 915.80 +/- 168.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 916      |\n",
      "|    mean_reward     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -194     |\n",
      "|    critic_loss     | 21.7     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 749899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 990      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4559     |\n",
      "|    total_timesteps | 750518   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.339    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 750417   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1000     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4603     |\n",
      "|    total_timesteps | 757999   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 8.28     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 757898   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=3421.78 +/- 39.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -183     |\n",
      "|    critic_loss     | 9.4      |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 759899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1010     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4663     |\n",
      "|    total_timesteps | 767999   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 9.1      |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.801   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 767898   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=3396.51 +/- 37.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -184     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 769899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1020     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4716     |\n",
      "|    total_timesteps | 776695   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -189     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.914    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 776594   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=2389.58 +/- 1226.13\n",
      "Episode length: 836.60 +/- 310.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 837      |\n",
      "|    mean_reward     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -186     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.296    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 779899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1030     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4771     |\n",
      "|    total_timesteps | 785900   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -183     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.296   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 785799   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=3029.40 +/- 966.68\n",
      "Episode length: 865.60 +/- 268.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 866      |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -192     |\n",
      "|    critic_loss     | 9.6      |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.124   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 789899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1040     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4817     |\n",
      "|    total_timesteps | 793473   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.266   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 793372   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=2791.49 +/- 657.41\n",
      "Episode length: 934.60 +/- 130.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 935      |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -207     |\n",
      "|    critic_loss     | 6.15     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.649   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1050     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4877     |\n",
      "|    total_timesteps | 803473   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 23.2     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 803372   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=3115.79 +/- 510.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -191     |\n",
      "|    critic_loss     | 9.67     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.768    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 809899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1060     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4932     |\n",
      "|    total_timesteps | 812658   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -199     |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | -0.558   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 812557   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=1850.40 +/- 1655.19\n",
      "Episode length: 770.00 +/- 298.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 770      |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 6.71     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -1.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 819899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1070     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4982     |\n",
      "|    total_timesteps | 820938   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -202     |\n",
      "|    critic_loss     | 9.26     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.475   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 820837   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1080     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5029     |\n",
      "|    total_timesteps | 828915   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -203     |\n",
      "|    critic_loss     | 9.18     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.184    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 828814   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=3606.73 +/- 52.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -194     |\n",
      "|    critic_loss     | 7.4      |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.183    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 829899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1090     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5082     |\n",
      "|    total_timesteps | 837741   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -199     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.175   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 837640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=2627.39 +/- 1382.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 6.44     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -2.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 839899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1100     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5126     |\n",
      "|    total_timesteps | 844951   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -208     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.358    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 844850   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=2947.99 +/- 997.30\n",
      "Episode length: 863.80 +/- 272.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 864      |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -200     |\n",
      "|    critic_loss     | 19       |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.349   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 849899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1110     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5168     |\n",
      "|    total_timesteps | 851915   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -206     |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.399    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 851814   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=3595.78 +/- 130.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -207     |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | 0.476    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 859899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 840      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1120     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5220     |\n",
      "|    total_timesteps | 860665   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -201     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 860564   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=2032.21 +/- 1343.51\n",
      "Episode length: 735.60 +/- 360.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 736      |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -209     |\n",
      "|    critic_loss     | 8.58     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 869899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1130     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5280     |\n",
      "|    total_timesteps | 870665   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -203     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.399    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 870564   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1140     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5333     |\n",
      "|    total_timesteps | 879697   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -205     |\n",
      "|    critic_loss     | 8.67     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -0.178   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879596   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=3022.98 +/- 583.70\n",
      "Episode length: 869.20 +/- 171.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 869      |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -210     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.0936   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1150     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5387     |\n",
      "|    total_timesteps | 888775   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -215     |\n",
      "|    critic_loss     | 7.06     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.677    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 888674   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=1971.82 +/- 1302.07\n",
      "Episode length: 816.60 +/- 366.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 817      |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 889899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1160     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5435     |\n",
      "|    total_timesteps | 896745   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 6.96     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 0.273    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 896644   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=3296.50 +/- 792.86\n",
      "Episode length: 900.40 +/- 199.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 900      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -224     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 840      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1170     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5485     |\n",
      "|    total_timesteps | 904907   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 19.9     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.241   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 904806   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=2310.64 +/- 1169.00\n",
      "Episode length: 931.20 +/- 137.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 931      |\n",
      "|    mean_reward     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 13.4     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.426    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 909899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1180     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5545     |\n",
      "|    total_timesteps | 914907   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -214     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.992    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 914806   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=2568.97 +/- 1824.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -212     |\n",
      "|    critic_loss     | 8.86     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -0.678   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 919899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1190     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5601     |\n",
      "|    total_timesteps | 924175   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 924074   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=2563.77 +/- 878.60\n",
      "Episode length: 814.60 +/- 241.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 815      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 22       |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 929899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1200     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5656     |\n",
      "|    total_timesteps | 933310   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -215     |\n",
      "|    critic_loss     | 15       |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.277    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 933209   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=2790.12 +/- 1155.61\n",
      "Episode length: 887.20 +/- 225.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 887      |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 0.189    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 939899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1210     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5705     |\n",
      "|    total_timesteps | 941497   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 9.11     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 941396   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=2745.33 +/- 1103.26\n",
      "Episode length: 840.60 +/- 318.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 841      |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -0.293   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 949899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1220     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5764     |\n",
      "|    total_timesteps | 951354   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.0873   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 951253   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1230     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5812     |\n",
      "|    total_timesteps | 959553   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 8.49     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.419    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959452   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=1431.12 +/- 1106.54\n",
      "Episode length: 724.20 +/- 338.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 724      |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -222     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | -0.418   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1240     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5857     |\n",
      "|    total_timesteps | 967115   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 967014   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=3281.88 +/- 659.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 18.3     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | 0.358    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 969899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1250     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5909     |\n",
      "|    total_timesteps | 975703   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -216     |\n",
      "|    critic_loss     | 9.31     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 0.848    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 975602   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=2781.90 +/- 1011.87\n",
      "Episode length: 921.20 +/- 157.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 921      |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 0.217    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 979899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1260     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5950     |\n",
      "|    total_timesteps | 982447   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -222     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | -0.771   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 982346   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=2907.40 +/- 1002.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.0081  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 989899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1270     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5997     |\n",
      "|    total_timesteps | 990210   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -1.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 990109   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1280     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6050     |\n",
      "|    total_timesteps | 999104   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -225     |\n",
      "|    critic_loss     | 6.75     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.196   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999003   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=3652.30 +/- 83.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 8.53     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 0.832    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1290     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6099     |\n",
      "|    total_timesteps | 1007254  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -0.727   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1007153  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=2201.93 +/- 959.98\n",
      "Episode length: 870.80 +/- 188.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 871      |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -225     |\n",
      "|    critic_loss     | 7.63     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1300     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6152     |\n",
      "|    total_timesteps | 1016121  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -228     |\n",
      "|    critic_loss     | 9.74     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.9      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1016020  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=3126.47 +/- 1253.87\n",
      "Episode length: 835.60 +/- 328.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 836      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -230     |\n",
      "|    critic_loss     | 7.59     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.531    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1310     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6214     |\n",
      "|    total_timesteps | 1026121  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -229     |\n",
      "|    critic_loss     | 9.01     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1026020  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=2444.46 +/- 1388.57\n",
      "Episode length: 859.80 +/- 280.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 22.4     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.181   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1320     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6264     |\n",
      "|    total_timesteps | 1034193  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 13.6     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.238   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1034092  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=2517.30 +/- 1152.45\n",
      "Episode length: 672.20 +/- 291.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 672      |\n",
      "|    mean_reward     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.147    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1330     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6317     |\n",
      "|    total_timesteps | 1042496  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -229     |\n",
      "|    critic_loss     | 8.1      |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 2.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1042395  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=3472.82 +/- 607.85\n",
      "Episode length: 919.60 +/- 160.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 920      |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -0.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1340     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6371     |\n",
      "|    total_timesteps | 1050969  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -236     |\n",
      "|    critic_loss     | 22.8     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.617    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1050868  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=2080.48 +/- 1559.12\n",
      "Episode length: 700.00 +/- 377.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 700      |\n",
      "|    mean_reward     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -0.642   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1350     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6430     |\n",
      "|    total_timesteps | 1060136  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -234     |\n",
      "|    critic_loss     | 5.99     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.263   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1060035  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1360     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6488     |\n",
      "|    total_timesteps | 1069463  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -240     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.501    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069362  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=2062.94 +/- 1330.74\n",
      "Episode length: 702.40 +/- 367.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 702      |\n",
      "|    mean_reward     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -238     |\n",
      "|    critic_loss     | 9.63     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.61     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1370     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6529     |\n",
      "|    total_timesteps | 1076323  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -235     |\n",
      "|    critic_loss     | 7.12     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.812   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1076222  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=3143.17 +/- 1350.97\n",
      "Episode length: 823.60 +/- 352.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -235     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.667   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1380     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6581     |\n",
      "|    total_timesteps | 1084974  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -233     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.788   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1084873  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=3886.55 +/- 142.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -244     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.538   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1089899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1390     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6637     |\n",
      "|    total_timesteps | 1094255  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 0.558    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1094154  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=3094.73 +/- 1041.57\n",
      "Episode length: 863.80 +/- 272.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 864      |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -243     |\n",
      "|    critic_loss     | 9.08     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1400     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6688     |\n",
      "|    total_timesteps | 1102633  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -243     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1102532  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=2809.93 +/- 1225.59\n",
      "Episode length: 896.60 +/- 206.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 897      |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -236     |\n",
      "|    critic_loss     | 8.06     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.505   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1410     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6742     |\n",
      "|    total_timesteps | 1111584  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -231     |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.356    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1111483  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=2807.10 +/- 1238.79\n",
      "Episode length: 894.20 +/- 211.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 894      |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -241     |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.888    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1420     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6793     |\n",
      "|    total_timesteps | 1120010  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -236     |\n",
      "|    critic_loss     | 17.7     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.658    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119909  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1430     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6849     |\n",
      "|    total_timesteps | 1129618  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129517  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=3177.11 +/- 1359.46\n",
      "Episode length: 824.20 +/- 351.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -244     |\n",
      "|    critic_loss     | 8.87     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.00847  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1440     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6900     |\n",
      "|    total_timesteps | 1138073  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -241     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.456   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1137972  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=3530.18 +/- 500.75\n",
      "Episode length: 936.20 +/- 127.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 936      |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 25       |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.145   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1450     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6940     |\n",
      "|    total_timesteps | 1144644  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -244     |\n",
      "|    critic_loss     | 20.5     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.177    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1144543  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=2866.66 +/- 1135.86\n",
      "Episode length: 881.40 +/- 237.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 881      |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -249     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.0529   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1460     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 7000     |\n",
      "|    total_timesteps | 1154566  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -245     |\n",
      "|    critic_loss     | 8.17     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -0.415   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1154465  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=2244.86 +/- 1404.41\n",
      "Episode length: 582.20 +/- 353.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 582      |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -243     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.393   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1470     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 7049     |\n",
      "|    total_timesteps | 1162889  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -253     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | 0.839    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1162788  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=3013.95 +/- 1026.40\n",
      "Episode length: 883.40 +/- 233.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 883      |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -247     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -0.142   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1480     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 7104     |\n",
      "|    total_timesteps | 1171999  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -0.858   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1171898  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=3572.42 +/- 635.99\n",
      "Episode length: 913.00 +/- 174.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 913      |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 16.8     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -0.235   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1490     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 7156     |\n",
      "|    total_timesteps | 1180580  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 9.47     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 0.245    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1180479  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1500     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7202     |\n",
      "|    total_timesteps | 1188621  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -253     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.927   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1188520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=1398.87 +/- 561.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 19.5     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.511   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1510     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7250     |\n",
      "|    total_timesteps | 1196446  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.353    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1196345  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=3802.45 +/- 245.40\n",
      "Episode length: 980.80 +/- 38.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 981      |\n",
      "|    mean_reward     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 31.4     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | -1.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1520     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7309     |\n",
      "|    total_timesteps | 1206295  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1206194  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=3740.05 +/- 239.20\n",
      "Episode length: 987.40 +/- 25.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 987      |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -249     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -0.408   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1530     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7363     |\n",
      "|    total_timesteps | 1215158  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -257     |\n",
      "|    critic_loss     | 9.68     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -1.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1215057  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=3322.53 +/- 1193.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -247     |\n",
      "|    critic_loss     | 8.49     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 0.0747   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1540     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7417     |\n",
      "|    total_timesteps | 1224101  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -259     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 1        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=3311.16 +/- 1085.65\n",
      "Episode length: 827.20 +/- 264.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -262     |\n",
      "|    critic_loss     | 6.66     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | 0.189    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1550     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7475     |\n",
      "|    total_timesteps | 1233892  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1233791  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=3482.51 +/- 519.92\n",
      "Episode length: 897.80 +/- 126.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 898      |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 14.3     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | 0.963    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1560     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7525     |\n",
      "|    total_timesteps | 1242104  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1242003  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=3919.03 +/- 108.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.858    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1249899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1570     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7573     |\n",
      "|    total_timesteps | 1250123  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -269     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -0.275   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1250022  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1580     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7628     |\n",
      "|    total_timesteps | 1259560  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -266     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.106    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259459  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=2040.35 +/- 1355.49\n",
      "Episode length: 671.80 +/- 352.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 672      |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 6.07     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.265   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1590     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7679     |\n",
      "|    total_timesteps | 1268044  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -271     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.798   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1267943  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=2732.32 +/- 1004.50\n",
      "Episode length: 703.40 +/- 259.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 703      |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -271     |\n",
      "|    critic_loss     | 14       |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | 0.378    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1600     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7729     |\n",
      "|    total_timesteps | 1276338  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -266     |\n",
      "|    critic_loss     | 37       |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1276237  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=3777.18 +/- 272.27\n",
      "Episode length: 965.40 +/- 69.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 965      |\n",
      "|    mean_reward     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | 0.365    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1610     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7785     |\n",
      "|    total_timesteps | 1285785  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -273     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.424   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1285684  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=3672.21 +/- 570.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -278     |\n",
      "|    critic_loss     | 73.1     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1620     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7835     |\n",
      "|    total_timesteps | 1294051  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 25.7     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | -0.799   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1293950  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=3358.36 +/- 1199.38\n",
      "Episode length: 850.60 +/- 298.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 851      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -275     |\n",
      "|    critic_loss     | 8.96     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.699    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1630     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7884     |\n",
      "|    total_timesteps | 1302048  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1301947  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1640     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7924     |\n",
      "|    total_timesteps | 1308883  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 8.11     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.979   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1308782  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=3786.33 +/- 502.08\n",
      "Episode length: 942.80 +/- 114.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 943      |\n",
      "|    mean_reward     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 24       |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | -0.407   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1650     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7978     |\n",
      "|    total_timesteps | 1318032  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.331   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1317931  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=3852.84 +/- 406.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -277     |\n",
      "|    critic_loss     | 6.25     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1660     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8033     |\n",
      "|    total_timesteps | 1327152  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1327051  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=2852.20 +/- 1535.80\n",
      "Episode length: 810.00 +/- 380.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 810      |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1670     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8087     |\n",
      "|    total_timesteps | 1336059  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -0.608   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1335958  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=2853.58 +/- 993.35\n",
      "Episode length: 713.60 +/- 248.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 714      |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -1.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1680     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8143     |\n",
      "|    total_timesteps | 1345494  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 9.32     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.657   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345393  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=3482.61 +/- 887.28\n",
      "Episode length: 882.00 +/- 217.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 882      |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.708   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1690     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8182     |\n",
      "|    total_timesteps | 1351983  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -285     |\n",
      "|    critic_loss     | 18.4     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.899    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1351882  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1700     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8220     |\n",
      "|    total_timesteps | 1358465  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.395   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1358364  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=3300.09 +/- 1132.59\n",
      "Episode length: 964.40 +/- 71.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 964      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -283     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.761   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1710     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8271     |\n",
      "|    total_timesteps | 1366834  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 20.9     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1366733  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=3641.57 +/- 931.80\n",
      "Episode length: 886.80 +/- 226.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 887      |\n",
      "|    mean_reward     | 3.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 22.1     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.316    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1720     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8321     |\n",
      "|    total_timesteps | 1375217  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 13.9     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.489    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1375116  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=-534.12 +/- 826.89\n",
      "Episode length: 860.80 +/- 278.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 861      |\n",
      "|    mean_reward     | -534     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | 0.599    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1730     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8363     |\n",
      "|    total_timesteps | 1382027  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 38.9     |\n",
      "|    ent_coef        | 0.0649   |\n",
      "|    ent_coef_loss   | 2.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1381926  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1740     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8401     |\n",
      "|    total_timesteps | 1388475  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0505   |\n",
      "|    ent_coef_loss   | 0.116    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1388374  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=-171.95 +/- 548.72\n",
      "Episode length: 816.80 +/- 366.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 817      |\n",
      "|    mean_reward     | -172     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 27.4     |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | 2.67     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1750     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8456     |\n",
      "|    total_timesteps | 1397590  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 16.2     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.191    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1397489  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=1803.26 +/- 773.45\n",
      "Episode length: 496.40 +/- 254.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 496      |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 31.8     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.0121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1760     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8498     |\n",
      "|    total_timesteps | 1404682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 29.2     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1404581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=3471.44 +/- 822.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 786      |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1770     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8558     |\n",
      "|    total_timesteps | 1414682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.131   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1414581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=2839.93 +/- 1692.76\n",
      "Episode length: 937.40 +/- 125.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 937      |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 17       |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1780     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8610     |\n",
      "|    total_timesteps | 1423182  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 13.9     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.312   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1423081  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=2996.22 +/- 721.55\n",
      "Episode length: 754.40 +/- 188.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 754      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1790     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8661     |\n",
      "|    total_timesteps | 1431739  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1431638  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=3371.77 +/- 1470.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 0.981    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1800     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8721     |\n",
      "|    total_timesteps | 1441739  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 47.1     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 0.326    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1441638  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1810     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8759     |\n",
      "|    total_timesteps | 1448293  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 18.5     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.887   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1448192  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=2767.53 +/- 1389.70\n",
      "Episode length: 880.80 +/- 187.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 881      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.899   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1820     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8812     |\n",
      "|    total_timesteps | 1457085  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 89.5     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | 0.758    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1456984  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=4138.79 +/- 499.07\n",
      "Episode length: 954.40 +/- 91.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 954      |\n",
      "|    mean_reward     | 4.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -292     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.872   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1459899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1830     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8864     |\n",
      "|    total_timesteps | 1465584  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 23.3     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.207   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1465483  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=3611.16 +/- 1288.53\n",
      "Episode length: 846.80 +/- 306.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 847      |\n",
      "|    mean_reward     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 10       |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.648    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1840     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8915     |\n",
      "|    total_timesteps | 1474096  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 21.2     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.862   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1473995  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=3716.95 +/- 844.86\n",
      "Episode length: 901.80 +/- 196.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 902      |\n",
      "|    mean_reward     | 3.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 19       |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1850     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8969     |\n",
      "|    total_timesteps | 1483115  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.413   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1483014  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 840      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1860     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9002     |\n",
      "|    total_timesteps | 1488723  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.557    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1488622  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=2659.81 +/- 1885.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.555   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1870     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9045     |\n",
      "|    total_timesteps | 1495816  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 9.11     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.286    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1495715  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=4101.09 +/- 101.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 63.7     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.453    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1880     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9086     |\n",
      "|    total_timesteps | 1502645  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 41.9     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.809    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1502544  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=4180.35 +/- 113.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 32.4     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -0.599   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1509899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1890     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9137     |\n",
      "|    total_timesteps | 1511057  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 0.876    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1510956  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=2223.62 +/- 1684.10\n",
      "Episode length: 885.60 +/- 228.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 886      |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 10       |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.409   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1900     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9196     |\n",
      "|    total_timesteps | 1520893  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | 0.087    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1520792  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1910     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9226     |\n",
      "|    total_timesteps | 1526001  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 24.5     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.317   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1525900  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=3943.31 +/- 300.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 19.7     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.695    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1920     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9273     |\n",
      "|    total_timesteps | 1533767  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 1.72     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1533666  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=1980.61 +/- 1789.95\n",
      "Episode length: 628.00 +/- 455.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 628      |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 18       |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.876    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1930     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9322     |\n",
      "|    total_timesteps | 1542000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 34.3     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 1.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1541899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1940     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9368     |\n",
      "|    total_timesteps | 1549858  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 23.6     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549757  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=3708.70 +/- 689.10\n",
      "Episode length: 967.40 +/- 65.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 967      |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 1.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 741      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1950     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9412     |\n",
      "|    total_timesteps | 1557172  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1557071  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=3514.88 +/- 1586.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.433    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1960     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9458     |\n",
      "|    total_timesteps | 1564734  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 18.3     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | -0.231   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1564633  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=1602.71 +/- 1735.62\n",
      "Episode length: 402.00 +/- 413.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 402      |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 44.9     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 2.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 753      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1970     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9496     |\n",
      "|    total_timesteps | 1571110  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 26.5     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1571009  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1980     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9546     |\n",
      "|    total_timesteps | 1579771  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | 0.208    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579670  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=726.66 +/- 556.64\n",
      "Episode length: 515.40 +/- 405.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 515      |\n",
      "|    mean_reward     | 727      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 18.5     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | 0.436    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1990     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9590     |\n",
      "|    total_timesteps | 1587024  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 16       |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.224   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1586923  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=3293.07 +/- 1034.27\n",
      "Episode length: 791.80 +/- 264.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 792      |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 18.2     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.0647   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2000     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9632     |\n",
      "|    total_timesteps | 1594016  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1593915  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=3133.37 +/- 1790.89\n",
      "Episode length: 938.00 +/- 124.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 938      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 9.05     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 769      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2010     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9685     |\n",
      "|    total_timesteps | 1602854  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1602753  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=2259.30 +/- 1173.08\n",
      "Episode length: 800.60 +/- 284.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 801      |\n",
      "|    mean_reward     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 20.3     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.0881   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2020     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9739     |\n",
      "|    total_timesteps | 1611822  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 16.9     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.00702  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1611721  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 769      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2030     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9780     |\n",
      "|    total_timesteps | 1618949  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 9.44     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1618848  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=4044.19 +/- 107.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 0.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2040     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9829     |\n",
      "|    total_timesteps | 1627083  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 87.7     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.707    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1626982  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=4266.50 +/- 132.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | -0.109   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1629899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2050     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9889     |\n",
      "|    total_timesteps | 1637056  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -0.994   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1636955  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=2834.31 +/- 1105.74\n",
      "Episode length: 684.80 +/- 263.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 685      |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2060     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9945     |\n",
      "|    total_timesteps | 1646349  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -0.761   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1646248  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=3603.95 +/- 1010.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.622   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2070     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9997     |\n",
      "|    total_timesteps | 1654948  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.506    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1654847  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=2654.35 +/- 1907.50\n",
      "Episode length: 637.80 +/- 446.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 638      |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 29       |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.0249   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2080     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10050    |\n",
      "|    total_timesteps | 1663954  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 66.6     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1663853  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=3690.26 +/- 1107.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 2.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2090     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10102    |\n",
      "|    total_timesteps | 1672513  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 6.88     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.734    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1672412  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=4110.08 +/- 158.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 23.3     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2100     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10149    |\n",
      "|    total_timesteps | 1680296  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 15.8     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | -0.157   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1680195  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2110     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10200    |\n",
      "|    total_timesteps | 1688968  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 0.769    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1688867  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=3355.18 +/- 853.15\n",
      "Episode length: 894.40 +/- 203.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 894      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 63.6     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2120     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10249    |\n",
      "|    total_timesteps | 1697096  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 14.3     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.0446  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1696995  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=3308.18 +/- 1260.67\n",
      "Episode length: 942.20 +/- 115.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 942      |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | 0.758    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2130     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10297    |\n",
      "|    total_timesteps | 1704966  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 16.2     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | 0.431    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1704865  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=3088.95 +/- 1314.45\n",
      "Episode length: 881.80 +/- 236.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 882      |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 22.2     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.629    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2140     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10339    |\n",
      "|    total_timesteps | 1711965  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 75.3     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.591    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1711864  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2150     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10386    |\n",
      "|    total_timesteps | 1719912  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.884   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719811  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=3838.01 +/- 915.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 14       |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2160     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10443    |\n",
      "|    total_timesteps | 1729439  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729338  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=2456.31 +/- 1652.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 30.9     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | 0.554    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2170     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10503    |\n",
      "|    total_timesteps | 1739335  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | -0.729   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739234  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=2358.41 +/- 1618.26\n",
      "Episode length: 802.60 +/- 394.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 803      |\n",
      "|    mean_reward     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | -0.933   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2180     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10548    |\n",
      "|    total_timesteps | 1746760  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 14       |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1746659  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=4236.68 +/- 115.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | -0.688   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2190     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10601    |\n",
      "|    total_timesteps | 1755509  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1755408  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=3233.95 +/- 1319.69\n",
      "Episode length: 892.20 +/- 215.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 892      |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | -0.357   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2200     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10648    |\n",
      "|    total_timesteps | 1763295  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 20.8     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | -0.716   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1763194  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=3163.87 +/- 1180.81\n",
      "Episode length: 903.40 +/- 193.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 903      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | 0.804    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 832      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2210     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10701    |\n",
      "|    total_timesteps | 1772150  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | -0.419   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1772049  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=3887.47 +/- 633.37\n",
      "Episode length: 931.20 +/- 137.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 931      |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | -0.102   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2220     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10753    |\n",
      "|    total_timesteps | 1780579  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 0.107    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1780478  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2230     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10804    |\n",
      "|    total_timesteps | 1789383  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | 0.227    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789282  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=4220.06 +/- 131.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 21.9     |\n",
      "|    ent_coef        | 0.0377   |\n",
      "|    ent_coef_loss   | -0.597   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2240     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10860    |\n",
      "|    total_timesteps | 1798695  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 22.3     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | 0.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1798594  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=3997.04 +/- 341.00\n",
      "Episode length: 958.60 +/- 82.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 959      |\n",
      "|    mean_reward     | 4e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.189   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2250     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10917    |\n",
      "|    total_timesteps | 1808144  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 73.6     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | 0.136    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1808043  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=3443.02 +/- 741.92\n",
      "Episode length: 912.20 +/- 175.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 912      |\n",
      "|    mean_reward     | 3.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 15.2     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | -2.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2260     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10966    |\n",
      "|    total_timesteps | 1816344  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 9.5      |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | -0.108   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1816243  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=3572.93 +/- 1082.08\n",
      "Episode length: 834.40 +/- 235.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 834      |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | 0.923    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2270     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11015    |\n",
      "|    total_timesteps | 1824374  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -0.745   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1824273  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=2577.75 +/- 1832.97\n",
      "Episode length: 907.40 +/- 185.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 907      |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | 0.0937   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2280     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11071    |\n",
      "|    total_timesteps | 1833671  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | -0.107   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1833570  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=1802.33 +/- 1026.71\n",
      "Episode length: 666.60 +/- 292.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 20.3     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -0.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2290     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11130    |\n",
      "|    total_timesteps | 1843598  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 23.9     |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.315    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1843497  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1850000, episode_reward=1981.18 +/- 1958.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -0.445   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2300     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11181    |\n",
      "|    total_timesteps | 1852002  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -0.417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1851901  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=2707.14 +/- 1778.83\n",
      "Episode length: 803.40 +/- 295.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 803      |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | -0.207   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2310     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11239    |\n",
      "|    total_timesteps | 1861753  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 2.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1861652  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=3207.76 +/- 1621.87\n",
      "Episode length: 743.60 +/- 367.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 744      |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 30.4     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | 0.678    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2320     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11289    |\n",
      "|    total_timesteps | 1870017  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | 0.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869916  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2330     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11331    |\n",
      "|    total_timesteps | 1877193  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -0.0795  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1877092  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=3932.03 +/- 1003.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 16.4     |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | 0.317    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2340     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11373    |\n",
      "|    total_timesteps | 1884132  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 0.151    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1884031  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=2623.07 +/- 1527.27\n",
      "Episode length: 611.60 +/- 342.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 612      |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 72.6     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | -0.0447  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2350     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11425    |\n",
      "|    total_timesteps | 1892887  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | 0.167    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1892786  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=3693.35 +/- 787.12\n",
      "Episode length: 856.40 +/- 199.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 856      |\n",
      "|    mean_reward     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 22       |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.721   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2360     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11474    |\n",
      "|    total_timesteps | 1900883  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1900782  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=3674.45 +/- 1094.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 34.4     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.954   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2370     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11533    |\n",
      "|    total_timesteps | 1910693  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 32.2     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1910592  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=3733.13 +/- 1041.01\n",
      "Episode length: 968.60 +/- 62.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 969      |\n",
      "|    mean_reward     | 3.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | 0.141    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2380     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11593    |\n",
      "|    total_timesteps | 1920693  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | -1.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1920592  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2390     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11638    |\n",
      "|    total_timesteps | 1928444  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.185   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1928343  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=4115.64 +/- 516.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | 0.145    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2400     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11694    |\n",
      "|    total_timesteps | 1937747  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.495   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1937646  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=3123.11 +/- 1614.41\n",
      "Episode length: 719.20 +/- 369.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 719      |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 66.6     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | -0.422   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2410     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11744    |\n",
      "|    total_timesteps | 1946105  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1946004  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=2892.90 +/- 1736.65\n",
      "Episode length: 679.20 +/- 393.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 679      |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 46.3     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 1.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2420     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11797    |\n",
      "|    total_timesteps | 1954922  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 25.7     |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.719    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1954821  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=3014.75 +/- 1066.83\n",
      "Episode length: 701.00 +/- 251.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 701      |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 13.6     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | 0.166    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2430     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11843    |\n",
      "|    total_timesteps | 1962640  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1962539  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=3739.48 +/- 1144.83\n",
      "Episode length: 872.20 +/- 255.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 872      |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | 0.316    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2440     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11891    |\n",
      "|    total_timesteps | 1970537  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 27.1     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | -0.664   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1970436  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2450     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11936    |\n",
      "|    total_timesteps | 1978294  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 59.4     |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | -0.409   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1978193  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=3560.93 +/- 1081.60\n",
      "Episode length: 803.60 +/- 243.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 16.1     |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.0963   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 852      |\n",
      "|    ep_rew_mean     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2460     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11983    |\n",
      "|    total_timesteps | 1986120  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 18.2     |\n",
      "|    ent_coef        | 0.0388   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1986019  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=3471.61 +/- 1281.42\n",
      "Episode length: 850.40 +/- 299.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 850      |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.665   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 852      |\n",
      "|    ep_rew_mean     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2470     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12042    |\n",
      "|    total_timesteps | 1995930  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 19.5     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1995829  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=4122.33 +/- 547.71\n",
      "Episode length: 961.00 +/- 78.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 961      |\n",
      "|    mean_reward     | 4.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 24.1     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | 0.813    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2480     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12084    |\n",
      "|    total_timesteps | 2002882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0391   |\n",
      "|    ent_coef_loss   | 0.487    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2002781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=2630.16 +/- 1369.56\n",
      "Episode length: 793.00 +/- 266.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 793      |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 16.4     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | 0.338    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2490     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12137    |\n",
      "|    total_timesteps | 2011733  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 16.2     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | -0.772   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2011632  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2500     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12172    |\n",
      "|    total_timesteps | 2017697  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2017596  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=3837.24 +/- 1166.81\n",
      "Episode length: 868.20 +/- 263.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 868      |\n",
      "|    mean_reward     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 40.8     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | 0.642    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 789      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2510     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12216    |\n",
      "|    total_timesteps | 2024985  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2024884  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=3178.08 +/- 1559.28\n",
      "Episode length: 741.60 +/- 353.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 742      |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 17.6     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.0727   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2520     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12261    |\n",
      "|    total_timesteps | 2032411  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.121    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2032310  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=3507.57 +/- 822.62\n",
      "Episode length: 853.20 +/- 180.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | 0.0374   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 782      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2530     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12312    |\n",
      "|    total_timesteps | 2040860  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 15       |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | 0.422    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2040759  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2540     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12357    |\n",
      "|    total_timesteps | 2048546  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 22.7     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2048445  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=3494.38 +/- 1508.35\n",
      "Episode length: 785.40 +/- 328.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 785      |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0407   |\n",
      "|    ent_coef_loss   | -0.153   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 778      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2550     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12402    |\n",
      "|    total_timesteps | 2056138  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 20.9     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | 0.202    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2056037  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=3810.61 +/- 1209.59\n",
      "Episode length: 866.40 +/- 266.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 866      |\n",
      "|    mean_reward     | 3.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | 0.217    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2560     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12459    |\n",
      "|    total_timesteps | 2065649  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | -0.138   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2065548  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=3226.37 +/- 1487.62\n",
      "Episode length: 732.80 +/- 327.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 733      |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 15.2     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.779   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2570     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12505    |\n",
      "|    total_timesteps | 2073342  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2073241  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=2862.57 +/- 1939.52\n",
      "Episode length: 650.40 +/- 430.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 650      |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 48.4     |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | -0.0815  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2580     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12562    |\n",
      "|    total_timesteps | 2082811  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | 0.952    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2082710  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=3420.70 +/- 1432.04\n",
      "Episode length: 776.00 +/- 325.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 776      |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 63.2     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | 0.756    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2590     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12621    |\n",
      "|    total_timesteps | 2092716  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | 1.83     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2092615  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 817       |\n",
      "|    ep_rew_mean     | 3.25e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2600      |\n",
      "|    fps             | 165       |\n",
      "|    time_elapsed    | 12660     |\n",
      "|    total_timesteps | 2099426   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -318      |\n",
      "|    critic_loss     | 30.8      |\n",
      "|    ent_coef        | 0.0411    |\n",
      "|    ent_coef_loss   | -0.000713 |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2099325   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=2930.46 +/- 1979.70\n",
      "Episode length: 653.40 +/- 428.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 653      |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 60       |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 0.551    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2610     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12704    |\n",
      "|    total_timesteps | 2106709  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 20.4     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 0.328    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2106608  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=3601.80 +/- 1608.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | 0.118    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2620     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12749    |\n",
      "|    total_timesteps | 2114107  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0416   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2114006  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=1987.98 +/- 1574.32\n",
      "Episode length: 604.80 +/- 402.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 605      |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.0425   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2630     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12803    |\n",
      "|    total_timesteps | 2123118  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 38.4     |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | -0.494   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2123017  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=4025.07 +/- 1039.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2640     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12858    |\n",
      "|    total_timesteps | 2132262  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 31.5     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | 0.668    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2132161  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2650     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12901    |\n",
      "|    total_timesteps | 2139581  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -0.0347  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=4034.37 +/- 773.15\n",
      "Episode length: 897.80 +/- 153.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 898      |\n",
      "|    mean_reward     | 4.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.0429   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2660     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12949    |\n",
      "|    total_timesteps | 2147532  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    ent_coef        | 0.0411   |\n",
      "|    ent_coef_loss   | -0.567   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2147431  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=3581.58 +/- 1097.16\n",
      "Episode length: 812.00 +/- 239.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 812      |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 26.3     |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | 0.308    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2670     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13008    |\n",
      "|    total_timesteps | 2157442  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 24.8     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | -0.629   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2157341  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=2565.43 +/- 1748.04\n",
      "Episode length: 582.00 +/- 380.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 582      |\n",
      "|    mean_reward     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 47.7     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | 0.533    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2680     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13051    |\n",
      "|    total_timesteps | 2164622  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 33.9     |\n",
      "|    ent_coef        | 0.0427   |\n",
      "|    ent_coef_loss   | 0.494    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2164521  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=4154.55 +/- 771.84\n",
      "Episode length: 914.60 +/- 170.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 915      |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 56.4     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2690     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13095    |\n",
      "|    total_timesteps | 2171885  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2171784  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=4050.37 +/- 621.67\n",
      "Episode length: 897.20 +/- 111.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 897      |\n",
      "|    mean_reward     | 4.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | 0.287    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2700     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13154    |\n",
      "|    total_timesteps | 2181736  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0428   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2181635  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2710     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13197    |\n",
      "|    total_timesteps | 2189085  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 47.6     |\n",
      "|    ent_coef        | 0.0427   |\n",
      "|    ent_coef_loss   | -0.983   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2188984  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=3350.69 +/- 1427.78\n",
      "Episode length: 872.80 +/- 254.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 873      |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0438   |\n",
      "|    ent_coef_loss   | 1.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2720     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13242    |\n",
      "|    total_timesteps | 2196493  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -0.596   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2196392  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=4468.63 +/- 110.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 45.2     |\n",
      "|    ent_coef        | 0.0441   |\n",
      "|    ent_coef_loss   | -0.507   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2199899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2730     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13294    |\n",
      "|    total_timesteps | 2205168  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 22       |\n",
      "|    ent_coef        | 0.0429   |\n",
      "|    ent_coef_loss   | -0.827   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2205067  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=2766.67 +/- 1413.26\n",
      "Episode length: 627.00 +/- 316.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 627      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2740     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13338    |\n",
      "|    total_timesteps | 2212572  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | 0.174    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2212471  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=3774.37 +/- 1602.62\n",
      "Episode length: 830.20 +/- 339.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 830      |\n",
      "|    mean_reward     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 46.1     |\n",
      "|    ent_coef        | 0.045    |\n",
      "|    ent_coef_loss   | 0.524    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2750     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13394    |\n",
      "|    total_timesteps | 2221884  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | 0.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2221783  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2760     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13420    |\n",
      "|    total_timesteps | 2226274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 27.2     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2226173  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=4369.71 +/- 215.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | -0.282   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 766      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2770     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13468    |\n",
      "|    total_timesteps | 2234040  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 68.5     |\n",
      "|    ent_coef        | 0.0451   |\n",
      "|    ent_coef_loss   | 0.407    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2233939  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=3846.00 +/- 1281.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 33.1     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 0.711    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2780     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13518    |\n",
      "|    total_timesteps | 2242233  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 20.4     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | 0.846    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2242132  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=4542.66 +/- 58.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 36.8     |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | -0.763   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2249899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 788      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2790     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13569    |\n",
      "|    total_timesteps | 2250700  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 42.9     |\n",
      "|    ent_coef        | 0.0446   |\n",
      "|    ent_coef_loss   | 0.908    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2250599  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2800     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13617    |\n",
      "|    total_timesteps | 2258799  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 18.9     |\n",
      "|    ent_coef        | 0.0449   |\n",
      "|    ent_coef_loss   | 0.236    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2258698  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=3134.40 +/- 1689.14\n",
      "Episode length: 878.40 +/- 243.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 878      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 38.2     |\n",
      "|    ent_coef        | 0.0449   |\n",
      "|    ent_coef_loss   | -0.501   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2810     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13674    |\n",
      "|    total_timesteps | 2268331  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0446   |\n",
      "|    ent_coef_loss   | -0.458   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2268230  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=3415.15 +/- 1439.58\n",
      "Episode length: 773.00 +/- 325.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 773      |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2820     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13722    |\n",
      "|    total_timesteps | 2276383  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 25.9     |\n",
      "|    ent_coef        | 0.0452   |\n",
      "|    ent_coef_loss   | 1.64     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2276282  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=3156.52 +/- 1183.53\n",
      "Episode length: 845.60 +/- 308.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 846      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | 0.194    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2830     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13766    |\n",
      "|    total_timesteps | 2283636  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 22.3     |\n",
      "|    ent_coef        | 0.0453   |\n",
      "|    ent_coef_loss   | 1.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2283535  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=3434.51 +/- 1166.83\n",
      "Episode length: 755.40 +/- 258.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 755      |\n",
      "|    mean_reward     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 21.6     |\n",
      "|    ent_coef        | 0.0451   |\n",
      "|    ent_coef_loss   | -0.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2840     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13817    |\n",
      "|    total_timesteps | 2292109  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 19.3     |\n",
      "|    ent_coef        | 0.0459   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2292008  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2850     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13855    |\n",
      "|    total_timesteps | 2298603  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    ent_coef        | 0.0462   |\n",
      "|    ent_coef_loss   | 0.584    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2298502  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=3724.46 +/- 1642.26\n",
      "Episode length: 822.40 +/- 355.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 822      |\n",
      "|    mean_reward     | 3.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 58.9     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -0.0946  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2860     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13901    |\n",
      "|    total_timesteps | 2306169  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 30.5     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | 0.618    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2306068  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310000, episode_reward=2465.29 +/- 1583.36\n",
      "Episode length: 744.20 +/- 315.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 744      |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    ent_coef        | 0.0464   |\n",
      "|    ent_coef_loss   | 0.0747   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2870     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13951    |\n",
      "|    total_timesteps | 2314575  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 30.6     |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | 0.91     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2314474  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=3696.63 +/- 1187.50\n",
      "Episode length: 812.80 +/- 242.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 813      |\n",
      "|    mean_reward     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.0457   |\n",
      "|    ent_coef_loss   | 0.0913   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2880     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13986    |\n",
      "|    total_timesteps | 2320289  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 21.9     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2320188  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 762      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2890     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14024    |\n",
      "|    total_timesteps | 2326882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 52.1     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2326781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=3530.77 +/- 1486.47\n",
      "Episode length: 779.40 +/- 322.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 779      |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | -0.384   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2900     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14084    |\n",
      "|    total_timesteps | 2336882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 15.9     |\n",
      "|    ent_coef        | 0.0463   |\n",
      "|    ent_coef_loss   | -0.992   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2336781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=4042.02 +/- 1091.34\n",
      "Episode length: 887.20 +/- 225.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 887      |\n",
      "|    mean_reward     | 4.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 25.4     |\n",
      "|    ent_coef        | 0.0462   |\n",
      "|    ent_coef_loss   | 0.0865   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2910     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14130    |\n",
      "|    total_timesteps | 2344384  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 40.7     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2344283  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=2246.29 +/- 2233.04\n",
      "Episode length: 695.40 +/- 399.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 695      |\n",
      "|    mean_reward     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | 0.386    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2920     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14182    |\n",
      "|    total_timesteps | 2353105  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 69.6     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | -0.562   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2353004  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2930     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14221    |\n",
      "|    total_timesteps | 2359761  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 22.7     |\n",
      "|    ent_coef        | 0.0473   |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359660  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=3417.75 +/- 1467.45\n",
      "Episode length: 752.00 +/- 304.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 752      |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 30.1     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 0.992    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2940     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14277    |\n",
      "|    total_timesteps | 2369126  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 95.2     |\n",
      "|    ent_coef        | 0.0469   |\n",
      "|    ent_coef_loss   | 0.621    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2369025  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=4214.05 +/- 612.92\n",
      "Episode length: 901.20 +/- 148.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 901      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 20.6     |\n",
      "|    ent_coef        | 0.048    |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2950     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14325    |\n",
      "|    total_timesteps | 2377086  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.0479   |\n",
      "|    ent_coef_loss   | -0.0972  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2376985  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=4679.78 +/- 99.19\n",
      "Episode length: 988.80 +/- 22.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 989      |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 28.3     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | -0.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2379899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 790      |\n",
      "|    ep_rew_mean     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2960     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14374    |\n",
      "|    total_timesteps | 2385161  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 44.4     |\n",
      "|    ent_coef        | 0.0479   |\n",
      "|    ent_coef_loss   | -0.828   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2385060  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=3352.59 +/- 1317.63\n",
      "Episode length: 880.80 +/- 159.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 881      |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 20.2     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -0.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2970     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14426    |\n",
      "|    total_timesteps | 2393753  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 70.7     |\n",
      "|    ent_coef        | 0.0476   |\n",
      "|    ent_coef_loss   | 0.0919   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2393652  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=3735.70 +/- 2008.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | -0.559   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2980     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14471    |\n",
      "|    total_timesteps | 2401204  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 19.5     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | 0.832    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2401103  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2990     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14518    |\n",
      "|    total_timesteps | 2409257  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409156  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=4486.04 +/- 323.44\n",
      "Episode length: 970.60 +/- 58.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 971      |\n",
      "|    mean_reward     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 22.4     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3000     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14566    |\n",
      "|    total_timesteps | 2417258  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 23.6     |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | 0.283    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2417157  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=4172.96 +/- 915.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3010     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14616    |\n",
      "|    total_timesteps | 2425462  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | -0.447   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2425361  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=2405.43 +/- 1820.00\n",
      "Episode length: 538.80 +/- 389.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 539      |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 57.6     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | 0.722    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3020     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14668    |\n",
      "|    total_timesteps | 2434096  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | -0.0645  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2433995  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=2196.66 +/- 1992.51\n",
      "Episode length: 480.20 +/- 427.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 480      |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    ent_coef        | 0.0474   |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3030     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14724    |\n",
      "|    total_timesteps | 2443567  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    ent_coef        | 0.0487   |\n",
      "|    ent_coef_loss   | -0.979   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2443466  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2450000, episode_reward=4667.49 +/- 64.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | -1.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3040     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14764    |\n",
      "|    total_timesteps | 2450235  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 29.8     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2450134  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3050     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14810    |\n",
      "|    total_timesteps | 2458134  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 46.2     |\n",
      "|    ent_coef        | 0.0474   |\n",
      "|    ent_coef_loss   | 0.85     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2458033  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=4285.70 +/- 1025.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    ent_coef        | 0.0475   |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3060     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14856    |\n",
      "|    total_timesteps | 2465570  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 32.8     |\n",
      "|    ent_coef        | 0.048    |\n",
      "|    ent_coef_loss   | -0.762   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2465469  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=4384.76 +/- 566.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.0486   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3070     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14904    |\n",
      "|    total_timesteps | 2473624  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 32.3     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | -0.437   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2473523  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=4173.34 +/- 1270.75\n",
      "Episode length: 866.80 +/- 266.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 867      |\n",
      "|    mean_reward     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 41.6     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 0.00924  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3080     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14959    |\n",
      "|    total_timesteps | 2482817  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2482716  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=3633.33 +/- 1177.81\n",
      "Episode length: 901.60 +/- 196.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 902      |\n",
      "|    mean_reward     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 90.4     |\n",
      "|    ent_coef        | 0.0475   |\n",
      "|    ent_coef_loss   | 0.937    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3090     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15006    |\n",
      "|    total_timesteps | 2490477  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | -0.435   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2490376  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 782      |\n",
      "|    ep_rew_mean     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3100     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15035    |\n",
      "|    total_timesteps | 2495496  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 40.7     |\n",
      "|    ent_coef        | 0.0491   |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2495395  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=3833.53 +/- 1722.66\n",
      "Episode length: 819.40 +/- 361.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 819      |\n",
      "|    mean_reward     | 3.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 22.5     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3110     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15090    |\n",
      "|    total_timesteps | 2504577  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.0488   |\n",
      "|    ent_coef_loss   | 0.307    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2504476  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=3676.15 +/- 1587.49\n",
      "Episode length: 761.80 +/- 309.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 762      |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 16.9     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | -0.772   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3120     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15133    |\n",
      "|    total_timesteps | 2511783  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 46       |\n",
      "|    ent_coef        | 0.0488   |\n",
      "|    ent_coef_loss   | 0.00404  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2511682  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3130     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15181    |\n",
      "|    total_timesteps | 2519975  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 59.2     |\n",
      "|    ent_coef        | 0.0487   |\n",
      "|    ent_coef_loss   | 0.494    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519874  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=2646.25 +/- 1504.12\n",
      "Episode length: 565.40 +/- 300.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 565      |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    ent_coef        | 0.0486   |\n",
      "|    ent_coef_loss   | -0.379   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3140     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15225    |\n",
      "|    total_timesteps | 2527368  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 19.4     |\n",
      "|    ent_coef        | 0.05     |\n",
      "|    ent_coef_loss   | 0.00295  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2527267  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=4180.41 +/- 747.15\n",
      "Episode length: 890.60 +/- 159.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 891      |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 27.9     |\n",
      "|    ent_coef        | 0.0503   |\n",
      "|    ent_coef_loss   | 0.461    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3150     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15273    |\n",
      "|    total_timesteps | 2535334  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.05     |\n",
      "|    ent_coef_loss   | 0.896    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2535233  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=3481.59 +/- 1265.91\n",
      "Episode length: 770.80 +/- 259.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 771      |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 48.5     |\n",
      "|    ent_coef        | 0.05     |\n",
      "|    ent_coef_loss   | 0.324    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3160     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15321    |\n",
      "|    total_timesteps | 2543285  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 20       |\n",
      "|    ent_coef        | 0.0499   |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2543184  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=4711.19 +/- 153.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 25.2     |\n",
      "|    ent_coef        | 0.0501   |\n",
      "|    ent_coef_loss   | -0.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2549899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3170     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15367    |\n",
      "|    total_timesteps | 2550804  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 32.3     |\n",
      "|    ent_coef        | 0.0513   |\n",
      "|    ent_coef_loss   | -0.883   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2550703  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3180     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15414    |\n",
      "|    total_timesteps | 2558837  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 22.8     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | 0.171    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2558736  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=3390.70 +/- 1399.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 27.4     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | -0.316   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3190     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15463    |\n",
      "|    total_timesteps | 2566994  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 53.7     |\n",
      "|    ent_coef        | 0.0506   |\n",
      "|    ent_coef_loss   | -0.548   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2566893  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=3524.06 +/- 1902.91\n",
      "Episode length: 912.40 +/- 175.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 912      |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 41.4     |\n",
      "|    ent_coef        | 0.0504   |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3200     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15516    |\n",
      "|    total_timesteps | 2575862  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.0505   |\n",
      "|    ent_coef_loss   | -0.932   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2575761  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=3581.97 +/- 1598.36\n",
      "Episode length: 909.40 +/- 181.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 909      |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 51.8     |\n",
      "|    ent_coef        | 0.0502   |\n",
      "|    ent_coef_loss   | -0.225   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 773      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3210     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15553    |\n",
      "|    total_timesteps | 2581867  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 26.3     |\n",
      "|    ent_coef        | 0.0509   |\n",
      "|    ent_coef_loss   | -0.0412  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2581766  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3220     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15597    |\n",
      "|    total_timesteps | 2589313  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    ent_coef        | 0.0505   |\n",
      "|    ent_coef_loss   | 0.161    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2589212  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=3050.39 +/- 1811.71\n",
      "Episode length: 812.20 +/- 255.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 812      |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 18.5     |\n",
      "|    ent_coef        | 0.0506   |\n",
      "|    ent_coef_loss   | 0.555    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3230     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15638    |\n",
      "|    total_timesteps | 2596035  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 19.3     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2595934  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=4838.94 +/- 45.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 51.2     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | 0.571    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2599899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 752      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3240     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15677    |\n",
      "|    total_timesteps | 2602554  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 25.2     |\n",
      "|    ent_coef        | 0.0501   |\n",
      "|    ent_coef_loss   | -0.317   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2602453  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=3711.37 +/- 933.13\n",
      "Episode length: 850.00 +/- 190.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 850      |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 13.8     |\n",
      "|    ent_coef        | 0.0507   |\n",
      "|    ent_coef_loss   | 0.255    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 747      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3250     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15723    |\n",
      "|    total_timesteps | 2610048  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609947  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 736      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3260     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15763    |\n",
      "|    total_timesteps | 2616887  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 43.1     |\n",
      "|    ent_coef        | 0.0515   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2616786  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=3497.70 +/- 1780.28\n",
      "Episode length: 794.40 +/- 372.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 794      |\n",
      "|    mean_reward     | 3.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 92.4     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | 0.483    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 738      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3270     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15809    |\n",
      "|    total_timesteps | 2624614  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.0514   |\n",
      "|    ent_coef_loss   | -0.945   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2624513  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2630000, episode_reward=3629.91 +/- 1616.52\n",
      "Episode length: 742.20 +/- 316.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 742      |\n",
      "|    mean_reward     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 23.2     |\n",
      "|    ent_coef        | 0.0507   |\n",
      "|    ent_coef_loss   | -0.541   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3280     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15864    |\n",
      "|    total_timesteps | 2633729  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 56.4     |\n",
      "|    ent_coef        | 0.0501   |\n",
      "|    ent_coef_loss   | -0.241   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2633628  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=4267.80 +/- 1089.44\n",
      "Episode length: 893.00 +/- 214.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 893      |\n",
      "|    mean_reward     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    ent_coef        | 0.0521   |\n",
      "|    ent_coef_loss   | -0.621   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 748      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3290     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15913    |\n",
      "|    total_timesteps | 2641842  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 21       |\n",
      "|    ent_coef        | 0.0532   |\n",
      "|    ent_coef_loss   | -0.485   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2641741  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 716      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3300     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15946    |\n",
      "|    total_timesteps | 2647446  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 32.1     |\n",
      "|    ent_coef        | 0.0523   |\n",
      "|    ent_coef_loss   | 0.357    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2647345  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=3157.47 +/- 2031.56\n",
      "Episode length: 833.00 +/- 334.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 833      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 15.3     |\n",
      "|    ent_coef        | 0.0529   |\n",
      "|    ent_coef_loss   | -0.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3310     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15980    |\n",
      "|    total_timesteps | 2653086  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 39.2     |\n",
      "|    ent_coef        | 0.0527   |\n",
      "|    ent_coef_loss   | -0.0205  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2652985  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=4695.96 +/- 159.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 73.8     |\n",
      "|    ent_coef        | 0.0528   |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3320     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16026    |\n",
      "|    total_timesteps | 2660614  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 38       |\n",
      "|    ent_coef        | 0.0524   |\n",
      "|    ent_coef_loss   | 0.71     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2660513  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 728      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3330     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16074    |\n",
      "|    total_timesteps | 2668873  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 43.7     |\n",
      "|    ent_coef        | 0.0534   |\n",
      "|    ent_coef_loss   | 0.349    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2668772  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=2090.28 +/- 1104.95\n",
      "Episode length: 584.40 +/- 271.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 584      |\n",
      "|    mean_reward     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0519   |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3340     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16129    |\n",
      "|    total_timesteps | 2678168  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    ent_coef        | 0.0532   |\n",
      "|    ent_coef_loss   | 0.688    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2678067  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=4403.84 +/- 712.64\n",
      "Episode length: 926.40 +/- 147.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 926      |\n",
      "|    mean_reward     | 4.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.0524   |\n",
      "|    ent_coef_loss   | 0.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3350     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16172    |\n",
      "|    total_timesteps | 2685150  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 21.9     |\n",
      "|    ent_coef        | 0.0528   |\n",
      "|    ent_coef_loss   | -0.532   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2685049  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=3776.88 +/- 1521.08\n",
      "Episode length: 962.60 +/- 74.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 963      |\n",
      "|    mean_reward     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 228      |\n",
      "|    ent_coef        | 0.0532   |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3360     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16221    |\n",
      "|    total_timesteps | 2693394  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 36.9     |\n",
      "|    ent_coef        | 0.0537   |\n",
      "|    ent_coef_loss   | -1.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2693293  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3370     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16248    |\n",
      "|    total_timesteps | 2697912  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 115      |\n",
      "|    ent_coef        | 0.0533   |\n",
      "|    ent_coef_loss   | 0.914    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2697811  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=3692.74 +/- 1344.17\n",
      "Episode length: 859.80 +/- 280.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 62.8     |\n",
      "|    ent_coef        | 0.053    |\n",
      "|    ent_coef_loss   | -0.654   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 717      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3380     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16293    |\n",
      "|    total_timesteps | 2705426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | 0.71     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2705325  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=3133.02 +/- 1933.33\n",
      "Episode length: 677.40 +/- 409.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 677      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 30.2     |\n",
      "|    ent_coef        | 0.0552   |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 715      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3390     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16341    |\n",
      "|    total_timesteps | 2713359  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 69.5     |\n",
      "|    ent_coef        | 0.0545   |\n",
      "|    ent_coef_loss   | 0.559    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2713258  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3400     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16379    |\n",
      "|    total_timesteps | 2719940  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.986   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719839  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=4664.00 +/- 406.99\n",
      "Episode length: 959.00 +/- 82.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 959      |\n",
      "|    mean_reward     | 4.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 75.8     |\n",
      "|    ent_coef        | 0.0543   |\n",
      "|    ent_coef_loss   | 0.126    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 735      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3410     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16419    |\n",
      "|    total_timesteps | 2726560  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 30.7     |\n",
      "|    ent_coef        | 0.0544   |\n",
      "|    ent_coef_loss   | -0.589   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2726459  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=3681.56 +/- 1511.80\n",
      "Episode length: 931.40 +/- 94.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 931      |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.0543   |\n",
      "|    ent_coef_loss   | 0.47     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3420     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16477    |\n",
      "|    total_timesteps | 2736185  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 54.2     |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | 0.739    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2736084  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=2285.50 +/- 1754.89\n",
      "Episode length: 854.40 +/- 191.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 854      |\n",
      "|    mean_reward     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.054    |\n",
      "|    ent_coef_loss   | -0.496   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3430     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16532    |\n",
      "|    total_timesteps | 2745334  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 47.6     |\n",
      "|    ent_coef        | 0.0544   |\n",
      "|    ent_coef_loss   | 0.809    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2745233  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2750000, episode_reward=4614.07 +/- 372.36\n",
      "Episode length: 956.20 +/- 87.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 956      |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 29.2     |\n",
      "|    ent_coef        | 0.0541   |\n",
      "|    ent_coef_loss   | -0.836   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 754      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3440     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16582    |\n",
      "|    total_timesteps | 2753576  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 25.3     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2753475  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 732      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3450     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16610    |\n",
      "|    total_timesteps | 2758343  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | -0.172   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2758242  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=4816.59 +/- 182.03\n",
      "Episode length: 985.60 +/- 28.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 986      |\n",
      "|    mean_reward     | 4.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 29       |\n",
      "|    ent_coef        | 0.0551   |\n",
      "|    ent_coef_loss   | -0.691   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3460     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16660    |\n",
      "|    total_timesteps | 2766682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    ent_coef        | 0.0551   |\n",
      "|    ent_coef_loss   | -0.349   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2766581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=3379.22 +/- 1996.07\n",
      "Episode length: 687.20 +/- 386.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 687      |\n",
      "|    mean_reward     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    ent_coef        | 0.0545   |\n",
      "|    ent_coef_loss   | 0.341    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3470     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16697    |\n",
      "|    total_timesteps | 2772786  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 37       |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | -0.141   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2772685  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=2629.72 +/- 2076.08\n",
      "Episode length: 654.20 +/- 430.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 654      |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    ent_coef        | 0.0546   |\n",
      "|    ent_coef_loss   | -0.0707  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3480     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16748    |\n",
      "|    total_timesteps | 2781404  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 42.3     |\n",
      "|    ent_coef        | 0.0553   |\n",
      "|    ent_coef_loss   | 0.512    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2781303  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3490     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16790    |\n",
      "|    total_timesteps | 2788500  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.0581   |\n",
      "|    ent_coef_loss   | -0.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2788399  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=2967.10 +/- 1357.33\n",
      "Episode length: 928.00 +/- 104.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 928      |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    ent_coef        | 0.0579   |\n",
      "|    ent_coef_loss   | -0.407   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3500     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16828    |\n",
      "|    total_timesteps | 2794870  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 28.7     |\n",
      "|    ent_coef        | 0.0557   |\n",
      "|    ent_coef_loss   | 0.146    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2794769  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 730      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3510     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16856    |\n",
      "|    total_timesteps | 2799571  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 61.6     |\n",
      "|    ent_coef        | 0.0563   |\n",
      "|    ent_coef_loss   | 1.79     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2799470  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=2285.57 +/- 1757.33\n",
      "Episode length: 798.60 +/- 260.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 799      |\n",
      "|    mean_reward     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 62.1     |\n",
      "|    ent_coef        | 0.0568   |\n",
      "|    ent_coef_loss   | 0.792    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3520     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16903    |\n",
      "|    total_timesteps | 2807343  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    ent_coef        | 0.0577   |\n",
      "|    ent_coef_loss   | -0.488   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2807242  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=4473.12 +/- 795.75\n",
      "Episode length: 925.80 +/- 148.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 926      |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 40.5     |\n",
      "|    ent_coef        | 0.0576   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 687      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3530     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16944    |\n",
      "|    total_timesteps | 2814075  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 29       |\n",
      "|    ent_coef        | 0.059    |\n",
      "|    ent_coef_loss   | 0.242    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2813974  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2820000, episode_reward=2201.91 +/- 1868.09\n",
      "Episode length: 697.20 +/- 374.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 697      |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 66.9     |\n",
      "|    ent_coef        | 0.0584   |\n",
      "|    ent_coef_loss   | 0.741    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 673      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3540     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16984    |\n",
      "|    total_timesteps | 2820832  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 46.1     |\n",
      "|    ent_coef        | 0.0581   |\n",
      "|    ent_coef_loss   | 0.518    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2820731  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 685      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3550     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17019    |\n",
      "|    total_timesteps | 2826818  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 20.4     |\n",
      "|    ent_coef        | 0.0582   |\n",
      "|    ent_coef_loss   | 0.00143  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2826717  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2830000, episode_reward=2993.40 +/- 1545.63\n",
      "Episode length: 805.60 +/- 177.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0574   |\n",
      "|    ent_coef_loss   | -0.472   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 690      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3560     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17073    |\n",
      "|    total_timesteps | 2835722  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0595   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2835621  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2840000, episode_reward=4375.03 +/- 790.62\n",
      "Episode length: 880.20 +/- 148.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 880      |\n",
      "|    mean_reward     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 61.6     |\n",
      "|    ent_coef        | 0.0596   |\n",
      "|    ent_coef_loss   | -0.124   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 704      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3570     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17118    |\n",
      "|    total_timesteps | 2843167  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 30.8     |\n",
      "|    ent_coef        | 0.0598   |\n",
      "|    ent_coef_loss   | 0.085    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2843066  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 682      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3580     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17156    |\n",
      "|    total_timesteps | 2849606  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 55.9     |\n",
      "|    ent_coef        | 0.0575   |\n",
      "|    ent_coef_loss   | -0.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2849505  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=3904.82 +/- 982.24\n",
      "Episode length: 826.60 +/- 224.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 3.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 33.8     |\n",
      "|    ent_coef        | 0.0578   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 685      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3590     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17200    |\n",
      "|    total_timesteps | 2857002  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 103      |\n",
      "|    ent_coef        | 0.0573   |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2856901  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=4390.91 +/- 1051.96\n",
      "Episode length: 962.80 +/- 74.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 963      |\n",
      "|    mean_reward     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 49.5     |\n",
      "|    ent_coef        | 0.06     |\n",
      "|    ent_coef_loss   | 0.558    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 688      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3600     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17241    |\n",
      "|    total_timesteps | 2863713  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 24.2     |\n",
      "|    ent_coef        | 0.0591   |\n",
      "|    ent_coef_loss   | -0.379   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2863612  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=4233.80 +/- 1015.80\n",
      "Episode length: 854.40 +/- 192.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 854      |\n",
      "|    mean_reward     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 27.6     |\n",
      "|    ent_coef        | 0.0567   |\n",
      "|    ent_coef_loss   | -0.442   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 714      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3610     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17285    |\n",
      "|    total_timesteps | 2870990  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 18.8     |\n",
      "|    ent_coef        | 0.0592   |\n",
      "|    ent_coef_loss   | -0.116   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2870889  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3620     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17330    |\n",
      "|    total_timesteps | 2878628  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    ent_coef        | 0.0578   |\n",
      "|    ent_coef_loss   | 0.914    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2878527  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=4327.55 +/- 1217.92\n",
      "Episode length: 875.40 +/- 249.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 875      |\n",
      "|    mean_reward     | 4.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 36.7     |\n",
      "|    ent_coef        | 0.0585   |\n",
      "|    ent_coef_loss   | -0.405   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3630     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17377    |\n",
      "|    total_timesteps | 2886531  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0579   |\n",
      "|    ent_coef_loss   | -0.878   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2886430  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=4604.43 +/- 753.29\n",
      "Episode length: 930.20 +/- 139.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 930      |\n",
      "|    mean_reward     | 4.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 277      |\n",
      "|    ent_coef        | 0.0586   |\n",
      "|    ent_coef_loss   | 0.804    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 738      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3640     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17426    |\n",
      "|    total_timesteps | 2894672  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.0587   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2894571  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=3248.99 +/- 2147.88\n",
      "Episode length: 838.80 +/- 313.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 839      |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 35.6     |\n",
      "|    ent_coef        | 0.0581   |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3650     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17473    |\n",
      "|    total_timesteps | 2902449  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 26.5     |\n",
      "|    ent_coef        | 0.0577   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2902348  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2910000, episode_reward=3540.48 +/- 1274.87\n",
      "Episode length: 867.40 +/- 171.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 867      |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 66.6     |\n",
      "|    ent_coef        | 0.0591   |\n",
      "|    ent_coef_loss   | 0.0607   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3660     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17524    |\n",
      "|    total_timesteps | 2910839  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 74.2     |\n",
      "|    ent_coef        | 0.0579   |\n",
      "|    ent_coef_loss   | -0.792   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2910738  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3670     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17567    |\n",
      "|    total_timesteps | 2918146  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 50.7     |\n",
      "|    ent_coef        | 0.0594   |\n",
      "|    ent_coef_loss   | 0.0161   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2918045  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=4001.32 +/- 1470.34\n",
      "Episode length: 852.80 +/- 294.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | 4e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0588   |\n",
      "|    ent_coef_loss   | -0.686   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 778      |\n",
      "|    ep_rew_mean     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3680     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17622    |\n",
      "|    total_timesteps | 2927388  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 39.1     |\n",
      "|    ent_coef        | 0.0593   |\n",
      "|    ent_coef_loss   | 0.756    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2927287  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=3999.37 +/- 1903.26\n",
      "Episode length: 810.00 +/- 380.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 810      |\n",
      "|    mean_reward     | 4e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 21       |\n",
      "|    ent_coef        | 0.0578   |\n",
      "|    ent_coef_loss   | -0.815   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3690     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17665    |\n",
      "|    total_timesteps | 2934455  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 26.4     |\n",
      "|    ent_coef        | 0.0597   |\n",
      "|    ent_coef_loss   | 0.399    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2934354  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=4272.09 +/- 856.95\n",
      "Episode length: 921.80 +/- 156.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 922      |\n",
      "|    mean_reward     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 39.2     |\n",
      "|    ent_coef        | 0.0588   |\n",
      "|    ent_coef_loss   | 0.818    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3700     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17711    |\n",
      "|    total_timesteps | 2942083  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 51.1     |\n",
      "|    ent_coef        | 0.0592   |\n",
      "|    ent_coef_loss   | -0.239   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2941982  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2950000, episode_reward=5177.34 +/- 153.08\n",
      "Episode length: 985.80 +/- 28.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 986      |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 43.8     |\n",
      "|    ent_coef        | 0.0589   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2949899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3710     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17764    |\n",
      "|    total_timesteps | 2950793  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 23.4     |\n",
      "|    ent_coef        | 0.0588   |\n",
      "|    ent_coef_loss   | -0.063   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2950692  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3720     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17813    |\n",
      "|    total_timesteps | 2959156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 15.9     |\n",
      "|    ent_coef        | 0.0601   |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2959055  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2960000, episode_reward=4810.81 +/- 248.49\n",
      "Episode length: 971.40 +/- 57.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 971      |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 28.8     |\n",
      "|    ent_coef        | 0.0582   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3730     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17867    |\n",
      "|    total_timesteps | 2968202  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 28.9     |\n",
      "|    ent_coef        | 0.0593   |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2968101  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=4952.67 +/- 119.27\n",
      "Episode length: 997.20 +/- 5.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 997      |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 33.8     |\n",
      "|    ent_coef        | 0.0607   |\n",
      "|    ent_coef_loss   | 0.0655   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3740     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17911    |\n",
      "|    total_timesteps | 2975426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.059    |\n",
      "|    ent_coef_loss   | 0.275    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2975325  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=3287.80 +/- 1961.53\n",
      "Episode length: 644.40 +/- 365.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 644      |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 31.8     |\n",
      "|    ent_coef        | 0.0599   |\n",
      "|    ent_coef_loss   | 0.526    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3750     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17960    |\n",
      "|    total_timesteps | 2983595  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 77.8     |\n",
      "|    ent_coef        | 0.0608   |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2983494  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=4901.77 +/- 114.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 102      |\n",
      "|    ent_coef        | 0.0594   |\n",
      "|    ent_coef_loss   | 0.468    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3760     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18016    |\n",
      "|    total_timesteps | 2992864  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 18       |\n",
      "|    ent_coef        | 0.0608   |\n",
      "|    ent_coef_loss   | -0.0306  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2992763  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=4843.41 +/- 194.50\n",
      "Episode length: 973.60 +/- 52.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 974      |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 87.2     |\n",
      "|    ent_coef        | 0.0619   |\n",
      "|    ent_coef_loss   | -0.378   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3770     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18070    |\n",
      "|    total_timesteps | 3001810  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 31.7     |\n",
      "|    ent_coef        | 0.0616   |\n",
      "|    ent_coef_loss   | 0.645    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3001709  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3010000, episode_reward=2043.77 +/- 2237.47\n",
      "Episode length: 671.20 +/- 325.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 671      |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 42.6     |\n",
      "|    ent_coef        | 0.0624   |\n",
      "|    ent_coef_loss   | -0.0856  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3780     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18121    |\n",
      "|    total_timesteps | 3010259  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 23.4     |\n",
      "|    ent_coef        | 0.0625   |\n",
      "|    ent_coef_loss   | -0.0941  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3010158  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3790     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18174    |\n",
      "|    total_timesteps | 3019344  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 37.6     |\n",
      "|    ent_coef        | 0.0633   |\n",
      "|    ent_coef_loss   | 0.435    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019243  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3020000, episode_reward=3820.43 +/- 1392.39\n",
      "Episode length: 779.20 +/- 276.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 779      |\n",
      "|    mean_reward     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    ent_coef        | 0.0622   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3800     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18217    |\n",
      "|    total_timesteps | 3026558  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 44.8     |\n",
      "|    ent_coef        | 0.0623   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3026457  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3030000, episode_reward=3245.50 +/- 2067.05\n",
      "Episode length: 819.40 +/- 325.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 819      |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 67       |\n",
      "|    ent_coef        | 0.0617   |\n",
      "|    ent_coef_loss   | -0.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3810     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18263    |\n",
      "|    total_timesteps | 3034221  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0623   |\n",
      "|    ent_coef_loss   | 0.227    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3034120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=3486.59 +/- 1967.73\n",
      "Episode length: 690.20 +/- 384.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 690      |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 47       |\n",
      "|    ent_coef        | 0.0618   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3820     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18319    |\n",
      "|    total_timesteps | 3043508  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 29.8     |\n",
      "|    ent_coef        | 0.0618   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3043407  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3050000, episode_reward=5002.74 +/- 75.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 39.7     |\n",
      "|    ent_coef        | 0.0621   |\n",
      "|    ent_coef_loss   | -0.0404  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3830     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18366    |\n",
      "|    total_timesteps | 3051221  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0615   |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3051120  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3840     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18412    |\n",
      "|    total_timesteps | 3059181  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 84.5     |\n",
      "|    ent_coef        | 0.0625   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3059080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060000, episode_reward=3785.41 +/- 1585.26\n",
      "Episode length: 775.80 +/- 286.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 776      |\n",
      "|    mean_reward     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 20.9     |\n",
      "|    ent_coef        | 0.0617   |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3850     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18450    |\n",
      "|    total_timesteps | 3065478  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 25.7     |\n",
      "|    ent_coef        | 0.0627   |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3065377  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3070000, episode_reward=3642.57 +/- 1558.28\n",
      "Episode length: 703.60 +/- 289.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 704      |\n",
      "|    mean_reward     | 3.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    ent_coef        | 0.0607   |\n",
      "|    ent_coef_loss   | -0.711   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3860     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18489    |\n",
      "|    total_timesteps | 3071980  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 21.9     |\n",
      "|    ent_coef        | 0.0628   |\n",
      "|    ent_coef_loss   | -0.141   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3071879  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 3.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3870     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18536    |\n",
      "|    total_timesteps | 3079921  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 20.3     |\n",
      "|    ent_coef        | 0.061    |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3079820  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3080000, episode_reward=4687.20 +/- 1127.69\n",
      "Episode length: 894.00 +/- 212.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 894      |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    ent_coef        | 0.061    |\n",
      "|    ent_coef_loss   | 0.341    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3880     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18583    |\n",
      "|    total_timesteps | 3087625  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 61.1     |\n",
      "|    ent_coef        | 0.061    |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3087524  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090000, episode_reward=4145.46 +/- 1128.89\n",
      "Episode length: 863.00 +/- 199.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 863      |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 31.2     |\n",
      "|    ent_coef        | 0.0626   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 757      |\n",
      "|    ep_rew_mean     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3890     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18627    |\n",
      "|    total_timesteps | 3095031  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.0617   |\n",
      "|    ent_coef_loss   | 0.0496   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3094930  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3100000, episode_reward=4647.77 +/- 721.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 50.1     |\n",
      "|    ent_coef        | 0.062    |\n",
      "|    ent_coef_loss   | 0.164    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3900     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18677    |\n",
      "|    total_timesteps | 3103240  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 36.4     |\n",
      "|    ent_coef        | 0.0616   |\n",
      "|    ent_coef_loss   | -0.449   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3103139  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3110000, episode_reward=4206.73 +/- 1819.45\n",
      "Episode length: 832.00 +/- 336.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 832      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 25.4     |\n",
      "|    ent_coef        | 0.0628   |\n",
      "|    ent_coef_loss   | -0.374   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3910     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18728    |\n",
      "|    total_timesteps | 3111792  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    ent_coef        | 0.0638   |\n",
      "|    ent_coef_loss   | -1.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3111691  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3120000, episode_reward=4605.37 +/- 894.35\n",
      "Episode length: 876.20 +/- 157.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 876      |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 53.3     |\n",
      "|    ent_coef        | 0.0626   |\n",
      "|    ent_coef_loss   | -0.224   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 783      |\n",
      "|    ep_rew_mean     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3920     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18788    |\n",
      "|    total_timesteps | 3121775  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 34.9     |\n",
      "|    ent_coef        | 0.0634   |\n",
      "|    ent_coef_loss   | 0.121    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3121674  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3930     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18826    |\n",
      "|    total_timesteps | 3128319  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 70.8     |\n",
      "|    ent_coef        | 0.0633   |\n",
      "|    ent_coef_loss   | 0.799    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3128218  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3130000, episode_reward=5246.08 +/- 179.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 61.7     |\n",
      "|    ent_coef        | 0.0623   |\n",
      "|    ent_coef_loss   | 0.214    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3129899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3940     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18872    |\n",
      "|    total_timesteps | 3135843  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 46.8     |\n",
      "|    ent_coef        | 0.0616   |\n",
      "|    ent_coef_loss   | -1.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3135742  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3140000, episode_reward=4694.54 +/- 482.53\n",
      "Episode length: 952.00 +/- 96.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 952      |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 53.3     |\n",
      "|    ent_coef        | 0.0633   |\n",
      "|    ent_coef_loss   | 0.764    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 797      |\n",
      "|    ep_rew_mean     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3950     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18928    |\n",
      "|    total_timesteps | 3145160  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 52.3     |\n",
      "|    ent_coef        | 0.0634   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3145059  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3150000, episode_reward=3363.58 +/- 2231.82\n",
      "Episode length: 893.40 +/- 213.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 893      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 36.8     |\n",
      "|    ent_coef        | 0.0622   |\n",
      "|    ent_coef_loss   | 0.728    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3960     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18986    |\n",
      "|    total_timesteps | 3154808  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 23       |\n",
      "|    ent_coef        | 0.0624   |\n",
      "|    ent_coef_loss   | -0.627   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3154707  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3160000, episode_reward=4050.84 +/- 1816.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 39.7     |\n",
      "|    ent_coef        | 0.0621   |\n",
      "|    ent_coef_loss   | 0.88     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3970     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19032    |\n",
      "|    total_timesteps | 3162367  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    ent_coef        | 0.0626   |\n",
      "|    ent_coef_loss   | 0.0721   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3162266  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3170000, episode_reward=2856.75 +/- 2189.99\n",
      "Episode length: 561.20 +/- 406.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 561      |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0639   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3980     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19087    |\n",
      "|    total_timesteps | 3171682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 26.6     |\n",
      "|    ent_coef        | 0.0634   |\n",
      "|    ent_coef_loss   | 0.435    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3171581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3180000, episode_reward=4093.35 +/- 2098.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    ent_coef        | 0.0637   |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 3.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3990     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19144    |\n",
      "|    total_timesteps | 3181106  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 93.2     |\n",
      "|    ent_coef        | 0.0647   |\n",
      "|    ent_coef_loss   | 0.848    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3181005  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4000     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19195    |\n",
      "|    total_timesteps | 3189933  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 41.9     |\n",
      "|    ent_coef        | 0.065    |\n",
      "|    ent_coef_loss   | 0.207    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3189832  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3190000, episode_reward=4964.46 +/- 379.12\n",
      "Episode length: 976.20 +/- 47.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 976      |\n",
      "|    mean_reward     | 4.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 28.8     |\n",
      "|    ent_coef        | 0.0649   |\n",
      "|    ent_coef_loss   | -0.726   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4010     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19250    |\n",
      "|    total_timesteps | 3198950  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0637   |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3198849  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=4531.43 +/- 1493.49\n",
      "Episode length: 860.40 +/- 279.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 4.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 65.5     |\n",
      "|    ent_coef        | 0.0652   |\n",
      "|    ent_coef_loss   | 0.906    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 3.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4020     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19296    |\n",
      "|    total_timesteps | 3206638  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 123      |\n",
      "|    ent_coef        | 0.0651   |\n",
      "|    ent_coef_loss   | 2.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3206537  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210000, episode_reward=5315.21 +/- 98.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 36.4     |\n",
      "|    ent_coef        | 0.0646   |\n",
      "|    ent_coef_loss   | -0.945   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3209899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4030     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19341    |\n",
      "|    total_timesteps | 3214020  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 20.8     |\n",
      "|    ent_coef        | 0.0671   |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3213919  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3220000, episode_reward=5198.78 +/- 135.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 25.2     |\n",
      "|    ent_coef        | 0.0652   |\n",
      "|    ent_coef_loss   | -0.504   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4040     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19380    |\n",
      "|    total_timesteps | 3220483  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 38.6     |\n",
      "|    ent_coef        | 0.0657   |\n",
      "|    ent_coef_loss   | -0.525   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3220382  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4050     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19424    |\n",
      "|    total_timesteps | 3227877  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 73.8     |\n",
      "|    ent_coef        | 0.0654   |\n",
      "|    ent_coef_loss   | 0.511    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3227776  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3230000, episode_reward=5254.48 +/- 213.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0648   |\n",
      "|    ent_coef_loss   | 0.105    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 3.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4060     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19481    |\n",
      "|    total_timesteps | 3237441  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 107      |\n",
      "|    ent_coef        | 0.0647   |\n",
      "|    ent_coef_loss   | 0.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3237340  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3240000, episode_reward=5227.33 +/- 115.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 41       |\n",
      "|    ent_coef        | 0.065    |\n",
      "|    ent_coef_loss   | -0.207   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 3.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4070     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19529    |\n",
      "|    total_timesteps | 3245363  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 56.8     |\n",
      "|    ent_coef        | 0.0639   |\n",
      "|    ent_coef_loss   | -0.00401 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3245262  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3250000, episode_reward=5200.08 +/- 175.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 42.7     |\n",
      "|    ent_coef        | 0.0662   |\n",
      "|    ent_coef_loss   | -0.326   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4080     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19576    |\n",
      "|    total_timesteps | 3253226  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 47.1     |\n",
      "|    ent_coef        | 0.0662   |\n",
      "|    ent_coef_loss   | -0.313   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3253125  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3260000, episode_reward=5183.23 +/- 204.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 30       |\n",
      "|    ent_coef        | 0.0659   |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4090     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19633    |\n",
      "|    total_timesteps | 3262687  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 56.6     |\n",
      "|    ent_coef        | 0.0666   |\n",
      "|    ent_coef_loss   | 0.588    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3262586  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270000, episode_reward=3748.85 +/- 1667.30\n",
      "Episode length: 807.20 +/- 287.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 807      |\n",
      "|    mean_reward     | 3.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    ent_coef        | 0.0667   |\n",
      "|    ent_coef_loss   | 0.0236   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 4.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4100     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19689    |\n",
      "|    total_timesteps | 3271978  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -374     |\n",
      "|    critic_loss     | 22       |\n",
      "|    ent_coef        | 0.0677   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3271877  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3280000, episode_reward=3754.41 +/- 1729.91\n",
      "Episode length: 843.20 +/- 313.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 843      |\n",
      "|    mean_reward     | 3.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 27.1     |\n",
      "|    ent_coef        | 0.068    |\n",
      "|    ent_coef_loss   | 0.0313   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 4e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4110     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19737    |\n",
      "|    total_timesteps | 3280003  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 49.1     |\n",
      "|    ent_coef        | 0.068    |\n",
      "|    ent_coef_loss   | 0.299    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3279902  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4120     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19791    |\n",
      "|    total_timesteps | 3289205  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    ent_coef        | 0.0679   |\n",
      "|    ent_coef_loss   | 0.831    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3289104  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3290000, episode_reward=4235.38 +/- 1740.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 40.4     |\n",
      "|    ent_coef        | 0.068    |\n",
      "|    ent_coef_loss   | 0.656    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4130     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19846    |\n",
      "|    total_timesteps | 3298329  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 72.1     |\n",
      "|    ent_coef        | 0.0726   |\n",
      "|    ent_coef_loss   | -0.108   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3298228  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300000, episode_reward=4220.01 +/- 1065.39\n",
      "Episode length: 819.80 +/- 202.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -382     |\n",
      "|    critic_loss     | 38.4     |\n",
      "|    ent_coef        | 0.0752   |\n",
      "|    ent_coef_loss   | 2.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4140     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19897    |\n",
      "|    total_timesteps | 3306880  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -389     |\n",
      "|    critic_loss     | 46.7     |\n",
      "|    ent_coef        | 0.1      |\n",
      "|    ent_coef_loss   | -0.658   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3306779  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3310000, episode_reward=-2065.99 +/- 198.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -2.07e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 3310000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -506      |\n",
      "|    critic_loss     | 215       |\n",
      "|    ent_coef        | 0.243     |\n",
      "|    ent_coef_loss   | 5.89      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3309899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4150     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19943    |\n",
      "|    total_timesteps | 3314487  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -824     |\n",
      "|    critic_loss     | 623      |\n",
      "|    ent_coef        | 0.416    |\n",
      "|    ent_coef_loss   | -0.717   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3314386  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3320000, episode_reward=-619.97 +/- 801.09\n",
      "Episode length: 412.60 +/- 479.63\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 413       |\n",
      "|    mean_reward     | -620      |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 3320000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 709       |\n",
      "|    ent_coef        | 0.459     |\n",
      "|    ent_coef_loss   | 0.261     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3319899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 831       |\n",
      "|    ep_rew_mean     | 3.17e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4160      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 19980     |\n",
      "|    total_timesteps | 3320552   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 1.92e+03  |\n",
      "|    ent_coef        | 0.469     |\n",
      "|    ent_coef_loss   | 0.185     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3320451   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 784       |\n",
      "|    ep_rew_mean     | 2.72e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4170      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 19998     |\n",
      "|    total_timesteps | 3323715   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.35e+03 |\n",
      "|    critic_loss     | 664       |\n",
      "|    ent_coef        | 0.509     |\n",
      "|    ent_coef_loss   | 0.225     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3323614   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 756       |\n",
      "|    ep_rew_mean     | 2.27e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4180      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 20028     |\n",
      "|    total_timesteps | 3328805   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.54e+03 |\n",
      "|    critic_loss     | 3.8e+03   |\n",
      "|    ent_coef        | 0.496     |\n",
      "|    ent_coef_loss   | -0.151    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3328704   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3330000, episode_reward=-382.53 +/- 454.41\n",
      "Episode length: 429.20 +/- 467.74\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 429       |\n",
      "|    mean_reward     | -383      |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 3330000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.56e+03 |\n",
      "|    critic_loss     | 726       |\n",
      "|    ent_coef        | 0.432     |\n",
      "|    ent_coef_loss   | 0.306     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3329899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 713       |\n",
      "|    ep_rew_mean     | 1.75e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4190      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 20060     |\n",
      "|    total_timesteps | 3334021   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.48e+03 |\n",
      "|    critic_loss     | 555       |\n",
      "|    ent_coef        | 0.293     |\n",
      "|    ent_coef_loss   | -0.215    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3333920   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 674      |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4200     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20091    |\n",
      "|    total_timesteps | 3339401  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3e+03 |\n",
      "|    critic_loss     | 301      |\n",
      "|    ent_coef        | 0.204    |\n",
      "|    ent_coef_loss   | -0.513   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3339300  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3340000, episode_reward=-17.21 +/- 199.94\n",
      "Episode length: 640.00 +/- 441.01\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 640       |\n",
      "|    mean_reward     | -17.2     |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 3340000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.28e+03 |\n",
      "|    critic_loss     | 533       |\n",
      "|    ent_coef        | 0.205     |\n",
      "|    ent_coef_loss   | -0.284    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3339899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 676       |\n",
      "|    ep_rew_mean     | 818       |\n",
      "| time/              |           |\n",
      "|    episodes        | 4210      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 20140     |\n",
      "|    total_timesteps | 3347582   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 236       |\n",
      "|    ent_coef        | 0.167     |\n",
      "|    ent_coef_loss   | 0.98      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3347481   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3350000, episode_reward=-276.10 +/- 379.61\n",
      "Episode length: 616.20 +/- 470.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 616      |\n",
      "|    mean_reward     | -276     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -980     |\n",
      "|    critic_loss     | 363      |\n",
      "|    ent_coef        | 0.165    |\n",
      "|    ent_coef_loss   | 0.704    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 649      |\n",
      "|    ep_rew_mean     | 303      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4220     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20180    |\n",
      "|    total_timesteps | 3354075  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -905     |\n",
      "|    critic_loss     | 618      |\n",
      "|    ent_coef        | 0.163    |\n",
      "|    ent_coef_loss   | 0.0708   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3353974  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 612      |\n",
      "|    ep_rew_mean     | -152     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4230     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20212    |\n",
      "|    total_timesteps | 3359574  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -803     |\n",
      "|    critic_loss     | 232      |\n",
      "|    ent_coef        | 0.137    |\n",
      "|    ent_coef_loss   | -0.608   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3359473  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=-270.59 +/- 640.22\n",
      "Episode length: 455.00 +/- 445.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | -271     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -784     |\n",
      "|    critic_loss     | 605      |\n",
      "|    ent_coef        | 0.138    |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 617      |\n",
      "|    ep_rew_mean     | -553     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4240     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20265    |\n",
      "|    total_timesteps | 3368588  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -658     |\n",
      "|    critic_loss     | 92.4     |\n",
      "|    ent_coef        | 0.112    |\n",
      "|    ent_coef_loss   | 0.584    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3368487  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3370000, episode_reward=-364.86 +/- 468.16\n",
      "Episode length: 811.00 +/- 378.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 811      |\n",
      "|    mean_reward     | -365     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -633     |\n",
      "|    critic_loss     | 128      |\n",
      "|    ent_coef        | 0.11     |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 613      |\n",
      "|    ep_rew_mean     | -453     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4250     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20309    |\n",
      "|    total_timesteps | 3375763  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -599     |\n",
      "|    critic_loss     | 134      |\n",
      "|    ent_coef        | 0.0985   |\n",
      "|    ent_coef_loss   | 0.214    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3375662  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3380000, episode_reward=-257.78 +/- 458.65\n",
      "Episode length: 803.80 +/- 392.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -538     |\n",
      "|    critic_loss     | 86.6     |\n",
      "|    ent_coef        | 0.0917   |\n",
      "|    ent_coef_loss   | -0.214   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 633      |\n",
      "|    ep_rew_mean     | -373     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4260     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20358    |\n",
      "|    total_timesteps | 3383849  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 89.8     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 0.392    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3383748  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3390000, episode_reward=-466.55 +/- 494.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -467     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 129      |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.00137  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 667      |\n",
      "|    ep_rew_mean     | -317     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4270     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20399    |\n",
      "|    total_timesteps | 3390434  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -486     |\n",
      "|    critic_loss     | 250      |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | -0.604   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3390333  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 671      |\n",
      "|    ep_rew_mean     | -246     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4280     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20431    |\n",
      "|    total_timesteps | 3395918  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -477     |\n",
      "|    critic_loss     | 431      |\n",
      "|    ent_coef        | 0.0826   |\n",
      "|    ent_coef_loss   | -0.874   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3395817  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3400000, episode_reward=109.28 +/- 404.56\n",
      "Episode length: 837.80 +/- 324.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 838      |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -462     |\n",
      "|    critic_loss     | 78.4     |\n",
      "|    ent_coef        | 0.0814   |\n",
      "|    ent_coef_loss   | 0.0185   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 686      |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4290     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20472    |\n",
      "|    total_timesteps | 3402639  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -453     |\n",
      "|    critic_loss     | 52.7     |\n",
      "|    ent_coef        | 0.0788   |\n",
      "|    ent_coef_loss   | 0.00694  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3402538  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 684      |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4300     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20502    |\n",
      "|    total_timesteps | 3407789  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 51.4     |\n",
      "|    ent_coef        | 0.079    |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3407688  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3410000, episode_reward=1030.91 +/- 799.43\n",
      "Episode length: 839.00 +/- 322.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 839      |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -422     |\n",
      "|    critic_loss     | 52.6     |\n",
      "|    ent_coef        | 0.0792   |\n",
      "|    ent_coef_loss   | -0.709   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | -52.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4310     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20547    |\n",
      "|    total_timesteps | 3415258  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -414     |\n",
      "|    critic_loss     | 71.6     |\n",
      "|    ent_coef        | 0.0805   |\n",
      "|    ent_coef_loss   | 0.0269   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3415157  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3420000, episode_reward=2115.87 +/- 1921.85\n",
      "Episode length: 749.00 +/- 256.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 749      |\n",
      "|    mean_reward     | 2.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -396     |\n",
      "|    critic_loss     | 36       |\n",
      "|    ent_coef        | 0.0806   |\n",
      "|    ent_coef_loss   | 0.367    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 695      |\n",
      "|    ep_rew_mean     | 57.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4320     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20597    |\n",
      "|    total_timesteps | 3423574  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -403     |\n",
      "|    critic_loss     | 239      |\n",
      "|    ent_coef        | 0.0807   |\n",
      "|    ent_coef_loss   | -0.298   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3423473  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 685      |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4330     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20624    |\n",
      "|    total_timesteps | 3428054  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -392     |\n",
      "|    critic_loss     | 32       |\n",
      "|    ent_coef        | 0.0808   |\n",
      "|    ent_coef_loss   | 0.81     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3427953  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3430000, episode_reward=1461.76 +/- 1961.22\n",
      "Episode length: 896.20 +/- 207.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 896      |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -393     |\n",
      "|    critic_loss     | 71.5     |\n",
      "|    ent_coef        | 0.0825   |\n",
      "|    ent_coef_loss   | 0.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 672      |\n",
      "|    ep_rew_mean     | 209      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4340     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20670    |\n",
      "|    total_timesteps | 3435760  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -386     |\n",
      "|    critic_loss     | 49.7     |\n",
      "|    ent_coef        | 0.0818   |\n",
      "|    ent_coef_loss   | 0.604    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3435659  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3440000, episode_reward=1024.67 +/- 2124.94\n",
      "Episode length: 673.20 +/- 400.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 673      |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -385     |\n",
      "|    critic_loss     | 45.5     |\n",
      "|    ent_coef        | 0.0812   |\n",
      "|    ent_coef_loss   | 1.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 665      |\n",
      "|    ep_rew_mean     | 317      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4350     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20709    |\n",
      "|    total_timesteps | 3442236  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -374     |\n",
      "|    critic_loss     | 50.8     |\n",
      "|    ent_coef        | 0.0842   |\n",
      "|    ent_coef_loss   | -0.663   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3442135  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 651      |\n",
      "|    ep_rew_mean     | 462      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4360     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20749    |\n",
      "|    total_timesteps | 3448989  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -377     |\n",
      "|    critic_loss     | 64.3     |\n",
      "|    ent_coef        | 0.0832   |\n",
      "|    ent_coef_loss   | 0.674    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3448888  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3450000, episode_reward=2557.41 +/- 2484.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -376     |\n",
      "|    critic_loss     | 50.4     |\n",
      "|    ent_coef        | 0.0833   |\n",
      "|    ent_coef_loss   | -0.527   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 674      |\n",
      "|    ep_rew_mean     | 655      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4370     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20803    |\n",
      "|    total_timesteps | 3457882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 97.5     |\n",
      "|    ent_coef        | 0.0811   |\n",
      "|    ent_coef_loss   | -0.515   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3457781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3460000, episode_reward=945.08 +/- 1256.67\n",
      "Episode length: 401.40 +/- 380.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 401      |\n",
      "|    mean_reward     | 945      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 46.4     |\n",
      "|    ent_coef        | 0.0811   |\n",
      "|    ent_coef_loss   | -0.678   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 688      |\n",
      "|    ep_rew_mean     | 928      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4380     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20844    |\n",
      "|    total_timesteps | 3464752  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 71.1     |\n",
      "|    ent_coef        | 0.0812   |\n",
      "|    ent_coef_loss   | -0.619   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3464651  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3470000, episode_reward=2897.50 +/- 1617.09\n",
      "Episode length: 643.00 +/- 354.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 643      |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 43.8     |\n",
      "|    ent_coef        | 0.0817   |\n",
      "|    ent_coef_loss   | -0.337   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 710      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4390     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20897    |\n",
      "|    total_timesteps | 3473597  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 32.6     |\n",
      "|    ent_coef        | 0.0819   |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3473496  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 721      |\n",
      "|    ep_rew_mean     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4400     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20934    |\n",
      "|    total_timesteps | 3479876  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 38.4     |\n",
      "|    ent_coef        | 0.0821   |\n",
      "|    ent_coef_loss   | -0.653   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3479775  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480000, episode_reward=3248.74 +/- 2317.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 42.4     |\n",
      "|    ent_coef        | 0.0824   |\n",
      "|    ent_coef_loss   | -0.701   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4410     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20985    |\n",
      "|    total_timesteps | 3488366  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 184      |\n",
      "|    ent_coef        | 0.0833   |\n",
      "|    ent_coef_loss   | 0.94     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3488265  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3490000, episode_reward=4300.17 +/- 1251.97\n",
      "Episode length: 882.40 +/- 235.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 882      |\n",
      "|    mean_reward     | 4.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 49.7     |\n",
      "|    ent_coef        | 0.0818   |\n",
      "|    ent_coef_loss   | 0.724    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4420     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21036    |\n",
      "|    total_timesteps | 3496877  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 52.1     |\n",
      "|    ent_coef        | 0.0835   |\n",
      "|    ent_coef_loss   | 0.646    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3496776  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500000, episode_reward=2707.92 +/- 1432.01\n",
      "Episode length: 559.20 +/- 279.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 559      |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 50       |\n",
      "|    ent_coef        | 0.0837   |\n",
      "|    ent_coef_loss   | -0.518   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 763      |\n",
      "|    ep_rew_mean     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4430     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21081    |\n",
      "|    total_timesteps | 3504396  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 41.4     |\n",
      "|    ent_coef        | 0.0847   |\n",
      "|    ent_coef_loss   | -0.566   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3504295  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3510000, episode_reward=2551.24 +/- 2500.19\n",
      "Episode length: 972.40 +/- 55.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 972      |\n",
      "|    mean_reward     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 46.3     |\n",
      "|    ent_coef        | 0.0832   |\n",
      "|    ent_coef_loss   | -0.903   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4440     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21135    |\n",
      "|    total_timesteps | 3513341  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 77.7     |\n",
      "|    ent_coef        | 0.0843   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3513240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=4294.65 +/- 1918.64\n",
      "Episode length: 820.20 +/- 359.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 50.1     |\n",
      "|    ent_coef        | 0.0834   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4450     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21190    |\n",
      "|    total_timesteps | 3522459  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 66.6     |\n",
      "|    ent_coef        | 0.0849   |\n",
      "|    ent_coef_loss   | -0.137   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3522358  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3530000, episode_reward=2576.93 +/- 2307.08\n",
      "Episode length: 747.00 +/- 312.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 747      |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 31.7     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | 0.242    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4460     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21243    |\n",
      "|    total_timesteps | 3531389  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 40.5     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | -0.517   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3531288  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4470     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21282    |\n",
      "|    total_timesteps | 3538041  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    ent_coef        | 0.0849   |\n",
      "|    ent_coef_loss   | -0.249   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3537940  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3540000, episode_reward=1859.60 +/- 1359.53\n",
      "Episode length: 547.00 +/- 304.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 547      |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 145      |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | 0.695    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4480     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21323    |\n",
      "|    total_timesteps | 3544881  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 38.2     |\n",
      "|    ent_coef        | 0.0853   |\n",
      "|    ent_coef_loss   | 0.334    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3544780  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3550000, episode_reward=4010.96 +/- 1978.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 58.2     |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | 0.413    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4490     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21368    |\n",
      "|    total_timesteps | 3552286  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.0852   |\n",
      "|    ent_coef_loss   | 0.734    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3552185  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3560000, episode_reward=4220.81 +/- 1278.72\n",
      "Episode length: 807.40 +/- 236.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 807      |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 44       |\n",
      "|    ent_coef        | 0.0849   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4500     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21420    |\n",
      "|    total_timesteps | 3560956  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 137      |\n",
      "|    ent_coef        | 0.0846   |\n",
      "|    ent_coef_loss   | -0.267   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3560855  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4510     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21460    |\n",
      "|    total_timesteps | 3567700  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 184      |\n",
      "|    ent_coef        | 0.0833   |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3567599  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570000, episode_reward=4129.09 +/- 1833.29\n",
      "Episode length: 822.40 +/- 355.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 822      |\n",
      "|    mean_reward     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 53.8     |\n",
      "|    ent_coef        | 0.084    |\n",
      "|    ent_coef_loss   | -0.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4520     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21506    |\n",
      "|    total_timesteps | 3575380  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 44.4     |\n",
      "|    ent_coef        | 0.0843   |\n",
      "|    ent_coef_loss   | 0.267    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3575279  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3580000, episode_reward=4564.01 +/- 1251.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 68.5     |\n",
      "|    ent_coef        | 0.084    |\n",
      "|    ent_coef_loss   | -0.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4530     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21556    |\n",
      "|    total_timesteps | 3583567  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 139      |\n",
      "|    ent_coef        | 0.0829   |\n",
      "|    ent_coef_loss   | -0.0707  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3583466  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3590000, episode_reward=4983.52 +/- 747.37\n",
      "Episode length: 936.20 +/- 127.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 936      |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 36.7     |\n",
      "|    ent_coef        | 0.0844   |\n",
      "|    ent_coef_loss   | -0.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4540     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21607    |\n",
      "|    total_timesteps | 3592074  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 50       |\n",
      "|    ent_coef        | 0.0856   |\n",
      "|    ent_coef_loss   | 0.738    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3591973  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3600000, episode_reward=4678.47 +/- 1226.18\n",
      "Episode length: 883.20 +/- 233.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 883      |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 64       |\n",
      "|    ent_coef        | 0.0851   |\n",
      "|    ent_coef_loss   | 0.741    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4550     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21665    |\n",
      "|    total_timesteps | 3601701  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3601600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3610000, episode_reward=4460.07 +/- 1074.88\n",
      "Episode length: 851.40 +/- 186.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 851      |\n",
      "|    mean_reward     | 4.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 34.7     |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | 0.353    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4560     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21723    |\n",
      "|    total_timesteps | 3611333  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 47.6     |\n",
      "|    ent_coef        | 0.0847   |\n",
      "|    ent_coef_loss   | -0.021   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3611232  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3620000, episode_reward=5107.94 +/- 193.81\n",
      "Episode length: 978.60 +/- 42.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 979      |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 81.1     |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | 0.662    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 3.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4570     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21776    |\n",
      "|    total_timesteps | 3620188  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 32.8     |\n",
      "|    ent_coef        | 0.0842   |\n",
      "|    ent_coef_loss   | -0.914   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3620087  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4580     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21821    |\n",
      "|    total_timesteps | 3627360  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 161      |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | 0.332    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3627259  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3630000, episode_reward=3438.25 +/- 2479.78\n",
      "Episode length: 635.40 +/- 446.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 635      |\n",
      "|    mean_reward     | 3.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 46.8     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | 0.668    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4590     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21865    |\n",
      "|    total_timesteps | 3634457  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 36.8     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | 0.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3634356  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3640000, episode_reward=4212.47 +/- 1873.71\n",
      "Episode length: 822.40 +/- 355.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 822      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 45.2     |\n",
      "|    ent_coef        | 0.0853   |\n",
      "|    ent_coef_loss   | -0.757   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4600     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21920    |\n",
      "|    total_timesteps | 3643188  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 44.1     |\n",
      "|    ent_coef        | 0.0872   |\n",
      "|    ent_coef_loss   | -0.644   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3643087  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3650000, episode_reward=2350.39 +/- 2045.54\n",
      "Episode length: 455.60 +/- 370.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 456      |\n",
      "|    mean_reward     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 51.7     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 3.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4610     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21967    |\n",
      "|    total_timesteps | 3650652  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.0837   |\n",
      "|    ent_coef_loss   | -0.414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3650551  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 3.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4620     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22014    |\n",
      "|    total_timesteps | 3658322  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.0833   |\n",
      "|    ent_coef_loss   | 0.444    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3658221  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660000, episode_reward=4175.41 +/- 1501.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 76       |\n",
      "|    ent_coef        | 0.0843   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4630     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22072    |\n",
      "|    total_timesteps | 3667655  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 77.5     |\n",
      "|    ent_coef        | 0.0869   |\n",
      "|    ent_coef_loss   | 0.209    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3667554  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3670000, episode_reward=3917.75 +/- 1502.73\n",
      "Episode length: 831.80 +/- 258.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 832      |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 41.5     |\n",
      "|    ent_coef        | 0.0877   |\n",
      "|    ent_coef_loss   | -0.502   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4640     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22116    |\n",
      "|    total_timesteps | 3674880  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 118      |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | 0.413    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3674779  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=5222.48 +/- 353.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 63.8     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 0.542    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4650     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22168    |\n",
      "|    total_timesteps | 3683443  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 63.7     |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | 0.995    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3683342  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690000, episode_reward=4408.32 +/- 2175.79\n",
      "Episode length: 806.00 +/- 388.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 71.4     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | 0.276    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4660     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22218    |\n",
      "|    total_timesteps | 3691717  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 52.6     |\n",
      "|    ent_coef        | 0.0877   |\n",
      "|    ent_coef_loss   | -0.666   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3691616  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4670     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22254    |\n",
      "|    total_timesteps | 3697882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 39.5     |\n",
      "|    ent_coef        | 0.0891   |\n",
      "|    ent_coef_loss   | -0.0171  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3697781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3700000, episode_reward=3761.42 +/- 2236.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 86.4     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | -0.413   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4680     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22294    |\n",
      "|    total_timesteps | 3704573  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.0885   |\n",
      "|    ent_coef_loss   | -0.842   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3704472  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3710000, episode_reward=4517.51 +/- 1461.24\n",
      "Episode length: 825.40 +/- 260.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 825      |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    ent_coef        | 0.0897   |\n",
      "|    ent_coef_loss   | 0.464    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4690     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22341    |\n",
      "|    total_timesteps | 3712344  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 39       |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | 0.526    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3712243  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3720000, episode_reward=4671.72 +/- 1634.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 29.3     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | -0.483   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4700     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22388    |\n",
      "|    total_timesteps | 3720169  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 43.1     |\n",
      "|    ent_coef        | 0.0896   |\n",
      "|    ent_coef_loss   | 0.0839   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3720068  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4710     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22435    |\n",
      "|    total_timesteps | 3728082  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 64.3     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | 0.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3727981  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3730000, episode_reward=5300.94 +/- 352.27\n",
      "Episode length: 977.80 +/- 44.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 978      |\n",
      "|    mean_reward     | 5.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | 0.0838   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4720     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22478    |\n",
      "|    total_timesteps | 3735277  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 68.3     |\n",
      "|    ent_coef        | 0.0912   |\n",
      "|    ent_coef_loss   | 1.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3735176  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3740000, episode_reward=3300.92 +/- 1862.28\n",
      "Episode length: 618.20 +/- 329.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 618      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -370     |\n",
      "|    critic_loss     | 70.6     |\n",
      "|    ent_coef        | 0.0914   |\n",
      "|    ent_coef_loss   | 0.224    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 752      |\n",
      "|    ep_rew_mean     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4730     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22523    |\n",
      "|    total_timesteps | 3742869  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 44       |\n",
      "|    ent_coef        | 0.0918   |\n",
      "|    ent_coef_loss   | -0.317   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3742768  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750000, episode_reward=5368.28 +/- 122.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 34.7     |\n",
      "|    ent_coef        | 0.0942   |\n",
      "|    ent_coef_loss   | -0.383   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3749899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4740     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22579    |\n",
      "|    total_timesteps | 3752118  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -380     |\n",
      "|    critic_loss     | 197      |\n",
      "|    ent_coef        | 0.0936   |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3752017  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3760000, episode_reward=2560.72 +/- 2227.31\n",
      "Episode length: 505.40 +/- 413.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 505      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    ent_coef        | 0.0944   |\n",
      "|    ent_coef_loss   | -0.235   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4750     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22631    |\n",
      "|    total_timesteps | 3760854  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    ent_coef        | 0.0934   |\n",
      "|    ent_coef_loss   | -0.464   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3760753  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 730      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4760     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22653    |\n",
      "|    total_timesteps | 3764719  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 34.6     |\n",
      "|    ent_coef        | 0.0912   |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3764618  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3770000, episode_reward=2244.18 +/- 1986.79\n",
      "Episode length: 557.40 +/- 430.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 557      |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 92.8     |\n",
      "|    ent_coef        | 0.0891   |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 732      |\n",
      "|    ep_rew_mean     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4770     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22692    |\n",
      "|    total_timesteps | 3771110  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 104      |\n",
      "|    ent_coef        | 0.0886   |\n",
      "|    ent_coef_loss   | 0.647    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3771009  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3780000, episode_reward=3764.09 +/- 2054.64\n",
      "Episode length: 863.60 +/- 272.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 864      |\n",
      "|    mean_reward     | 3.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 48.6     |\n",
      "|    ent_coef        | 0.0879   |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4780     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22748    |\n",
      "|    total_timesteps | 3780590  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0887   |\n",
      "|    ent_coef_loss   | -0.184   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3780489  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 3.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4790     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22794    |\n",
      "|    total_timesteps | 3788442  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 36.1     |\n",
      "|    ent_coef        | 0.0882   |\n",
      "|    ent_coef_loss   | -0.886   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3788341  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3790000, episode_reward=5175.86 +/- 530.48\n",
      "Episode length: 954.40 +/- 91.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 954      |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 43.9     |\n",
      "|    ent_coef        | 0.0862   |\n",
      "|    ent_coef_loss   | 0.361    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4800     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22841    |\n",
      "|    total_timesteps | 3796141  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 108      |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3796040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3800000, episode_reward=4755.78 +/- 1090.93\n",
      "Episode length: 905.40 +/- 189.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 905      |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 46.2     |\n",
      "|    ent_coef        | 0.088    |\n",
      "|    ent_coef_loss   | 0.337    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 3.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4810     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22901    |\n",
      "|    total_timesteps | 3806103  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 47.9     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | 0.165    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3806002  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810000, episode_reward=4661.12 +/- 1671.40\n",
      "Episode length: 856.60 +/- 286.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 857      |\n",
      "|    mean_reward     | 4.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -368     |\n",
      "|    critic_loss     | 26.5     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | -0.187   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 786      |\n",
      "|    ep_rew_mean     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4820     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22948    |\n",
      "|    total_timesteps | 3813848  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 31.4     |\n",
      "|    ent_coef        | 0.0869   |\n",
      "|    ent_coef_loss   | -0.507   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3813747  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3820000, episode_reward=5377.82 +/- 346.74\n",
      "Episode length: 989.60 +/- 20.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 990      |\n",
      "|    mean_reward     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 158      |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -0.336   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3819899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 806      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4830     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23006    |\n",
      "|    total_timesteps | 3823474  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 44.2     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | -0.161   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3823373  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3830000, episode_reward=1544.03 +/- 1890.53\n",
      "Episode length: 483.60 +/- 426.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 484      |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4840     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23057    |\n",
      "|    total_timesteps | 3832038  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 53.3     |\n",
      "|    ent_coef        | 0.0869   |\n",
      "|    ent_coef_loss   | 0.233    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3831937  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4850     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23091    |\n",
      "|    total_timesteps | 3837911  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3837810  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=3728.30 +/- 2333.76\n",
      "Episode length: 905.80 +/- 188.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 906      |\n",
      "|    mean_reward     | 3.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 37.9     |\n",
      "|    ent_coef        | 0.0847   |\n",
      "|    ent_coef_loss   | -0.603   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4860     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23139    |\n",
      "|    total_timesteps | 3845871  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 112      |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -0.609   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3845770  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850000, episode_reward=4296.77 +/- 1626.12\n",
      "Episode length: 860.40 +/- 279.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 4.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 43.1     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4870     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23196    |\n",
      "|    total_timesteps | 3855292  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 35.8     |\n",
      "|    ent_coef        | 0.0876   |\n",
      "|    ent_coef_loss   | 0.565    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3855191  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3860000, episode_reward=4580.11 +/- 1174.13\n",
      "Episode length: 885.80 +/- 228.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 886      |\n",
      "|    mean_reward     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 37.1     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | -0.872   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4880     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23250    |\n",
      "|    total_timesteps | 3864259  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 36.9     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | 0.912    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3864158  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870000, episode_reward=5410.08 +/- 203.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 88.4     |\n",
      "|    ent_coef        | 0.0874   |\n",
      "|    ent_coef_loss   | 0.481    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3869899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4890     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23306    |\n",
      "|    total_timesteps | 3873703  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 66.3     |\n",
      "|    ent_coef        | 0.085    |\n",
      "|    ent_coef_loss   | 0.471    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3873602  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3880000, episode_reward=4611.16 +/- 1526.63\n",
      "Episode length: 860.20 +/- 279.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.0857   |\n",
      "|    ent_coef_loss   | -0.251   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4900     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23352    |\n",
      "|    total_timesteps | 3881229  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 54.1     |\n",
      "|    ent_coef        | 0.0843   |\n",
      "|    ent_coef_loss   | -0.211   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3881128  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 3.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4910     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23402    |\n",
      "|    total_timesteps | 3889838  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 41.5     |\n",
      "|    ent_coef        | 0.0868   |\n",
      "|    ent_coef_loss   | -0.509   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3889737  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890000, episode_reward=4502.96 +/- 1722.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 63.2     |\n",
      "|    ent_coef        | 0.086    |\n",
      "|    ent_coef_loss   | 0.0673   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4920     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23451    |\n",
      "|    total_timesteps | 3897994  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 46.5     |\n",
      "|    ent_coef        | 0.0871   |\n",
      "|    ent_coef_loss   | 0.257    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3897893  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900000, episode_reward=5114.73 +/- 660.02\n",
      "Episode length: 937.60 +/- 124.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 938      |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 47.1     |\n",
      "|    ent_coef        | 0.0856   |\n",
      "|    ent_coef_loss   | 0.236    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4930     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23508    |\n",
      "|    total_timesteps | 3907402  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 42.1     |\n",
      "|    ent_coef        | 0.0874   |\n",
      "|    ent_coef_loss   | -0.551   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3907301  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3910000, episode_reward=5433.94 +/- 46.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 46.8     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | -0.138   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3909899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4940     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23568    |\n",
      "|    total_timesteps | 3917402  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 43.3     |\n",
      "|    ent_coef        | 0.0885   |\n",
      "|    ent_coef_loss   | 0.871    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3917301  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3920000, episode_reward=4182.61 +/- 1991.71\n",
      "Episode length: 815.20 +/- 369.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 815      |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 54.5     |\n",
      "|    ent_coef        | 0.0876   |\n",
      "|    ent_coef_loss   | 0.0538   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4950     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23623    |\n",
      "|    total_timesteps | 3926617  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 59.9     |\n",
      "|    ent_coef        | 0.0887   |\n",
      "|    ent_coef_loss   | -0.136   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3926516  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3930000, episode_reward=5520.89 +/- 120.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 65.2     |\n",
      "|    ent_coef        | 0.0879   |\n",
      "|    ent_coef_loss   | 0.468    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3929899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 4.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4960     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23673    |\n",
      "|    total_timesteps | 3935005  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 31.4     |\n",
      "|    ent_coef        | 0.0868   |\n",
      "|    ent_coef_loss   | -0.471   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3934904  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3940000, episode_reward=5277.85 +/- 525.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 46.8     |\n",
      "|    ent_coef        | 0.0885   |\n",
      "|    ent_coef_loss   | 0.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4970     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23722    |\n",
      "|    total_timesteps | 3942982  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 133      |\n",
      "|    ent_coef        | 0.089    |\n",
      "|    ent_coef_loss   | -0.228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3942881  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3950000, episode_reward=5202.07 +/- 892.75\n",
      "Episode length: 926.40 +/- 147.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 926      |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 56.3     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | -0.0354  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 4.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4980     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23769    |\n",
      "|    total_timesteps | 3950845  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 42.4     |\n",
      "|    ent_coef        | 0.0902   |\n",
      "|    ent_coef_loss   | 0.349    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3950744  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4990     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23820    |\n",
      "|    total_timesteps | 3959622  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 44.3     |\n",
      "|    ent_coef        | 0.0888   |\n",
      "|    ent_coef_loss   | -0.285   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3959521  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960000, episode_reward=4340.35 +/- 2595.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 37.9     |\n",
      "|    ent_coef        | 0.0877   |\n",
      "|    ent_coef_loss   | -0.536   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 4.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5000     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23871    |\n",
      "|    total_timesteps | 3967999  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 52.4     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 0.355    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3967898  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3970000, episode_reward=4723.65 +/- 1046.99\n",
      "Episode length: 858.20 +/- 199.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 858      |\n",
      "|    mean_reward     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 55.8     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 1.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 4.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5010     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23915    |\n",
      "|    total_timesteps | 3975273  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 58.5     |\n",
      "|    ent_coef        | 0.089    |\n",
      "|    ent_coef_loss   | 0.129    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3975172  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3980000, episode_reward=3403.07 +/- 2454.90\n",
      "Episode length: 611.60 +/- 430.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 612      |\n",
      "|    mean_reward     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | -0.449   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 4.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5020     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23968    |\n",
      "|    total_timesteps | 3984296  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | -0.997   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3984195  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990000, episode_reward=5575.40 +/- 112.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | -0.436   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3989899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5030     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24006    |\n",
      "|    total_timesteps | 3990493  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 54.3     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | -0.0863  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3990392  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5040     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24061    |\n",
      "|    total_timesteps | 3999853  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 34.9     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3999752  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000000, episode_reward=4938.17 +/- 1396.60\n",
      "Episode length: 878.40 +/- 243.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 878      |\n",
      "|    mean_reward     | 4.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 122      |\n",
      "|    ent_coef        | 0.0904   |\n",
      "|    ent_coef_loss   | -0.541   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 813      |\n",
      "|    ep_rew_mean     | 4.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5050     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24109    |\n",
      "|    total_timesteps | 4007890  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    ent_coef        | 0.0902   |\n",
      "|    ent_coef_loss   | 0.0192   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4007789  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4010000, episode_reward=5450.10 +/- 65.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 52.8     |\n",
      "|    ent_coef        | 0.0906   |\n",
      "|    ent_coef_loss   | 0.339    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5060     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24163    |\n",
      "|    total_timesteps | 4016913  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 46       |\n",
      "|    ent_coef        | 0.0913   |\n",
      "|    ent_coef_loss   | 1.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4016812  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4020000, episode_reward=4706.09 +/- 1490.20\n",
      "Episode length: 874.40 +/- 251.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 874      |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 29.3     |\n",
      "|    ent_coef        | 0.093    |\n",
      "|    ent_coef_loss   | 0.521    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5070     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24217    |\n",
      "|    total_timesteps | 4025976  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 48.8     |\n",
      "|    ent_coef        | 0.092    |\n",
      "|    ent_coef_loss   | -0.192   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4025875  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4030000, episode_reward=5009.61 +/- 954.09\n",
      "Episode length: 919.40 +/- 161.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 919      |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 69.3     |\n",
      "|    ent_coef        | 0.0905   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5080     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24265    |\n",
      "|    total_timesteps | 4033882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 32       |\n",
      "|    ent_coef        | 0.0909   |\n",
      "|    ent_coef_loss   | -0.691   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4033781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4040000, episode_reward=3197.51 +/- 2014.13\n",
      "Episode length: 700.60 +/- 375.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 701      |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 25.3     |\n",
      "|    ent_coef        | 0.0903   |\n",
      "|    ent_coef_loss   | 0.406    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5090     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24315    |\n",
      "|    total_timesteps | 4042313  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 36.6     |\n",
      "|    ent_coef        | 0.0919   |\n",
      "|    ent_coef_loss   | -0.454   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4042212  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050000, episode_reward=5639.18 +/- 121.34\n",
      "Episode length: 998.60 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | 5.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 41.3     |\n",
      "|    ent_coef        | 0.0918   |\n",
      "|    ent_coef_loss   | 1.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4049899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5100     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24370    |\n",
      "|    total_timesteps | 4051353  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 38.9     |\n",
      "|    ent_coef        | 0.0918   |\n",
      "|    ent_coef_loss   | 0.427    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4051252  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4060000, episode_reward=4945.23 +/- 1186.36\n",
      "Episode length: 897.20 +/- 205.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 897      |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 52.2     |\n",
      "|    ent_coef        | 0.0932   |\n",
      "|    ent_coef_loss   | -0.874   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5110     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24427    |\n",
      "|    total_timesteps | 4060866  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 98.6     |\n",
      "|    ent_coef        | 0.0931   |\n",
      "|    ent_coef_loss   | -0.732   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4060765  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4070000, episode_reward=3600.16 +/- 2241.79\n",
      "Episode length: 805.80 +/- 388.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 47.5     |\n",
      "|    ent_coef        | 0.0921   |\n",
      "|    ent_coef_loss   | 0.0543   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 4.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5120     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24482    |\n",
      "|    total_timesteps | 4070097  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 53.1     |\n",
      "|    ent_coef        | 0.0921   |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4069996  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 883      |\n",
      "|    ep_rew_mean     | 4.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5130     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24533    |\n",
      "|    total_timesteps | 4078803  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 30.5     |\n",
      "|    ent_coef        | 0.0924   |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4078702  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4080000, episode_reward=5674.43 +/- 100.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -383     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0932   |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4079899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5140     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24581    |\n",
      "|    total_timesteps | 4086754  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 27       |\n",
      "|    ent_coef        | 0.0943   |\n",
      "|    ent_coef_loss   | 0.559    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4086653  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4090000, episode_reward=5611.69 +/- 72.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -379     |\n",
      "|    critic_loss     | 27.9     |\n",
      "|    ent_coef        | 0.0938   |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 4.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5150     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24641    |\n",
      "|    total_timesteps | 4096754  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 40.6     |\n",
      "|    ent_coef        | 0.0956   |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4096653  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4100000, episode_reward=2560.07 +/- 2607.91\n",
      "Episode length: 705.60 +/- 361.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 706      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -373     |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    ent_coef        | 0.0944   |\n",
      "|    ent_coef_loss   | -0.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5160     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24686    |\n",
      "|    total_timesteps | 4104370  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 44.7     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4104269  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110000, episode_reward=4016.89 +/- 1593.24\n",
      "Episode length: 711.20 +/- 265.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 711      |\n",
      "|    mean_reward     | 4.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    ent_coef        | 0.0937   |\n",
      "|    ent_coef_loss   | 0.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5170     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24740    |\n",
      "|    total_timesteps | 4113439  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 252      |\n",
      "|    ent_coef        | 0.0928   |\n",
      "|    ent_coef_loss   | 0.00525  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4113338  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4120000, episode_reward=4506.73 +/- 1711.13\n",
      "Episode length: 959.40 +/- 81.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 959      |\n",
      "|    mean_reward     | 4.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -375     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    ent_coef        | 0.0931   |\n",
      "|    ent_coef_loss   | 0.647    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5180     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24798    |\n",
      "|    total_timesteps | 4123138  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 63.8     |\n",
      "|    ent_coef        | 0.0952   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4123037  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4130000, episode_reward=5722.72 +/- 86.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -376     |\n",
      "|    critic_loss     | 286      |\n",
      "|    ent_coef        | 0.0953   |\n",
      "|    ent_coef_loss   | -0.302   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4129899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 4.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5190     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24853    |\n",
      "|    total_timesteps | 4132184  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -374     |\n",
      "|    critic_loss     | 69.9     |\n",
      "|    ent_coef        | 0.095    |\n",
      "|    ent_coef_loss   | 0.616    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4132083  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5200     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24890    |\n",
      "|    total_timesteps | 4138540  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 32.9     |\n",
      "|    ent_coef        | 0.0941   |\n",
      "|    ent_coef_loss   | -0.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4138439  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4140000, episode_reward=4792.32 +/- 1702.76\n",
      "Episode length: 854.40 +/- 291.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 854      |\n",
      "|    mean_reward     | 4.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.0923   |\n",
      "|    ent_coef_loss   | -0.156   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 852      |\n",
      "|    ep_rew_mean     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5210     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24935    |\n",
      "|    total_timesteps | 4146061  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -375     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    ent_coef        | 0.0949   |\n",
      "|    ent_coef_loss   | 0.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4145960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4150000, episode_reward=5553.20 +/- 139.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 56.1     |\n",
      "|    ent_coef        | 0.095    |\n",
      "|    ent_coef_loss   | 0.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5220     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24996    |\n",
      "|    total_timesteps | 4156061  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 29.3     |\n",
      "|    ent_coef        | 0.0938   |\n",
      "|    ent_coef_loss   | 0.416    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4155960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4160000, episode_reward=4838.31 +/- 1481.15\n",
      "Episode length: 872.80 +/- 254.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 873      |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 43.5     |\n",
      "|    ent_coef        | 0.0932   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5230     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25047    |\n",
      "|    total_timesteps | 4164559  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    ent_coef        | 0.0941   |\n",
      "|    ent_coef_loss   | -0.466   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4164458  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4170000, episode_reward=5653.29 +/- 139.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 44.8     |\n",
      "|    ent_coef        | 0.0945   |\n",
      "|    ent_coef_loss   | -0.392   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5240     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25094    |\n",
      "|    total_timesteps | 4172341  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.0938   |\n",
      "|    ent_coef_loss   | 0.532    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4172240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4180000, episode_reward=4689.25 +/- 2013.06\n",
      "Episode length: 827.20 +/- 345.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 62       |\n",
      "|    ent_coef        | 0.0921   |\n",
      "|    ent_coef_loss   | 1.64     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5250     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25154    |\n",
      "|    total_timesteps | 4182341  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 54.4     |\n",
      "|    ent_coef        | 0.0917   |\n",
      "|    ent_coef_loss   | -1.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4182240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4190000, episode_reward=5619.35 +/- 129.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 39.9     |\n",
      "|    ent_coef        | 0.0919   |\n",
      "|    ent_coef_loss   | -0.498   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5260     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25201    |\n",
      "|    total_timesteps | 4190013  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 53.8     |\n",
      "|    ent_coef        | 0.0919   |\n",
      "|    ent_coef_loss   | 0.664    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4189912  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200000, episode_reward=4888.40 +/- 1311.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    ent_coef        | 0.0911   |\n",
      "|    ent_coef_loss   | 0.719    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5270     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25261    |\n",
      "|    total_timesteps | 4200013  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 44.6     |\n",
      "|    ent_coef        | 0.0912   |\n",
      "|    ent_coef_loss   | -0.937   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4199912  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5280     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25307    |\n",
      "|    total_timesteps | 4207880  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 35.5     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4207779  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4210000, episode_reward=5678.28 +/- 94.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 58.6     |\n",
      "|    ent_coef        | 0.091    |\n",
      "|    ent_coef_loss   | 0.292    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5290     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25349    |\n",
      "|    total_timesteps | 4214912  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 87.4     |\n",
      "|    ent_coef        | 0.0923   |\n",
      "|    ent_coef_loss   | -0.538   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4214811  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4220000, episode_reward=4946.70 +/- 934.56\n",
      "Episode length: 957.40 +/- 85.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 957      |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.0928   |\n",
      "|    ent_coef_loss   | 0.106    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5300     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25404    |\n",
      "|    total_timesteps | 4224004  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 38.5     |\n",
      "|    ent_coef        | 0.0928   |\n",
      "|    ent_coef_loss   | 0.463    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4223903  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4230000, episode_reward=5645.13 +/- 136.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -370     |\n",
      "|    critic_loss     | 31.2     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 4.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5310     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25464    |\n",
      "|    total_timesteps | 4233898  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0938   |\n",
      "|    ent_coef_loss   | 0.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4233797  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4240000, episode_reward=5250.03 +/- 781.60\n",
      "Episode length: 938.20 +/- 123.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 938      |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 38       |\n",
      "|    ent_coef        | 0.0909   |\n",
      "|    ent_coef_loss   | 0.454    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5320     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25523    |\n",
      "|    total_timesteps | 4243898  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 52.8     |\n",
      "|    ent_coef        | 0.0934   |\n",
      "|    ent_coef_loss   | 1.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4243797  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4250000, episode_reward=4577.27 +/- 2280.06\n",
      "Episode length: 802.80 +/- 394.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 803      |\n",
      "|    mean_reward     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 39.6     |\n",
      "|    ent_coef        | 0.0953   |\n",
      "|    ent_coef_loss   | 0.395    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5330     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25579    |\n",
      "|    total_timesteps | 4253174  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -383     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.094    |\n",
      "|    ent_coef_loss   | -0.618   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4253073  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4260000, episode_reward=4748.06 +/- 2104.75\n",
      "Episode length: 823.60 +/- 352.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 41       |\n",
      "|    ent_coef        | 0.0945   |\n",
      "|    ent_coef_loss   | 0.905    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5340     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25625    |\n",
      "|    total_timesteps | 4260927  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 56.1     |\n",
      "|    ent_coef        | 0.0968   |\n",
      "|    ent_coef_loss   | 0.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4260826  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 4.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5350     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25677    |\n",
      "|    total_timesteps | 4269785  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 96       |\n",
      "|    ent_coef        | 0.0971   |\n",
      "|    ent_coef_loss   | 0.874    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269684  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4270000, episode_reward=3528.09 +/- 2563.88\n",
      "Episode length: 841.80 +/- 316.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 842      |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 31.8     |\n",
      "|    ent_coef        | 0.0966   |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5360     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25723    |\n",
      "|    total_timesteps | 4277465  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 39.5     |\n",
      "|    ent_coef        | 0.098    |\n",
      "|    ent_coef_loss   | 0.449    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4277364  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4280000, episode_reward=2496.15 +/- 1612.71\n",
      "Episode length: 462.20 +/- 278.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 462      |\n",
      "|    mean_reward     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 26.4     |\n",
      "|    ent_coef        | 0.0965   |\n",
      "|    ent_coef_loss   | 0.279    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5370     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25766    |\n",
      "|    total_timesteps | 4284728  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -368     |\n",
      "|    critic_loss     | 73.3     |\n",
      "|    ent_coef        | 0.0968   |\n",
      "|    ent_coef_loss   | -0.696   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4284627  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4290000, episode_reward=4484.19 +/- 2243.22\n",
      "Episode length: 802.40 +/- 395.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 802      |\n",
      "|    mean_reward     | 4.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 44.7     |\n",
      "|    ent_coef        | 0.0942   |\n",
      "|    ent_coef_loss   | -0.631   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5380     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25819    |\n",
      "|    total_timesteps | 4293436  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 34       |\n",
      "|    ent_coef        | 0.0961   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4293335  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4300000, episode_reward=5610.35 +/- 98.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 53.4     |\n",
      "|    ent_coef        | 0.096    |\n",
      "|    ent_coef_loss   | 0.358    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 4.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5390     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25866    |\n",
      "|    total_timesteps | 4301188  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 31.7     |\n",
      "|    ent_coef        | 0.0962   |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4301087  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5400     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25916    |\n",
      "|    total_timesteps | 4309865  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 40.6     |\n",
      "|    ent_coef        | 0.096    |\n",
      "|    ent_coef_loss   | -0.248   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4309764  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4310000, episode_reward=5782.85 +/- 97.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0955   |\n",
      "|    ent_coef_loss   | -0.514   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4309899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5410     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25966    |\n",
      "|    total_timesteps | 4318215  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0975   |\n",
      "|    ent_coef_loss   | -0.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4318114  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320000, episode_reward=4657.99 +/- 2203.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.096    |\n",
      "|    ent_coef_loss   | -0.0003  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5420     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26018    |\n",
      "|    total_timesteps | 4326745  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -376     |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    ent_coef        | 0.0964   |\n",
      "|    ent_coef_loss   | 0.234    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4326644  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4330000, episode_reward=4341.75 +/- 2621.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 255      |\n",
      "|    ent_coef        | 0.096    |\n",
      "|    ent_coef_loss   | 0.279    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5430     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26074    |\n",
      "|    total_timesteps | 4336070  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 31       |\n",
      "|    ent_coef        | 0.0962   |\n",
      "|    ent_coef_loss   | -0.595   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4335969  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4340000, episode_reward=5659.32 +/- 64.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -385     |\n",
      "|    critic_loss     | 47.8     |\n",
      "|    ent_coef        | 0.0964   |\n",
      "|    ent_coef_loss   | 0.352    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 4.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5440     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26131    |\n",
      "|    total_timesteps | 4345478  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -378     |\n",
      "|    critic_loss     | 51.8     |\n",
      "|    ent_coef        | 0.0974   |\n",
      "|    ent_coef_loss   | 0.846    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4345377  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4350000, episode_reward=5324.89 +/- 667.07\n",
      "Episode length: 939.00 +/- 122.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 939      |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -380     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    ent_coef        | 0.0963   |\n",
      "|    ent_coef_loss   | -0.444   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5450     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26183    |\n",
      "|    total_timesteps | 4354251  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -385     |\n",
      "|    critic_loss     | 70.8     |\n",
      "|    ent_coef        | 0.098    |\n",
      "|    ent_coef_loss   | -0.323   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4354150  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4360000, episode_reward=4996.83 +/- 1241.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -389     |\n",
      "|    critic_loss     | 37.1     |\n",
      "|    ent_coef        | 0.0966   |\n",
      "|    ent_coef_loss   | 0.353    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 4.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5460     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26239    |\n",
      "|    total_timesteps | 4363558  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -386     |\n",
      "|    critic_loss     | 29.1     |\n",
      "|    ent_coef        | 0.0941   |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4363457  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4370000, episode_reward=5668.97 +/- 52.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -386     |\n",
      "|    critic_loss     | 36.3     |\n",
      "|    ent_coef        | 0.0926   |\n",
      "|    ent_coef_loss   | -0.0872  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 4.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5470     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26281    |\n",
      "|    total_timesteps | 4370388  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -391     |\n",
      "|    critic_loss     | 48.5     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | -0.428   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4370287  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5480     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26331    |\n",
      "|    total_timesteps | 4379040  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -398     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0936   |\n",
      "|    ent_coef_loss   | -0.817   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4378939  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380000, episode_reward=5319.99 +/- 943.71\n",
      "Episode length: 923.40 +/- 153.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 923      |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -394     |\n",
      "|    critic_loss     | 41.1     |\n",
      "|    ent_coef        | 0.093    |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 4.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5490     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26385    |\n",
      "|    total_timesteps | 4387970  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -386     |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    ent_coef        | 0.0904   |\n",
      "|    ent_coef_loss   | -0.00818 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4387869  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4390000, episode_reward=5570.01 +/- 505.16\n",
      "Episode length: 957.20 +/- 85.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 957      |\n",
      "|    mean_reward     | 5.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -380     |\n",
      "|    critic_loss     | 59.1     |\n",
      "|    ent_coef        | 0.0914   |\n",
      "|    ent_coef_loss   | 0.392    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 4.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5500     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26437    |\n",
      "|    total_timesteps | 4396657  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -381     |\n",
      "|    critic_loss     | 43       |\n",
      "|    ent_coef        | 0.0917   |\n",
      "|    ent_coef_loss   | 0.722    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4396556  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4400000, episode_reward=4881.70 +/- 1406.95\n",
      "Episode length: 874.60 +/- 250.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 875      |\n",
      "|    mean_reward     | 4.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -395     |\n",
      "|    critic_loss     | 40.2     |\n",
      "|    ent_coef        | 0.0907   |\n",
      "|    ent_coef_loss   | -0.205   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5510     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26489    |\n",
      "|    total_timesteps | 4405171  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -392     |\n",
      "|    critic_loss     | 61.6     |\n",
      "|    ent_coef        | 0.0924   |\n",
      "|    ent_coef_loss   | 0.423    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4405070  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410000, episode_reward=4734.94 +/- 1308.11\n",
      "Episode length: 842.80 +/- 214.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 843      |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -389     |\n",
      "|    critic_loss     | 50.2     |\n",
      "|    ent_coef        | 0.0896   |\n",
      "|    ent_coef_loss   | -0.613   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 4.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5520     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26548    |\n",
      "|    total_timesteps | 4415171  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -388     |\n",
      "|    critic_loss     | 59       |\n",
      "|    ent_coef        | 0.0897   |\n",
      "|    ent_coef_loss   | -0.00567 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4415070  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4420000, episode_reward=5312.09 +/- 899.55\n",
      "Episode length: 923.40 +/- 153.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 923      |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -388     |\n",
      "|    critic_loss     | 137      |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | 0.0626   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5530     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26587    |\n",
      "|    total_timesteps | 4421596  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -399     |\n",
      "|    critic_loss     | 437      |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4421495  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5540     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26638    |\n",
      "|    total_timesteps | 4429978  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -402     |\n",
      "|    critic_loss     | 85.6     |\n",
      "|    ent_coef        | 0.088    |\n",
      "|    ent_coef_loss   | -0.419   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4429877  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4430000, episode_reward=5653.74 +/- 72.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -396     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.088    |\n",
      "|    ent_coef_loss   | -1.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 4.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5550     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26698    |\n",
      "|    total_timesteps | 4439978  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -404     |\n",
      "|    critic_loss     | 46.4     |\n",
      "|    ent_coef        | 0.0856   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4439877  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4440000, episode_reward=5831.57 +/- 133.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 42.8     |\n",
      "|    ent_coef        | 0.0857   |\n",
      "|    ent_coef_loss   | -0.0682  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4439899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 4.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5560     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26756    |\n",
      "|    total_timesteps | 4449628  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -394     |\n",
      "|    critic_loss     | 50.4     |\n",
      "|    ent_coef        | 0.0855   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4449527  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4450000, episode_reward=5405.60 +/- 513.89\n",
      "Episode length: 952.80 +/- 94.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 953      |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -404     |\n",
      "|    critic_loss     | 87.5     |\n",
      "|    ent_coef        | 0.0862   |\n",
      "|    ent_coef_loss   | 0.479    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5570     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26809    |\n",
      "|    total_timesteps | 4458520  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -0.109   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4458419  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4460000, episode_reward=4805.27 +/- 1780.59\n",
      "Episode length: 993.40 +/- 13.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 993      |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -408     |\n",
      "|    critic_loss     | 87.4     |\n",
      "|    ent_coef        | 0.0842   |\n",
      "|    ent_coef_loss   | 0.533    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5580     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26858    |\n",
      "|    total_timesteps | 4466706  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -397     |\n",
      "|    critic_loss     | 43.6     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -0.449   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4466605  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470000, episode_reward=4645.29 +/- 1340.07\n",
      "Episode length: 826.40 +/- 232.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 826      |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 31.6     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -0.136   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 4.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5590     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26899    |\n",
      "|    total_timesteps | 4473500  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 47.7     |\n",
      "|    ent_coef        | 0.0857   |\n",
      "|    ent_coef_loss   | -0.576   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4473399  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4480000, episode_reward=4035.19 +/- 2100.65\n",
      "Episode length: 709.00 +/- 363.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 709      |\n",
      "|    mean_reward     | 4.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -410     |\n",
      "|    critic_loss     | 56.3     |\n",
      "|    ent_coef        | 0.0842   |\n",
      "|    ent_coef_loss   | -0.149   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5600     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26951    |\n",
      "|    total_timesteps | 4482213  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    ent_coef        | 0.0851   |\n",
      "|    ent_coef_loss   | -0.579   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4482112  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4490000, episode_reward=4517.43 +/- 2222.15\n",
      "Episode length: 808.40 +/- 383.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 808      |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -410     |\n",
      "|    critic_loss     | 19.9     |\n",
      "|    ent_coef        | 0.0862   |\n",
      "|    ent_coef_loss   | -0.0943  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 4.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5610     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27004    |\n",
      "|    total_timesteps | 4490921  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | -0.266   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4490820  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5620     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27058    |\n",
      "|    total_timesteps | 4499938  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 48.8     |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4499837  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500000, episode_reward=4937.23 +/- 1599.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 45.5     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | -0.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5630     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27109    |\n",
      "|    total_timesteps | 4508453  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -414     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    ent_coef        | 0.0837   |\n",
      "|    ent_coef_loss   | 0.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4508352  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4510000, episode_reward=4465.94 +/- 2704.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 80.5     |\n",
      "|    ent_coef        | 0.086    |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5640     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27164    |\n",
      "|    total_timesteps | 4517334  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -419     |\n",
      "|    critic_loss     | 95.6     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4517233  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4520000, episode_reward=3362.36 +/- 2998.65\n",
      "Episode length: 980.40 +/- 39.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 980      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -419     |\n",
      "|    critic_loss     | 45.7     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -0.616   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5650     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27224    |\n",
      "|    total_timesteps | 4527298  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 37       |\n",
      "|    ent_coef        | 0.084    |\n",
      "|    ent_coef_loss   | 0.524    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4527197  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4530000, episode_reward=5680.73 +/- 111.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.517    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 4.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5660     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27278    |\n",
      "|    total_timesteps | 4536271  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 25.6     |\n",
      "|    ent_coef        | 0.0862   |\n",
      "|    ent_coef_loss   | -0.892   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4536170  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4540000, episode_reward=4767.49 +/- 1466.97\n",
      "Episode length: 940.40 +/- 119.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 940      |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 62       |\n",
      "|    ent_coef        | 0.0864   |\n",
      "|    ent_coef_loss   | 0.495    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5670     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27331    |\n",
      "|    total_timesteps | 4545066  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 42.2     |\n",
      "|    ent_coef        | 0.0879   |\n",
      "|    ent_coef_loss   | -0.319   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4544965  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4550000, episode_reward=4496.74 +/- 2241.29\n",
      "Episode length: 803.60 +/- 392.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 4.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    ent_coef        | 0.0855   |\n",
      "|    ent_coef_loss   | -0.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5680     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27368    |\n",
      "|    total_timesteps | 4551293  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -422     |\n",
      "|    critic_loss     | 34.9     |\n",
      "|    ent_coef        | 0.0874   |\n",
      "|    ent_coef_loss   | -0.553   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4551192  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 4.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5690     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27409    |\n",
      "|    total_timesteps | 4558282  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 36.7     |\n",
      "|    ent_coef        | 0.0882   |\n",
      "|    ent_coef_loss   | 0.354    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4558181  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4560000, episode_reward=4300.82 +/- 1904.92\n",
      "Episode length: 843.60 +/- 312.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 844      |\n",
      "|    mean_reward     | 4.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 398      |\n",
      "|    ent_coef        | 0.0888   |\n",
      "|    ent_coef_loss   | -0.268   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 4.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5700     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27456    |\n",
      "|    total_timesteps | 4566084  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 40.1     |\n",
      "|    ent_coef        | 0.088    |\n",
      "|    ent_coef_loss   | -0.387   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4565983  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4570000, episode_reward=4535.27 +/- 2582.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 46.2     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | 0.271    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5710     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27503    |\n",
      "|    total_timesteps | 4573859  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.0867   |\n",
      "|    ent_coef_loss   | 0.198    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4573758  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4580000, episode_reward=4694.87 +/- 1337.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 47       |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | 0.734    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 3.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5720     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27545    |\n",
      "|    total_timesteps | 4580685  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 141      |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | 0.196    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4580584  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5730     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27593    |\n",
      "|    total_timesteps | 4588917  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -422     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.085    |\n",
      "|    ent_coef_loss   | 0.828    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4588816  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4590000, episode_reward=3939.49 +/- 2366.26\n",
      "Episode length: 689.40 +/- 386.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 689      |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 36.3     |\n",
      "|    ent_coef        | 0.0853   |\n",
      "|    ent_coef_loss   | -0.373   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5740     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27650    |\n",
      "|    total_timesteps | 4598531  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -0.708   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4598430  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4600000, episode_reward=5809.90 +/- 113.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -0.969   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5750     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27688    |\n",
      "|    total_timesteps | 4604797  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 24.7     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.307    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4604696  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4610000, episode_reward=4152.57 +/- 2023.81\n",
      "Episode length: 730.00 +/- 346.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 730      |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 53.4     |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5760     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27742    |\n",
      "|    total_timesteps | 4613829  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 39       |\n",
      "|    ent_coef        | 0.085    |\n",
      "|    ent_coef_loss   | -0.399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4613728  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620000, episode_reward=4745.04 +/- 1515.90\n",
      "Episode length: 945.80 +/- 108.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 946      |\n",
      "|    mean_reward     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.0868   |\n",
      "|    ent_coef_loss   | -0.221   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5770     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27784    |\n",
      "|    total_timesteps | 4620671  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 26.9     |\n",
      "|    ent_coef        | 0.0867   |\n",
      "|    ent_coef_loss   | -0.835   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4620570  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5780     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27837    |\n",
      "|    total_timesteps | 4629740  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 87.8     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4629639  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4630000, episode_reward=5306.98 +/- 885.84\n",
      "Episode length: 928.20 +/- 143.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 928      |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 68.7     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | -0.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5790     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27877    |\n",
      "|    total_timesteps | 4636313  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -416     |\n",
      "|    critic_loss     | 227      |\n",
      "|    ent_coef        | 0.0855   |\n",
      "|    ent_coef_loss   | -0.0474  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4636212  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4640000, episode_reward=3883.60 +/- 2384.34\n",
      "Episode length: 677.00 +/- 410.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 677      |\n",
      "|    mean_reward     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 36.4     |\n",
      "|    ent_coef        | 0.0852   |\n",
      "|    ent_coef_loss   | 0.328    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 3.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5800     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27925    |\n",
      "|    total_timesteps | 4644497  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 37.6     |\n",
      "|    ent_coef        | 0.0877   |\n",
      "|    ent_coef_loss   | -0.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4644396  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4650000, episode_reward=5729.87 +/- 159.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 47.2     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | -0.686   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5810     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27981    |\n",
      "|    total_timesteps | 4653740  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 31.8     |\n",
      "|    ent_coef        | 0.089    |\n",
      "|    ent_coef_loss   | 0.853    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4653639  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4660000, episode_reward=4558.00 +/- 1846.57\n",
      "Episode length: 805.20 +/- 314.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 805      |\n",
      "|    mean_reward     | 4.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 56.9     |\n",
      "|    ent_coef        | 0.0879   |\n",
      "|    ent_coef_loss   | 2.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 3.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5820     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28020    |\n",
      "|    total_timesteps | 4660208  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 37.7     |\n",
      "|    ent_coef        | 0.0882   |\n",
      "|    ent_coef_loss   | -0.194   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4660107  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4670000, episode_reward=4759.31 +/- 1867.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 38.7     |\n",
      "|    ent_coef        | 0.0881   |\n",
      "|    ent_coef_loss   | 0.0889   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5830     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28079    |\n",
      "|    total_timesteps | 4670000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5840     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28132    |\n",
      "|    total_timesteps | 4679025  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    ent_coef        | 0.0868   |\n",
      "|    ent_coef_loss   | 0.0677   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4678924  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680000, episode_reward=3895.41 +/- 2144.59\n",
      "Episode length: 697.80 +/- 373.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 698      |\n",
      "|    mean_reward     | 3.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 65.9     |\n",
      "|    ent_coef        | 0.0872   |\n",
      "|    ent_coef_loss   | -0.186   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5850     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28185    |\n",
      "|    total_timesteps | 4687883  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | -0.152   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4687782  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4690000, episode_reward=3963.29 +/- 2182.47\n",
      "Episode length: 718.00 +/- 385.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 718      |\n",
      "|    mean_reward     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 295      |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | 0.0273   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5860     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28236    |\n",
      "|    total_timesteps | 4696486  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 65.3     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.161    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4696385  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4700000, episode_reward=2980.55 +/- 3483.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -437     |\n",
      "|    critic_loss     | 29       |\n",
      "|    ent_coef        | 0.0858   |\n",
      "|    ent_coef_loss   | -0.199   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5870     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28294    |\n",
      "|    total_timesteps | 4706036  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 35.8     |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -0.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4705935  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710000, episode_reward=2620.45 +/- 2617.60\n",
      "Episode length: 792.40 +/- 330.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 792      |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 30.1     |\n",
      "|    ent_coef        | 0.085    |\n",
      "|    ent_coef_loss   | -0.872   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5880     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28342    |\n",
      "|    total_timesteps | 4714027  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 30       |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | 0.161    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4713926  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4720000, episode_reward=3129.21 +/- 3272.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    ent_coef        | 0.0864   |\n",
      "|    ent_coef_loss   | -0.671   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5890     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28397    |\n",
      "|    total_timesteps | 4723207  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -438     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -0.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4723106  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4730000, episode_reward=2586.02 +/- 2571.18\n",
      "Episode length: 683.80 +/- 404.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 684      |\n",
      "|    mean_reward     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 43.4     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | -0.399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 3.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5900     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28446    |\n",
      "|    total_timesteps | 4731446  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 35.3     |\n",
      "|    ent_coef        | 0.0881   |\n",
      "|    ent_coef_loss   | 0.641    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4731345  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5910     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28484    |\n",
      "|    total_timesteps | 4737859  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -432     |\n",
      "|    critic_loss     | 26.1     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4737758  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4740000, episode_reward=687.72 +/- 2754.81\n",
      "Episode length: 806.40 +/- 387.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 688      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 43.4     |\n",
      "|    ent_coef        | 0.0878   |\n",
      "|    ent_coef_loss   | -0.358   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5920     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28527    |\n",
      "|    total_timesteps | 4745114  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | -0.836   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4745013  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4750000, episode_reward=4706.40 +/- 2080.69\n",
      "Episode length: 824.40 +/- 351.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0888   |\n",
      "|    ent_coef_loss   | -0.675   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5930     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28564    |\n",
      "|    total_timesteps | 4751099  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 63       |\n",
      "|    ent_coef        | 0.0881   |\n",
      "|    ent_coef_loss   | 0.115    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4750998  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5940     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28613    |\n",
      "|    total_timesteps | 4759463  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | -0.827   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4759362  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4760000, episode_reward=5304.14 +/- 1021.59\n",
      "Episode length: 916.40 +/- 167.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 916      |\n",
      "|    mean_reward     | 5.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 67.8     |\n",
      "|    ent_coef        | 0.0887   |\n",
      "|    ent_coef_loss   | -0.324   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5950     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28664    |\n",
      "|    total_timesteps | 4768013  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 87.3     |\n",
      "|    ent_coef        | 0.0886   |\n",
      "|    ent_coef_loss   | 0.595    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4767912  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4770000, episode_reward=5626.98 +/- 118.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 23.9     |\n",
      "|    ent_coef        | 0.0884   |\n",
      "|    ent_coef_loss   | -1.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5960     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28714    |\n",
      "|    total_timesteps | 4776274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 61.7     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | 1.47     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4776173  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4780000, episode_reward=5739.96 +/- 118.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -438     |\n",
      "|    critic_loss     | 49.1     |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5970     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28768    |\n",
      "|    total_timesteps | 4785268  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -437     |\n",
      "|    critic_loss     | 33.2     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | -0.393   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4785167  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4790000, episode_reward=5610.98 +/- 145.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -432     |\n",
      "|    critic_loss     | 66.3     |\n",
      "|    ent_coef        | 0.0884   |\n",
      "|    ent_coef_loss   | 0.336    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5980     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28803    |\n",
      "|    total_timesteps | 4791006  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 67.6     |\n",
      "|    ent_coef        | 0.0887   |\n",
      "|    ent_coef_loss   | -0.406   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4790905  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 763      |\n",
      "|    ep_rew_mean     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5990     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28853    |\n",
      "|    total_timesteps | 4799490  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 48.4     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | 0.0611   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4799389  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800000, episode_reward=3801.66 +/- 2409.76\n",
      "Episode length: 672.40 +/- 405.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 672      |\n",
      "|    mean_reward     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    ent_coef        | 0.0896   |\n",
      "|    ent_coef_loss   | -0.191   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6000     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28899    |\n",
      "|    total_timesteps | 4807215  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 61.5     |\n",
      "|    ent_coef        | 0.0891   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4807114  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4810000, episode_reward=4960.28 +/- 1523.31\n",
      "Episode length: 872.00 +/- 256.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 872      |\n",
      "|    mean_reward     | 4.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    ent_coef        | 0.0878   |\n",
      "|    ent_coef_loss   | 0.294    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6010     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28950    |\n",
      "|    total_timesteps | 4815532  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 30.8     |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | -0.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4815431  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4820000, episode_reward=5785.24 +/- 65.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 50.6     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | 0.456    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6020     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28995    |\n",
      "|    total_timesteps | 4822844  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.0885   |\n",
      "|    ent_coef_loss   | -0.968   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4822743  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4830000, episode_reward=5659.66 +/- 194.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 49.5     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6030     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29052    |\n",
      "|    total_timesteps | 4832120  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 31.9     |\n",
      "|    ent_coef        | 0.09     |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4832019  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4840000, episode_reward=3827.06 +/- 2441.77\n",
      "Episode length: 816.20 +/- 367.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 816      |\n",
      "|    mean_reward     | 3.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    ent_coef        | 0.0884   |\n",
      "|    ent_coef_loss   | 0.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 4.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6040     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29104    |\n",
      "|    total_timesteps | 4840894  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 37.5     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | -0.638   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4840793  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 4.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6050     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29156    |\n",
      "|    total_timesteps | 4849849  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 49.4     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | 0.131    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4849748  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4850000, episode_reward=4898.70 +/- 1830.50\n",
      "Episode length: 846.60 +/- 306.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 847      |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 164      |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | -0.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6060     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29200    |\n",
      "|    total_timesteps | 4857189  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 459      |\n",
      "|    ent_coef        | 0.0904   |\n",
      "|    ent_coef_loss   | 0.991    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4857088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4860000, episode_reward=4898.87 +/- 1710.04\n",
      "Episode length: 852.00 +/- 296.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 852      |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 74.5     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | 0.627    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6070     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29240    |\n",
      "|    total_timesteps | 4863788  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 32.6     |\n",
      "|    ent_coef        | 0.0908   |\n",
      "|    ent_coef_loss   | 0.054    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4863687  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4870000, episode_reward=5825.41 +/- 59.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    ent_coef        | 0.0909   |\n",
      "|    ent_coef_loss   | -1.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6080     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29290    |\n",
      "|    total_timesteps | 4872100  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -416     |\n",
      "|    critic_loss     | 22.2     |\n",
      "|    ent_coef        | 0.0884   |\n",
      "|    ent_coef_loss   | 0.239    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4871999  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4880000, episode_reward=5796.86 +/- 109.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 32.3     |\n",
      "|    ent_coef        | 0.0905   |\n",
      "|    ent_coef_loss   | 0.00703  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6090     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29340    |\n",
      "|    total_timesteps | 4880173  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    ent_coef        | 0.0906   |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4880072  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6100     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29394    |\n",
      "|    total_timesteps | 4889495  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 39.1     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | -0.903   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4889394  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4890000, episode_reward=5784.38 +/- 136.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 51.3     |\n",
      "|    ent_coef        | 0.0925   |\n",
      "|    ent_coef_loss   | 0.473    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6110     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29446    |\n",
      "|    total_timesteps | 4898303  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 37       |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | -0.0241  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4898202  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4900000, episode_reward=4757.74 +/- 1179.47\n",
      "Episode length: 833.00 +/- 193.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 833      |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 41.8     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | 0.433    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6120     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29494    |\n",
      "|    total_timesteps | 4906365  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    ent_coef        | 0.0897   |\n",
      "|    ent_coef_loss   | -0.285   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4906264  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4910000, episode_reward=3563.41 +/- 2034.68\n",
      "Episode length: 633.00 +/- 348.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 633      |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 60.9     |\n",
      "|    ent_coef        | 0.0911   |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6130     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29541    |\n",
      "|    total_timesteps | 4914349  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 37.3     |\n",
      "|    ent_coef        | 0.092    |\n",
      "|    ent_coef_loss   | 0.0778   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4914248  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4920000, episode_reward=5235.24 +/- 867.32\n",
      "Episode length: 928.60 +/- 142.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 929      |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -419     |\n",
      "|    critic_loss     | 52.9     |\n",
      "|    ent_coef        | 0.0916   |\n",
      "|    ent_coef_loss   | -0.434   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 4.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6140     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29594    |\n",
      "|    total_timesteps | 4923189  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0926   |\n",
      "|    ent_coef_loss   | -0.635   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4923088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4930000, episode_reward=3146.29 +/- 3114.27\n",
      "Episode length: 804.20 +/- 391.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -416     |\n",
      "|    critic_loss     | 22.1     |\n",
      "|    ent_coef        | 0.0905   |\n",
      "|    ent_coef_loss   | -0.329   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6150     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29650    |\n",
      "|    total_timesteps | 4932648  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 32.2     |\n",
      "|    ent_coef        | 0.0906   |\n",
      "|    ent_coef_loss   | -0.579   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4932547  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4940000, episode_reward=5764.24 +/- 83.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 50.9     |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | 0.658    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6160     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29706    |\n",
      "|    total_timesteps | 4941946  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 43.6     |\n",
      "|    ent_coef        | 0.0904   |\n",
      "|    ent_coef_loss   | 0.643    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4941845  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4950000, episode_reward=5690.82 +/- 108.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    ent_coef        | 0.0914   |\n",
      "|    ent_coef_loss   | -0.716   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6170     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29758    |\n",
      "|    total_timesteps | 4950812  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 33.7     |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | -0.242   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4950711  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 4.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6180     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29802    |\n",
      "|    total_timesteps | 4958484  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -408     |\n",
      "|    critic_loss     | 38.5     |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | 0.301    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4958383  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4960000, episode_reward=5832.43 +/- 97.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 32.1     |\n",
      "|    ent_coef        | 0.0902   |\n",
      "|    ent_coef_loss   | 0.558    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4959899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 4.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6190     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29856    |\n",
      "|    total_timesteps | 4967520  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 26.8     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | 0.516    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4967419  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4970000, episode_reward=5726.31 +/- 50.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -422     |\n",
      "|    critic_loss     | 79.9     |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | -0.873   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 4.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6200     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29910    |\n",
      "|    total_timesteps | 4976625  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 49       |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -0.288   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4976524  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4980000, episode_reward=5055.23 +/- 1547.19\n",
      "Episode length: 875.40 +/- 249.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 875      |\n",
      "|    mean_reward     | 5.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | -0.0167  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6210     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29964    |\n",
      "|    total_timesteps | 4985588  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4985487  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4990000, episode_reward=5873.83 +/- 81.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 30       |\n",
      "|    ent_coef        | 0.09     |\n",
      "|    ent_coef_loss   | 0.223    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4989899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 4.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6220     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 30016    |\n",
      "|    total_timesteps | 4994288  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -419     |\n",
      "|    critic_loss     | 59       |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | 0.819    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4994187  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000000, episode_reward=5803.81 +/- 79.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 41.6     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | 0.205    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4999899  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ant-v4\n",
    "env_id = 'Ant-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "sac_model = SAC(\"MlpPolicy\", env, verbose=10)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=f'logs/{env_id}')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'logs/{env_id}/best_model',\n",
    "                             log_path=f'logs/{env_id}/results', eval_freq=10000)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "sac_model.learn(total_timesteps=5e6, log_interval=10, callback=callback)\n",
    "sac_model.save(f\"logs/expert/{env_id}-sac/model5e6\")\n",
    "sac_model.save_replay_buffer(f\"logs/expert/{env_id}-sac/buffer5e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=10000, episode_reward=408.27 +/- 53.35\n",
      "Episode length: 207.80 +/- 22.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 208      |\n",
      "|    mean_reward     | 408      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69.2    |\n",
      "|    critic_loss     | 8.6      |\n",
      "|    ent_coef        | 0.0685   |\n",
      "|    ent_coef_loss   | -6.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=399.41 +/- 104.15\n",
      "Episode length: 277.40 +/- 50.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 277      |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.2    |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=490.37 +/- 252.29\n",
      "Episode length: 458.40 +/- 240.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 458      |\n",
      "|    mean_reward     | 490      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -103     |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=356.83 +/- 53.57\n",
      "Episode length: 194.60 +/- 25.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 195      |\n",
      "|    mean_reward     | 357      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -113     |\n",
      "|    critic_loss     | 13.4     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | 0.251    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=469.70 +/- 268.04\n",
      "Episode length: 555.40 +/- 273.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 555      |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -114     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | -0.794   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=446.23 +/- 24.68\n",
      "Episode length: 241.20 +/- 29.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 241      |\n",
      "|    mean_reward     | 446      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -114     |\n",
      "|    critic_loss     | 8.74     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=414.28 +/- 48.80\n",
      "Episode length: 231.60 +/- 32.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 232      |\n",
      "|    mean_reward     | 414      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -115     |\n",
      "|    critic_loss     | 7.38     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.569    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=442.93 +/- 133.99\n",
      "Episode length: 200.20 +/- 38.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 443      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 7.41     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 1.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=635.34 +/- 111.12\n",
      "Episode length: 300.00 +/- 54.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 300      |\n",
      "|    mean_reward     | 635      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 7.1      |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | -0.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=426.65 +/- 26.27\n",
      "Episode length: 238.20 +/- 14.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 238      |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -116     |\n",
      "|    critic_loss     | 8.55     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 2.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=715.57 +/- 142.91\n",
      "Episode length: 283.20 +/- 42.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 283      |\n",
      "|    mean_reward     | 716      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -119     |\n",
      "|    critic_loss     | 8.78     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.989    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=621.91 +/- 56.61\n",
      "Episode length: 256.00 +/- 42.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 256      |\n",
      "|    mean_reward     | 622      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -125     |\n",
      "|    critic_loss     | 8.52     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 119899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=515.15 +/- 79.61\n",
      "Episode length: 212.20 +/- 20.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 212      |\n",
      "|    mean_reward     | 515      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -126     |\n",
      "|    critic_loss     | 8.18     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | -0.228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 129899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=512.45 +/- 87.56\n",
      "Episode length: 223.20 +/- 31.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 223      |\n",
      "|    mean_reward     | 512      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -129     |\n",
      "|    critic_loss     | 7.84     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | -0.824   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 139899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=854.85 +/- 450.56\n",
      "Episode length: 519.20 +/- 228.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 519      |\n",
      "|    mean_reward     | 855      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 8.1      |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=690.69 +/- 67.98\n",
      "Episode length: 265.20 +/- 30.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 265      |\n",
      "|    mean_reward     | 691      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -134     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | -0.0493  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=482.58 +/- 338.21\n",
      "Episode length: 228.40 +/- 116.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | 483      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.527    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=750.04 +/- 284.56\n",
      "Episode length: 301.00 +/- 80.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 301      |\n",
      "|    mean_reward     | 750      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -140     |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | -0.861   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=714.61 +/- 340.60\n",
      "Episode length: 283.00 +/- 106.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 283      |\n",
      "|    mean_reward     | 715      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 8.91     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | -0.314   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=875.77 +/- 67.07\n",
      "Episode length: 308.80 +/- 16.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 309      |\n",
      "|    mean_reward     | 876      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -149     |\n",
      "|    critic_loss     | 9.35     |\n",
      "|    ent_coef        | 0.0473   |\n",
      "|    ent_coef_loss   | -0.179   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 199899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=210000, episode_reward=1122.77 +/- 543.00\n",
      "Episode length: 418.80 +/- 146.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 419      |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -157     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 209899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=2060.36 +/- 769.33\n",
      "Episode length: 660.40 +/- 230.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 660      |\n",
      "|    mean_reward     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -158     |\n",
      "|    critic_loss     | 8.94     |\n",
      "|    ent_coef        | 0.0479   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=230000, episode_reward=1214.06 +/- 198.09\n",
      "Episode length: 390.60 +/- 48.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 391      |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 12.2     |\n",
      "|    ent_coef        | 0.0496   |\n",
      "|    ent_coef_loss   | 0.424    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 229899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=1736.82 +/- 400.21\n",
      "Episode length: 521.00 +/- 105.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 521      |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0501   |\n",
      "|    ent_coef_loss   | -0.677   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 239899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=1444.84 +/- 769.48\n",
      "Episode length: 438.40 +/- 208.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 438      |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -171     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.0493   |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 249899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=3430.55 +/- 19.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 9.93     |\n",
      "|    ent_coef        | 0.0517   |\n",
      "|    ent_coef_loss   | -0.428   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=270000, episode_reward=1225.30 +/- 972.31\n",
      "Episode length: 411.80 +/- 294.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 412      |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 12.2     |\n",
      "|    ent_coef        | 0.0521   |\n",
      "|    ent_coef_loss   | 0.638    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=3390.18 +/- 41.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -189     |\n",
      "|    critic_loss     | 9.63     |\n",
      "|    ent_coef        | 0.0529   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 279899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=2562.49 +/- 1069.88\n",
      "Episode length: 761.80 +/- 299.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 762      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -200     |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | 0.749    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 289899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=2449.80 +/- 1231.23\n",
      "Episode length: 726.20 +/- 335.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 726      |\n",
      "|    mean_reward     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -201     |\n",
      "|    critic_loss     | 16       |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | -0.374   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=3524.42 +/- 34.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -206     |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    ent_coef        | 0.0557   |\n",
      "|    ent_coef_loss   | -0.406   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=3010.03 +/- 605.71\n",
      "Episode length: 868.00 +/- 166.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 868      |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    ent_coef        | 0.0561   |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=960.67 +/- 275.88\n",
      "Episode length: 314.60 +/- 77.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 315      |\n",
      "|    mean_reward     | 961      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -212     |\n",
      "|    critic_loss     | 15       |\n",
      "|    ent_coef        | 0.0549   |\n",
      "|    ent_coef_loss   | 0.332    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 329899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=3362.16 +/- 419.34\n",
      "Episode length: 910.20 +/- 114.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 910      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0562   |\n",
      "|    ent_coef_loss   | 0.131    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 339899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=3540.57 +/- 69.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -215     |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.0553   |\n",
      "|    ent_coef_loss   | -0.611   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=2849.77 +/- 1188.28\n",
      "Episode length: 792.60 +/- 308.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 793      |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    ent_coef        | 0.0555   |\n",
      "|    ent_coef_loss   | 0.118    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=3679.63 +/- 25.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 9.3      |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | 1.13     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=380000, episode_reward=3707.91 +/- 25.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -231     |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 379899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=390000, episode_reward=3823.44 +/- 32.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -245     |\n",
      "|    critic_loss     | 9.05     |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | -0.162   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 389899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=3765.66 +/- 98.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -229     |\n",
      "|    critic_loss     | 9.98     |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | -0.304   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=3598.67 +/- 15.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0528   |\n",
      "|    ent_coef_loss   | 0.356    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 409899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=3794.83 +/- 14.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0542   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 419899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=3784.91 +/- 30.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -259     |\n",
      "|    critic_loss     | 9.56     |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | -0.475   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 429899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=3921.59 +/- 35.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -249     |\n",
      "|    critic_loss     | 6.95     |\n",
      "|    ent_coef        | 0.0549   |\n",
      "|    ent_coef_loss   | 0.719    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=3788.99 +/- 58.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 8.45     |\n",
      "|    ent_coef        | 0.0552   |\n",
      "|    ent_coef_loss   | 0.742    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 449899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=3807.14 +/- 88.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 7.74     |\n",
      "|    ent_coef        | 0.0541   |\n",
      "|    ent_coef_loss   | 0.0269   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 459899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=3812.52 +/- 69.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -271     |\n",
      "|    critic_loss     | 9.56     |\n",
      "|    ent_coef        | 0.0577   |\n",
      "|    ent_coef_loss   | 0.934    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 469899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=3457.70 +/- 781.48\n",
      "Episode length: 906.00 +/- 188.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 906      |\n",
      "|    mean_reward     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0587   |\n",
      "|    ent_coef_loss   | -0.0502  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 479899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=3854.13 +/- 53.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 11.6     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.589   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 489899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=3857.03 +/- 72.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -270     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0566   |\n",
      "|    ent_coef_loss   | 0.492    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=3681.98 +/- 151.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0567   |\n",
      "|    ent_coef_loss   | -0.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 509899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=3890.88 +/- 23.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -269     |\n",
      "|    critic_loss     | 12.2     |\n",
      "|    ent_coef        | 0.0566   |\n",
      "|    ent_coef_loss   | -0.418   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 519899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=3858.91 +/- 21.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -278     |\n",
      "|    critic_loss     | 7.45     |\n",
      "|    ent_coef        | 0.0562   |\n",
      "|    ent_coef_loss   | -0.653   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 529899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=3841.96 +/- 10.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.057    |\n",
      "|    ent_coef_loss   | -0.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 539899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=3779.72 +/- 44.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    ent_coef        | 0.0585   |\n",
      "|    ent_coef_loss   | 0.246    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=3880.91 +/- 58.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.0582   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 559899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=3882.41 +/- 54.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -285     |\n",
      "|    critic_loss     | 7.29     |\n",
      "|    ent_coef        | 0.0555   |\n",
      "|    ent_coef_loss   | 0.368    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 569899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=3880.59 +/- 43.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 9.35     |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | 0.482    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 579899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=3927.54 +/- 71.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 9.14     |\n",
      "|    ent_coef        | 0.0552   |\n",
      "|    ent_coef_loss   | 1.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 589899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=600000, episode_reward=3976.94 +/- 16.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | -0.671   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 599899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=610000, episode_reward=4012.27 +/- 65.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 7.22     |\n",
      "|    ent_coef        | 0.0549   |\n",
      "|    ent_coef_loss   | -0.446   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=620000, episode_reward=3961.03 +/- 37.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 7.36     |\n",
      "|    ent_coef        | 0.0541   |\n",
      "|    ent_coef_loss   | -0.0989  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=3924.82 +/- 70.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 15.9     |\n",
      "|    ent_coef        | 0.056    |\n",
      "|    ent_coef_loss   | 1.94     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 629899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=3854.13 +/- 15.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.0539   |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 639899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=3926.85 +/- 61.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 8.39     |\n",
      "|    ent_coef        | 0.0571   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 649899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=3967.35 +/- 94.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 7.72     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | -0.501   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 659899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=3765.07 +/- 521.36\n",
      "Episode length: 940.60 +/- 118.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 941      |\n",
      "|    mean_reward     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.0534   |\n",
      "|    ent_coef_loss   | -0.377   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=4009.96 +/- 76.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 679899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=3937.87 +/- 26.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 9.31     |\n",
      "|    ent_coef        | 0.0539   |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 689899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=4057.15 +/- 21.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=710000, episode_reward=4079.59 +/- 28.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 8.06     |\n",
      "|    ent_coef        | 0.0543   |\n",
      "|    ent_coef_loss   | 0.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 709899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=720000, episode_reward=4097.65 +/- 23.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 6.57     |\n",
      "|    ent_coef        | 0.0537   |\n",
      "|    ent_coef_loss   | -0.959   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 719899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=730000, episode_reward=3952.66 +/- 24.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0526   |\n",
      "|    ent_coef_loss   | 0.299    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 729899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=3542.31 +/- 1147.49\n",
      "Episode length: 869.40 +/- 261.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 869      |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 6.12     |\n",
      "|    ent_coef        | 0.0528   |\n",
      "|    ent_coef_loss   | -0.678   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 739899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=4122.15 +/- 56.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 7.49     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | 0.103    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 749899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=760000, episode_reward=4094.15 +/- 51.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 9.28     |\n",
      "|    ent_coef        | 0.0517   |\n",
      "|    ent_coef_loss   | 0.231    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 759899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=4113.74 +/- 41.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 9.11     |\n",
      "|    ent_coef        | 0.0519   |\n",
      "|    ent_coef_loss   | 0.0729   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 769899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=4136.93 +/- 26.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 7.77     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | 0.495    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 779899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=790000, episode_reward=4107.96 +/- 20.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.0485   |\n",
      "|    ent_coef_loss   | 1.94     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 789899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=4058.88 +/- 79.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 5.16     |\n",
      "|    ent_coef        | 0.0498   |\n",
      "|    ent_coef_loss   | -0.446   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=4176.67 +/- 69.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 16.8     |\n",
      "|    ent_coef        | 0.0489   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 809899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=820000, episode_reward=4024.43 +/- 16.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 7.08     |\n",
      "|    ent_coef        | 0.0492   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 819899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=4137.73 +/- 103.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 7.6      |\n",
      "|    ent_coef        | 0.0493   |\n",
      "|    ent_coef_loss   | -0.929   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 829899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=4125.03 +/- 22.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 6.6      |\n",
      "|    ent_coef        | 0.0495   |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 839899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=4103.09 +/- 14.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 5        |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | 0.205    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 849899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=4125.55 +/- 13.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 7.89     |\n",
      "|    ent_coef        | 0.0476   |\n",
      "|    ent_coef_loss   | -0.463   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 859899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=4179.23 +/- 24.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | -0.533   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 869899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=880000, episode_reward=4189.38 +/- 17.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 7.48     |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=890000, episode_reward=4213.61 +/- 19.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 7.91     |\n",
      "|    ent_coef        | 0.048    |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 889899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=900000, episode_reward=4089.97 +/- 15.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 5.26     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=4070.50 +/- 25.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 4.76     |\n",
      "|    ent_coef        | 0.0487   |\n",
      "|    ent_coef_loss   | -0.287   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 909899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=4216.11 +/- 41.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 5.53     |\n",
      "|    ent_coef        | 0.0489   |\n",
      "|    ent_coef_loss   | -0.205   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 919899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=930000, episode_reward=4242.63 +/- 8.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 8.17     |\n",
      "|    ent_coef        | 0.0485   |\n",
      "|    ent_coef_loss   | -2.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 929899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=940000, episode_reward=4099.18 +/- 26.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 0.256    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 939899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=4132.09 +/- 44.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 8.37     |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 949899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=4201.02 +/- 12.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 5.72     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | 0.288    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=4195.53 +/- 26.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | -0.736   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 969899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=4228.62 +/- 28.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 5.01     |\n",
      "|    ent_coef        | 0.0486   |\n",
      "|    ent_coef_loss   | -0.742   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 979899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=4224.25 +/- 12.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    ent_coef        | 0.0475   |\n",
      "|    ent_coef_loss   | 0.767    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 989899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=4275.06 +/- 13.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 11.5     |\n",
      "|    ent_coef        | 0.0469   |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1010000, episode_reward=4208.59 +/- 31.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 6.66     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1009899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=4151.75 +/- 41.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 6.71     |\n",
      "|    ent_coef        | 0.0456   |\n",
      "|    ent_coef_loss   | -0.145   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=4256.00 +/- 52.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 5.35     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 0.685    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=4230.50 +/- 45.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 7.99     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | -2.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1039899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=4171.50 +/- 39.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 4.61     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 0.396    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=4284.03 +/- 17.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 7.92     |\n",
      "|    ent_coef        | 0.0452   |\n",
      "|    ent_coef_loss   | 0.489    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1059899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1070000, episode_reward=4250.48 +/- 41.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 5.4      |\n",
      "|    ent_coef        | 0.0447   |\n",
      "|    ent_coef_loss   | -0.238   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=4241.09 +/- 24.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 5.42     |\n",
      "|    ent_coef        | 0.0429   |\n",
      "|    ent_coef_loss   | 1.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1079899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=4152.80 +/- 26.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 4.35     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1089899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=4275.62 +/- 56.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 4.66     |\n",
      "|    ent_coef        | 0.0413   |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=4219.86 +/- 25.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 4.13     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | -0.934   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1109899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=4276.29 +/- 8.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 7.05     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | -0.604   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=4205.60 +/- 26.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 5.53     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | 0.317    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=4093.62 +/- 11.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | 0.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1139899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=4239.08 +/- 48.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -376     |\n",
      "|    critic_loss     | 3.9      |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1149899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=4208.41 +/- 38.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -382     |\n",
      "|    critic_loss     | 7.07     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | 0.0584   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1159899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=4316.35 +/- 21.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -383     |\n",
      "|    critic_loss     | 3.3      |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.975   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1169899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1180000, episode_reward=3459.55 +/- 1534.72\n",
      "Episode length: 830.80 +/- 338.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 831      |\n",
      "|    mean_reward     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -384     |\n",
      "|    critic_loss     | 3.54     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | 0.134    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1179899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=4224.59 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -382     |\n",
      "|    critic_loss     | 3.69     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1189899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=4157.94 +/- 21.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -387     |\n",
      "|    critic_loss     | 2.36     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.812    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1199899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=2632.82 +/- 1920.99\n",
      "Episode length: 646.60 +/- 433.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 647      |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -388     |\n",
      "|    critic_loss     | 2.74     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.438   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1209899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=4261.47 +/- 31.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -388     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.255    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1219899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=4271.86 +/- 12.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -392     |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.487   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1229899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=4354.03 +/- 18.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -398     |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 2.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1250000, episode_reward=4342.11 +/- 18.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -390     |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.556    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1249899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=4235.24 +/- 105.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -396     |\n",
      "|    critic_loss     | 2.71     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -0.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=4310.55 +/- 33.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -399     |\n",
      "|    critic_loss     | 3.52     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.514   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1269899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=4386.33 +/- 17.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -394     |\n",
      "|    critic_loss     | 3.17     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.561   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1279899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1290000, episode_reward=4353.83 +/- 47.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -398     |\n",
      "|    critic_loss     | 3.6      |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.993    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1289899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=4412.15 +/- 27.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -402     |\n",
      "|    critic_loss     | 2.91     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.155    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1299899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1310000, episode_reward=3921.28 +/- 696.27\n",
      "Episode length: 926.20 +/- 147.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 926      |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -398     |\n",
      "|    critic_loss     | 2.42     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1309899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=4441.93 +/- 33.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -403     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.208   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1319899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1330000, episode_reward=4388.87 +/- 18.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | -0.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1329899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=4344.66 +/- 31.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -403     |\n",
      "|    critic_loss     | 2.11     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -1.63    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1339899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=4124.85 +/- 660.95\n",
      "Episode length: 935.80 +/- 128.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 936      |\n",
      "|    mean_reward     | 4.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1349899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=3391.38 +/- 1612.75\n",
      "Episode length: 820.60 +/- 358.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 821      |\n",
      "|    mean_reward     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -406     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.322   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1359899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=4348.10 +/- 50.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 2.49     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 2.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1369899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=0.02 +/- 0.05\n",
      "Episode length: 9.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | 0.0238   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 3.31     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1379899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=4468.98 +/- 31.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 0.942    |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | -0.739   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1389899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1400000, episode_reward=4477.53 +/- 24.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | -0.252   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1399899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1410000, episode_reward=4550.61 +/- 82.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -413     |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1409899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1420000, episode_reward=4607.08 +/- 30.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -410     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -0.0365  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1419899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1430000, episode_reward=4551.19 +/- 27.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 8.99     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=4494.48 +/- 46.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -413     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -0.952   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1439899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=3574.99 +/- 1718.78\n",
      "Episode length: 816.80 +/- 366.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 817      |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -410     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 1.49     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1449899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=4335.29 +/- 121.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -413     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1459899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=4411.98 +/- 54.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 65.7     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.0793  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1469899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=3577.09 +/- 1699.87\n",
      "Episode length: 818.20 +/- 363.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 818      |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1479899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=4581.62 +/- 10.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | -0.828   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1489899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=3891.70 +/- 1234.57\n",
      "Episode length: 875.20 +/- 249.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 875      |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 3.31     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.384   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=4387.85 +/- 25.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -2.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1509899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=4522.30 +/- 50.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 2.58     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1519899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=4448.15 +/- 37.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 2.52     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.355   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1529899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=4375.87 +/- 96.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 2.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=3039.53 +/- 1874.17\n",
      "Episode length: 693.40 +/- 386.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 693      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 2.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=4542.89 +/- 23.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 0.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=4407.56 +/- 101.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 2.74     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 0.677    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1569899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=4452.51 +/- 113.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.735   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=4473.64 +/- 33.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 1.6      |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=4586.31 +/- 44.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 4.65     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1599899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=3368.35 +/- 1347.72\n",
      "Episode length: 776.00 +/- 277.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 776      |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 4.26     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1609899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=4640.51 +/- 53.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 3.48     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 0.615    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1619899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1630000, episode_reward=4578.99 +/- 31.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 2.27     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -1.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1629899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=4526.13 +/- 33.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -0.579   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1639899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=3679.55 +/- 1746.41\n",
      "Episode length: 818.00 +/- 364.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 818      |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 0.876    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1649899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=4645.01 +/- 21.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.137   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1659899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1670000, episode_reward=4634.99 +/- 56.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1669899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=4422.90 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | 0.373    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1679899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=4590.05 +/- 18.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 0.97     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | -2.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1689899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=4053.81 +/- 1028.09\n",
      "Episode length: 897.40 +/- 205.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 897      |\n",
      "|    mean_reward     | 4.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1699899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=4205.10 +/- 588.09\n",
      "Episode length: 942.60 +/- 114.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 943      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 1.95     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 2.74     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=4519.28 +/- 18.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.361   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=4627.86 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 6.17     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=4486.53 +/- 26.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 2.01     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=4634.98 +/- 11.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -432     |\n",
      "|    critic_loss     | 2.72     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1749899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=4650.89 +/- 9.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 0.872    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1759899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1770000, episode_reward=3736.09 +/- 1783.75\n",
      "Episode length: 823.60 +/- 352.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 3.11     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.275   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1769899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=4633.45 +/- 30.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -437     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1779899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=4672.79 +/- 55.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.227    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1800000, episode_reward=4649.06 +/- 21.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -0.329   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1799899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=3003.71 +/- 1836.82\n",
      "Episode length: 693.20 +/- 375.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 693      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.233   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1809899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=4651.41 +/- 65.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 2.14     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -0.441   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1819899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=4513.40 +/- 368.00\n",
      "Episode length: 965.20 +/- 69.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 965      |\n",
      "|    mean_reward     | 4.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -434     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | -0.202   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1829899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=4717.83 +/- 22.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -0.306   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1839899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1850000, episode_reward=4611.62 +/- 24.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -434     |\n",
      "|    critic_loss     | 4.24     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.527   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1849899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=4629.46 +/- 64.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.309    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1859899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=4737.74 +/- 7.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.666   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1880000, episode_reward=4594.34 +/- 172.88\n",
      "Episode length: 970.20 +/- 36.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 970      |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -434     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.624    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1879899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=4693.14 +/- 51.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.0412   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1889899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=4730.35 +/- 24.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 2.36     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.0803  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=4482.73 +/- 418.64\n",
      "Episode length: 958.80 +/- 82.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 959      |\n",
      "|    mean_reward     | 4.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -432     |\n",
      "|    critic_loss     | 3.14     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 1.86     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1909899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=3050.81 +/- 2116.59\n",
      "Episode length: 670.60 +/- 405.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 671      |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.815   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=3805.46 +/- 1768.54\n",
      "Episode length: 830.40 +/- 339.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 830      |\n",
      "|    mean_reward     | 3.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 2.73     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | 2.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1929899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=4736.31 +/- 12.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.575   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1939899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=4706.76 +/- 36.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1949899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=4615.45 +/- 96.93\n",
      "Episode length: 991.20 +/- 17.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 991      |\n",
      "|    mean_reward     | 4.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -0.284   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1959899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=4698.95 +/- 15.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -443     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.362   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1969899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=4770.34 +/- 11.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 2.1      |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.925   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1979899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1990000, episode_reward=4585.55 +/- 21.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.00122 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=4689.76 +/- 21.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -434     |\n",
      "|    critic_loss     | 3.57     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | -1.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1999899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=4681.37 +/- 32.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -444     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.236    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2009899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=4680.34 +/- 93.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 0.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2019899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=4723.62 +/- 21.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -440     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.859    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2029899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=4694.20 +/- 20.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -443     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 1.94     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2039899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=4684.23 +/- 11.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -438     |\n",
      "|    critic_loss     | 3        |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2049899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=4078.65 +/- 1265.45\n",
      "Episode length: 880.20 +/- 239.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 880      |\n",
      "|    mean_reward     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -445     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.679   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2059899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=3663.15 +/- 1682.70\n",
      "Episode length: 801.40 +/- 325.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 801      |\n",
      "|    mean_reward     | 3.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -443     |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.0738   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2069899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=4605.27 +/- 16.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.376    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2079899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=2950.48 +/- 2150.61\n",
      "Episode length: 658.40 +/- 420.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 658      |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -441     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 0.609    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2089899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=1948.92 +/- 2207.44\n",
      "Episode length: 452.80 +/- 446.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 453      |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.103    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2099899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=4630.94 +/- 6.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | -2.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2109899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=4455.20 +/- 42.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -446     |\n",
      "|    critic_loss     | 17       |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.644    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2119899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=3804.87 +/- 1798.17\n",
      "Episode length: 819.80 +/- 360.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -446     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 0.603    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2129899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=4754.51 +/- 9.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -441     |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.579    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=4742.96 +/- 38.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -448     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.0187  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2149899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=4732.66 +/- 33.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -445     |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -0.758   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2159899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=4781.78 +/- 21.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -0.129   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2169899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2180000, episode_reward=4569.57 +/- 378.43\n",
      "Episode length: 965.60 +/- 68.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 966      |\n",
      "|    mean_reward     | 4.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -451     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -0.271   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2179899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=4763.87 +/- 12.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -451     |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -2.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2189899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=4715.51 +/- 67.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 2.98     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 2.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2199899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=4759.05 +/- 49.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -448     |\n",
      "|    critic_loss     | 3.68     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2209899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=4734.06 +/- 46.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -445     |\n",
      "|    critic_loss     | 2.97     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 1.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2219899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=4757.93 +/- 28.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 3.27     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.233    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2229899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=4726.23 +/- 15.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | 2.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2239899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=4742.24 +/- 13.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -449     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -1.95    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2249899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=4759.73 +/- 55.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 2.88     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.283    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2259899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=4774.02 +/- 26.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -455     |\n",
      "|    critic_loss     | 2.73     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2269899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=4015.33 +/- 1547.31\n",
      "Episode length: 852.60 +/- 294.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | 4.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 2.73     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.355   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2279899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=4753.33 +/- 21.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 3.04     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.96     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2289899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=4798.04 +/- 17.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -453     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 2.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2310000, episode_reward=4816.94 +/- 11.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -455     |\n",
      "|    critic_loss     | 2.8      |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2309899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2320000, episode_reward=4811.92 +/- 52.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -454     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2319899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=4782.08 +/- 23.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2329899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=4769.85 +/- 13.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -457     |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2339899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=4835.95 +/- 20.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -459     |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.521   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2349899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2360000, episode_reward=1295.84 +/- 1769.70\n",
      "Episode length: 331.00 +/- 334.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 331      |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -457     |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=4831.61 +/- 9.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -458     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.458    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2369899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=4831.45 +/- 9.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -455     |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 1.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2379899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=4806.88 +/- 30.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -454     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.35     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2389899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=4778.63 +/- 29.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -453     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | -0.546   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2399899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=4839.98 +/- 14.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -465     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2420000, episode_reward=2913.33 +/- 2257.12\n",
      "Episode length: 637.20 +/- 444.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 637      |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -453     |\n",
      "|    critic_loss     | 2.9      |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.0874  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2419899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=4836.57 +/- 13.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -459     |\n",
      "|    critic_loss     | 3.28     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | 0.132    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2429899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=4873.52 +/- 22.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -460     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.509    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2439899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2450000, episode_reward=4877.11 +/- 5.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -458     |\n",
      "|    critic_loss     | 2.74     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2449899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2460000, episode_reward=4765.63 +/- 43.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -458     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2459899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=3737.12 +/- 1797.46\n",
      "Episode length: 820.20 +/- 359.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -465     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.517    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2469899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=4826.09 +/- 64.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -461     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | 1.97     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2479899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=3046.48 +/- 2188.83\n",
      "Episode length: 690.20 +/- 383.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 690      |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -465     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | -0.267   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2489899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=4791.58 +/- 47.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -466     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.884   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2499899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=4805.32 +/- 31.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -466     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.0881   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2509899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=4851.38 +/- 6.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -464     |\n",
      "|    critic_loss     | 2.92     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 0.718    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=4831.28 +/- 40.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -460     |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | 0.845    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2529899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=4799.90 +/- 63.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -466     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.0459  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2539899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=4899.25 +/- 14.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -464     |\n",
      "|    critic_loss     | 3.34     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 0.0624   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2549899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2560000, episode_reward=4834.03 +/- 40.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -467     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 2.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2559899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=4834.54 +/- 44.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -465     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.328   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2569899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=3937.82 +/- 1878.22\n",
      "Episode length: 821.20 +/- 357.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 821      |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -464     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 0.149    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2579899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=4844.09 +/- 26.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.867   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2589899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=4879.24 +/- 43.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -464     |\n",
      "|    critic_loss     | 2.02     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.648    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2599899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=4869.25 +/- 31.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -470     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=4924.14 +/- 10.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.00651 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2619899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2630000, episode_reward=4838.72 +/- 18.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -472     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2629899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=4875.02 +/- 8.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 2.81     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.245   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2639899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=4711.23 +/- 20.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -470     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.356   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2649899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=3591.07 +/- 1576.53\n",
      "Episode length: 758.40 +/- 297.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 758      |\n",
      "|    mean_reward     | 3.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -0.217   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2659899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=4867.93 +/- 18.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -0.884   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2669899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=4806.95 +/- 25.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 2.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2679899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=4797.18 +/- 25.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.917    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2689899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=4891.22 +/- 11.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -470     |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.145   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2699899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=4834.72 +/- 65.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -471     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.843    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2709899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=4852.03 +/- 25.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -473     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.221    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=4824.65 +/- 83.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -473     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2729899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=4961.98 +/- 7.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2739899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2750000, episode_reward=4891.57 +/- 23.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 111      |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 2.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2749899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=4894.64 +/- 11.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -468     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2759899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=4929.87 +/- 14.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -470     |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.0864  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2769899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=4915.49 +/- 11.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 3.49     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.658    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2779899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=2308.10 +/- 2096.88\n",
      "Episode length: 517.80 +/- 394.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 518      |\n",
      "|    mean_reward     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | 0.608    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2789899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=4916.34 +/- 17.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -475     |\n",
      "|    critic_loss     | 2.21     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2799899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=4983.02 +/- 37.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | 0.833    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2809899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2820000, episode_reward=4983.50 +/- 13.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2819899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2830000, episode_reward=5005.29 +/- 54.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -2.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2829899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2840000, episode_reward=4978.53 +/- 10.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2839899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=4936.47 +/- 60.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2849899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=4923.63 +/- 69.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -477     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.0881  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2859899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=4923.96 +/- 55.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=4933.37 +/- 19.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -478     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | -0.131   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2879899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=4992.04 +/- 22.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -473     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.803    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2889899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=4979.60 +/- 27.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.179    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2910000, episode_reward=339.27 +/- 73.76\n",
      "Episode length: 127.80 +/- 16.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 128      |\n",
      "|    mean_reward     | 339      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -478     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -1.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2909899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=4895.44 +/- 48.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 8.78     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -0.808   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2919899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=4982.59 +/- 32.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -476     |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2929899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=4991.87 +/- 42.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 3.89     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.717   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2939899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2950000, episode_reward=5041.59 +/- 24.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2949899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2960000, episode_reward=4740.19 +/- 189.53\n",
      "Episode length: 986.20 +/- 27.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 986      |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -480     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.837   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2959899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=4818.58 +/- 23.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 2.53     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 2.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2969899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=4788.32 +/- 47.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 3.6      |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.42     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2979899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=5012.84 +/- 50.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2989899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=4900.41 +/- 96.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -477     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.588    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3010000, episode_reward=5041.97 +/- 31.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 3.64     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -0.432   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3009899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3020000, episode_reward=4993.32 +/- 74.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 3.57     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3030000, episode_reward=5008.56 +/- 17.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 0.928    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3029899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=3863.88 +/- 1752.79\n",
      "Episode length: 832.60 +/- 334.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 833      |\n",
      "|    mean_reward     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | -0.659   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3039899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3050000, episode_reward=5014.17 +/- 41.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.707    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3049899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060000, episode_reward=5033.62 +/- 22.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -488     |\n",
      "|    critic_loss     | 3.2      |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3059899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3070000, episode_reward=4970.43 +/- 39.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -0.414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3069899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3080000, episode_reward=4992.80 +/- 63.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -0.233   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3079899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090000, episode_reward=4947.44 +/- 32.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 5.07     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | 0.114    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3089899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3100000, episode_reward=4533.10 +/- 550.97\n",
      "Episode length: 932.80 +/- 83.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 933      |\n",
      "|    mean_reward     | 4.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -478     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.576    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3110000, episode_reward=5083.56 +/- 6.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.33     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3109899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3120000, episode_reward=5002.32 +/- 69.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -486     |\n",
      "|    critic_loss     | 88.6     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.382   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3119899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3130000, episode_reward=4925.19 +/- 100.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 0.331    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3129899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3140000, episode_reward=5064.31 +/- 17.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -487     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3139899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3150000, episode_reward=2144.65 +/- 1937.83\n",
      "Episode length: 516.80 +/- 404.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 517      |\n",
      "|    mean_reward     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -489     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3149899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3160000, episode_reward=5007.53 +/- 76.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -0.891   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3159899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3170000, episode_reward=5021.26 +/- 46.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3169899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3180000, episode_reward=5045.98 +/- 20.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -488     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3179899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3190000, episode_reward=5022.27 +/- 42.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 2.33     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.972    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3189899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=4948.02 +/- 79.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -480     |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3199899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210000, episode_reward=5044.43 +/- 40.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -490     |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.968    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3209899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3220000, episode_reward=1096.16 +/- 1826.81\n",
      "Episode length: 281.80 +/- 359.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 282      |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -488     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | -2.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3219899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3230000, episode_reward=4989.08 +/- 44.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | -0.148   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3229899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3240000, episode_reward=4996.54 +/- 55.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 4.11     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | -1.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3239899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3250000, episode_reward=5040.79 +/- 34.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 3.24     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -1.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3249899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3260000, episode_reward=4994.58 +/- 35.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -486     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | 0.661    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3259899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270000, episode_reward=4958.61 +/- 29.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | 0.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3269899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3280000, episode_reward=4999.87 +/- 66.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 106      |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 0.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3279899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3290000, episode_reward=5044.99 +/- 6.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 4.74     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | 0.0417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3289899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300000, episode_reward=5077.79 +/- 23.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 2.02     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -0.986   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3299899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3310000, episode_reward=5035.72 +/- 8.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 4.07     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3309899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3320000, episode_reward=5114.15 +/- 15.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -489     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.525   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3319899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3330000, episode_reward=2278.97 +/- 2274.49\n",
      "Episode length: 492.40 +/- 414.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 492      |\n",
      "|    mean_reward     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -487     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3329899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3340000, episode_reward=5040.13 +/- 17.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -487     |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.705   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3339899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3350000, episode_reward=5073.78 +/- 8.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 2.8      |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.796    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3349899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=5115.17 +/- 18.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3359899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3370000, episode_reward=5114.80 +/- 19.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -480     |\n",
      "|    critic_loss     | 14.4     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3369899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3380000, episode_reward=5068.05 +/- 21.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -488     |\n",
      "|    critic_loss     | 3.39     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.308    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3379899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3390000, episode_reward=5112.44 +/- 14.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 3.84     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.667    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3389899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3400000, episode_reward=5110.25 +/- 6.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 2.89     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -1.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3399899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3410000, episode_reward=5163.69 +/- 28.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | -0.155   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3409899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3420000, episode_reward=5136.77 +/- 15.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.158    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3419899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3430000, episode_reward=5120.19 +/- 14.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.464    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3429899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3440000, episode_reward=5174.55 +/- 20.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 2.62     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3439899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3450000, episode_reward=5101.29 +/- 33.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.0191   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3449899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3460000, episode_reward=5165.69 +/- 12.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 3.5      |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.0767  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3459899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3470000, episode_reward=5156.79 +/- 19.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -486     |\n",
      "|    critic_loss     | 6        |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3469899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480000, episode_reward=5133.58 +/- 24.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3479899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3490000, episode_reward=4324.73 +/- 1731.61\n",
      "Episode length: 849.40 +/- 301.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 849      |\n",
      "|    mean_reward     | 4.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -490     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.179    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3489899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500000, episode_reward=5105.99 +/- 23.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 1.69     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3499899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3510000, episode_reward=2271.57 +/- 2327.29\n",
      "Episode length: 478.80 +/- 425.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 479      |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -493     |\n",
      "|    critic_loss     | 5.21     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 0.428    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3509899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=5210.94 +/- 17.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -492     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3519899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3530000, episode_reward=5172.61 +/- 32.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -487     |\n",
      "|    critic_loss     | 3.85     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.345    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3529899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3540000, episode_reward=5111.13 +/- 58.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -495     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3539899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3550000, episode_reward=5177.67 +/- 7.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.727    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3549899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3560000, episode_reward=4313.72 +/- 1733.78\n",
      "Episode length: 852.40 +/- 295.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 852      |\n",
      "|    mean_reward     | 4.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 1.84     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3559899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570000, episode_reward=5084.67 +/- 118.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.533   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3569899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3580000, episode_reward=4087.33 +/- 1892.69\n",
      "Episode length: 825.00 +/- 350.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 825      |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3579899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3590000, episode_reward=5232.20 +/- 12.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.724    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3589899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3600000, episode_reward=4222.41 +/- 1670.28\n",
      "Episode length: 849.20 +/- 301.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 849      |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3599899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3610000, episode_reward=5211.32 +/- 31.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -0.651   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3609899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3620000, episode_reward=4204.36 +/- 1936.04\n",
      "Episode length: 824.40 +/- 351.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 4.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -497     |\n",
      "|    critic_loss     | 3.08     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3619899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3630000, episode_reward=5207.18 +/- 7.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -495     |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3629899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3640000, episode_reward=5135.32 +/- 34.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.431   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3639899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3650000, episode_reward=5194.63 +/- 23.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -497     |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3649899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660000, episode_reward=3677.80 +/- 1872.28\n",
      "Episode length: 733.60 +/- 326.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 734      |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3659899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3670000, episode_reward=5184.29 +/- 29.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -501     |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.073   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3669899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=5213.02 +/- 38.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 1.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3679899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690000, episode_reward=5167.38 +/- 9.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -501     |\n",
      "|    critic_loss     | 2.06     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3689899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3700000, episode_reward=5209.38 +/- 22.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -498     |\n",
      "|    critic_loss     | 2.19     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.82     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3699899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3710000, episode_reward=1182.86 +/- 1802.63\n",
      "Episode length: 314.80 +/- 342.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 315      |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 0.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3709899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3720000, episode_reward=5134.59 +/- 21.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.842   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3719899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3730000, episode_reward=5152.91 +/- 63.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -498     |\n",
      "|    critic_loss     | 2.06     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.628   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3729899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3740000, episode_reward=5201.09 +/- 28.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -503     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 1.61     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3739899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750000, episode_reward=5114.41 +/- 116.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.181   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3749899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3760000, episode_reward=5185.21 +/- 65.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 3.36     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.225    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3759899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3770000, episode_reward=5231.93 +/- 19.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 3.79     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.164    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3769899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3780000, episode_reward=5241.20 +/- 17.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.783   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3779899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3790000, episode_reward=5263.24 +/- 31.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.474   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3789899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3800000, episode_reward=5243.95 +/- 23.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -2.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3799899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810000, episode_reward=5173.49 +/- 31.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3809899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3820000, episode_reward=4405.03 +/- 1692.79\n",
      "Episode length: 853.40 +/- 293.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.834   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3819899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3830000, episode_reward=5226.15 +/- 25.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3829899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=5238.60 +/- 13.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -0.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3839899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850000, episode_reward=5213.18 +/- 21.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | -0.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3849899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3860000, episode_reward=5238.35 +/- 25.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 5.29     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.0437   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3859899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870000, episode_reward=5250.90 +/- 23.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -498     |\n",
      "|    critic_loss     | 2.25     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3869899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3880000, episode_reward=5245.29 +/- 16.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -498     |\n",
      "|    critic_loss     | 2.78     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3879899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890000, episode_reward=5252.91 +/- 19.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -504     |\n",
      "|    critic_loss     | 2.66     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 1.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3889899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900000, episode_reward=5213.95 +/- 17.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.424    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3910000, episode_reward=5282.41 +/- 9.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -503     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.053   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3909899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3920000, episode_reward=5240.38 +/- 23.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.188    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3919899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3930000, episode_reward=5246.35 +/- 12.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 0.962    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3929899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3940000, episode_reward=4287.43 +/- 1766.05\n",
      "Episode length: 845.20 +/- 309.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 845      |\n",
      "|    mean_reward     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 0.913    |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -0.142   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3939899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3950000, episode_reward=5189.97 +/- 16.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -503     |\n",
      "|    critic_loss     | 2.3      |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.0414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3949899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960000, episode_reward=5280.04 +/- 19.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.9      |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 0.508    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3959899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3970000, episode_reward=5265.11 +/- 21.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 2.15     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.852   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3969899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3980000, episode_reward=4192.80 +/- 2043.00\n",
      "Episode length: 812.60 +/- 374.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 813      |\n",
      "|    mean_reward     | 4.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 3.22     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | 0.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3979899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990000, episode_reward=5237.45 +/- 18.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 0.858    |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.751   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3989899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000000, episode_reward=5306.21 +/- 16.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 5.2      |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.254    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3999899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4010000, episode_reward=5202.78 +/- 34.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 2.42     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.825   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4009899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4020000, episode_reward=5240.95 +/- 49.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4019899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4030000, episode_reward=4534.31 +/- 1419.42\n",
      "Episode length: 880.00 +/- 240.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 880      |\n",
      "|    mean_reward     | 4.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -518     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -0.288   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4029899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4040000, episode_reward=4245.81 +/- 1902.79\n",
      "Episode length: 843.40 +/- 313.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 843      |\n",
      "|    mean_reward     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 3.87     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.499    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4039899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050000, episode_reward=3035.74 +/- 2229.14\n",
      "Episode length: 712.00 +/- 352.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 712      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 8.79     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.565   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4049899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4060000, episode_reward=-2.18 +/- 0.10\n",
      "Episode length: 7.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -2.18    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 5.84     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.621    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4059899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4070000, episode_reward=5295.54 +/- 16.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -504     |\n",
      "|    critic_loss     | 3.47     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 1.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4069899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4080000, episode_reward=5270.01 +/- 42.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 3.88     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.401    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4079899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4090000, episode_reward=5264.76 +/- 29.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 2.95     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | -0.649   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4089899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4100000, episode_reward=5227.58 +/- 45.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | 0.0218   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4099899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110000, episode_reward=5229.02 +/- 173.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -501     |\n",
      "|    critic_loss     | 2.56     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 0.457    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4109899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4120000, episode_reward=4377.67 +/- 1670.50\n",
      "Episode length: 857.00 +/- 286.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 857      |\n",
      "|    mean_reward     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4119899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4130000, episode_reward=3259.57 +/- 1882.76\n",
      "Episode length: 686.20 +/- 335.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 686      |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 6.99     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | -0.443   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4129899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4140000, episode_reward=5161.52 +/- 64.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -503     |\n",
      "|    critic_loss     | 3.4      |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.419    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4139899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4150000, episode_reward=4606.39 +/- 1266.85\n",
      "Episode length: 898.20 +/- 203.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 898      |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 4.41     |\n",
      "|    ent_coef        | 0.0424   |\n",
      "|    ent_coef_loss   | 0.035    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4149899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4160000, episode_reward=4027.28 +/- 1492.03\n",
      "Episode length: 808.40 +/- 234.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 808      |\n",
      "|    mean_reward     | 4.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 3.74     |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | 0.123    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4159899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4170000, episode_reward=5142.60 +/- 34.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 3.29     |\n",
      "|    ent_coef        | 0.0428   |\n",
      "|    ent_coef_loss   | -0.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4169899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4180000, episode_reward=5111.69 +/- 209.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 2.99     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.704    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4179899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4190000, episode_reward=4998.08 +/- 32.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 2.22     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | -0.556   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4189899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200000, episode_reward=5178.78 +/- 34.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.79     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4199899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4210000, episode_reward=5224.89 +/- 19.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 3.35     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | 0.994    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4209899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4220000, episode_reward=5132.24 +/- 49.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.509    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4219899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4230000, episode_reward=5236.67 +/- 8.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.8      |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -0.273   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4229899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4240000, episode_reward=5291.39 +/- 23.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.68     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -0.991   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4239899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4250000, episode_reward=4276.59 +/- 1955.46\n",
      "Episode length: 827.20 +/- 345.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.57     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | -0.424   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4249899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4260000, episode_reward=5282.99 +/- 59.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 2.11     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -0.887   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4259899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4270000, episode_reward=5331.75 +/- 15.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 2.47     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.0488   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4280000, episode_reward=4519.05 +/- 1679.97\n",
      "Episode length: 858.20 +/- 283.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 858      |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.366    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4279899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4290000, episode_reward=4643.79 +/- 1324.49\n",
      "Episode length: 891.20 +/- 217.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 891      |\n",
      "|    mean_reward     | 4.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 5.56     |\n",
      "|    ent_coef        | 0.0388   |\n",
      "|    ent_coef_loss   | 1.02     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4289899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4300000, episode_reward=4360.94 +/- 1738.72\n",
      "Episode length: 850.40 +/- 299.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 850      |\n",
      "|    mean_reward     | 4.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 5.04     |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.557    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4299899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4310000, episode_reward=3193.96 +/- 2370.22\n",
      "Episode length: 649.00 +/- 429.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 649      |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -1.83    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4309899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320000, episode_reward=4208.74 +/- 1995.35\n",
      "Episode length: 823.80 +/- 352.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 2.87     |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | -0.0259  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4319899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4330000, episode_reward=5269.82 +/- 21.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 5.71     |\n",
      "|    ent_coef        | 0.0388   |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4329899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4340000, episode_reward=5334.51 +/- 10.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0407   |\n",
      "|    ent_coef_loss   | 0.377    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4339899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4350000, episode_reward=3464.57 +/- 2173.59\n",
      "Episode length: 695.00 +/- 379.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 695      |\n",
      "|    mean_reward     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 5.2      |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.684    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4349899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4360000, episode_reward=5209.13 +/- 184.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 6.86     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | -0.849   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4359899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4370000, episode_reward=5274.98 +/- 41.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | 0.263    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4369899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380000, episode_reward=5284.53 +/- 15.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -504     |\n",
      "|    critic_loss     | 3.66     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | 0.265    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4379899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4390000, episode_reward=5320.73 +/- 14.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.63     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | -2.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4389899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4400000, episode_reward=5278.83 +/- 32.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 2.21     |\n",
      "|    ent_coef        | 0.0433   |\n",
      "|    ent_coef_loss   | 0.0408   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4399899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410000, episode_reward=5238.73 +/- 11.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 4.16     |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -0.952   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4409899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4420000, episode_reward=5290.57 +/- 19.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 3.78     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4419899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4430000, episode_reward=5291.15 +/- 78.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 2.77     |\n",
      "|    ent_coef        | 0.0435   |\n",
      "|    ent_coef_loss   | -0.202   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4429899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4440000, episode_reward=5230.70 +/- 85.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -514     |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4439899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4450000, episode_reward=5306.89 +/- 2.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 4.15     |\n",
      "|    ent_coef        | 0.045    |\n",
      "|    ent_coef_loss   | -0.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4449899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4460000, episode_reward=5333.39 +/- 15.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.115    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4459899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470000, episode_reward=5359.62 +/- 30.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 7.28     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | 0.889    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4469899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4480000, episode_reward=5415.04 +/- 33.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.0813  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4479899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4490000, episode_reward=5381.79 +/- 19.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 5.19     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4489899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500000, episode_reward=5382.74 +/- 12.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4499899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4510000, episode_reward=1819.69 +/- 1722.03\n",
      "Episode length: 543.60 +/- 373.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 544      |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 6.35     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | 1.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4509899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4520000, episode_reward=4488.52 +/- 1770.71\n",
      "Episode length: 852.00 +/- 296.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 852      |\n",
      "|    mean_reward     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.0272   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4519899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4530000, episode_reward=4432.80 +/- 1865.73\n",
      "Episode length: 842.00 +/- 316.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 842      |\n",
      "|    mean_reward     | 4.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.0436   |\n",
      "|    ent_coef_loss   | -1.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4529899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4540000, episode_reward=5308.15 +/- 10.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.58     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | -0.0521  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4539899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4550000, episode_reward=4335.82 +/- 2084.68\n",
      "Episode length: 817.60 +/- 364.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 818      |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 2.91     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | -0.533   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4549899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4560000, episode_reward=5337.04 +/- 8.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | 0.983    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4559899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4570000, episode_reward=5356.02 +/- 9.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4569899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4580000, episode_reward=5402.51 +/- 7.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.045    |\n",
      "|    ent_coef_loss   | -0.774   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4579899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4590000, episode_reward=5363.13 +/- 7.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 2.72     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.274    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4589899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4600000, episode_reward=2468.02 +/- 2361.82\n",
      "Episode length: 503.20 +/- 406.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 503      |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 3.1      |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4599899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4610000, episode_reward=5317.67 +/- 25.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 5.25     |\n",
      "|    ent_coef        | 0.0411   |\n",
      "|    ent_coef_loss   | 0.442    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4609899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620000, episode_reward=5370.05 +/- 7.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -514     |\n",
      "|    critic_loss     | 3.89     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -1.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4619899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4630000, episode_reward=5346.21 +/- 15.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 2.68     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | 0.214    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4629899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4640000, episode_reward=5396.75 +/- 23.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | -0.637   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4639899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4650000, episode_reward=5323.48 +/- 34.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -526     |\n",
      "|    critic_loss     | 4.02     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4649899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4660000, episode_reward=5392.34 +/- 27.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 2.87     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.533    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4659899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4670000, episode_reward=5410.18 +/- 6.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.59     |\n",
      "|    ent_coef        | 0.0413   |\n",
      "|    ent_coef_loss   | 0.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4669899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680000, episode_reward=5346.79 +/- 14.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -522     |\n",
      "|    critic_loss     | 3.49     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | -0.842   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4679899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4690000, episode_reward=5149.55 +/- 103.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    ent_coef        | 0.0438   |\n",
      "|    ent_coef_loss   | -0.483   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4689899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4700000, episode_reward=5402.43 +/- 7.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 9.53     |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -0.181   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710000, episode_reward=3361.75 +/- 2354.28\n",
      "Episode length: 666.00 +/- 414.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 666      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 3.55     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4709899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4720000, episode_reward=4184.15 +/- 2067.65\n",
      "Episode length: 810.80 +/- 378.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 811      |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 2.84     |\n",
      "|    ent_coef        | 0.0459   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4719899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4730000, episode_reward=5329.87 +/- 26.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 6.79     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | 0.0293   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4729899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4740000, episode_reward=5389.06 +/- 7.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.488    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4739899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4750000, episode_reward=5412.04 +/- 50.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 4.7      |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4749899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4760000, episode_reward=5377.22 +/- 12.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4759899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4770000, episode_reward=5414.75 +/- 15.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 3.42     |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | -0.494   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4769899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4780000, episode_reward=5387.53 +/- 14.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 6.29     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | -0.289   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4779899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4790000, episode_reward=5414.16 +/- 39.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 9.8      |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | 0.401    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4789899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800000, episode_reward=5374.07 +/- 11.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 2.64     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -0.809   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4799899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4810000, episode_reward=5297.64 +/- 60.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 3.09     |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | -2.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4809899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4820000, episode_reward=5421.82 +/- 17.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -525     |\n",
      "|    critic_loss     | 4.28     |\n",
      "|    ent_coef        | 0.044    |\n",
      "|    ent_coef_loss   | -0.202   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4819899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4830000, episode_reward=5194.64 +/- 42.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | 0.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4829899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4840000, episode_reward=5401.89 +/- 16.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 5.81     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | 0.909    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4839899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4850000, episode_reward=5415.30 +/- 8.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 4.77     |\n",
      "|    ent_coef        | 0.0436   |\n",
      "|    ent_coef_loss   | 0.484    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4849899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4860000, episode_reward=5407.19 +/- 22.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 2.57     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 1.5      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4859899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4870000, episode_reward=5448.83 +/- 8.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 3.26     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -0.525   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4869899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4880000, episode_reward=5368.54 +/- 48.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.2      |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | 0.523    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4879899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4890000, episode_reward=5414.62 +/- 35.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 2.46     |\n",
      "|    ent_coef        | 0.0442   |\n",
      "|    ent_coef_loss   | -0.782   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4889899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4900000, episode_reward=1483.95 +/- 2061.16\n",
      "Episode length: 327.80 +/- 361.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 328      |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 3.75     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | -0.191   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4899899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4910000, episode_reward=4593.31 +/- 1639.80\n",
      "Episode length: 868.60 +/- 262.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 869      |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 4.02     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | 0.797    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4909899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4920000, episode_reward=5420.81 +/- 45.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 2.59     |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | 0.167    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4919899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4930000, episode_reward=5438.31 +/- 8.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -521     |\n",
      "|    critic_loss     | 8.3      |\n",
      "|    ent_coef        | 0.0453   |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4929899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4940000, episode_reward=5461.29 +/- 10.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -521     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | 0.269    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4939899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4950000, episode_reward=3127.91 +/- 2340.61\n",
      "Episode length: 661.00 +/- 415.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 661      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 4.11     |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | 0.853    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4949899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4960000, episode_reward=5411.60 +/- 12.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 3.79     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 0.712    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4959899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4970000, episode_reward=5373.03 +/- 12.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -528     |\n",
      "|    critic_loss     | 9.81     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4969899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4980000, episode_reward=5464.75 +/- 17.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 7.64     |\n",
      "|    ent_coef        | 0.0483   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4979899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4990000, episode_reward=5478.88 +/- 9.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 6.78     |\n",
      "|    ent_coef        | 0.0513   |\n",
      "|    ent_coef_loss   | 0.0208   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4989899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000000, episode_reward=5317.08 +/- 17.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0474   |\n",
      "|    ent_coef_loss   | -0.148   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4999899  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Walker2d-v4\n",
    "env_id = 'Walker2d-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "sac_model = SAC(\"MlpPolicy\", env, verbose=10)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1e5, save_path=f'../logs/{env_id}')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'./logs/{env_id}/best_model',\n",
    "                             log_path=f'./logs/{env_id}/results', eval_freq=10000)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "sac_model.learn(total_timesteps=5e6, log_interval=10000, callback=callback)\n",
    "sac_model.save(f\"logs/expert/{env_id}-sac/model5e6\")\n",
    "sac_model.save_replay_buffer(f\"logs/expert/{env_id}-sac/buffer5e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/miniforge3/envs/irl/lib/python3.10/site-packages/stable_baselines3/common/buffers.py:229: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 6.09GB > 5.86GB\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.8     |\n",
      "|    ep_rew_mean     | 119      |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 238      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.7    |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    ent_coef        | 0.957    |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 137      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.6     |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 128      |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 432      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -35.9    |\n",
      "|    critic_loss     | 16.6     |\n",
      "|    ent_coef        | 0.901    |\n",
      "|    ent_coef_loss   | -2.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 331      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 20.9     |\n",
      "|    ep_rew_mean     | 104      |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 118      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 627      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -45.7    |\n",
      "|    critic_loss     | 6.84     |\n",
      "|    ent_coef        | 0.848    |\n",
      "|    ent_coef_loss   | -4.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 526      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.9     |\n",
      "|    ep_rew_mean     | 99.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 113      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 798      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -52.3    |\n",
      "|    critic_loss     | 8.3      |\n",
      "|    ent_coef        | 0.806    |\n",
      "|    ent_coef_loss   | -5.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 697      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.3     |\n",
      "|    ep_rew_mean     | 95.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 111      |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 966      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58      |\n",
      "|    critic_loss     | 8.1      |\n",
      "|    ent_coef        | 0.767    |\n",
      "|    ent_coef_loss   | -7.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 865      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.9     |\n",
      "|    ep_rew_mean     | 93.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 109      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1133     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -60.7    |\n",
      "|    critic_loss     | 8.96     |\n",
      "|    ent_coef        | 0.729    |\n",
      "|    ent_coef_loss   | -8.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1032     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.7     |\n",
      "|    ep_rew_mean     | 92.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 70       |\n",
      "|    fps             | 108      |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 1309     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -68.8    |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.692    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1208     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.5     |\n",
      "|    ep_rew_mean     | 91.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 107      |\n",
      "|    time_elapsed    | 13       |\n",
      "|    total_timesteps | 1478     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -72.7    |\n",
      "|    critic_loss     | 13.3     |\n",
      "|    ent_coef        | 0.657    |\n",
      "|    ent_coef_loss   | -11.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1377     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | 93.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 103      |\n",
      "|    time_elapsed    | 16       |\n",
      "|    total_timesteps | 1696     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -79.5    |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    ent_coef        | 0.616    |\n",
      "|    ent_coef_loss   | -13.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1595     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.7     |\n",
      "|    ep_rew_mean     | 98.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 1973     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -87.6    |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.567    |\n",
      "|    ent_coef_loss   | -15.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1872     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.3     |\n",
      "|    ep_rew_mean     | 112      |\n",
      "| time/              |          |\n",
      "|    episodes        | 110      |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 25       |\n",
      "|    total_timesteps | 2468     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -99.7    |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.49     |\n",
      "|    ent_coef_loss   | -18.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2367     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 24.7     |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 2898     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -106     |\n",
      "|    critic_loss     | 27.1     |\n",
      "|    ent_coef        | 0.433    |\n",
      "|    ent_coef_loss   | -20.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2797     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 25.9     |\n",
      "|    ep_rew_mean     | 131      |\n",
      "| time/              |          |\n",
      "|    episodes        | 130      |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 3216     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -117     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    ent_coef        | 0.396    |\n",
      "|    ent_coef_loss   | -21.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3115     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.3     |\n",
      "|    ep_rew_mean     | 149      |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 3727     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -128     |\n",
      "|    critic_loss     | 26.9     |\n",
      "|    ent_coef        | 0.345    |\n",
      "|    ent_coef_loss   | -23.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3626     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 33.2     |\n",
      "|    ep_rew_mean     | 170      |\n",
      "| time/              |          |\n",
      "|    episodes        | 150      |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 4290     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -138     |\n",
      "|    critic_loss     | 33.5     |\n",
      "|    ent_coef        | 0.297    |\n",
      "|    ent_coef_loss   | -24.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4189     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 37.3     |\n",
      "|    ep_rew_mean     | 190      |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 49       |\n",
      "|    total_timesteps | 4863     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    ent_coef        | 0.255    |\n",
      "|    ent_coef_loss   | -26.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4762     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 42.1     |\n",
      "|    ep_rew_mean     | 215      |\n",
      "| time/              |          |\n",
      "|    episodes        | 170      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 5516     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -153     |\n",
      "|    critic_loss     | 36       |\n",
      "|    ent_coef        | 0.214    |\n",
      "|    ent_coef_loss   | -27.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5415     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 46.1     |\n",
      "|    ep_rew_mean     | 237      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 6093     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -153     |\n",
      "|    critic_loss     | 44.9     |\n",
      "|    ent_coef        | 0.185    |\n",
      "|    ent_coef_loss   | -25      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5992     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 50.1     |\n",
      "|    ep_rew_mean     | 259      |\n",
      "| time/              |          |\n",
      "|    episodes        | 190      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 6708     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -163     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.158    |\n",
      "|    ent_coef_loss   | -28.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6607     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 54.1     |\n",
      "|    ep_rew_mean     | 281      |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 7381     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -164     |\n",
      "|    critic_loss     | 40.6     |\n",
      "|    ent_coef        | 0.133    |\n",
      "|    ent_coef_loss   | -29.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7280     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 55.8     |\n",
      "|    ep_rew_mean     | 290      |\n",
      "| time/              |          |\n",
      "|    episodes        | 210      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 80       |\n",
      "|    total_timesteps | 8045     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -163     |\n",
      "|    critic_loss     | 46.3     |\n",
      "|    ent_coef        | 0.111    |\n",
      "|    ent_coef_loss   | -27.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7944     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 58.4     |\n",
      "|    ep_rew_mean     | 304      |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 87       |\n",
      "|    total_timesteps | 8739     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -159     |\n",
      "|    critic_loss     | 42.7     |\n",
      "|    ent_coef        | 0.0934   |\n",
      "|    ent_coef_loss   | -26.4    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8638     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 63       |\n",
      "|    ep_rew_mean     | 328      |\n",
      "| time/              |          |\n",
      "|    episodes        | 230      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 9513     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -160     |\n",
      "|    critic_loss     | 52.2     |\n",
      "|    ent_coef        | 0.0771   |\n",
      "|    ent_coef_loss   | -25.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9412     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 64.6     |\n",
      "|    ep_rew_mean     | 338      |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 101      |\n",
      "|    total_timesteps | 10189    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -157     |\n",
      "|    critic_loss     | 55.3     |\n",
      "|    ent_coef        | 0.0661   |\n",
      "|    ent_coef_loss   | -19.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10088    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 67       |\n",
      "|    ep_rew_mean     | 351      |\n",
      "| time/              |          |\n",
      "|    episodes        | 250      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 10991    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 32.7     |\n",
      "|    ent_coef        | 0.0551   |\n",
      "|    ent_coef_loss   | -17.6    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 69.4     |\n",
      "|    ep_rew_mean     | 365      |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 11802    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -153     |\n",
      "|    critic_loss     | 38.9     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11701    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 70.5     |\n",
      "|    ep_rew_mean     | 371      |\n",
      "| time/              |          |\n",
      "|    episodes        | 270      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 12569    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -157     |\n",
      "|    critic_loss     | 41.3     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -10.8    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 12468    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.1     |\n",
      "|    ep_rew_mean     | 377      |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 13299    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -165     |\n",
      "|    critic_loss     | 29.3     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -7.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13198    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73       |\n",
      "|    ep_rew_mean     | 382      |\n",
      "| time/              |          |\n",
      "|    episodes        | 290      |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 140      |\n",
      "|    total_timesteps | 14003    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 32.3     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -5.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13902    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.6     |\n",
      "|    ep_rew_mean     | 379      |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 146      |\n",
      "|    total_timesteps | 14638    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -152     |\n",
      "|    critic_loss     | 30.5     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | -3.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14537    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.1     |\n",
      "|    ep_rew_mean     | 379      |\n",
      "| time/              |          |\n",
      "|    episodes        | 310      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 153      |\n",
      "|    total_timesteps | 15356    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -150     |\n",
      "|    critic_loss     | 38.9     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -7.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15255    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74       |\n",
      "|    ep_rew_mean     | 384      |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 16141    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -160     |\n",
      "|    critic_loss     | 29.8     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16040    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.5     |\n",
      "|    ep_rew_mean     | 386      |\n",
      "| time/              |          |\n",
      "|    episodes        | 330      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 168      |\n",
      "|    total_timesteps | 16964    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -145     |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | 3.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16863    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.9     |\n",
      "|    ep_rew_mean     | 385      |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 175      |\n",
      "|    total_timesteps | 17676    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -151     |\n",
      "|    critic_loss     | 26.7     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | -3.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17575    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74       |\n",
      "|    ep_rew_mean     | 382      |\n",
      "| time/              |          |\n",
      "|    episodes        | 350      |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 182      |\n",
      "|    total_timesteps | 18396    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -149     |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -2.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18295    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 72.6     |\n",
      "|    ep_rew_mean     | 374      |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 188      |\n",
      "|    total_timesteps | 19064    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -0.781   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18963    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 73.8     |\n",
      "|    ep_rew_mean     | 379      |\n",
      "| time/              |          |\n",
      "|    episodes        | 370      |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 196      |\n",
      "|    total_timesteps | 19952    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -140     |\n",
      "|    critic_loss     | 33.2     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 3.71     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19851    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.7     |\n",
      "|    ep_rew_mean     | 383      |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 20772    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -145     |\n",
      "|    critic_loss     | 42.2     |\n",
      "|    ent_coef        | 0.0241   |\n",
      "|    ent_coef_loss   | -2.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20671    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 75.5     |\n",
      "|    ep_rew_mean     | 385      |\n",
      "| time/              |          |\n",
      "|    episodes        | 390      |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 212      |\n",
      "|    total_timesteps | 21556    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 27.8     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21455    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.6     |\n",
      "|    ep_rew_mean     | 391      |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 219      |\n",
      "|    total_timesteps | 22295    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 23.1     |\n",
      "|    ent_coef        | 0.0241   |\n",
      "|    ent_coef_loss   | -1.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22194    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.2     |\n",
      "|    ep_rew_mean     | 395      |\n",
      "| time/              |          |\n",
      "|    episodes        | 410      |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 226      |\n",
      "|    total_timesteps | 23080    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 40.5     |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | 2.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22979    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.8     |\n",
      "|    ep_rew_mean     | 388      |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 233      |\n",
      "|    total_timesteps | 23817    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    ent_coef        | 0.0241   |\n",
      "|    ent_coef_loss   | 0.333    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23716    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.2     |\n",
      "|    ep_rew_mean     | 388      |\n",
      "| time/              |          |\n",
      "|    episodes        | 430      |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 241      |\n",
      "|    total_timesteps | 24585    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -136     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | 3.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 24484    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.7     |\n",
      "|    ep_rew_mean     | 392      |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 248      |\n",
      "|    total_timesteps | 25349    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 24.1     |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -0.873   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25248    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.1     |\n",
      "|    ep_rew_mean     | 393      |\n",
      "| time/              |          |\n",
      "|    episodes        | 450      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 255      |\n",
      "|    total_timesteps | 26103    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -133     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26002    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.6     |\n",
      "|    ep_rew_mean     | 402      |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 26925    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -139     |\n",
      "|    critic_loss     | 26.2     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | -1.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26824    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.8     |\n",
      "|    ep_rew_mean     | 398      |\n",
      "| time/              |          |\n",
      "|    episodes        | 470      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 27735    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -140     |\n",
      "|    critic_loss     | 26.2     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -0.00208 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27634    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.7     |\n",
      "|    ep_rew_mean     | 397      |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 278      |\n",
      "|    total_timesteps | 28540    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -133     |\n",
      "|    critic_loss     | 22       |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | -0.823   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28439    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.1     |\n",
      "|    ep_rew_mean     | 401      |\n",
      "| time/              |          |\n",
      "|    episodes        | 490      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 286      |\n",
      "|    total_timesteps | 29370    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 32.7     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | -1.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29269    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 78.9     |\n",
      "|    ep_rew_mean     | 405      |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 30188    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -134     |\n",
      "|    critic_loss     | 16.8     |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -2.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30087    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.1     |\n",
      "|    ep_rew_mean     | 409      |\n",
      "| time/              |          |\n",
      "|    episodes        | 510      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 303      |\n",
      "|    total_timesteps | 30990    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 25.7     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | 6.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30889    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.1     |\n",
      "|    ep_rew_mean     | 413      |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 310      |\n",
      "|    total_timesteps | 31724    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -138     |\n",
      "|    critic_loss     | 23.2     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | 0.234    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31623    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.1     |\n",
      "|    ep_rew_mean     | 413      |\n",
      "| time/              |          |\n",
      "|    episodes        | 530      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 317      |\n",
      "|    total_timesteps | 32497    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -130     |\n",
      "|    critic_loss     | 21.2     |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | -3.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32396    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 79.9     |\n",
      "|    ep_rew_mean     | 414      |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 325      |\n",
      "|    total_timesteps | 33335    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -141     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33234    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81       |\n",
      "|    ep_rew_mean     | 420      |\n",
      "| time/              |          |\n",
      "|    episodes        | 550      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 333      |\n",
      "|    total_timesteps | 34202    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -134     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 8.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34101    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 80.1     |\n",
      "|    ep_rew_mean     | 414      |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 340      |\n",
      "|    total_timesteps | 34934    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 19.9     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 0.389    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 34833    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.2     |\n",
      "|    ep_rew_mean     | 420      |\n",
      "| time/              |          |\n",
      "|    episodes        | 570      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 349      |\n",
      "|    total_timesteps | 35851    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 22.4     |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35750    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.7     |\n",
      "|    ep_rew_mean     | 423      |\n",
      "| time/              |          |\n",
      "|    episodes        | 580      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 358      |\n",
      "|    total_timesteps | 36706    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 22.7     |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 0.645    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36605    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.3     |\n",
      "|    ep_rew_mean     | 424      |\n",
      "| time/              |          |\n",
      "|    episodes        | 590      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 366      |\n",
      "|    total_timesteps | 37603    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 24.8     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 37502    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.4     |\n",
      "|    ep_rew_mean     | 429      |\n",
      "| time/              |          |\n",
      "|    episodes        | 600      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 376      |\n",
      "|    total_timesteps | 38529    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38428    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.3     |\n",
      "|    ep_rew_mean     | 432      |\n",
      "| time/              |          |\n",
      "|    episodes        | 610      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 384      |\n",
      "|    total_timesteps | 39422    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -140     |\n",
      "|    critic_loss     | 25.7     |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | -2.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39321    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 85.6     |\n",
      "|    ep_rew_mean     | 439      |\n",
      "| time/              |          |\n",
      "|    episodes        | 620      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 393      |\n",
      "|    total_timesteps | 40284    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -148     |\n",
      "|    critic_loss     | 22.4     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | 4.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40183    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.6     |\n",
      "|    ep_rew_mean     | 449      |\n",
      "| time/              |          |\n",
      "|    episodes        | 630      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 402      |\n",
      "|    total_timesteps | 41258    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 23       |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 41157    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 88.4     |\n",
      "|    ep_rew_mean     | 452      |\n",
      "| time/              |          |\n",
      "|    episodes        | 640      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 411      |\n",
      "|    total_timesteps | 42173    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 30.5     |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | 0.253    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42072    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.8     |\n",
      "|    ep_rew_mean     | 449      |\n",
      "| time/              |          |\n",
      "|    episodes        | 650      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 419      |\n",
      "|    total_timesteps | 42978    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -146     |\n",
      "|    critic_loss     | 24.2     |\n",
      "|    ent_coef        | 0.0188   |\n",
      "|    ent_coef_loss   | -0.671   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42877    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.5     |\n",
      "|    ep_rew_mean     | 457      |\n",
      "| time/              |          |\n",
      "|    episodes        | 660      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 428      |\n",
      "|    total_timesteps | 43887    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 20.6     |\n",
      "|    ent_coef        | 0.0188   |\n",
      "|    ent_coef_loss   | -2.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 43786    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.3     |\n",
      "|    ep_rew_mean     | 460      |\n",
      "| time/              |          |\n",
      "|    episodes        | 670      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 437      |\n",
      "|    total_timesteps | 44884    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 23.2     |\n",
      "|    ent_coef        | 0.0188   |\n",
      "|    ent_coef_loss   | -2.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 44783    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.8     |\n",
      "|    ep_rew_mean     | 466      |\n",
      "| time/              |          |\n",
      "|    episodes        | 680      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 447      |\n",
      "|    total_timesteps | 45888    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 31.7     |\n",
      "|    ent_coef        | 0.0185   |\n",
      "|    ent_coef_loss   | -3.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 45787    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.4     |\n",
      "|    ep_rew_mean     | 464      |\n",
      "| time/              |          |\n",
      "|    episodes        | 690      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 455      |\n",
      "|    total_timesteps | 46740    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | 4.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46639    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91       |\n",
      "|    ep_rew_mean     | 462      |\n",
      "| time/              |          |\n",
      "|    episodes        | 700      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 464      |\n",
      "|    total_timesteps | 47631    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -152     |\n",
      "|    critic_loss     | 20       |\n",
      "|    ent_coef        | 0.0183   |\n",
      "|    ent_coef_loss   | -0.108   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 47530    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.8     |\n",
      "|    ep_rew_mean     | 466      |\n",
      "| time/              |          |\n",
      "|    episodes        | 710      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 473      |\n",
      "|    total_timesteps | 48601    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -151     |\n",
      "|    critic_loss     | 24.5     |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 48500    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.2     |\n",
      "|    ep_rew_mean     | 470      |\n",
      "| time/              |          |\n",
      "|    episodes        | 720      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 483      |\n",
      "|    total_timesteps | 49602    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 26.5     |\n",
      "|    ent_coef        | 0.0182   |\n",
      "|    ent_coef_loss   | 8.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49501    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.6     |\n",
      "|    ep_rew_mean     | 463      |\n",
      "| time/              |          |\n",
      "|    episodes        | 730      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 491      |\n",
      "|    total_timesteps | 50414    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 23.6     |\n",
      "|    ent_coef        | 0.0182   |\n",
      "|    ent_coef_loss   | -0.865   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 50313    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.1     |\n",
      "|    ep_rew_mean     | 469      |\n",
      "| time/              |          |\n",
      "|    episodes        | 740      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 500      |\n",
      "|    total_timesteps | 51384    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -154     |\n",
      "|    critic_loss     | 23       |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | 4.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 51283    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.1     |\n",
      "|    ep_rew_mean     | 472      |\n",
      "| time/              |          |\n",
      "|    episodes        | 750      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 509      |\n",
      "|    total_timesteps | 52291    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -153     |\n",
      "|    critic_loss     | 23.3     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | -5.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 52190    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92       |\n",
      "|    ep_rew_mean     | 469      |\n",
      "| time/              |          |\n",
      "|    episodes        | 760      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 517      |\n",
      "|    total_timesteps | 53086    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -141     |\n",
      "|    critic_loss     | 19       |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | -2.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 52985    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92       |\n",
      "|    ep_rew_mean     | 472      |\n",
      "| time/              |          |\n",
      "|    episodes        | 770      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 527      |\n",
      "|    total_timesteps | 54089    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -153     |\n",
      "|    critic_loss     | 25.9     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | 0.495    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 53988    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 90.6     |\n",
      "|    ep_rew_mean     | 469      |\n",
      "| time/              |          |\n",
      "|    episodes        | 780      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 535      |\n",
      "|    total_timesteps | 54950    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -152     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | 3.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 54849    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.2     |\n",
      "|    ep_rew_mean     | 479      |\n",
      "| time/              |          |\n",
      "|    episodes        | 790      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 545      |\n",
      "|    total_timesteps | 55960    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 19.9     |\n",
      "|    ent_coef        | 0.0179   |\n",
      "|    ent_coef_loss   | -4.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 55859    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.4     |\n",
      "|    ep_rew_mean     | 481      |\n",
      "| time/              |          |\n",
      "|    episodes        | 800      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 554      |\n",
      "|    total_timesteps | 56871    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -148     |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    ent_coef        | 0.0182   |\n",
      "|    ent_coef_loss   | 4.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 56770    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.4     |\n",
      "|    ep_rew_mean     | 481      |\n",
      "| time/              |          |\n",
      "|    episodes        | 810      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 563      |\n",
      "|    total_timesteps | 57842    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -154     |\n",
      "|    critic_loss     | 21.4     |\n",
      "|    ent_coef        | 0.0182   |\n",
      "|    ent_coef_loss   | 4.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 57741    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.5     |\n",
      "|    ep_rew_mean     | 484      |\n",
      "| time/              |          |\n",
      "|    episodes        | 820      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 573      |\n",
      "|    total_timesteps | 58847    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 25.9     |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | -1.79    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 58746    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94.6     |\n",
      "|    ep_rew_mean     | 490      |\n",
      "| time/              |          |\n",
      "|    episodes        | 830      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 582      |\n",
      "|    total_timesteps | 59872    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -151     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | -2.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59771    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95       |\n",
      "|    ep_rew_mean     | 490      |\n",
      "| time/              |          |\n",
      "|    episodes        | 840      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 592      |\n",
      "|    total_timesteps | 60882    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -155     |\n",
      "|    critic_loss     | 26.1     |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | 3.74     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 60781    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96       |\n",
      "|    ep_rew_mean     | 496      |\n",
      "| time/              |          |\n",
      "|    episodes        | 850      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 602      |\n",
      "|    total_timesteps | 61896    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | -0.465   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 61795    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99       |\n",
      "|    ep_rew_mean     | 509      |\n",
      "| time/              |          |\n",
      "|    episodes        | 860      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 612      |\n",
      "|    total_timesteps | 62991    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -155     |\n",
      "|    critic_loss     | 22.7     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | -0.588   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 62890    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 99.7     |\n",
      "|    ep_rew_mean     | 510      |\n",
      "| time/              |          |\n",
      "|    episodes        | 870      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 622      |\n",
      "|    total_timesteps | 64058    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -154     |\n",
      "|    critic_loss     | 15.8     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | -1.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 63957    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | 516      |\n",
      "| time/              |          |\n",
      "|    episodes        | 880      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 633      |\n",
      "|    total_timesteps | 65092    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -151     |\n",
      "|    critic_loss     | 26.9     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | 2.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 64991    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 102      |\n",
      "|    ep_rew_mean     | 518      |\n",
      "| time/              |          |\n",
      "|    episodes        | 890      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 643      |\n",
      "|    total_timesteps | 66140    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -153     |\n",
      "|    critic_loss     | 24.1     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | 2.35     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 66039    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 104      |\n",
      "|    ep_rew_mean     | 530      |\n",
      "| time/              |          |\n",
      "|    episodes        | 900      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 655      |\n",
      "|    total_timesteps | 67301    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -159     |\n",
      "|    critic_loss     | 24.7     |\n",
      "|    ent_coef        | 0.0182   |\n",
      "|    ent_coef_loss   | -2.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 67200    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | 536      |\n",
      "| time/              |          |\n",
      "|    episodes        | 910      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 665      |\n",
      "|    total_timesteps | 68424    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -152     |\n",
      "|    critic_loss     | 26       |\n",
      "|    ent_coef        | 0.0186   |\n",
      "|    ent_coef_loss   | 4.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 68323    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | 537      |\n",
      "| time/              |          |\n",
      "|    episodes        | 920      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 675      |\n",
      "|    total_timesteps | 69442    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -156     |\n",
      "|    critic_loss     | 19.3     |\n",
      "|    ent_coef        | 0.0185   |\n",
      "|    ent_coef_loss   | -0.371   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69341    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 105      |\n",
      "|    ep_rew_mean     | 538      |\n",
      "| time/              |          |\n",
      "|    episodes        | 930      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 684      |\n",
      "|    total_timesteps | 70408    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -155     |\n",
      "|    critic_loss     | 22.3     |\n",
      "|    ent_coef        | 0.0184   |\n",
      "|    ent_coef_loss   | -2.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 70307    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | 539      |\n",
      "| time/              |          |\n",
      "|    episodes        | 940      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 694      |\n",
      "|    total_timesteps | 71439    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -153     |\n",
      "|    critic_loss     | 21.9     |\n",
      "|    ent_coef        | 0.0183   |\n",
      "|    ent_coef_loss   | -2.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 71338    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | 546      |\n",
      "| time/              |          |\n",
      "|    episodes        | 950      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 705      |\n",
      "|    total_timesteps | 72598    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    ent_coef        | 0.0186   |\n",
      "|    ent_coef_loss   | 0.00858  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 72497    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 552      |\n",
      "| time/              |          |\n",
      "|    episodes        | 960      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 717      |\n",
      "|    total_timesteps | 73834    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -163     |\n",
      "|    critic_loss     | 20.3     |\n",
      "|    ent_coef        | 0.0184   |\n",
      "|    ent_coef_loss   | 4.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 73733    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 553      |\n",
      "| time/              |          |\n",
      "|    episodes        | 970      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 728      |\n",
      "|    total_timesteps | 74908    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    ent_coef        | 0.0182   |\n",
      "|    ent_coef_loss   | -0.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 74807    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | 557      |\n",
      "| time/              |          |\n",
      "|    episodes        | 980      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 739      |\n",
      "|    total_timesteps | 76063    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 25.2     |\n",
      "|    ent_coef        | 0.0184   |\n",
      "|    ent_coef_loss   | -3.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 75962    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | 562      |\n",
      "| time/              |          |\n",
      "|    episodes        | 990      |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 751      |\n",
      "|    total_timesteps | 77188    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    ent_coef        | 0.0187   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 77087    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | 555      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1000     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 760      |\n",
      "|    total_timesteps | 78181    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -156     |\n",
      "|    critic_loss     | 25.4     |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | -4.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 78080    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | 557      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1010     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 771      |\n",
      "|    total_timesteps | 79292    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | 0.861    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79191    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | 556      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1020     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 781      |\n",
      "|    total_timesteps | 80298    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -163     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 1.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 80197    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 568      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1030     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 793      |\n",
      "|    total_timesteps | 81533    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -164     |\n",
      "|    critic_loss     | 22.8     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | -2.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 81432    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 567      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1040     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 802      |\n",
      "|    total_timesteps | 82533    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -156     |\n",
      "|    critic_loss     | 28.3     |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 82432    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | 562      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1050     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 812      |\n",
      "|    total_timesteps | 83570    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -159     |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -2.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 83469    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 554      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1060     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 822      |\n",
      "|    total_timesteps | 84608    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | 0.884    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 84507    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | 558      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1070     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 834      |\n",
      "|    total_timesteps | 85788    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -165     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | -1.83    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 85687    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | 568      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1080     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 846      |\n",
      "|    total_timesteps | 87099    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -173     |\n",
      "|    critic_loss     | 30.5     |\n",
      "|    ent_coef        | 0.0194   |\n",
      "|    ent_coef_loss   | -1.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 86998    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 115      |\n",
      "|    ep_rew_mean     | 590      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1090     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 862      |\n",
      "|    total_timesteps | 88716    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -3.94    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 88615    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 117      |\n",
      "|    ep_rew_mean     | 600      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1100     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 874      |\n",
      "|    total_timesteps | 89919    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -170     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | 2.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89818    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 118      |\n",
      "|    ep_rew_mean     | 603      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1110     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 885      |\n",
      "|    total_timesteps | 91118    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -169     |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | -3.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 91017    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | 624      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1120     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 899      |\n",
      "|    total_timesteps | 92544    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 23.9     |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | 3.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 92443    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | 630      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1130     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 912      |\n",
      "|    total_timesteps | 93883    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -173     |\n",
      "|    critic_loss     | 26       |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 93782    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | 643      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1140     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 924      |\n",
      "|    total_timesteps | 95147    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -175     |\n",
      "|    critic_loss     | 32.5     |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 0.108    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 95046    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | 650      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1150     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 935      |\n",
      "|    total_timesteps | 96329    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -173     |\n",
      "|    critic_loss     | 26.6     |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 0.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 96228    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | 659      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1160     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 948      |\n",
      "|    total_timesteps | 97575    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -174     |\n",
      "|    critic_loss     | 25.5     |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | -7.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 97474    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | 669      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1170     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 961      |\n",
      "|    total_timesteps | 98889    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -168     |\n",
      "|    critic_loss     | 25       |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 98788    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | 664      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1180     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 972      |\n",
      "|    total_timesteps | 100085   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 23.8     |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | -0.348   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99984    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 129      |\n",
      "|    ep_rew_mean     | 662      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1190     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 986      |\n",
      "|    total_timesteps | 101575   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 26.8     |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | -0.128   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 101474   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 129      |\n",
      "|    ep_rew_mean     | 666      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1200     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 999      |\n",
      "|    total_timesteps | 102838   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -174     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 102737   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | 678      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1210     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1012     |\n",
      "|    total_timesteps | 104263   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -173     |\n",
      "|    critic_loss     | 26.2     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | 3.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 104162   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | 676      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1220     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1025     |\n",
      "|    total_timesteps | 105614   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -170     |\n",
      "|    critic_loss     | 29.2     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.942    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 105513   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 133      |\n",
      "|    ep_rew_mean     | 689      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1230     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1040     |\n",
      "|    total_timesteps | 107164   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 25.3     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 107063   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 136      |\n",
      "|    ep_rew_mean     | 705      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1240     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1055     |\n",
      "|    total_timesteps | 108709   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -176     |\n",
      "|    critic_loss     | 24.8     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | -1.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 108608   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 139      |\n",
      "|    ep_rew_mean     | 723      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1250     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1070     |\n",
      "|    total_timesteps | 110227   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -181     |\n",
      "|    critic_loss     | 26       |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -2.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 110126   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 140      |\n",
      "|    ep_rew_mean     | 732      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1260     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1083     |\n",
      "|    total_timesteps | 111579   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -159     |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 111478   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 140      |\n",
      "|    ep_rew_mean     | 735      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1270     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1097     |\n",
      "|    total_timesteps | 112929   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -184     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 0.686    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 112828   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | 742      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1280     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1110     |\n",
      "|    total_timesteps | 114285   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -177     |\n",
      "|    critic_loss     | 24.5     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | -2.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 114184   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 143      |\n",
      "|    ep_rew_mean     | 740      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1290     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1125     |\n",
      "|    total_timesteps | 115847   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -177     |\n",
      "|    critic_loss     | 27.4     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 0.866    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 115746   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 145      |\n",
      "|    ep_rew_mean     | 748      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1300     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1140     |\n",
      "|    total_timesteps | 117307   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -169     |\n",
      "|    critic_loss     | 33.1     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -2.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 117206   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 741      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1310     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1153     |\n",
      "|    total_timesteps | 118654   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -179     |\n",
      "|    critic_loss     | 34.6     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -2.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 118553   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 145      |\n",
      "|    ep_rew_mean     | 745      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1320     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1167     |\n",
      "|    total_timesteps | 120134   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -187     |\n",
      "|    critic_loss     | 25.3     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | -3.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 120033   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 737      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1330     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1182     |\n",
      "|    total_timesteps | 121587   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -187     |\n",
      "|    critic_loss     | 32.8     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -2.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 121486   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 143      |\n",
      "|    ep_rew_mean     | 729      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1340     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1196     |\n",
      "|    total_timesteps | 123035   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | 0.391    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 122934   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 732      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1350     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1211     |\n",
      "|    total_timesteps | 124613   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -3.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 124512   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 148      |\n",
      "|    ep_rew_mean     | 750      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1360     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1229     |\n",
      "|    total_timesteps | 126363   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 30       |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 0.0434   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 126262   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 148      |\n",
      "|    ep_rew_mean     | 748      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1370     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1242     |\n",
      "|    total_timesteps | 127717   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -190     |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 127616   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 152      |\n",
      "|    ep_rew_mean     | 769      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1380     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1260     |\n",
      "|    total_timesteps | 129513   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -191     |\n",
      "|    critic_loss     | 31       |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | 0.551    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 129412   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 156      |\n",
      "|    ep_rew_mean     | 789      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1390     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1278     |\n",
      "|    total_timesteps | 131411   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -187     |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | 2.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 131310   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 157      |\n",
      "|    ep_rew_mean     | 792      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1400     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1293     |\n",
      "|    total_timesteps | 132998   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -188     |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | -0.424   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 132897   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 161      |\n",
      "|    ep_rew_mean     | 813      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1410     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1310     |\n",
      "|    total_timesteps | 134705   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 28.6     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 6.49     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 134604   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 158      |\n",
      "|    ep_rew_mean     | 801      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1420     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1322     |\n",
      "|    total_timesteps | 135950   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -191     |\n",
      "|    critic_loss     | 31.6     |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | -0.543   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 135849   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 160      |\n",
      "|    ep_rew_mean     | 812      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1430     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1338     |\n",
      "|    total_timesteps | 137592   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -180     |\n",
      "|    critic_loss     | 33.4     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 137491   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 161      |\n",
      "|    ep_rew_mean     | 817      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1440     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1352     |\n",
      "|    total_timesteps | 139091   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -187     |\n",
      "|    critic_loss     | 32.2     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | -1.89    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 138990   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 163      |\n",
      "|    ep_rew_mean     | 833      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1450     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1370     |\n",
      "|    total_timesteps | 140941   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -192     |\n",
      "|    critic_loss     | 28.6     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | 4.85     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 140840   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 163      |\n",
      "|    ep_rew_mean     | 833      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1460     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1387     |\n",
      "|    total_timesteps | 142709   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -207     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | 6.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 142608   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 165      |\n",
      "|    ep_rew_mean     | 840      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1470     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1401     |\n",
      "|    total_timesteps | 144172   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -183     |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | 1.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 144071   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 163      |\n",
      "|    ep_rew_mean     | 831      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1480     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1418     |\n",
      "|    total_timesteps | 145847   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -192     |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | 3.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 145746   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 161      |\n",
      "|    ep_rew_mean     | 817      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1490     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1434     |\n",
      "|    total_timesteps | 147475   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -195     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | -0.883   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 147374   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 164      |\n",
      "|    ep_rew_mean     | 839      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1500     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1452     |\n",
      "|    total_timesteps | 149413   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -195     |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -2.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149312   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 164      |\n",
      "|    ep_rew_mean     | 835      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1510     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1468     |\n",
      "|    total_timesteps | 151089   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -193     |\n",
      "|    critic_loss     | 30.2     |\n",
      "|    ent_coef        | 0.0218   |\n",
      "|    ent_coef_loss   | -0.686   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 150988   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 168      |\n",
      "|    ep_rew_mean     | 855      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1520     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1484     |\n",
      "|    total_timesteps | 152706   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -195     |\n",
      "|    critic_loss     | 52.6     |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 152605   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 169      |\n",
      "|    ep_rew_mean     | 857      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1530     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1504     |\n",
      "|    total_timesteps | 154443   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -198     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -2.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 154342   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 172      |\n",
      "|    ep_rew_mean     | 870      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1540     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1521     |\n",
      "|    total_timesteps | 156241   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -195     |\n",
      "|    critic_loss     | 30.4     |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | -6.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 156140   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 171      |\n",
      "|    ep_rew_mean     | 871      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1550     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1539     |\n",
      "|    total_timesteps | 158089   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -194     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | 2.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 157988   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 175      |\n",
      "|    ep_rew_mean     | 888      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1560     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1560     |\n",
      "|    total_timesteps | 160205   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.0218   |\n",
      "|    ent_coef_loss   | 3.64     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 160104   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 183      |\n",
      "|    ep_rew_mean     | 925      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1570     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1581     |\n",
      "|    total_timesteps | 162445   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -203     |\n",
      "|    critic_loss     | 43.8     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | 2.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 162344   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 183      |\n",
      "|    ep_rew_mean     | 929      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1580     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1597     |\n",
      "|    total_timesteps | 164141   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -206     |\n",
      "|    critic_loss     | 40.3     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | 1.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 164040   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 184      |\n",
      "|    ep_rew_mean     | 932      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1590     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1614     |\n",
      "|    total_timesteps | 165843   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 0.261    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 165742   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 186      |\n",
      "|    ep_rew_mean     | 943      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1600     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1636     |\n",
      "|    total_timesteps | 167982   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -202     |\n",
      "|    critic_loss     | 31.4     |\n",
      "|    ent_coef        | 0.0226   |\n",
      "|    ent_coef_loss   | -3       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 167881   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 187      |\n",
      "|    ep_rew_mean     | 950      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1610     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1657     |\n",
      "|    total_timesteps | 169799   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -200     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | -3.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169698   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 193      |\n",
      "|    ep_rew_mean     | 977      |\n",
      "| time/              |          |\n",
      "|    episodes        | 1620     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1680     |\n",
      "|    total_timesteps | 171964   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -186     |\n",
      "|    critic_loss     | 30.9     |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | -3.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 171863   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1630     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1706     |\n",
      "|    total_timesteps | 174572   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -210     |\n",
      "|    critic_loss     | 35.3     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 2        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 174471   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 204      |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1640     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1727     |\n",
      "|    total_timesteps | 176645   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -204     |\n",
      "|    critic_loss     | 30.6     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | 0.82     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 176544   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 208      |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1650     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1748     |\n",
      "|    total_timesteps | 178883   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -210     |\n",
      "|    critic_loss     | 34.4     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 3.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 178782   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 206      |\n",
      "|    ep_rew_mean     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1660     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1767     |\n",
      "|    total_timesteps | 180840   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -196     |\n",
      "|    critic_loss     | 52.6     |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | 1.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 180739   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 205      |\n",
      "|    ep_rew_mean     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1670     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1787     |\n",
      "|    total_timesteps | 182897   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -206     |\n",
      "|    critic_loss     | 34.6     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 182796   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1680     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1818     |\n",
      "|    total_timesteps | 186000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 43.8     |\n",
      "|    ent_coef        | 0.0236   |\n",
      "|    ent_coef_loss   | 0.982    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 185899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 222      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1690     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1838     |\n",
      "|    total_timesteps | 188005   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -207     |\n",
      "|    critic_loss     | 41.7     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -3.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 187904   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1700     |\n",
      "|    fps             | 102      |\n",
      "|    time_elapsed    | 1857     |\n",
      "|    total_timesteps | 189879   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -203     |\n",
      "|    critic_loss     | 38       |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 0.0369   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189778   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 223      |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1710     |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 1883     |\n",
      "|    total_timesteps | 192142   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 49.3     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 5.97     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 192041   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 225      |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1720     |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 1908     |\n",
      "|    total_timesteps | 194464   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -208     |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | -1.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 194363   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 219      |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1730     |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 1928     |\n",
      "|    total_timesteps | 196470   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -222     |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 1.85     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 196369   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 218      |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1740     |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 1948     |\n",
      "|    total_timesteps | 198479   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -223     |\n",
      "|    critic_loss     | 44.4     |\n",
      "|    ent_coef        | 0.0241   |\n",
      "|    ent_coef_loss   | -0.866   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 198378   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 225      |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1750     |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 1977     |\n",
      "|    total_timesteps | 201380   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -215     |\n",
      "|    critic_loss     | 49.4     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 201279   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 228      |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1760     |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 2004     |\n",
      "|    total_timesteps | 203667   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 39.3     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -0.759   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 203566   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 233      |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1770     |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 2033     |\n",
      "|    total_timesteps | 206184   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 48.1     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | 3        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 206083   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 217      |\n",
      "|    ep_rew_mean     | 1.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1780     |\n",
      "|    fps             | 101      |\n",
      "|    time_elapsed    | 2050     |\n",
      "|    total_timesteps | 207687   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 46.2     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -3.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 207586   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 224      |\n",
      "|    ep_rew_mean     | 1.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1790     |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 2083     |\n",
      "|    total_timesteps | 210423   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 46.4     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | 6.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 210322   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 233      |\n",
      "|    ep_rew_mean     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1800     |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 2114     |\n",
      "|    total_timesteps | 213145   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 44       |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -0.141   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 213044   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 233      |\n",
      "|    ep_rew_mean     | 1.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1810     |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 2141     |\n",
      "|    total_timesteps | 215464   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -223     |\n",
      "|    critic_loss     | 55.8     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -2.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 215363   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 247      |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1820     |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 2182     |\n",
      "|    total_timesteps | 219166   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -228     |\n",
      "|    critic_loss     | 41.8     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | 3.81     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219065   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 244      |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1830     |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 2200     |\n",
      "|    total_timesteps | 220860   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 47.9     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | 0.391    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 220759   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 254      |\n",
      "|    ep_rew_mean     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1840     |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 2234     |\n",
      "|    total_timesteps | 223882   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 52.8     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | -0.473   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 223781   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 253      |\n",
      "|    ep_rew_mean     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1850     |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 2263     |\n",
      "|    total_timesteps | 226668   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -223     |\n",
      "|    critic_loss     | 47       |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | 0.519    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 226567   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 247      |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1860     |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 2282     |\n",
      "|    total_timesteps | 228404   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -231     |\n",
      "|    critic_loss     | 51.5     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | 4.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 228303   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 246      |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1870     |\n",
      "|    fps             | 100      |\n",
      "|    time_elapsed    | 2307     |\n",
      "|    total_timesteps | 230744   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -252     |\n",
      "|    critic_loss     | 39.2     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 0.0396   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 230643   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 259      |\n",
      "|    ep_rew_mean     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1880     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2336     |\n",
      "|    total_timesteps | 233570   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -223     |\n",
      "|    critic_loss     | 45.6     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | 3.31     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 233469   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 255      |\n",
      "|    ep_rew_mean     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1890     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2361     |\n",
      "|    total_timesteps | 235970   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -247     |\n",
      "|    critic_loss     | 50.5     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | -3.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 235869   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 256      |\n",
      "|    ep_rew_mean     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1900     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2390     |\n",
      "|    total_timesteps | 238785   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -239     |\n",
      "|    critic_loss     | 50.6     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 6.82     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 238684   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 266      |\n",
      "|    ep_rew_mean     | 1.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1910     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2424     |\n",
      "|    total_timesteps | 242024   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -235     |\n",
      "|    critic_loss     | 39.9     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | 0.539    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 241923   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 263      |\n",
      "|    ep_rew_mean     | 1.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1920     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2460     |\n",
      "|    total_timesteps | 245427   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 55.9     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | -2.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 245326   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 283      |\n",
      "|    ep_rew_mean     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1930     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2501     |\n",
      "|    total_timesteps | 249163   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -236     |\n",
      "|    critic_loss     | 53       |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 249062   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1940     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2549     |\n",
      "|    total_timesteps | 253617   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -231     |\n",
      "|    critic_loss     | 49.8     |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -2.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 253516   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 302      |\n",
      "|    ep_rew_mean     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1950     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2584     |\n",
      "|    total_timesteps | 256891   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -242     |\n",
      "|    critic_loss     | 55       |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | 3.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 256790   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 313      |\n",
      "|    ep_rew_mean     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1960     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2614     |\n",
      "|    total_timesteps | 259711   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -241     |\n",
      "|    critic_loss     | 47.1     |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | -0.244   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259610   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 318      |\n",
      "|    ep_rew_mean     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1970     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2646     |\n",
      "|    total_timesteps | 262570   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -238     |\n",
      "|    critic_loss     | 49.5     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 1.94     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 262469   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 323      |\n",
      "|    ep_rew_mean     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1980     |\n",
      "|    fps             | 99       |\n",
      "|    time_elapsed    | 2684     |\n",
      "|    total_timesteps | 265893   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -239     |\n",
      "|    critic_loss     | 42.6     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | 2.58     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 265792   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 331      |\n",
      "|    ep_rew_mean     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1990     |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 2720     |\n",
      "|    total_timesteps | 269056   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -251     |\n",
      "|    critic_loss     | 44.1     |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | 3.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 268955   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 327      |\n",
      "|    ep_rew_mean     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2000     |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 2748     |\n",
      "|    total_timesteps | 271439   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -256     |\n",
      "|    critic_loss     | 50.6     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 4.86     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 271338   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 344      |\n",
      "|    ep_rew_mean     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2010     |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 2803     |\n",
      "|    total_timesteps | 276455   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -249     |\n",
      "|    critic_loss     | 46.6     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | 0.971    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 276354   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 352      |\n",
      "|    ep_rew_mean     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2020     |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 2846     |\n",
      "|    total_timesteps | 280594   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 55.7     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | -2.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 280493   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 355      |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2030     |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 2891     |\n",
      "|    total_timesteps | 284619   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -265     |\n",
      "|    critic_loss     | 73.2     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | 0.534    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 284518   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 352      |\n",
      "|    ep_rew_mean     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2040     |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 2940     |\n",
      "|    total_timesteps | 288773   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -263     |\n",
      "|    critic_loss     | 46.5     |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -1.81    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 288672   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 380      |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2050     |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 3005     |\n",
      "|    total_timesteps | 294876   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 51.5     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | -0.00148 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 294775   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 390      |\n",
      "|    ep_rew_mean     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2060     |\n",
      "|    fps             | 98       |\n",
      "|    time_elapsed    | 3047     |\n",
      "|    total_timesteps | 298710   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -277     |\n",
      "|    critic_loss     | 55.4     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | 0.639    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 298609   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 415      |\n",
      "|    ep_rew_mean     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2070     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 3107     |\n",
      "|    total_timesteps | 304022   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 58       |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 3.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 303921   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 431      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2080     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 3163     |\n",
      "|    total_timesteps | 308986   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -258     |\n",
      "|    critic_loss     | 60.7     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 7.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 308885   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 430      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2090     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 3199     |\n",
      "|    total_timesteps | 312098   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 64.2     |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | 0.566    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 311997   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 477      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2100     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 3280     |\n",
      "|    total_timesteps | 319183   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -262     |\n",
      "|    critic_loss     | 57.6     |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | 3.69     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319082   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 471      |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2110     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 3329     |\n",
      "|    total_timesteps | 323596   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 60.1     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 2.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 323495   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 483      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2120     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 3386     |\n",
      "|    total_timesteps | 328906   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 44.7     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -2.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 328805   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 516      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2130     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 3470     |\n",
      "|    total_timesteps | 336247   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 83.6     |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | -0.188   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 336146   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 535      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2140     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 3539     |\n",
      "|    total_timesteps | 342302   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -267     |\n",
      "|    critic_loss     | 50.7     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 3.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 342201   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 534      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2150     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 3607     |\n",
      "|    total_timesteps | 348240   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 57.4     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | 2.83     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 348139   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 561      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2160     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 3680     |\n",
      "|    total_timesteps | 354791   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 58.9     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -3.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 354690   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 556      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2170     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 3728     |\n",
      "|    total_timesteps | 359587   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 57.9     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | -0.265   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359486   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 595      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2180     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 3824     |\n",
      "|    total_timesteps | 368454   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 59.8     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 368353   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 635      |\n",
      "|    ep_rew_mean     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2190     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 3899     |\n",
      "|    total_timesteps | 375607   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 52.7     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 0.249    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 375506   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 631      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2200     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 3972     |\n",
      "|    total_timesteps | 382311   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 55.6     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 382210   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 664      |\n",
      "|    ep_rew_mean     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2210     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 4057     |\n",
      "|    total_timesteps | 389967   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 62.4     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 0.183    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 389866   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 684      |\n",
      "|    ep_rew_mean     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2220     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 4139     |\n",
      "|    total_timesteps | 397290   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 47.4     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -2.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 397189   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 698      |\n",
      "|    ep_rew_mean     | 3.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2230     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 4238     |\n",
      "|    total_timesteps | 406061   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 67.4     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | -2.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 405960   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 720      |\n",
      "|    ep_rew_mean     | 3.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2240     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 4335     |\n",
      "|    total_timesteps | 414310   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 75.7     |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | 4.49     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 414209   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 748      |\n",
      "|    ep_rew_mean     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2250     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 4431     |\n",
      "|    total_timesteps | 423008   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 76.6     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -0.774   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 422907   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 757      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2260     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 4517     |\n",
      "|    total_timesteps | 430519   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 46.1     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -3.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 430418   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2270     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 4611     |\n",
      "|    total_timesteps | 439604   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 61.6     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | -6.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439503   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2280     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 4713     |\n",
      "|    total_timesteps | 448754   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 53.4     |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | -1.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 448653   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2290     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 4819     |\n",
      "|    total_timesteps | 458019   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 69.9     |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | -0.766   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 457918   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 4.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2300     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 4912     |\n",
      "|    total_timesteps | 466145   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 54.7     |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | -0.954   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 466044   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 4.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2310     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 5020     |\n",
      "|    total_timesteps | 475398   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 49.5     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | -8.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 475297   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 4.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2320     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 5099     |\n",
      "|    total_timesteps | 482626   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 54.8     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | -1.55    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 482525   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 4.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2330     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 5205     |\n",
      "|    total_timesteps | 491781   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 63.5     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | 3.87     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 491680   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2340     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 5318     |\n",
      "|    total_timesteps | 501214   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 55.9     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | -0.792   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 501113   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 4.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2350     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 5414     |\n",
      "|    total_timesteps | 509234   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -379     |\n",
      "|    critic_loss     | 46.6     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -3.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 509133   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 4.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2360     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 5518     |\n",
      "|    total_timesteps | 518186   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | 5.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 518085   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 4.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2370     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 5628     |\n",
      "|    total_timesteps | 528186   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -381     |\n",
      "|    critic_loss     | 55.7     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 8.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 528085   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 4.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2380     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 5729     |\n",
      "|    total_timesteps | 537352   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -374     |\n",
      "|    critic_loss     | 71.3     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 2.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 537251   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 4.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2390     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 5829     |\n",
      "|    total_timesteps | 546505   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -370     |\n",
      "|    critic_loss     | 54.5     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | 4.35     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 546404   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 4.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2400     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 5932     |\n",
      "|    total_timesteps | 555471   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -382     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 555370   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 4.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2410     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6037     |\n",
      "|    total_timesteps | 564999   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -376     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 1.83     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 564898   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 922      |\n",
      "|    ep_rew_mean     | 4.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2420     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6151     |\n",
      "|    total_timesteps | 574825   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -387     |\n",
      "|    critic_loss     | 45.8     |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | 0.868    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 574724   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2430     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6255     |\n",
      "|    total_timesteps | 584257   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -405     |\n",
      "|    critic_loss     | 59.3     |\n",
      "|    ent_coef        | 0.0231   |\n",
      "|    ent_coef_loss   | -4.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 584156   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 922      |\n",
      "|    ep_rew_mean     | 4.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2440     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6356     |\n",
      "|    total_timesteps | 593457   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -386     |\n",
      "|    critic_loss     | 47.4     |\n",
      "|    ent_coef        | 0.0241   |\n",
      "|    ent_coef_loss   | -2.93    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 593356   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2450     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6429     |\n",
      "|    total_timesteps | 600182   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -399     |\n",
      "|    critic_loss     | 44.9     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -3.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 600081   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2460     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6525     |\n",
      "|    total_timesteps | 609091   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -396     |\n",
      "|    critic_loss     | 45.9     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -2.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 608990   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 4.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2470     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6619     |\n",
      "|    total_timesteps | 617798   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -404     |\n",
      "|    critic_loss     | 40       |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 617697   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 4.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2480     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6712     |\n",
      "|    total_timesteps | 626441   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -394     |\n",
      "|    critic_loss     | 50.2     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | 1.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 626340   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2490     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6821     |\n",
      "|    total_timesteps | 636441   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -392     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | 0.695    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 636340   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2500     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6898     |\n",
      "|    total_timesteps | 643659   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 81.6     |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | 4.88     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 643558   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2510     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 6994     |\n",
      "|    total_timesteps | 652324   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -404     |\n",
      "|    critic_loss     | 45.7     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -0.593   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 652223   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 4.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2520     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 7096     |\n",
      "|    total_timesteps | 661373   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -398     |\n",
      "|    critic_loss     | 45.2     |\n",
      "|    ent_coef        | 0.0218   |\n",
      "|    ent_coef_loss   | 0.722    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 661272   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 4.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2530     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 7199     |\n",
      "|    total_timesteps | 670497   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 38.6     |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | 0.495    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 670396   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2540     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 7299     |\n",
      "|    total_timesteps | 679703   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -404     |\n",
      "|    critic_loss     | 56.8     |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | 8.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 679602   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2550     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 7408     |\n",
      "|    total_timesteps | 689703   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -406     |\n",
      "|    critic_loss     | 45.2     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 689602   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2560     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 7515     |\n",
      "|    total_timesteps | 699361   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -402     |\n",
      "|    critic_loss     | 34.6     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | 8.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699260   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 910      |\n",
      "|    ep_rew_mean     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2570     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 7618     |\n",
      "|    total_timesteps | 708837   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -404     |\n",
      "|    critic_loss     | 44.6     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | -0.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 708736   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 924      |\n",
      "|    ep_rew_mean     | 4.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2580     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 7731     |\n",
      "|    total_timesteps | 718837   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | 1.67     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 718736   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 918      |\n",
      "|    ep_rew_mean     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2590     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 7833     |\n",
      "|    total_timesteps | 728291   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 49.2     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 728190   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 943      |\n",
      "|    ep_rew_mean     | 4.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2600     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 7942     |\n",
      "|    total_timesteps | 738006   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    ent_coef        | 0.0194   |\n",
      "|    ent_coef_loss   | 1.63     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 737905   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 955      |\n",
      "|    ep_rew_mean     | 5.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2610     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 8046     |\n",
      "|    total_timesteps | 747785   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 38.4     |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 747684   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 964      |\n",
      "|    ep_rew_mean     | 5.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2620     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 8158     |\n",
      "|    total_timesteps | 757775   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 32.5     |\n",
      "|    ent_coef        | 0.0188   |\n",
      "|    ent_coef_loss   | 1.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 757674   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2630     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 8273     |\n",
      "|    total_timesteps | 767775   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    ent_coef        | 0.0183   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 767674   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 978      |\n",
      "|    ep_rew_mean     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2640     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 8367     |\n",
      "|    total_timesteps | 777509   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -419     |\n",
      "|    critic_loss     | 37.8     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | 2.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 777408   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 972      |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2650     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 8458     |\n",
      "|    total_timesteps | 786869   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 37.3     |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | -4.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 786768   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 972      |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2660     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 8552     |\n",
      "|    total_timesteps | 796595   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 39       |\n",
      "|    ent_coef        | 0.0184   |\n",
      "|    ent_coef_loss   | -2.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 796494   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 969      |\n",
      "|    ep_rew_mean     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2670     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 8648     |\n",
      "|    total_timesteps | 805786   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 36       |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | -10.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 805685   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 969      |\n",
      "|    ep_rew_mean     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2680     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 8749     |\n",
      "|    total_timesteps | 815786   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -443     |\n",
      "|    critic_loss     | 22.4     |\n",
      "|    ent_coef        | 0.0179   |\n",
      "|    ent_coef_loss   | -3.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 815685   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 975      |\n",
      "|    ep_rew_mean     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2690     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 8867     |\n",
      "|    total_timesteps | 825786   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 49       |\n",
      "|    ent_coef        | 0.0184   |\n",
      "|    ent_coef_loss   | 4.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 825685   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 972      |\n",
      "|    ep_rew_mean     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2700     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 9015     |\n",
      "|    total_timesteps | 835182   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -459     |\n",
      "|    critic_loss     | 33.7     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | -5.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 835081   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 971      |\n",
      "|    ep_rew_mean     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2710     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 9165     |\n",
      "|    total_timesteps | 844903   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -440     |\n",
      "|    critic_loss     | 35.5     |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | 1.45     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 844802   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2720     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 9295     |\n",
      "|    total_timesteps | 854518   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    ent_coef        | 0.0181   |\n",
      "|    ent_coef_loss   | 2.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 854417   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2730     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 9440     |\n",
      "|    total_timesteps | 864518   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 44.7     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | 5.71     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 864417   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 960      |\n",
      "|    ep_rew_mean     | 5.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2740     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 9572     |\n",
      "|    total_timesteps | 873539   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 43.3     |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | -3.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 873438   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 953      |\n",
      "|    ep_rew_mean     | 5.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2750     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 9700     |\n",
      "|    total_timesteps | 882218   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 30.8     |\n",
      "|    ent_coef        | 0.0175   |\n",
      "|    ent_coef_loss   | 5.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 882117   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 951      |\n",
      "|    ep_rew_mean     | 5.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2760     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 9840     |\n",
      "|    total_timesteps | 891733   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 31.4     |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 891632   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 959      |\n",
      "|    ep_rew_mean     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2770     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 9988     |\n",
      "|    total_timesteps | 901733   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -452     |\n",
      "|    critic_loss     | 25.9     |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 901632   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 959      |\n",
      "|    ep_rew_mean     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2780     |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 10134    |\n",
      "|    total_timesteps | 911663   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -443     |\n",
      "|    critic_loss     | 34       |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | -4.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 911562   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 959      |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2790     |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 10284    |\n",
      "|    total_timesteps | 921663   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -448     |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -0.749   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 921562   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 965      |\n",
      "|    ep_rew_mean     | 5.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2800     |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 10383    |\n",
      "|    total_timesteps | 931663   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -460     |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -4.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 931562   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 968      |\n",
      "|    ep_rew_mean     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2810     |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 10480    |\n",
      "|    total_timesteps | 941663   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -446     |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -7.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 941562   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 5.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2820     |\n",
      "|    fps             | 89       |\n",
      "|    time_elapsed    | 10573    |\n",
      "|    total_timesteps | 951251   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -449     |\n",
      "|    critic_loss     | 35.6     |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | 6.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 951150   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 960      |\n",
      "|    ep_rew_mean     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2830     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 10666    |\n",
      "|    total_timesteps | 960479   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -449     |\n",
      "|    critic_loss     | 30.5     |\n",
      "|    ent_coef        | 0.0159   |\n",
      "|    ent_coef_loss   | 6.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 960378   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 969      |\n",
      "|    ep_rew_mean     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2840     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 10764    |\n",
      "|    total_timesteps | 970479   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -467     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | -3.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 970378   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 983      |\n",
      "|    ep_rew_mean     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2850     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 10862    |\n",
      "|    total_timesteps | 980479   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -441     |\n",
      "|    critic_loss     | 27.9     |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | 5.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 980378   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 984      |\n",
      "|    ep_rew_mean     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2860     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 10963    |\n",
      "|    total_timesteps | 990113   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -465     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 990012   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 984      |\n",
      "|    ep_rew_mean     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2870     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11061    |\n",
      "|    total_timesteps | 1000113  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -452     |\n",
      "|    critic_loss     | 46       |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | 3.67     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1000012  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 984      |\n",
      "|    ep_rew_mean     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2880     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11184    |\n",
      "|    total_timesteps | 1010113  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -463     |\n",
      "|    critic_loss     | 22.3     |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | -7.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1010012  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 983      |\n",
      "|    ep_rew_mean     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2890     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11285    |\n",
      "|    total_timesteps | 1020008  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -454     |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    ent_coef        | 0.0152   |\n",
      "|    ent_coef_loss   | 8.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019907  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 983      |\n",
      "|    ep_rew_mean     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2900     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11380    |\n",
      "|    total_timesteps | 1030008  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -459     |\n",
      "|    critic_loss     | 19       |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029907  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 983      |\n",
      "|    ep_rew_mean     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2910     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11485    |\n",
      "|    total_timesteps | 1040008  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -462     |\n",
      "|    critic_loss     | 22.9     |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | -6.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1039907  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 988      |\n",
      "|    ep_rew_mean     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2920     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11592    |\n",
      "|    total_timesteps | 1050008  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -467     |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    ent_coef        | 0.0142   |\n",
      "|    ent_coef_loss   | 1.82     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049907  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 985      |\n",
      "|    ep_rew_mean     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2930     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11678    |\n",
      "|    total_timesteps | 1058983  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -463     |\n",
      "|    critic_loss     | 26.2     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | 6.72     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1058882  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 985      |\n",
      "|    ep_rew_mean     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2940     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11773    |\n",
      "|    total_timesteps | 1068983  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 24       |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 2.47     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1068882  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 977      |\n",
      "|    ep_rew_mean     | 5.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2950     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11862    |\n",
      "|    total_timesteps | 1078173  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -473     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0129   |\n",
      "|    ent_coef_loss   | 7.69     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1078072  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 968      |\n",
      "|    ep_rew_mean     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2960     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 11957    |\n",
      "|    total_timesteps | 1086925  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -472     |\n",
      "|    critic_loss     | 29       |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | 6.74     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1086824  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 954      |\n",
      "|    ep_rew_mean     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2970     |\n",
      "|    fps             | 90       |\n",
      "|    time_elapsed    | 12038    |\n",
      "|    total_timesteps | 1095525  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 22.3     |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -0.677   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1095424  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 954      |\n",
      "|    ep_rew_mean     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2980     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12133    |\n",
      "|    total_timesteps | 1105472  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 28.7     |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -3.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1105371  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 934      |\n",
      "|    ep_rew_mean     | 5.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2990     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12208    |\n",
      "|    total_timesteps | 1113406  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | 2.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1113305  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 4.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3000     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12289    |\n",
      "|    total_timesteps | 1121932  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -489     |\n",
      "|    critic_loss     | 19.5     |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 5.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1121831  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 913      |\n",
      "|    ep_rew_mean     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3010     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12378    |\n",
      "|    total_timesteps | 1131321  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 0.878    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1131220  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 901      |\n",
      "|    ep_rew_mean     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3020     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12462    |\n",
      "|    total_timesteps | 1140130  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -493     |\n",
      "|    critic_loss     | 13.3     |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | 4.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1140029  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 905      |\n",
      "|    ep_rew_mean     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3030     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12550    |\n",
      "|    total_timesteps | 1149475  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -493     |\n",
      "|    critic_loss     | 24.8     |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1149374  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 895      |\n",
      "|    ep_rew_mean     | 4.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3040     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12635    |\n",
      "|    total_timesteps | 1158523  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 10.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1158422  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3050     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12733    |\n",
      "|    total_timesteps | 1168198  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -497     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | 3.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1168097  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 4.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3060     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12807    |\n",
      "|    total_timesteps | 1175942  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -498     |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | -7.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1175841  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3070     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12888    |\n",
      "|    total_timesteps | 1184475  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -495     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0111   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1184374  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 4.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3080     |\n",
      "|    fps             | 91       |\n",
      "|    time_elapsed    | 12983    |\n",
      "|    total_timesteps | 1194475  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -5.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1194374  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 4.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3090     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13077    |\n",
      "|    total_timesteps | 1204337  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | 5.96     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1204236  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 918      |\n",
      "|    ep_rew_mean     | 5.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3100     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13167    |\n",
      "|    total_timesteps | 1213762  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | 4.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1213661  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 5.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3110     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13259    |\n",
      "|    total_timesteps | 1223360  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 13.3     |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 8.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1223259  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 5.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3120     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13343    |\n",
      "|    total_timesteps | 1232237  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -0.429   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1232136  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 5.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3130     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13430    |\n",
      "|    total_timesteps | 1241400  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | -0.678   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1241299  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 5.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3140     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13517    |\n",
      "|    total_timesteps | 1250533  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 18.4     |\n",
      "|    ent_coef        | 0.00959  |\n",
      "|    ent_coef_loss   | 3.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1250432  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 5.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3150     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13603    |\n",
      "|    total_timesteps | 1259592  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 5.64     |\n",
      "|    ent_coef        | 0.00939  |\n",
      "|    ent_coef_loss   | 0.614    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259491  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3160     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13690    |\n",
      "|    total_timesteps | 1268706  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0097   |\n",
      "|    ent_coef_loss   | -0.819   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1268605  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 939      |\n",
      "|    ep_rew_mean     | 5.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3170     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13781    |\n",
      "|    total_timesteps | 1278404  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.00929  |\n",
      "|    ent_coef_loss   | -7.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1278303  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 938      |\n",
      "|    ep_rew_mean     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3180     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13875    |\n",
      "|    total_timesteps | 1288277  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 9.2      |\n",
      "|    ent_coef        | 0.00955  |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1288176  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3190     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 13965    |\n",
      "|    total_timesteps | 1297176  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 7.94     |\n",
      "|    ent_coef        | 0.00947  |\n",
      "|    ent_coef_loss   | -3.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1297075  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 927      |\n",
      "|    ep_rew_mean     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3200     |\n",
      "|    fps             | 92       |\n",
      "|    time_elapsed    | 14053    |\n",
      "|    total_timesteps | 1306450  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.00902  |\n",
      "|    ent_coef_loss   | -0.837   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1306349  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 906      |\n",
      "|    ep_rew_mean     | 5.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3210     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14125    |\n",
      "|    total_timesteps | 1313984  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.00929  |\n",
      "|    ent_coef_loss   | 5.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1313883  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 5.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3220     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14197    |\n",
      "|    total_timesteps | 1321626  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -522     |\n",
      "|    critic_loss     | 20.1     |\n",
      "|    ent_coef        | 0.0091   |\n",
      "|    ent_coef_loss   | 1.83     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1321525  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3230     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14276    |\n",
      "|    total_timesteps | 1329973  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 5.98     |\n",
      "|    ent_coef        | 0.00889  |\n",
      "|    ent_coef_loss   | -6.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1329872  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3240     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14362    |\n",
      "|    total_timesteps | 1339028  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -514     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    ent_coef        | 0.00977  |\n",
      "|    ent_coef_loss   | 2.72     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1338927  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 4.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3250     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14428    |\n",
      "|    total_timesteps | 1345972  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -514     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | -4.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345871  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 832      |\n",
      "|    ep_rew_mean     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3260     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14484    |\n",
      "|    total_timesteps | 1351909  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 33.8     |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -14.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1351808  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 4.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3270     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14563    |\n",
      "|    total_timesteps | 1359979  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -523     |\n",
      "|    critic_loss     | 11.5     |\n",
      "|    ent_coef        | 0.00991  |\n",
      "|    ent_coef_loss   | -3.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1359878  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3280     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14634    |\n",
      "|    total_timesteps | 1367434  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 16.2     |\n",
      "|    ent_coef        | 0.00991  |\n",
      "|    ent_coef_loss   | -3.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1367333  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 786      |\n",
      "|    ep_rew_mean     | 4.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3290     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14712    |\n",
      "|    total_timesteps | 1375747  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 23.2     |\n",
      "|    ent_coef        | 0.00951  |\n",
      "|    ent_coef_loss   | 14.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1375646  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 4.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3300     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14802    |\n",
      "|    total_timesteps | 1385178  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.00948  |\n",
      "|    ent_coef_loss   | -13.9    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1385077  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3310     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14863    |\n",
      "|    total_timesteps | 1391624  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.0091   |\n",
      "|    ent_coef_loss   | 2.31     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1391523  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3320     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 14940    |\n",
      "|    total_timesteps | 1399715  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    ent_coef        | 0.00989  |\n",
      "|    ent_coef_loss   | 3.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1399614  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3330     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 15014    |\n",
      "|    total_timesteps | 1407574  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -518     |\n",
      "|    critic_loss     | 13.9     |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1407473  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 782      |\n",
      "|    ep_rew_mean     | 4.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3340     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 15105    |\n",
      "|    total_timesteps | 1417183  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0101   |\n",
      "|    ent_coef_loss   | 4.88     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1417082  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3350     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 15200    |\n",
      "|    total_timesteps | 1427183  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -521     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    ent_coef        | 0.01     |\n",
      "|    ent_coef_loss   | -2.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1427082  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3360     |\n",
      "|    fps             | 93       |\n",
      "|    time_elapsed    | 15294    |\n",
      "|    total_timesteps | 1437172  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | -5.83    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1437071  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3370     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 15377    |\n",
      "|    total_timesteps | 1445927  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 17.7     |\n",
      "|    ent_coef        | 0.00975  |\n",
      "|    ent_coef_loss   | 2.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1445826  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 4.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3380     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 15462    |\n",
      "|    total_timesteps | 1454800  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1454699  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 5.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3390     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 15566    |\n",
      "|    total_timesteps | 1464800  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -521     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | 3.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1464699  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 5.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3400     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 15661    |\n",
      "|    total_timesteps | 1474765  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 15.2     |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | -3.96    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1474664  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 925      |\n",
      "|    ep_rew_mean     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3410     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 15753    |\n",
      "|    total_timesteps | 1484170  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -521     |\n",
      "|    critic_loss     | 30.8     |\n",
      "|    ent_coef        | 0.00952  |\n",
      "|    ent_coef_loss   | 2.9      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1484069  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 945      |\n",
      "|    ep_rew_mean     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3420     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 15848    |\n",
      "|    total_timesteps | 1494170  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -523     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.00979  |\n",
      "|    ent_coef_loss   | 0.914    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1494069  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 960      |\n",
      "|    ep_rew_mean     | 5.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3430     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 15936    |\n",
      "|    total_timesteps | 1503541  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -523     |\n",
      "|    critic_loss     | 9.8      |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -6.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1503440  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3440     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16017    |\n",
      "|    total_timesteps | 1512164  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -526     |\n",
      "|    critic_loss     | 11.5     |\n",
      "|    ent_coef        | 0.00998  |\n",
      "|    ent_coef_loss   | 0.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1512063  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3450     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16105    |\n",
      "|    total_timesteps | 1521378  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -526     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0102   |\n",
      "|    ent_coef_loss   | 5.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1521277  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 5.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3460     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16200    |\n",
      "|    total_timesteps | 1531378  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -531     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.00964  |\n",
      "|    ent_coef_loss   | -3.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1531277  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 940      |\n",
      "|    ep_rew_mean     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3470     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16283    |\n",
      "|    total_timesteps | 1539966  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -529     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0106   |\n",
      "|    ent_coef_loss   | -0.883   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539865  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 952      |\n",
      "|    ep_rew_mean     | 5.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3480     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16378    |\n",
      "|    total_timesteps | 1549966  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -528     |\n",
      "|    critic_loss     | 22.6     |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 0.973    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549865  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 945      |\n",
      "|    ep_rew_mean     | 5.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3490     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16466    |\n",
      "|    total_timesteps | 1559260  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -528     |\n",
      "|    critic_loss     | 15.4     |\n",
      "|    ent_coef        | 0.00935  |\n",
      "|    ent_coef_loss   | 0.138    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559159  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 934      |\n",
      "|    ep_rew_mean     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3500     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16551    |\n",
      "|    total_timesteps | 1568116  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -529     |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    ent_coef        | 0.0094   |\n",
      "|    ent_coef_loss   | 2.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1568015  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 939      |\n",
      "|    ep_rew_mean     | 5.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3510     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16646    |\n",
      "|    total_timesteps | 1578116  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -528     |\n",
      "|    critic_loss     | 14.3     |\n",
      "|    ent_coef        | 0.0107   |\n",
      "|    ent_coef_loss   | 5.69     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1578015  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 939      |\n",
      "|    ep_rew_mean     | 5.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3520     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16741    |\n",
      "|    total_timesteps | 1588116  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -527     |\n",
      "|    critic_loss     | 19       |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | 3.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1588015  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 940      |\n",
      "|    ep_rew_mean     | 5.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3530     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16830    |\n",
      "|    total_timesteps | 1597511  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -534     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | -6.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1597410  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 5.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3540     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 16922    |\n",
      "|    total_timesteps | 1607199  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -530     |\n",
      "|    critic_loss     | 16.8     |\n",
      "|    ent_coef        | 0.0108   |\n",
      "|    ent_coef_loss   | -3.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1607098  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 958      |\n",
      "|    ep_rew_mean     | 5.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3550     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 17016    |\n",
      "|    total_timesteps | 1617199  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -531     |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 0.453    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1617098  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 5.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3560     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 17103    |\n",
      "|    total_timesteps | 1626376  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -531     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0104   |\n",
      "|    ent_coef_loss   | 0.707    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1626275  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 947      |\n",
      "|    ep_rew_mean     | 5.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3570     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 17181    |\n",
      "|    total_timesteps | 1634692  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -538     |\n",
      "|    critic_loss     | 9        |\n",
      "|    ent_coef        | 0.0105   |\n",
      "|    ent_coef_loss   | 1.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1634591  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 938      |\n",
      "|    ep_rew_mean     | 5.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3580     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 17267    |\n",
      "|    total_timesteps | 1643795  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -538     |\n",
      "|    critic_loss     | 13.9     |\n",
      "|    ent_coef        | 0.0103   |\n",
      "|    ent_coef_loss   | -2.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1643694  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 936      |\n",
      "|    ep_rew_mean     | 5.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3590     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 17354    |\n",
      "|    total_timesteps | 1652910  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -540     |\n",
      "|    critic_loss     | 9.24     |\n",
      "|    ent_coef        | 0.01     |\n",
      "|    ent_coef_loss   | -7.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1652809  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 948      |\n",
      "|    ep_rew_mean     | 5.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3600     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 17447    |\n",
      "|    total_timesteps | 1662910  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -536     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.00986  |\n",
      "|    ent_coef_loss   | 1.85     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1662809  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 936      |\n",
      "|    ep_rew_mean     | 5.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3610     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 17530    |\n",
      "|    total_timesteps | 1671729  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -538     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | 7.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1671628  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 931      |\n",
      "|    ep_rew_mean     | 5.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3620     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 17620    |\n",
      "|    total_timesteps | 1681248  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -537     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.0109   |\n",
      "|    ent_coef_loss   | 8.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1681147  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 930      |\n",
      "|    ep_rew_mean     | 5.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3630     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 17710    |\n",
      "|    total_timesteps | 1690531  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -536     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0114   |\n",
      "|    ent_coef_loss   | -2.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1690430  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 933      |\n",
      "|    ep_rew_mean     | 5.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3640     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 17939    |\n",
      "|    total_timesteps | 1700531  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -539     |\n",
      "|    critic_loss     | 14.3     |\n",
      "|    ent_coef        | 0.0112   |\n",
      "|    ent_coef_loss   | 5.79     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1700430  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 924      |\n",
      "|    ep_rew_mean     | 5.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3650     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18033    |\n",
      "|    total_timesteps | 1709586  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -542     |\n",
      "|    critic_loss     | 8.2      |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | -7.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709485  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 931      |\n",
      "|    ep_rew_mean     | 5.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3660     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18128    |\n",
      "|    total_timesteps | 1719449  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -541     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    ent_coef        | 0.0113   |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719348  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 936      |\n",
      "|    ep_rew_mean     | 5.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3670     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18212    |\n",
      "|    total_timesteps | 1728318  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -541     |\n",
      "|    critic_loss     | 14.3     |\n",
      "|    ent_coef        | 0.0115   |\n",
      "|    ent_coef_loss   | 2.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1728217  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 5.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3680     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18310    |\n",
      "|    total_timesteps | 1737960  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -538     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | 3.42     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1737859  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 5.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3690     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18418    |\n",
      "|    total_timesteps | 1747960  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -542     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | 5.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1747859  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 945      |\n",
      "|    ep_rew_mean     | 5.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3700     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18520    |\n",
      "|    total_timesteps | 1757437  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -545     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | 4.58     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1757336  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 957      |\n",
      "|    ep_rew_mean     | 5.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3710     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18632    |\n",
      "|    total_timesteps | 1767437  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -541     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.011    |\n",
      "|    ent_coef_loss   | -0.542   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1767336  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 956      |\n",
      "|    ep_rew_mean     | 5.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3720     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18737    |\n",
      "|    total_timesteps | 1776886  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -540     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.0118   |\n",
      "|    ent_coef_loss   | 13.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1776785  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 964      |\n",
      "|    ep_rew_mean     | 5.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3730     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18846    |\n",
      "|    total_timesteps | 1786886  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -547     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.0116   |\n",
      "|    ent_coef_loss   | 4.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1786785  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 954      |\n",
      "|    ep_rew_mean     | 5.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3740     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 18945    |\n",
      "|    total_timesteps | 1795978  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -550     |\n",
      "|    critic_loss     | 7.63     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -8.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1795877  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 953      |\n",
      "|    ep_rew_mean     | 5.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3750     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19042    |\n",
      "|    total_timesteps | 1804916  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -546     |\n",
      "|    critic_loss     | 9.56     |\n",
      "|    ent_coef        | 0.0123   |\n",
      "|    ent_coef_loss   | -3.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1804815  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 954      |\n",
      "|    ep_rew_mean     | 5.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3760     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19158    |\n",
      "|    total_timesteps | 1814848  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -550     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -5.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1814747  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 951      |\n",
      "|    ep_rew_mean     | 5.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3770     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19251    |\n",
      "|    total_timesteps | 1823436  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -546     |\n",
      "|    critic_loss     | 80.6     |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | -4.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1823335  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 955      |\n",
      "|    ep_rew_mean     | 5.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3780     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19352    |\n",
      "|    total_timesteps | 1833436  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -543     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | 4.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1833335  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 955      |\n",
      "|    ep_rew_mean     | 5.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3790     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19455    |\n",
      "|    total_timesteps | 1843436  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -538     |\n",
      "|    critic_loss     | 38       |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 16.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1843335  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 960      |\n",
      "|    ep_rew_mean     | 5.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3800     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19563    |\n",
      "|    total_timesteps | 1853436  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -535     |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 17.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1853335  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 956      |\n",
      "|    ep_rew_mean     | 5.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3810     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19660    |\n",
      "|    total_timesteps | 1862989  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -552     |\n",
      "|    critic_loss     | 8.69     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 2.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1862888  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 953      |\n",
      "|    ep_rew_mean     | 5.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3820     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19750    |\n",
      "|    total_timesteps | 1872156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -549     |\n",
      "|    critic_loss     | 9.57     |\n",
      "|    ent_coef        | 0.0117   |\n",
      "|    ent_coef_loss   | 1.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1872055  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 953      |\n",
      "|    ep_rew_mean     | 5.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3830     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19852    |\n",
      "|    total_timesteps | 1882156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -553     |\n",
      "|    critic_loss     | 6.09     |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -6.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1882055  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 962      |\n",
      "|    ep_rew_mean     | 5.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3840     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 19946    |\n",
      "|    total_timesteps | 1892156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -552     |\n",
      "|    critic_loss     | 8.34     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | -7.65    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1892055  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 972      |\n",
      "|    ep_rew_mean     | 6.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3850     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 20043    |\n",
      "|    total_timesteps | 1902156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -549     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0125   |\n",
      "|    ent_coef_loss   | 14.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1902055  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 6.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3860     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 20143    |\n",
      "|    total_timesteps | 1912156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -552     |\n",
      "|    critic_loss     | 28.8     |\n",
      "|    ent_coef        | 0.0122   |\n",
      "|    ent_coef_loss   | -6.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1912055  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 987      |\n",
      "|    ep_rew_mean     | 6.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3870     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 20239    |\n",
      "|    total_timesteps | 1922156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -550     |\n",
      "|    critic_loss     | 17.7     |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | 5.13     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1922055  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 987      |\n",
      "|    ep_rew_mean     | 6.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3880     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 20334    |\n",
      "|    total_timesteps | 1932156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -555     |\n",
      "|    critic_loss     | 6.85     |\n",
      "|    ent_coef        | 0.0127   |\n",
      "|    ent_coef_loss   | -0.201   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1932055  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 987      |\n",
      "|    ep_rew_mean     | 6.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3890     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 20430    |\n",
      "|    total_timesteps | 1942156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -556     |\n",
      "|    critic_loss     | 5.23     |\n",
      "|    ent_coef        | 0.0124   |\n",
      "|    ent_coef_loss   | -6.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1942055  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 978      |\n",
      "|    ep_rew_mean     | 6.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3900     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 20521    |\n",
      "|    total_timesteps | 1951237  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -561     |\n",
      "|    critic_loss     | 6.95     |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | -2.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1951136  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 966      |\n",
      "|    ep_rew_mean     | 6.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3910     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 20604    |\n",
      "|    total_timesteps | 1959601  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -562     |\n",
      "|    critic_loss     | 11.6     |\n",
      "|    ent_coef        | 0.0126   |\n",
      "|    ent_coef_loss   | -7.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1959500  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 974      |\n",
      "|    ep_rew_mean     | 6.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3920     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 20705    |\n",
      "|    total_timesteps | 1969601  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -558     |\n",
      "|    critic_loss     | 15.9     |\n",
      "|    ent_coef        | 0.013    |\n",
      "|    ent_coef_loss   | 2.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1969500  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 6.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3930     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 20798    |\n",
      "|    total_timesteps | 1979435  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -558     |\n",
      "|    critic_loss     | 26       |\n",
      "|    ent_coef        | 0.0134   |\n",
      "|    ent_coef_loss   | 8.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1979334  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 970      |\n",
      "|    ep_rew_mean     | 6.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3940     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 20904    |\n",
      "|    total_timesteps | 1989151  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -564     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0139   |\n",
      "|    ent_coef_loss   | -4.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989050  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 970      |\n",
      "|    ep_rew_mean     | 6.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3950     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21012    |\n",
      "|    total_timesteps | 1999151  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -564     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0141   |\n",
      "|    ent_coef_loss   | -3.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1999050  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 955      |\n",
      "|    ep_rew_mean     | 6.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3960     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21103    |\n",
      "|    total_timesteps | 2007638  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -567     |\n",
      "|    critic_loss     | 5.6      |\n",
      "|    ent_coef        | 0.0132   |\n",
      "|    ent_coef_loss   | -7.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2007537  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 941      |\n",
      "|    ep_rew_mean     | 5.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3970     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21199    |\n",
      "|    total_timesteps | 2016208  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -564     |\n",
      "|    critic_loss     | 24.2     |\n",
      "|    ent_coef        | 0.014    |\n",
      "|    ent_coef_loss   | -6.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2016107  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 5.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3980     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21276    |\n",
      "|    total_timesteps | 2024274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -560     |\n",
      "|    critic_loss     | 34.3     |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 1.86     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2024173  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 5.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3990     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21380    |\n",
      "|    total_timesteps | 2034274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -558     |\n",
      "|    critic_loss     | 43.8     |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 3.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2034173  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 930      |\n",
      "|    ep_rew_mean     | 5.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4000     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21482    |\n",
      "|    total_timesteps | 2044274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -557     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    ent_coef        | 0.0151   |\n",
      "|    ent_coef_loss   | 10.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2044173  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 947      |\n",
      "|    ep_rew_mean     | 6.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4010     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21577    |\n",
      "|    total_timesteps | 2054274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -565     |\n",
      "|    critic_loss     | 13.4     |\n",
      "|    ent_coef        | 0.0146   |\n",
      "|    ent_coef_loss   | -1.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2054173  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 947      |\n",
      "|    ep_rew_mean     | 6.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4020     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21675    |\n",
      "|    total_timesteps | 2064274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -561     |\n",
      "|    critic_loss     | 53.7     |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -3.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2064173  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 948      |\n",
      "|    ep_rew_mean     | 6.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4030     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21781    |\n",
      "|    total_timesteps | 2074274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -568     |\n",
      "|    critic_loss     | 6.66     |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -4.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2074173  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 947      |\n",
      "|    ep_rew_mean     | 6.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4040     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21888    |\n",
      "|    total_timesteps | 2083833  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -566     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | 2.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2083732  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 947      |\n",
      "|    ep_rew_mean     | 6.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4050     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 21985    |\n",
      "|    total_timesteps | 2093833  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -565     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0145   |\n",
      "|    ent_coef_loss   | 6.91     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2093732  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 955      |\n",
      "|    ep_rew_mean     | 6.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4060     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22076    |\n",
      "|    total_timesteps | 2103148  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -567     |\n",
      "|    critic_loss     | 13.3     |\n",
      "|    ent_coef        | 0.0144   |\n",
      "|    ent_coef_loss   | -2.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2103047  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 969      |\n",
      "|    ep_rew_mean     | 6.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4070     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22175    |\n",
      "|    total_timesteps | 2113148  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -570     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.0148   |\n",
      "|    ent_coef_loss   | -7.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2113047  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 989      |\n",
      "|    ep_rew_mean     | 6.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4080     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22275    |\n",
      "|    total_timesteps | 2123148  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -569     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2123047  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 982      |\n",
      "|    ep_rew_mean     | 6.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4090     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22363    |\n",
      "|    total_timesteps | 2132466  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -573     |\n",
      "|    critic_loss     | 9.3      |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | 0.752    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2132365  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 982      |\n",
      "|    ep_rew_mean     | 6.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4100     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22457    |\n",
      "|    total_timesteps | 2142466  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -572     |\n",
      "|    critic_loss     | 8.76     |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 0.102    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2142365  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 6.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4110     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22543    |\n",
      "|    total_timesteps | 2151602  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -573     |\n",
      "|    critic_loss     | 7.73     |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 5.72     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2151501  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 6.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4120     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22637    |\n",
      "|    total_timesteps | 2161602  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -573     |\n",
      "|    critic_loss     | 7.21     |\n",
      "|    ent_coef        | 0.0156   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2161501  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 6.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4130     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22730    |\n",
      "|    total_timesteps | 2171602  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -579     |\n",
      "|    critic_loss     | 7.87     |\n",
      "|    ent_coef        | 0.0153   |\n",
      "|    ent_coef_loss   | 0.116    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2171501  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 6.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4140     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22818    |\n",
      "|    total_timesteps | 2181135  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -575     |\n",
      "|    critic_loss     | 6.41     |\n",
      "|    ent_coef        | 0.0162   |\n",
      "|    ent_coef_loss   | 4        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2181034  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 972      |\n",
      "|    ep_rew_mean     | 6.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4150     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 22909    |\n",
      "|    total_timesteps | 2191017  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -578     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.015    |\n",
      "|    ent_coef_loss   | -4.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2190916  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 979      |\n",
      "|    ep_rew_mean     | 6.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4160     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 23001    |\n",
      "|    total_timesteps | 2201017  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -579     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | -3.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2200916  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 979      |\n",
      "|    ep_rew_mean     | 6.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4170     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 23095    |\n",
      "|    total_timesteps | 2211017  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -583     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0154   |\n",
      "|    ent_coef_loss   | 2.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2210916  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 975      |\n",
      "|    ep_rew_mean     | 6.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4180     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 23183    |\n",
      "|    total_timesteps | 2220671  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -579     |\n",
      "|    critic_loss     | 11.5     |\n",
      "|    ent_coef        | 0.0163   |\n",
      "|    ent_coef_loss   | -4.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2220570  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 982      |\n",
      "|    ep_rew_mean     | 6.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4190     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 23275    |\n",
      "|    total_timesteps | 2230671  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -573     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0161   |\n",
      "|    ent_coef_loss   | 1.66     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2230570  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 982      |\n",
      "|    ep_rew_mean     | 6.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4200     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 23366    |\n",
      "|    total_timesteps | 2240671  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -578     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    ent_coef        | 0.016    |\n",
      "|    ent_coef_loss   | 11.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2240570  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 980      |\n",
      "|    ep_rew_mean     | 6.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4210     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 23449    |\n",
      "|    total_timesteps | 2249652  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -580     |\n",
      "|    critic_loss     | 26.1     |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | 6.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2249551  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 978      |\n",
      "|    ep_rew_mean     | 6.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4220     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 23539    |\n",
      "|    total_timesteps | 2259354  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -580     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0165   |\n",
      "|    ent_coef_loss   | 3.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2259253  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 971      |\n",
      "|    ep_rew_mean     | 6.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4230     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 23631    |\n",
      "|    total_timesteps | 2268740  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -582     |\n",
      "|    critic_loss     | 22.7     |\n",
      "|    ent_coef        | 0.0173   |\n",
      "|    ent_coef_loss   | 4.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2268639  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 976      |\n",
      "|    ep_rew_mean     | 6.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4240     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 23725    |\n",
      "|    total_timesteps | 2278740  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -582     |\n",
      "|    critic_loss     | 7.72     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | -2.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2278639  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 975      |\n",
      "|    ep_rew_mean     | 6.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4250     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 23817    |\n",
      "|    total_timesteps | 2288500  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -588     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0172   |\n",
      "|    ent_coef_loss   | 3.35     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2288399  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 975      |\n",
      "|    ep_rew_mean     | 6.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4260     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 23911    |\n",
      "|    total_timesteps | 2298500  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -585     |\n",
      "|    critic_loss     | 17.6     |\n",
      "|    ent_coef        | 0.017    |\n",
      "|    ent_coef_loss   | 5.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2298399  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 970      |\n",
      "|    ep_rew_mean     | 6.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4270     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24000    |\n",
      "|    total_timesteps | 2308011  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -586     |\n",
      "|    critic_loss     | 16.9     |\n",
      "|    ent_coef        | 0.0166   |\n",
      "|    ent_coef_loss   | 2.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2307910  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 6.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4280     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24094    |\n",
      "|    total_timesteps | 2318011  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -590     |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2317910  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 6.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4290     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24190    |\n",
      "|    total_timesteps | 2328011  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -590     |\n",
      "|    critic_loss     | 9.39     |\n",
      "|    ent_coef        | 0.0183   |\n",
      "|    ent_coef_loss   | 3.02     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2327910  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 6.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4300     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24278    |\n",
      "|    total_timesteps | 2337341  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -591     |\n",
      "|    critic_loss     | 9.63     |\n",
      "|    ent_coef        | 0.0176   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2337240  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 976      |\n",
      "|    ep_rew_mean     | 6.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4310     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24373    |\n",
      "|    total_timesteps | 2347245  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -594     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0172   |\n",
      "|    ent_coef_loss   | -3.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2347144  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 973      |\n",
      "|    ep_rew_mean     | 6.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4320     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24462    |\n",
      "|    total_timesteps | 2356627  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -592     |\n",
      "|    critic_loss     | 9.51     |\n",
      "|    ent_coef        | 0.0171   |\n",
      "|    ent_coef_loss   | -6.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2356526  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 979      |\n",
      "|    ep_rew_mean     | 6.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4330     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24556    |\n",
      "|    total_timesteps | 2366627  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -594     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | 3.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2366526  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 979      |\n",
      "|    ep_rew_mean     | 6.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4340     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24651    |\n",
      "|    total_timesteps | 2376627  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -596     |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    ent_coef        | 0.0179   |\n",
      "|    ent_coef_loss   | -0.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2376526  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 6.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4350     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24732    |\n",
      "|    total_timesteps | 2385169  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -595     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | -0.561   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2385068  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 960      |\n",
      "|    ep_rew_mean     | 6.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4360     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24821    |\n",
      "|    total_timesteps | 2394529  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -594     |\n",
      "|    critic_loss     | 5.17     |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | 0.0842   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2394428  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 958      |\n",
      "|    ep_rew_mean     | 6.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4370     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 24914    |\n",
      "|    total_timesteps | 2403851  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -596     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.018    |\n",
      "|    ent_coef_loss   | 0.278    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2403750  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 951      |\n",
      "|    ep_rew_mean     | 6.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4380     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25001    |\n",
      "|    total_timesteps | 2413091  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -600     |\n",
      "|    critic_loss     | 20.2     |\n",
      "|    ent_coef        | 0.0171   |\n",
      "|    ent_coef_loss   | -1.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2412990  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 951      |\n",
      "|    ep_rew_mean     | 6.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4390     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25094    |\n",
      "|    total_timesteps | 2423091  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -599     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    ent_coef        | 0.0169   |\n",
      "|    ent_coef_loss   | 2.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2422990  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 955      |\n",
      "|    ep_rew_mean     | 6.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4400     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25192    |\n",
      "|    total_timesteps | 2432808  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -599     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0178   |\n",
      "|    ent_coef_loss   | 7.83     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2432707  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 949      |\n",
      "|    ep_rew_mean     | 6.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4410     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25292    |\n",
      "|    total_timesteps | 2442105  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -600     |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | 4.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2442004  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 931      |\n",
      "|    ep_rew_mean     | 6.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4420     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25370    |\n",
      "|    total_timesteps | 2449774  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -603     |\n",
      "|    critic_loss     | 53.6     |\n",
      "|    ent_coef        | 0.0172   |\n",
      "|    ent_coef_loss   | -3.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2449673  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 6.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4430     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25462    |\n",
      "|    total_timesteps | 2458910  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -603     |\n",
      "|    critic_loss     | 26.1     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | 5.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2458809  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 914      |\n",
      "|    ep_rew_mean     | 6.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4440     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25550    |\n",
      "|    total_timesteps | 2468055  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -601     |\n",
      "|    critic_loss     | 7.64     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | -2.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2467954  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 929      |\n",
      "|    ep_rew_mean     | 6.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4450     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25647    |\n",
      "|    total_timesteps | 2478055  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -593     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | -3.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2477954  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 919      |\n",
      "|    ep_rew_mean     | 6.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4460     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25728    |\n",
      "|    total_timesteps | 2486447  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -591     |\n",
      "|    critic_loss     | 47.7     |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | 6.86     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2486346  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 922      |\n",
      "|    ep_rew_mean     | 6.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4470     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25825    |\n",
      "|    total_timesteps | 2496077  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -591     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 8.47     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2495976  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 921      |\n",
      "|    ep_rew_mean     | 6.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4480     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 25914    |\n",
      "|    total_timesteps | 2505185  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -592     |\n",
      "|    critic_loss     | 23.6     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | -2.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2505084  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 917      |\n",
      "|    ep_rew_mean     | 6.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4490     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26007    |\n",
      "|    total_timesteps | 2514759  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -599     |\n",
      "|    critic_loss     | 7.28     |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | -2.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2514658  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 902      |\n",
      "|    ep_rew_mean     | 5.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4500     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26087    |\n",
      "|    total_timesteps | 2522997  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -593     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | 4.85     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2522896  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 909      |\n",
      "|    ep_rew_mean     | 6e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4510     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26185    |\n",
      "|    total_timesteps | 2532997  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -596     |\n",
      "|    critic_loss     | 20.5     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | -5.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2532896  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 932      |\n",
      "|    ep_rew_mean     | 6.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4520     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26285    |\n",
      "|    total_timesteps | 2542997  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -595     |\n",
      "|    critic_loss     | 53.4     |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | 5.82     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2542896  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 936      |\n",
      "|    ep_rew_mean     | 6.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4530     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26376    |\n",
      "|    total_timesteps | 2552484  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -597     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | -10.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2552383  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 944      |\n",
      "|    ep_rew_mean     | 6.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4540     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26472    |\n",
      "|    total_timesteps | 2562484  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -597     |\n",
      "|    critic_loss     | 21.1     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 0.771    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2562383  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 944      |\n",
      "|    ep_rew_mean     | 6.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4550     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26568    |\n",
      "|    total_timesteps | 2572484  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -591     |\n",
      "|    critic_loss     | 84.5     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 3.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2572383  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 952      |\n",
      "|    ep_rew_mean     | 6.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4560     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26655    |\n",
      "|    total_timesteps | 2581625  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -600     |\n",
      "|    critic_loss     | 16       |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | 5.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2581524  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 955      |\n",
      "|    ep_rew_mean     | 6.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4570     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26752    |\n",
      "|    total_timesteps | 2591625  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -603     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 1.88     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2591524  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 955      |\n",
      "|    ep_rew_mean     | 6.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4580     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26841    |\n",
      "|    total_timesteps | 2600720  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -602     |\n",
      "|    critic_loss     | 9.05     |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -3.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2600619  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 948      |\n",
      "|    ep_rew_mean     | 6.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4590     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 26930    |\n",
      "|    total_timesteps | 2609577  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -601     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | -2.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609476  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 960      |\n",
      "|    ep_rew_mean     | 6.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4600     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27025    |\n",
      "|    total_timesteps | 2619024  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -604     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    ent_coef        | 0.0218   |\n",
      "|    ent_coef_loss   | 3.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2618923  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 960      |\n",
      "|    ep_rew_mean     | 6.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4610     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27123    |\n",
      "|    total_timesteps | 2629024  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -607     |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2628923  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 945      |\n",
      "|    ep_rew_mean     | 6.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4620     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27210    |\n",
      "|    total_timesteps | 2637464  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -608     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | -0.626   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2637363  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 6.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4630     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27306    |\n",
      "|    total_timesteps | 2646693  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -605     |\n",
      "|    critic_loss     | 76.1     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2646592  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 6.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4640     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27407    |\n",
      "|    total_timesteps | 2656693  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -610     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | 5.58     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2656592  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 942      |\n",
      "|    ep_rew_mean     | 6.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4650     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27510    |\n",
      "|    total_timesteps | 2666693  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -610     |\n",
      "|    critic_loss     | 11.6     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 5.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2666592  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 948      |\n",
      "|    ep_rew_mean     | 6.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4660     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27608    |\n",
      "|    total_timesteps | 2676413  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -614     |\n",
      "|    critic_loss     | 4.26     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | -2.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2676312  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 939      |\n",
      "|    ep_rew_mean     | 6.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4670     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27699    |\n",
      "|    total_timesteps | 2685534  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -611     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | -3.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2685433  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 948      |\n",
      "|    ep_rew_mean     | 6.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4680     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27796    |\n",
      "|    total_timesteps | 2695534  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -612     |\n",
      "|    critic_loss     | 16       |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 0.0238   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2695433  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 945      |\n",
      "|    ep_rew_mean     | 6.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4690     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 27879    |\n",
      "|    total_timesteps | 2704038  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -611     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -3.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2703937  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 6.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4700     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 27978    |\n",
      "|    total_timesteps | 2714038  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -612     |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -3.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2713937  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 941      |\n",
      "|    ep_rew_mean     | 6.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4710     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 28067    |\n",
      "|    total_timesteps | 2723124  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -613     |\n",
      "|    critic_loss     | 27.8     |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2723023  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 957      |\n",
      "|    ep_rew_mean     | 6.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4720     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 28166    |\n",
      "|    total_timesteps | 2733124  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -610     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 5.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2733023  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 964      |\n",
      "|    ep_rew_mean     | 6.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4730     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 28267    |\n",
      "|    total_timesteps | 2743124  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -615     |\n",
      "|    critic_loss     | 9.85     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -2.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2743023  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 963      |\n",
      "|    ep_rew_mean     | 6.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4740     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 28367    |\n",
      "|    total_timesteps | 2753039  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -611     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -0.448   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2752938  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 927      |\n",
      "|    ep_rew_mean     | 6.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4750     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 28434    |\n",
      "|    total_timesteps | 2759411  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -616     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | -6.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2759310  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 903      |\n",
      "|    ep_rew_mean     | 6.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4760     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 28511    |\n",
      "|    total_timesteps | 2766733  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -619     |\n",
      "|    critic_loss     | 35.5     |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | -2.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2766632  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 6.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4770     |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 28610    |\n",
      "|    total_timesteps | 2776733  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -615     |\n",
      "|    critic_loss     | 19.4     |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | -4.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2776632  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 912      |\n",
      "|    ep_rew_mean     | 6.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4780     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 28765    |\n",
      "|    total_timesteps | 2786733  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -614     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -3.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2786632  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 927      |\n",
      "|    ep_rew_mean     | 6.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4790     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 28920    |\n",
      "|    total_timesteps | 2796733  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -613     |\n",
      "|    critic_loss     | 33       |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 8.73     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2796632  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 927      |\n",
      "|    ep_rew_mean     | 6.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4800     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 29053    |\n",
      "|    total_timesteps | 2806707  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -622     |\n",
      "|    critic_loss     | 8.13     |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | -4.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2806606  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 928      |\n",
      "|    ep_rew_mean     | 6.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4810     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 29176    |\n",
      "|    total_timesteps | 2815969  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -615     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | 2.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2815868  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 920      |\n",
      "|    ep_rew_mean     | 6.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4820     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 29300    |\n",
      "|    total_timesteps | 2825074  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -618     |\n",
      "|    critic_loss     | 8        |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | -4.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2824973  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 918      |\n",
      "|    ep_rew_mean     | 6.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4830     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 29430    |\n",
      "|    total_timesteps | 2834876  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -621     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.111    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2834775  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 911      |\n",
      "|    ep_rew_mean     | 6.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4840     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 29557    |\n",
      "|    total_timesteps | 2844136  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -614     |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2844035  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 943      |\n",
      "|    ep_rew_mean     | 6.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4850     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 29685    |\n",
      "|    total_timesteps | 2853674  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -618     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | -3.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2853573  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 6.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4860     |\n",
      "|    fps             | 96       |\n",
      "|    time_elapsed    | 29817    |\n",
      "|    total_timesteps | 2863440  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -623     |\n",
      "|    critic_loss     | 26.4     |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -8.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2863339  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 6.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4870     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 29951    |\n",
      "|    total_timesteps | 2873440  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -621     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -2.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2873339  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 6.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4880     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 30089    |\n",
      "|    total_timesteps | 2883440  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -621     |\n",
      "|    critic_loss     | 9.72     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -10.7    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2883339  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 6.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4890     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 30235    |\n",
      "|    total_timesteps | 2893440  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -626     |\n",
      "|    critic_loss     | 9.02     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -2.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2893339  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 959      |\n",
      "|    ep_rew_mean     | 6.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4900     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 30356    |\n",
      "|    total_timesteps | 2902581  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -628     |\n",
      "|    critic_loss     | 12.2     |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 1.73     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2902480  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 966      |\n",
      "|    ep_rew_mean     | 6.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4910     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 30488    |\n",
      "|    total_timesteps | 2912581  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -627     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -2.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2912480  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 975      |\n",
      "|    ep_rew_mean     | 6.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4920     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 30622    |\n",
      "|    total_timesteps | 2922581  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -631     |\n",
      "|    critic_loss     | 7.92     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -4.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2922480  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 967      |\n",
      "|    ep_rew_mean     | 6.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4930     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 30742    |\n",
      "|    total_timesteps | 2931541  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -621     |\n",
      "|    critic_loss     | 11       |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 1.79     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2931440  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 970      |\n",
      "|    ep_rew_mean     | 6.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4940     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 30875    |\n",
      "|    total_timesteps | 2941123  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -628     |\n",
      "|    critic_loss     | 15.8     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -4.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2941022  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 957      |\n",
      "|    ep_rew_mean     | 6.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4950     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 30994    |\n",
      "|    total_timesteps | 2949355  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -620     |\n",
      "|    critic_loss     | 23.6     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 5.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2949254  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 6.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4960     |\n",
      "|    fps             | 95       |\n",
      "|    time_elapsed    | 31124    |\n",
      "|    total_timesteps | 2958426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -628     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | -2.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2958325  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 6.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4970     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 31266    |\n",
      "|    total_timesteps | 2968426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -623     |\n",
      "|    critic_loss     | 38.6     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | 5.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2968325  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 950      |\n",
      "|    ep_rew_mean     | 6.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4980     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 31425    |\n",
      "|    total_timesteps | 2978426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -627     |\n",
      "|    critic_loss     | 25.9     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 7.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2978325  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 941      |\n",
      "|    ep_rew_mean     | 6.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4990     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 31545    |\n",
      "|    total_timesteps | 2987563  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -630     |\n",
      "|    critic_loss     | 8.4      |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | -5.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2987462  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 940      |\n",
      "|    ep_rew_mean     | 6.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5000     |\n",
      "|    fps             | 94       |\n",
      "|    time_elapsed    | 31664    |\n",
      "|    total_timesteps | 2996592  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -629     |\n",
      "|    critic_loss     | 26.5     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | -0.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2996491  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/miniforge3/envs/irl/lib/python3.10/site-packages/stable_baselines3/common/save_util.py:276: UserWarning: Path 'logs/expert/Humanoid-v4-sac' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "# Humanoid-v4\n",
    "env_id = 'Humanoid-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "sac_model = SAC(\"MlpPolicy\", env, verbose=10)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=100000, save_path=f'../logs/{env_id}')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'./logs/{env_id}/best_model',\n",
    "                             log_path=f'./logs/{env_id}/results', eval_freq=10000)\n",
    "callback = CallbackList([checkpoint_callback, ])\n",
    "sac_model.learn(total_timesteps=3e6, log_interval=10, callback=callback)\n",
    "sac_model.save(f\"logs/expert/{env_id}-sac/model5e6\")\n",
    "sac_model.save_replay_buffer(f\"logs/expert/{env_id}-sac/buffer5e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('irl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d2e891c9d48cbc5657a17ab4ab08b2c1d2ec0060cc3e51694592beb2a6aa825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
