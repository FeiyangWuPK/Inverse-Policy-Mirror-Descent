{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/miniforge3/envs/irl/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py:4: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if not hasattr(tensorboard, \"__version__\") or LooseVersion(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Hopper-v4\n",
    "env_id = 'Hopper-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "sac_model = SAC(\"MlpPolicy\", env, verbose=10)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1e5, save_path=f'../logs/{env_id}')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'./logs/{env_id}/best_model',\n",
    "                             log_path=f'./logs/{env_id}/results', eval_freq=10000)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19       |\n",
      "|    ep_rew_mean     | 14.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 322      |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 190      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.05    |\n",
      "|    critic_loss     | 0.486    |\n",
      "|    ent_coef        | 0.974    |\n",
      "|    ent_coef_loss   | -0.133   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89       |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19.6     |\n",
      "|    ep_rew_mean     | 19.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 231      |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 391      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7       |\n",
      "|    critic_loss     | 0.845    |\n",
      "|    ent_coef        | 0.917    |\n",
      "|    ent_coef_loss   | -0.436   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 290      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | 19.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 214      |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 643      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.1     |\n",
      "|    critic_loss     | 0.543    |\n",
      "|    ent_coef        | 0.851    |\n",
      "|    ent_coef_loss   | -0.788   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 542      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 29.7     |\n",
      "|    ep_rew_mean     | 33.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 1188     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.4    |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.726    |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1087     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 38.5     |\n",
      "|    ep_rew_mean     | 56.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 1923     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -25.3    |\n",
      "|    critic_loss     | 7.45     |\n",
      "|    ent_coef        | 0.59     |\n",
      "|    ent_coef_loss   | -2.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1822     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 43.6     |\n",
      "|    ep_rew_mean     | 70.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 2618     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -31.5    |\n",
      "|    critic_loss     | 9.5      |\n",
      "|    ent_coef        | 0.486    |\n",
      "|    ent_coef_loss   | -2.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2517     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 52.7     |\n",
      "|    ep_rew_mean     | 94.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 70       |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 3688     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -43.3    |\n",
      "|    critic_loss     | 20.1     |\n",
      "|    ent_coef        | 0.364    |\n",
      "|    ent_coef_loss   | -3.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3587     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 59.4     |\n",
      "|    ep_rew_mean     | 113      |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 4750     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.8    |\n",
      "|    critic_loss     | 6.48     |\n",
      "|    ent_coef        | 0.274    |\n",
      "|    ent_coef_loss   | -3.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4649     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 63.8     |\n",
      "|    ep_rew_mean     | 126      |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 35       |\n",
      "|    total_timesteps | 5744     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.9    |\n",
      "|    critic_loss     | 7.02     |\n",
      "|    ent_coef        | 0.211    |\n",
      "|    ent_coef_loss   | -4.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5643     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 66.5     |\n",
      "|    ep_rew_mean     | 134      |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 6652     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69.7    |\n",
      "|    critic_loss     | 6.11     |\n",
      "|    ent_coef        | 0.169    |\n",
      "|    ent_coef_loss   | -3.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6551     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.4     |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    episodes        | 110      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 7632     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -76.4    |\n",
      "|    critic_loss     | 3.64     |\n",
      "|    ent_coef        | 0.135    |\n",
      "|    ent_coef_loss   | -3.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7531     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.3     |\n",
      "|    ep_rew_mean     | 180      |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 8724     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84.7    |\n",
      "|    critic_loss     | 9.55     |\n",
      "|    ent_coef        | 0.108    |\n",
      "|    ent_coef_loss   | -2.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8623     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92       |\n",
      "|    ep_rew_mean     | 205      |\n",
      "| time/              |          |\n",
      "|    episodes        | 130      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 9840     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -90      |\n",
      "|    critic_loss     | 8.22     |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -1.91    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9739     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=309.17 +/- 6.04\n",
      "Episode length: 126.20 +/- 2.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 126      |\n",
      "|    mean_reward     | 309      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -90.8    |\n",
      "|    critic_loss     | 6.63     |\n",
      "|    ent_coef        | 0.0851   |\n",
      "|    ent_coef_loss   | -1.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98       |\n",
      "|    ep_rew_mean     | 224      |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 10988    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -96.8    |\n",
      "|    critic_loss     | 25.6     |\n",
      "|    ent_coef        | 0.076    |\n",
      "|    ent_coef_loss   | -0.222   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10887    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 101      |\n",
      "|    ep_rew_mean     | 231      |\n",
      "| time/              |          |\n",
      "|    episodes        | 150      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 70       |\n",
      "|    total_timesteps | 12002    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -103     |\n",
      "|    critic_loss     | 6.45     |\n",
      "|    ent_coef        | 0.0758   |\n",
      "|    ent_coef_loss   | -0.539   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 11901    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | 243      |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 13201    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -106     |\n",
      "|    critic_loss     | 20.6     |\n",
      "|    ent_coef        | 0.0791   |\n",
      "|    ent_coef_loss   | -0.438   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 13100    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 107      |\n",
      "|    ep_rew_mean     | 247      |\n",
      "| time/              |          |\n",
      "|    episodes        | 170      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 14342    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -113     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.0774   |\n",
      "|    ent_coef_loss   | 0.0511   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14241    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 251      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 15523    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -118     |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    ent_coef        | 0.0769   |\n",
      "|    ent_coef_loss   | 0.0429   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 15422    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 109      |\n",
      "|    ep_rew_mean     | 254      |\n",
      "| time/              |          |\n",
      "|    episodes        | 190      |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 95       |\n",
      "|    total_timesteps | 16609    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -121     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0788   |\n",
      "|    ent_coef_loss   | 0.213    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 16508    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 111      |\n",
      "|    ep_rew_mean     | 260      |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 102      |\n",
      "|    total_timesteps | 17703    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -119     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.0844   |\n",
      "|    ent_coef_loss   | -0.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 17602    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 112      |\n",
      "|    ep_rew_mean     | 266      |\n",
      "| time/              |          |\n",
      "|    episodes        | 210      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 109      |\n",
      "|    total_timesteps | 18848    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -125     |\n",
      "|    critic_loss     | 6.47     |\n",
      "|    ent_coef        | 0.089    |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18747    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=311.81 +/- 6.90\n",
      "Episode length: 131.40 +/- 3.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 131      |\n",
      "|    mean_reward     | 312      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -124     |\n",
      "|    critic_loss     | 11       |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | 1.49     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 114      |\n",
      "|    ep_rew_mean     | 272      |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 20116    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -127     |\n",
      "|    critic_loss     | 9.5      |\n",
      "|    ent_coef        | 0.09     |\n",
      "|    ent_coef_loss   | -0.415   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 20015    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 116      |\n",
      "|    ep_rew_mean     | 278      |\n",
      "| time/              |          |\n",
      "|    episodes        | 230      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 125      |\n",
      "|    total_timesteps | 21449    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -126     |\n",
      "|    critic_loss     | 7.2      |\n",
      "|    ent_coef        | 0.0923   |\n",
      "|    ent_coef_loss   | -0.151   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 21348    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 118      |\n",
      "|    ep_rew_mean     | 283      |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 133      |\n",
      "|    total_timesteps | 22738    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -126     |\n",
      "|    critic_loss     | 9.91     |\n",
      "|    ent_coef        | 0.0863   |\n",
      "|    ent_coef_loss   | 0.746    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 22637    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | 293      |\n",
      "| time/              |          |\n",
      "|    episodes        | 250      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 141      |\n",
      "|    total_timesteps | 24036    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -131     |\n",
      "|    critic_loss     | 21.2     |\n",
      "|    ent_coef        | 0.0833   |\n",
      "|    ent_coef_loss   | -0.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23935    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 120      |\n",
      "|    ep_rew_mean     | 295      |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 148      |\n",
      "|    total_timesteps | 25197    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -138     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | 1.35     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 25096    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 121      |\n",
      "|    ep_rew_mean     | 300      |\n",
      "| time/              |          |\n",
      "|    episodes        | 270      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 155      |\n",
      "|    total_timesteps | 26491    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 7.81     |\n",
      "|    ent_coef        | 0.0908   |\n",
      "|    ent_coef_loss   | 0.0973   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26390    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 122      |\n",
      "|    ep_rew_mean     | 302      |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 162      |\n",
      "|    total_timesteps | 27757    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 7.58     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | 0.472    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 27656    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | 307      |\n",
      "| time/              |          |\n",
      "|    episodes        | 290      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 28995    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 7.36     |\n",
      "|    ent_coef        | 0.0926   |\n",
      "|    ent_coef_loss   | 0.619    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 28894    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=321.30 +/- 2.11\n",
      "Episode length: 132.20 +/- 0.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 132      |\n",
      "|    mean_reward     | 321      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -138     |\n",
      "|    critic_loss     | 9.49     |\n",
      "|    ent_coef        | 0.0888   |\n",
      "|    ent_coef_loss   | -0.145   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | 312      |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 178      |\n",
      "|    total_timesteps | 30253    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -139     |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.0869   |\n",
      "|    ent_coef_loss   | 0.589    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 30152    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | 317      |\n",
      "| time/              |          |\n",
      "|    episodes        | 310      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 186      |\n",
      "|    total_timesteps | 31610    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 5.56     |\n",
      "|    ent_coef        | 0.081    |\n",
      "|    ent_coef_loss   | -0.163   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 31509    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | 314      |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 193      |\n",
      "|    total_timesteps | 32820    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -131     |\n",
      "|    critic_loss     | 8.12     |\n",
      "|    ent_coef        | 0.0764   |\n",
      "|    ent_coef_loss   | -0.988   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 32719    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | 305      |\n",
      "| time/              |          |\n",
      "|    episodes        | 330      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 199      |\n",
      "|    total_timesteps | 33831    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -136     |\n",
      "|    critic_loss     | 7.03     |\n",
      "|    ent_coef        | 0.0778   |\n",
      "|    ent_coef_loss   | -0.269   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33730    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | 313      |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 207      |\n",
      "|    total_timesteps | 35306    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -134     |\n",
      "|    critic_loss     | 4.86     |\n",
      "|    ent_coef        | 0.0792   |\n",
      "|    ent_coef_loss   | 0.109    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 35205    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 129      |\n",
      "|    ep_rew_mean     | 329      |\n",
      "| time/              |          |\n",
      "|    episodes        | 350      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 217      |\n",
      "|    total_timesteps | 36953    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -132     |\n",
      "|    critic_loss     | 6.53     |\n",
      "|    ent_coef        | 0.0807   |\n",
      "|    ent_coef_loss   | 0.554    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36852    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 132      |\n",
      "|    ep_rew_mean     | 340      |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 225      |\n",
      "|    total_timesteps | 38392    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -140     |\n",
      "|    critic_loss     | 55.8     |\n",
      "|    ent_coef        | 0.0816   |\n",
      "|    ent_coef_loss   | -0.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 38291    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=552.25 +/- 119.81\n",
      "Episode length: 187.40 +/- 28.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 187      |\n",
      "|    mean_reward     | 552      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -136     |\n",
      "|    critic_loss     | 60.2     |\n",
      "|    ent_coef        | 0.0851   |\n",
      "|    ent_coef_loss   | 0.197    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 135      |\n",
      "|    ep_rew_mean     | 353      |\n",
      "| time/              |          |\n",
      "|    episodes        | 370      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 234      |\n",
      "|    total_timesteps | 40032    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -136     |\n",
      "|    critic_loss     | 4.77     |\n",
      "|    ent_coef        | 0.0852   |\n",
      "|    ent_coef_loss   | -0.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39931    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 136      |\n",
      "|    ep_rew_mean     | 352      |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 242      |\n",
      "|    total_timesteps | 41308    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -139     |\n",
      "|    critic_loss     | 5.14     |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | -0.623   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 41207    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 132      |\n",
      "|    ep_rew_mean     | 335      |\n",
      "| time/              |          |\n",
      "|    episodes        | 390      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 247      |\n",
      "|    total_timesteps | 42163    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -139     |\n",
      "|    critic_loss     | 6.84     |\n",
      "|    ent_coef        | 0.0876   |\n",
      "|    ent_coef_loss   | 0.601    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42062    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | 314      |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 252      |\n",
      "|    total_timesteps | 42955    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -138     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0813   |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 42854    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | 325      |\n",
      "| time/              |          |\n",
      "|    episodes        | 410      |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 262      |\n",
      "|    total_timesteps | 44597    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -136     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | 0.206    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 44496    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 134      |\n",
      "|    ep_rew_mean     | 341      |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 271      |\n",
      "|    total_timesteps | 46230    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -133     |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    ent_coef        | 0.0852   |\n",
      "|    ent_coef_loss   | -0.634   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46129    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 143      |\n",
      "|    ep_rew_mean     | 374      |\n",
      "| time/              |          |\n",
      "|    episodes        | 430      |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 280      |\n",
      "|    total_timesteps | 48140    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -140     |\n",
      "|    critic_loss     | 6.88     |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -0.568   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 48039    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=772.28 +/- 33.33\n",
      "Episode length: 252.20 +/- 5.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 252      |\n",
      "|    mean_reward     | 772      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 7.32     |\n",
      "|    ent_coef        | 0.084    |\n",
      "|    ent_coef_loss   | 0.00803  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 151      |\n",
      "|    ep_rew_mean     | 401      |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 292      |\n",
      "|    total_timesteps | 50403    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 7.03     |\n",
      "|    ent_coef        | 0.0846   |\n",
      "|    ent_coef_loss   | -0.532   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 50302    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 158      |\n",
      "|    ep_rew_mean     | 419      |\n",
      "| time/              |          |\n",
      "|    episodes        | 450      |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 305      |\n",
      "|    total_timesteps | 52781    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -150     |\n",
      "|    critic_loss     | 8.39     |\n",
      "|    ent_coef        | 0.0821   |\n",
      "|    ent_coef_loss   | -0.342   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 52680    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 162      |\n",
      "|    ep_rew_mean     | 425      |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 314      |\n",
      "|    total_timesteps | 54612    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -150     |\n",
      "|    critic_loss     | 9.54     |\n",
      "|    ent_coef        | 0.088    |\n",
      "|    ent_coef_loss   | 0.155    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 54511    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 173      |\n",
      "|    ep_rew_mean     | 461      |\n",
      "| time/              |          |\n",
      "|    episodes        | 470      |\n",
      "|    fps             | 174      |\n",
      "|    time_elapsed    | 328      |\n",
      "|    total_timesteps | 57327    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -156     |\n",
      "|    critic_loss     | 95       |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | 0.444    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 57226    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 182      |\n",
      "|    ep_rew_mean     | 498      |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 339      |\n",
      "|    total_timesteps | 59537    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -151     |\n",
      "|    critic_loss     | 9.99     |\n",
      "|    ent_coef        | 0.0846   |\n",
      "|    ent_coef_loss   | -0.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59436    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=740.56 +/- 20.45\n",
      "Episode length: 232.00 +/- 11.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 232      |\n",
      "|    mean_reward     | 741      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -152     |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | 0.891    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 199      |\n",
      "|    ep_rew_mean     | 561      |\n",
      "| time/              |          |\n",
      "|    episodes        | 490      |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 352      |\n",
      "|    total_timesteps | 62068    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -160     |\n",
      "|    critic_loss     | 6.76     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | 0.124    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 61967    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 217      |\n",
      "|    ep_rew_mean     | 627      |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 176      |\n",
      "|    time_elapsed    | 365      |\n",
      "|    total_timesteps | 64613    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | -0.0396  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 64512    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 221      |\n",
      "|    ep_rew_mean     | 638      |\n",
      "| time/              |          |\n",
      "|    episodes        | 510      |\n",
      "|    fps             | 177      |\n",
      "|    time_elapsed    | 376      |\n",
      "|    total_timesteps | 66694    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 16.2     |\n",
      "|    ent_coef        | 0.0891   |\n",
      "|    ent_coef_loss   | 0.238    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 66593    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 234      |\n",
      "|    ep_rew_mean     | 679      |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 391      |\n",
      "|    total_timesteps | 69644    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 6.15     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | -0.0412  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69543    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=1025.13 +/- 96.78\n",
      "Episode length: 329.40 +/- 25.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 329      |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -160     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.086    |\n",
      "|    ent_coef_loss   | -0.362   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 254      |\n",
      "|    ep_rew_mean     | 741      |\n",
      "| time/              |          |\n",
      "|    episodes        | 530      |\n",
      "|    fps             | 178      |\n",
      "|    time_elapsed    | 411      |\n",
      "|    total_timesteps | 73519    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -169     |\n",
      "|    critic_loss     | 19       |\n",
      "|    ent_coef        | 0.0914   |\n",
      "|    ent_coef_loss   | 0.495    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 73418    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 266      |\n",
      "|    ep_rew_mean     | 779      |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 179      |\n",
      "|    time_elapsed    | 428      |\n",
      "|    total_timesteps | 76963    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | 0.638    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 76862    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 267      |\n",
      "|    ep_rew_mean     | 794      |\n",
      "| time/              |          |\n",
      "|    episodes        | 550      |\n",
      "|    fps             | 179      |\n",
      "|    time_elapsed    | 441      |\n",
      "|    total_timesteps | 79445    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -184     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0921   |\n",
      "|    ent_coef_loss   | 0.172    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79344    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=1116.29 +/- 102.52\n",
      "Episode length: 348.00 +/- 30.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 348      |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -185     |\n",
      "|    critic_loss     | 30.4     |\n",
      "|    ent_coef        | 0.0921   |\n",
      "|    ent_coef_loss   | 0.629    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 275      |\n",
      "|    ep_rew_mean     | 829      |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 455      |\n",
      "|    total_timesteps | 82100    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 7.29     |\n",
      "|    ent_coef        | 0.0922   |\n",
      "|    ent_coef_loss   | -0.388   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 81999    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 279      |\n",
      "|    ep_rew_mean     | 847      |\n",
      "| time/              |          |\n",
      "|    episodes        | 570      |\n",
      "|    fps             | 180      |\n",
      "|    time_elapsed    | 471      |\n",
      "|    total_timesteps | 85199    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -192     |\n",
      "|    critic_loss     | 32.6     |\n",
      "|    ent_coef        | 0.0891   |\n",
      "|    ent_coef_loss   | -0.129   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 85098    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 293      |\n",
      "|    ep_rew_mean     | 898      |\n",
      "| time/              |          |\n",
      "|    episodes        | 580      |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 489      |\n",
      "|    total_timesteps | 88809    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -187     |\n",
      "|    critic_loss     | 65       |\n",
      "|    ent_coef        | 0.0903   |\n",
      "|    ent_coef_loss   | 0.198    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 88708    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=922.77 +/- 134.26\n",
      "Episode length: 278.20 +/- 44.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 278      |\n",
      "|    mean_reward     | 923      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -193     |\n",
      "|    critic_loss     | 8.25     |\n",
      "|    ent_coef        | 0.0882   |\n",
      "|    ent_coef_loss   | 0.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 297      |\n",
      "|    ep_rew_mean     | 916      |\n",
      "| time/              |          |\n",
      "|    episodes        | 590      |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 505      |\n",
      "|    total_timesteps | 91764    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -195     |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.0912   |\n",
      "|    ent_coef_loss   | 0.0228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 91663    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 309      |\n",
      "|    ep_rew_mean     | 963      |\n",
      "| time/              |          |\n",
      "|    episodes        | 600      |\n",
      "|    fps             | 182      |\n",
      "|    time_elapsed    | 524      |\n",
      "|    total_timesteps | 95493    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 20.4     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 95392    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 325      |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 610      |\n",
      "|    fps             | 182      |\n",
      "|    time_elapsed    | 542      |\n",
      "|    total_timesteps | 99173    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -202     |\n",
      "|    critic_loss     | 7.73     |\n",
      "|    ent_coef        | 0.0885   |\n",
      "|    ent_coef_loss   | 0.162    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99072    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=2098.08 +/- 275.58\n",
      "Episode length: 659.80 +/- 90.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 660      |\n",
      "|    mean_reward     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -202     |\n",
      "|    critic_loss     | 65       |\n",
      "|    ent_coef        | 0.0907   |\n",
      "|    ent_coef_loss   | 0.671    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 336      |\n",
      "|    ep_rew_mean     | 1.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 620      |\n",
      "|    fps             | 182      |\n",
      "|    time_elapsed    | 564      |\n",
      "|    total_timesteps | 103255   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -203     |\n",
      "|    critic_loss     | 6.21     |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | 0.265    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 103154   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 354      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 630      |\n",
      "|    fps             | 183      |\n",
      "|    time_elapsed    | 593      |\n",
      "|    total_timesteps | 108959   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -209     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | -0.152   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 108858   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=864.56 +/- 412.26\n",
      "Episode length: 268.40 +/- 130.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 268      |\n",
      "|    mean_reward     | 865      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -205     |\n",
      "|    critic_loss     | 8.56     |\n",
      "|    ent_coef        | 0.086    |\n",
      "|    ent_coef_loss   | 0.468    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 362      |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 640      |\n",
      "|    fps             | 183      |\n",
      "|    time_elapsed    | 614      |\n",
      "|    total_timesteps | 113126   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -204     |\n",
      "|    critic_loss     | 8.57     |\n",
      "|    ent_coef        | 0.0823   |\n",
      "|    ent_coef_loss   | 0.224    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 113025   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 378      |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 650      |\n",
      "|    fps             | 184      |\n",
      "|    time_elapsed    | 635      |\n",
      "|    total_timesteps | 117199   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -207     |\n",
      "|    critic_loss     | 9.4      |\n",
      "|    ent_coef        | 0.0819   |\n",
      "|    ent_coef_loss   | 0.485    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 117098   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=823.02 +/- 66.85\n",
      "Episode length: 248.80 +/- 21.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 249      |\n",
      "|    mean_reward     | 823      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -214     |\n",
      "|    critic_loss     | 7.14     |\n",
      "|    ent_coef        | 0.0816   |\n",
      "|    ent_coef_loss   | 0.495    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 119899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 390      |\n",
      "|    ep_rew_mean     | 1.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 660      |\n",
      "|    fps             | 184      |\n",
      "|    time_elapsed    | 656      |\n",
      "|    total_timesteps | 121146   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -213     |\n",
      "|    critic_loss     | 8.48     |\n",
      "|    ent_coef        | 0.0816   |\n",
      "|    ent_coef_loss   | -0.661   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 121045   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 396      |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 670      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 674      |\n",
      "|    total_timesteps | 124847   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 4.11     |\n",
      "|    ent_coef        | 0.0807   |\n",
      "|    ent_coef_loss   | 0.509    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 124746   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 398      |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 680      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 694      |\n",
      "|    total_timesteps | 128637   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -213     |\n",
      "|    critic_loss     | 5.88     |\n",
      "|    ent_coef        | 0.0806   |\n",
      "|    ent_coef_loss   | -0.275   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 128536   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=993.68 +/- 121.86\n",
      "Episode length: 306.40 +/- 39.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 306      |\n",
      "|    mean_reward     | 994      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -214     |\n",
      "|    critic_loss     | 5.43     |\n",
      "|    ent_coef        | 0.0798   |\n",
      "|    ent_coef_loss   | -0.348   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 129899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 397      |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 690      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 708      |\n",
      "|    total_timesteps | 131477   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 4.64     |\n",
      "|    ent_coef        | 0.0785   |\n",
      "|    ent_coef_loss   | -0.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 131376   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 390      |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 700      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 724      |\n",
      "|    total_timesteps | 134539   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 4.4      |\n",
      "|    ent_coef        | 0.0778   |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 134438   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 385      |\n",
      "|    ep_rew_mean     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 710      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 740      |\n",
      "|    total_timesteps | 137659   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0764   |\n",
      "|    ent_coef_loss   | -0.141   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 137558   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=1355.57 +/- 417.74\n",
      "Episode length: 416.80 +/- 127.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 417      |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0768   |\n",
      "|    ent_coef_loss   | 0.183    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 139899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 375      |\n",
      "|    ep_rew_mean     | 1.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 720      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 758      |\n",
      "|    total_timesteps | 140765   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -207     |\n",
      "|    critic_loss     | 4.97     |\n",
      "|    ent_coef        | 0.0775   |\n",
      "|    ent_coef_loss   | 0.66     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 140664   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 351      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 730      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 776      |\n",
      "|    total_timesteps | 144101   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -216     |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.0759   |\n",
      "|    ent_coef_loss   | 0.132    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 144000   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 348      |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 740      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 796      |\n",
      "|    total_timesteps | 147945   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -216     |\n",
      "|    critic_loss     | 5.97     |\n",
      "|    ent_coef        | 0.0735   |\n",
      "|    ent_coef_loss   | 0.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 147844   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=811.81 +/- 195.57\n",
      "Episode length: 252.20 +/- 56.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 252      |\n",
      "|    mean_reward     | 812      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0733   |\n",
      "|    ent_coef_loss   | 0.364    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 348      |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 750      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 818      |\n",
      "|    total_timesteps | 151989   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 6.43     |\n",
      "|    ent_coef        | 0.0701   |\n",
      "|    ent_coef_loss   | -0.663   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 151888   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 352      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 760      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 840      |\n",
      "|    total_timesteps | 156320   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 9.4      |\n",
      "|    ent_coef        | 0.0698   |\n",
      "|    ent_coef_loss   | 0.104    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 156219   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 351      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 770      |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 859      |\n",
      "|    total_timesteps | 159915   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -216     |\n",
      "|    critic_loss     | 14.4     |\n",
      "|    ent_coef        | 0.0711   |\n",
      "|    ent_coef_loss   | 0.783    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159814   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=906.84 +/- 543.84\n",
      "Episode length: 287.40 +/- 169.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 287      |\n",
      "|    mean_reward     | 907      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -213     |\n",
      "|    critic_loss     | 6.69     |\n",
      "|    ent_coef        | 0.0716   |\n",
      "|    ent_coef_loss   | 0.0109   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 348      |\n",
      "|    ep_rew_mean     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 780      |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 878      |\n",
      "|    total_timesteps | 163478   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -225     |\n",
      "|    critic_loss     | 7.55     |\n",
      "|    ent_coef        | 0.0704   |\n",
      "|    ent_coef_loss   | 0.311    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 163377   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 361      |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 790      |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 899      |\n",
      "|    total_timesteps | 167530   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 5.01     |\n",
      "|    ent_coef        | 0.0676   |\n",
      "|    ent_coef_loss   | -0.435   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 167429   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=3140.93 +/- 20.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -229     |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.0688   |\n",
      "|    ent_coef_loss   | -0.118   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 369      |\n",
      "|    ep_rew_mean     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 800      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 922      |\n",
      "|    total_timesteps | 171401   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 5.12     |\n",
      "|    ent_coef        | 0.0674   |\n",
      "|    ent_coef_loss   | 0.287    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 171300   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 377      |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 810      |\n",
      "|    fps             | 185      |\n",
      "|    time_elapsed    | 943      |\n",
      "|    total_timesteps | 175402   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 7.7      |\n",
      "|    ent_coef        | 0.067    |\n",
      "|    ent_coef_loss   | -0.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 175301   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=1489.89 +/- 838.45\n",
      "Episode length: 467.80 +/- 271.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 468      |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -230     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.066    |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 411      |\n",
      "|    ep_rew_mean     | 1.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 820      |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 976      |\n",
      "|    total_timesteps | 181818   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 4.91     |\n",
      "|    ent_coef        | 0.0649   |\n",
      "|    ent_coef_loss   | -0.469   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 181717   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 440      |\n",
      "|    ep_rew_mean     | 1.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 830      |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 1008     |\n",
      "|    total_timesteps | 188145   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -231     |\n",
      "|    critic_loss     | 6.03     |\n",
      "|    ent_coef        | 0.0651   |\n",
      "|    ent_coef_loss   | 0.645    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 188044   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=1368.55 +/- 851.03\n",
      "Episode length: 444.60 +/- 277.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 445      |\n",
      "|    mean_reward     | 1.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 3.98     |\n",
      "|    ent_coef        | 0.0643   |\n",
      "|    ent_coef_loss   | 0.358    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 475      |\n",
      "|    ep_rew_mean     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 840      |\n",
      "|    fps             | 186      |\n",
      "|    time_elapsed    | 1045     |\n",
      "|    total_timesteps | 195491   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -235     |\n",
      "|    critic_loss     | 6.46     |\n",
      "|    ent_coef        | 0.0637   |\n",
      "|    ent_coef_loss   | -0.161   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 195390   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=1362.13 +/- 865.95\n",
      "Episode length: 442.00 +/- 279.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 442      |\n",
      "|    mean_reward     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -231     |\n",
      "|    critic_loss     | 9.21     |\n",
      "|    ent_coef        | 0.0656   |\n",
      "|    ent_coef_loss   | 0.732    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 199899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 507      |\n",
      "|    ep_rew_mean     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 850      |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 1081     |\n",
      "|    total_timesteps | 202735   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -232     |\n",
      "|    critic_loss     | 5.61     |\n",
      "|    ent_coef        | 0.0638   |\n",
      "|    ent_coef_loss   | 0.00339  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 202634   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=3008.01 +/- 7.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 4.73     |\n",
      "|    ent_coef        | 0.0617   |\n",
      "|    ent_coef_loss   | -0.192   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 209899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 540      |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 860      |\n",
      "|    fps             | 187      |\n",
      "|    time_elapsed    | 1119     |\n",
      "|    total_timesteps | 210353   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -228     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.062    |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 210252   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 590      |\n",
      "|    ep_rew_mean     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 870      |\n",
      "|    fps             | 188      |\n",
      "|    time_elapsed    | 1161     |\n",
      "|    total_timesteps | 218943   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 3.33     |\n",
      "|    ent_coef        | 0.0598   |\n",
      "|    ent_coef_loss   | 0.569    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 218842   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=3110.11 +/- 6.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -238     |\n",
      "|    critic_loss     | 4.22     |\n",
      "|    ent_coef        | 0.0587   |\n",
      "|    ent_coef_loss   | 0.509    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 636      |\n",
      "|    ep_rew_mean     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 880      |\n",
      "|    fps             | 188      |\n",
      "|    time_elapsed    | 1202     |\n",
      "|    total_timesteps | 227040   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -239     |\n",
      "|    critic_loss     | 7.24     |\n",
      "|    ent_coef        | 0.0591   |\n",
      "|    ent_coef_loss   | -0.267   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 226939   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=3073.27 +/- 6.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -234     |\n",
      "|    critic_loss     | 3.89     |\n",
      "|    ent_coef        | 0.0567   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 229899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 668      |\n",
      "|    ep_rew_mean     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 890      |\n",
      "|    fps             | 188      |\n",
      "|    time_elapsed    | 1240     |\n",
      "|    total_timesteps | 234295   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -239     |\n",
      "|    critic_loss     | 5.04     |\n",
      "|    ent_coef        | 0.056    |\n",
      "|    ent_coef_loss   | 0.448    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 234194   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=3178.70 +/- 10.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -248     |\n",
      "|    critic_loss     | 4.72     |\n",
      "|    ent_coef        | 0.0553   |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 239899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 697      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 900      |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 1274     |\n",
      "|    total_timesteps | 241104   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -242     |\n",
      "|    critic_loss     | 5.33     |\n",
      "|    ent_coef        | 0.0555   |\n",
      "|    ent_coef_loss   | 0.0467   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 241003   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 910      |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 1301     |\n",
      "|    total_timesteps | 246558   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 86.6     |\n",
      "|    ent_coef        | 0.0554   |\n",
      "|    ent_coef_loss   | 0.0376   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 246457   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=3086.95 +/- 170.58\n",
      "Episode length: 968.00 +/- 64.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 968      |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -247     |\n",
      "|    critic_loss     | 3.36     |\n",
      "|    ent_coef        | 0.0524   |\n",
      "|    ent_coef_loss   | -0.057   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 249899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 707      |\n",
      "|    ep_rew_mean     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 920      |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 1331     |\n",
      "|    total_timesteps | 252494   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -247     |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.0539   |\n",
      "|    ent_coef_loss   | 0.579    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 252393   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 686      |\n",
      "|    ep_rew_mean     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 930      |\n",
      "|    fps             | 189      |\n",
      "|    time_elapsed    | 1351     |\n",
      "|    total_timesteps | 256756   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -244     |\n",
      "|    critic_loss     | 2.66     |\n",
      "|    ent_coef        | 0.0515   |\n",
      "|    ent_coef_loss   | -0.356   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 256655   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=3071.62 +/- 191.25\n",
      "Episode length: 962.60 +/- 74.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 963      |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -251     |\n",
      "|    critic_loss     | 4.55     |\n",
      "|    ent_coef        | 0.0518   |\n",
      "|    ent_coef_loss   | 0.327    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 663      |\n",
      "|    ep_rew_mean     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 940      |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 1377     |\n",
      "|    total_timesteps | 261803   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -244     |\n",
      "|    critic_loss     | 9.27     |\n",
      "|    ent_coef        | 0.0516   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 261702   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 643      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 950      |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 1403     |\n",
      "|    total_timesteps | 267081   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 4.98     |\n",
      "|    ent_coef        | 0.0511   |\n",
      "|    ent_coef_loss   | -0.585   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 266980   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=3163.62 +/- 6.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 2.47     |\n",
      "|    ent_coef        | 0.0505   |\n",
      "|    ent_coef_loss   | -0.0266  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 624      |\n",
      "|    ep_rew_mean     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 960      |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 1432     |\n",
      "|    total_timesteps | 272741   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 2.85     |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | -0.013   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 272640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=2894.71 +/- 485.38\n",
      "Episode length: 918.00 +/- 164.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 918      |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -252     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | -0.295   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 279899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 619      |\n",
      "|    ep_rew_mean     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 970      |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 1472     |\n",
      "|    total_timesteps | 280861   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -243     |\n",
      "|    critic_loss     | 8.32     |\n",
      "|    ent_coef        | 0.0507   |\n",
      "|    ent_coef_loss   | 0.0466   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 280760   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 613      |\n",
      "|    ep_rew_mean     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 980      |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 1509     |\n",
      "|    total_timesteps | 288304   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 7.63     |\n",
      "|    ent_coef        | 0.0484   |\n",
      "|    ent_coef_loss   | 0.639    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 288203   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=3188.50 +/- 15.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -249     |\n",
      "|    critic_loss     | 3.3      |\n",
      "|    ent_coef        | 0.0486   |\n",
      "|    ent_coef_loss   | 0.827    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 289899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 601      |\n",
      "|    ep_rew_mean     | 1.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 990      |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 1539     |\n",
      "|    total_timesteps | 294372   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 1.95     |\n",
      "|    ent_coef        | 0.0485   |\n",
      "|    ent_coef_loss   | -0.503   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 294271   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=2997.98 +/- 418.83\n",
      "Episode length: 930.40 +/- 139.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 930      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -259     |\n",
      "|    critic_loss     | 5.38     |\n",
      "|    ent_coef        | 0.0496   |\n",
      "|    ent_coef_loss   | 0.266    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 613      |\n",
      "|    ep_rew_mean     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1000     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 1580     |\n",
      "|    total_timesteps | 302413   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -252     |\n",
      "|    critic_loss     | 6.92     |\n",
      "|    ent_coef        | 0.0476   |\n",
      "|    ent_coef_loss   | -0.605   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 302312   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=2634.43 +/- 396.91\n",
      "Episode length: 804.40 +/- 134.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -248     |\n",
      "|    critic_loss     | 16.9     |\n",
      "|    ent_coef        | 0.048    |\n",
      "|    ent_coef_loss   | -0.0903  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 639      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1010     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 1620     |\n",
      "|    total_timesteps | 310503   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -244     |\n",
      "|    critic_loss     | 5.21     |\n",
      "|    ent_coef        | 0.0483   |\n",
      "|    ent_coef_loss   | -0.0119  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 310402   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 660      |\n",
      "|    ep_rew_mean     | 2.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1020     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 1659     |\n",
      "|    total_timesteps | 318492   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -259     |\n",
      "|    critic_loss     | 2.96     |\n",
      "|    ent_coef        | 0.0467   |\n",
      "|    ent_coef_loss   | -0.246   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 318391   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=3211.58 +/- 20.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -256     |\n",
      "|    critic_loss     | 5.06     |\n",
      "|    ent_coef        | 0.0464   |\n",
      "|    ent_coef_loss   | -0.274   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 703      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1030     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 1702     |\n",
      "|    total_timesteps | 327026   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -253     |\n",
      "|    critic_loss     | 3.38     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | -0.353   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 326925   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=3213.23 +/- 4.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 3.94     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | -0.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 329899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 735      |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1040     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 1743     |\n",
      "|    total_timesteps | 335263   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -252     |\n",
      "|    critic_loss     | 2.66     |\n",
      "|    ent_coef        | 0.0463   |\n",
      "|    ent_coef_loss   | 0.565    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 335162   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=2785.70 +/- 874.79\n",
      "Episode length: 862.60 +/- 274.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 863      |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 6.04     |\n",
      "|    ent_coef        | 0.0464   |\n",
      "|    ent_coef_loss   | -0.467   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 339899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1050     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 1785     |\n",
      "|    total_timesteps | 343573   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -257     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | -0.551   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 343472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=3204.56 +/- 2.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 2.45     |\n",
      "|    ent_coef        | 0.0453   |\n",
      "|    ent_coef_loss   | 0.805    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1060     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 1835     |\n",
      "|    total_timesteps | 353573   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -258     |\n",
      "|    critic_loss     | 5.41     |\n",
      "|    ent_coef        | 0.0429   |\n",
      "|    ent_coef_loss   | 0.638    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 353472   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=3227.24 +/- 5.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -259     |\n",
      "|    critic_loss     | 3.47     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | 0.023    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1070     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 1874     |\n",
      "|    total_timesteps | 361403   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -265     |\n",
      "|    critic_loss     | 2.34     |\n",
      "|    ent_coef        | 0.0416   |\n",
      "|    ent_coef_loss   | -0.681   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 361302   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1080     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 1915     |\n",
      "|    total_timesteps | 369666   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 2.07     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | -0.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369565   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=3158.04 +/- 1.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 5.4      |\n",
      "|    ent_coef        | 0.0415   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1090     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 1961     |\n",
      "|    total_timesteps | 378982   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -267     |\n",
      "|    critic_loss     | 2.17     |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | 0.0536   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 378881   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=3214.71 +/- 17.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -266     |\n",
      "|    critic_loss     | 2.79     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | 0.0995   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 379899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1100     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 2002     |\n",
      "|    total_timesteps | 387048   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 4.76     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | 0.475    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 386947   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=3212.70 +/- 21.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -263     |\n",
      "|    critic_loss     | 2.42     |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | -0.655   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 389899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1110     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 2048     |\n",
      "|    total_timesteps | 396344   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -269     |\n",
      "|    critic_loss     | 2.47     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | -0.699   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 396243   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=3185.16 +/- 86.26\n",
      "Episode length: 983.20 +/- 33.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 983      |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -266     |\n",
      "|    critic_loss     | 9.7      |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | 0.189    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1120     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 2094     |\n",
      "|    total_timesteps | 405461   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0388   |\n",
      "|    ent_coef_loss   | 0.266    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 405360   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=3201.35 +/- 2.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | 0.0962   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 409899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1130     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 2140     |\n",
      "|    total_timesteps | 414695   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -269     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.624   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 414594   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=3214.15 +/- 2.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.0377   |\n",
      "|    ent_coef_loss   | -0.447   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 419899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 888      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1140     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 2187     |\n",
      "|    total_timesteps | 424084   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | -0.227   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 423983   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=3221.45 +/- 9.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -275     |\n",
      "|    critic_loss     | 2.53     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 429899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1150     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 2231     |\n",
      "|    total_timesteps | 432891   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -278     |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | -0.229   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 432790   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=3184.57 +/- 17.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 2.01     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.0544   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1160     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2281     |\n",
      "|    total_timesteps | 442891   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -270     |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | -0.559   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 442790   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1170     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2315     |\n",
      "|    total_timesteps | 449765   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.356    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 449664   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=1673.02 +/- 786.76\n",
      "Episode length: 513.20 +/- 243.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 513      |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -277     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | -0.0796  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 449899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1180     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2342     |\n",
      "|    total_timesteps | 455169   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -275     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | -0.018   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 455068   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=1812.64 +/- 538.88\n",
      "Episode length: 546.00 +/- 166.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 546      |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -270     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.162   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 459899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1190     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2374     |\n",
      "|    total_timesteps | 461600   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 2.43     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -0.431   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 461499   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1200     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2409     |\n",
      "|    total_timesteps | 468841   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -267     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | -0.393   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 468740   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=1477.98 +/- 157.87\n",
      "Episode length: 439.40 +/- 45.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 439      |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -270     |\n",
      "|    critic_loss     | 2.49     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | -0.652   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 469899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1210     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2439     |\n",
      "|    total_timesteps | 474784   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 9.41     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.861    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 474683   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=2898.87 +/- 550.95\n",
      "Episode length: 878.00 +/- 176.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 878      |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -278     |\n",
      "|    critic_loss     | 2.67     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -0.0485  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 479899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 754      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1220     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2470     |\n",
      "|    total_timesteps | 480890   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -285     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -0.257   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 480789   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 719      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1230     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2498     |\n",
      "|    total_timesteps | 486638   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 8.88     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.301   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 486537   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=3190.65 +/- 3.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -277     |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.501    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 489899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 697      |\n",
      "|    ep_rew_mean     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1240     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2534     |\n",
      "|    total_timesteps | 493763   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -271     |\n",
      "|    critic_loss     | 2.62     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | -0.233   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 493662   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=1605.18 +/- 821.46\n",
      "Episode length: 484.00 +/- 258.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 484      |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 6.42     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.489    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 676      |\n",
      "|    ep_rew_mean     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1250     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2567     |\n",
      "|    total_timesteps | 500466   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.546    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 500365   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 628      |\n",
      "|    ep_rew_mean     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1260     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 2593     |\n",
      "|    total_timesteps | 505677   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -283     |\n",
      "|    critic_loss     | 2.52     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | -0.668   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 505576   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=2867.99 +/- 842.38\n",
      "Episode length: 869.80 +/- 260.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 870      |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -0.165   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 509899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 621      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1270     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 2624     |\n",
      "|    total_timesteps | 511836   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | -0.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 511735   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 633      |\n",
      "|    ep_rew_mean     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1280     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 2657     |\n",
      "|    total_timesteps | 518501   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.297    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 518400   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=3237.09 +/- 7.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -281     |\n",
      "|    critic_loss     | 2.15     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.969    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 519899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 643      |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1290     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 2694     |\n",
      "|    total_timesteps | 525856   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.933    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 525755   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=2250.77 +/- 914.59\n",
      "Episode length: 680.00 +/- 287.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 680      |\n",
      "|    mean_reward     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -273     |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.253   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 529899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 658      |\n",
      "|    ep_rew_mean     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1300     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 2738     |\n",
      "|    total_timesteps | 534604   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -275     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.422   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 534503   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=3210.07 +/- 0.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 4.13     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.0786   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 539899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 692      |\n",
      "|    ep_rew_mean     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1310     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 2785     |\n",
      "|    total_timesteps | 543993   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.449   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 543892   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=3236.23 +/- 4.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 2.66     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.089   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1320     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 2835     |\n",
      "|    total_timesteps | 553993   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 5.35     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.742   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 553892   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=3060.59 +/- 351.15\n",
      "Episode length: 942.60 +/- 114.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 943      |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 3.94     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 559899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 768      |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1330     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 2882     |\n",
      "|    total_timesteps | 563441   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -282     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.0554  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 563340   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=3220.67 +/- 2.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -282     |\n",
      "|    critic_loss     | 0.937    |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.454    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 569899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1340     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 2922     |\n",
      "|    total_timesteps | 571154   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | 0.067    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 571053   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=3220.53 +/- 0.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -283     |\n",
      "|    critic_loss     | 3.32     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 0.0237   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 579899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1350     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 2971     |\n",
      "|    total_timesteps | 580399   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 3.01     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.605   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 580298   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 840      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1360     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3019     |\n",
      "|    total_timesteps | 589685   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 2.43     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | 0.177    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 589584   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=3227.16 +/- 5.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -275     |\n",
      "|    critic_loss     | 2.57     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | 0.789    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 589899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1370     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3065     |\n",
      "|    total_timesteps | 598375   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 598274   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=3236.53 +/- 14.09\n",
      "Episode length: 989.20 +/- 21.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 989      |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 599899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1380     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3104     |\n",
      "|    total_timesteps | 605653   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 3.57     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.779    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 605552   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=3247.29 +/- 11.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 15.2     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.286    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1390     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 3150     |\n",
      "|    total_timesteps | 614334   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 2.53     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.538   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 614233   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=3275.41 +/- 5.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -0.182   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1400     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 3200     |\n",
      "|    total_timesteps | 623806   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.169    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 623705   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=3243.10 +/- 1.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -282     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | -0.116   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 629899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1410     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 3239     |\n",
      "|    total_timesteps | 631308   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 2.54     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -1.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 631207   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1420     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 3279     |\n",
      "|    total_timesteps | 639352   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 5.05     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | 0.0273   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 639251   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=1232.04 +/- 149.46\n",
      "Episode length: 362.40 +/- 45.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -285     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -0.954   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 639899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1430     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3304     |\n",
      "|    total_timesteps | 644424   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | 0.738    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 644323   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=3256.85 +/- 2.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -285     |\n",
      "|    critic_loss     | 3.03     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | 0.711    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 649899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1440     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3347     |\n",
      "|    total_timesteps | 653077   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | -0.139   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 652976   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=1978.89 +/- 1037.60\n",
      "Episode length: 604.00 +/- 323.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 604      |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 2.73     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 0.197    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 659899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1450     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3387     |\n",
      "|    total_timesteps | 661084   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 7.36     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 0.291    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 660983   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1460     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3428     |\n",
      "|    total_timesteps | 669294   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 2.07     |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669193   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=3259.67 +/- 2.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -283     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | -0.315   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 783      |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1470     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3465     |\n",
      "|    total_timesteps | 676641   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | 0.569    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 676540   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=3203.93 +/- 4.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | 0.646    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 679899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 790      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1480     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3505     |\n",
      "|    total_timesteps | 684606   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -282     |\n",
      "|    critic_loss     | 2.81     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -0.421   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 684505   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=3245.47 +/- 5.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | -0.421   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 689899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1490     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3542     |\n",
      "|    total_timesteps | 692041   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 2.42     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | 0.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 691940   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 753      |\n",
      "|    ep_rew_mean     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1500     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3577     |\n",
      "|    total_timesteps | 699131   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 3.77     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | 0.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699030   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=1507.75 +/- 10.68\n",
      "Episode length: 435.20 +/- 2.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 435      |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | 0.854    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1510     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3605     |\n",
      "|    total_timesteps | 704745   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -292     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | -0.299   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 704644   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 706      |\n",
      "|    ep_rew_mean     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1520     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3631     |\n",
      "|    total_timesteps | 709999   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -285     |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | 1.02     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 709898   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=1642.02 +/- 812.45\n",
      "Episode length: 494.60 +/- 252.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 495      |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | -0.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 709899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 710      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1530     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3658     |\n",
      "|    total_timesteps | 715380   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 2.98     |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | -0.788   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 715279   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=3026.92 +/- 297.12\n",
      "Episode length: 920.60 +/- 105.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 921      |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 4.61     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | 0.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 719899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 687      |\n",
      "|    ep_rew_mean     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1540     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3690     |\n",
      "|    total_timesteps | 721753   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 2.27     |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 721652   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 681      |\n",
      "|    ep_rew_mean     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1550     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3727     |\n",
      "|    total_timesteps | 729155   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | -0.608   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 729054   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=1226.22 +/- 8.82\n",
      "Episode length: 346.40 +/- 2.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 346      |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | 0.0268   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 729899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 648      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1560     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3751     |\n",
      "|    total_timesteps | 734063   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 0.0874   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 733962   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=2406.47 +/- 699.18\n",
      "Episode length: 716.40 +/- 220.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 716      |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | 0.349    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 739899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 647      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1570     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3787     |\n",
      "|    total_timesteps | 741304   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 0.923    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 741203   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 641      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1580     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3824     |\n",
      "|    total_timesteps | 748714   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | -0.441   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 748613   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=1252.22 +/- 60.92\n",
      "Episode length: 372.40 +/- 11.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 372      |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 0.895    |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | 0.182    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 749899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 617      |\n",
      "|    ep_rew_mean     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1590     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3849     |\n",
      "|    total_timesteps | 753772   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 31.8     |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | -0.982   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 753671   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=3101.99 +/- 296.35\n",
      "Episode length: 957.40 +/- 85.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 957      |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 0.779    |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | 0.177    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 759899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 616      |\n",
      "|    ep_rew_mean     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1600     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3884     |\n",
      "|    total_timesteps | 760688   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | 0.536    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 760587   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 633      |\n",
      "|    ep_rew_mean     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1610     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3920     |\n",
      "|    total_timesteps | 768008   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | 0.829    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 767907   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=1613.23 +/- 135.95\n",
      "Episode length: 475.20 +/- 34.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 475      |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | 0.193    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 769899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 646      |\n",
      "|    ep_rew_mean     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1620     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3952     |\n",
      "|    total_timesteps | 774586   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | 0.901    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 774485   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=2586.83 +/- 602.81\n",
      "Episode length: 777.00 +/- 199.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 777      |\n",
      "|    mean_reward     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | 0.307    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 779899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 659      |\n",
      "|    ep_rew_mean     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1630     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 3986     |\n",
      "|    total_timesteps | 781277   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | -0.529   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 781176   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=3044.77 +/- 489.44\n",
      "Episode length: 917.00 +/- 166.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 917      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 789899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 690      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1640     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4034     |\n",
      "|    total_timesteps | 790755   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 790654   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 699      |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1650     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4074     |\n",
      "|    total_timesteps | 799017   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 5.86     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | 1.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 798916   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=3279.07 +/- 8.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | -0.939   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 742      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1660     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4121     |\n",
      "|    total_timesteps | 808264   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | 0.442    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 808163   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=3072.39 +/- 509.05\n",
      "Episode length: 916.80 +/- 166.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 917      |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -292     |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | 0.174    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 809899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1670     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4164     |\n",
      "|    total_timesteps | 816915   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | -0.0545  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 816814   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=2929.04 +/- 594.10\n",
      "Episode length: 855.20 +/- 190.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 855      |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -0.0225  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 819899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1680     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4203     |\n",
      "|    total_timesteps | 824689   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 0.117    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 824588   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=3301.39 +/- 21.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | 0.0083   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 829899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 806      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1690     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4251     |\n",
      "|    total_timesteps | 834327   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -292     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | 0.296    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 834226   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=3339.08 +/- 22.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | -0.609   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 839899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1700     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4298     |\n",
      "|    total_timesteps | 843679   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | 0.63     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 843578   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=2018.33 +/- 530.56\n",
      "Episode length: 587.00 +/- 157.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 587      |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 2.49     |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | -0.387   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 849899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1710     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4344     |\n",
      "|    total_timesteps | 852954   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 1.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 852853   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1720     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4374     |\n",
      "|    total_timesteps | 859138   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | -0.612   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 859037   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=3356.10 +/- 10.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 5.71     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | -0.764   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 859899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1730     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4413     |\n",
      "|    total_timesteps | 866813   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | -0.0191  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 866712   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=1445.18 +/- 61.24\n",
      "Episode length: 393.60 +/- 18.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 394      |\n",
      "|    mean_reward     | 1.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | 0.681    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 869899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1740     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4455     |\n",
      "|    total_timesteps | 875365   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 2.01     |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 875264   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=3306.33 +/- 2.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1750     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4495     |\n",
      "|    total_timesteps | 883282   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 0.114    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 883181   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=1854.14 +/- 743.69\n",
      "Episode length: 522.00 +/- 239.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 522      |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 889899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1760     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4534     |\n",
      "|    total_timesteps | 890958   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 0.916    |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | 0.0986   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 890857   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1770     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4574     |\n",
      "|    total_timesteps | 899157   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -0.474   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899056   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=3158.70 +/- 438.19\n",
      "Episode length: 928.00 +/- 144.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 928      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -0.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1780     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4615     |\n",
      "|    total_timesteps | 907353   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | 0.336    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 907252   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=3306.72 +/- 6.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 6.83     |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | 0.84     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 909899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1790     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4650     |\n",
      "|    total_timesteps | 914258   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 0.637    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 914157   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=3339.85 +/- 9.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 919899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1800     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4691     |\n",
      "|    total_timesteps | 922426   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 0.938    |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | -0.409   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 922325   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1810     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4722     |\n",
      "|    total_timesteps | 928740   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | 0.0653   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 928639   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=1486.21 +/- 174.29\n",
      "Episode length: 405.80 +/- 39.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 406      |\n",
      "|    mean_reward     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 3.7      |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | 0.5      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 929899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1820     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4760     |\n",
      "|    total_timesteps | 936383   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | 0.431    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 936282   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=3341.11 +/- 19.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -0.355   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 939899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1830     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4800     |\n",
      "|    total_timesteps | 944404   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | -0.619   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 944303   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=1994.52 +/- 687.17\n",
      "Episode length: 561.60 +/- 220.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 562      |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -0.585   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 949899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1840     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4841     |\n",
      "|    total_timesteps | 952587   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | 0.757    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 952486   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 737      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1850     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4863     |\n",
      "|    total_timesteps | 956956   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 0.451    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 956855   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=1874.97 +/- 776.15\n",
      "Episode length: 527.60 +/- 236.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 528      |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 0.133    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 703      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1860     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4885     |\n",
      "|    total_timesteps | 961249   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 1.86     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 0.248    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 961148   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 697      |\n",
      "|    ep_rew_mean     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1870     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4922     |\n",
      "|    total_timesteps | 968901   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | 0.414    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 968800   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=3077.04 +/- 577.09\n",
      "Episode length: 903.00 +/- 194.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 903      |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 969899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 684      |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1880     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4957     |\n",
      "|    total_timesteps | 975741   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 0.922    |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | -0.614   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 975640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=3327.68 +/- 11.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 0.865    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 979899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 689      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1890     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 4995     |\n",
      "|    total_timesteps | 983156   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 0.559    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 983055   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=3378.94 +/- 34.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 0.924    |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -0.515   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 989899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 683      |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1900     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 5033     |\n",
      "|    total_timesteps | 990704   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | 0.797    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 990603   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 668      |\n",
      "|    ep_rew_mean     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1910     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 5056     |\n",
      "|    total_timesteps | 995528   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 0.491    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 995427   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=3039.00 +/- 676.37\n",
      "Episode length: 891.20 +/- 217.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 891      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 0.64     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 659      |\n",
      "|    ep_rew_mean     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1920     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 5091     |\n",
      "|    total_timesteps | 1002313  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 2.71     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1002212  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=1909.45 +/- 579.58\n",
      "Episode length: 534.40 +/- 164.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 534      |\n",
      "|    mean_reward     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 0.691    |\n",
      "|    ent_coef        | 0.0239   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 661      |\n",
      "|    ep_rew_mean     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1930     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 5131     |\n",
      "|    total_timesteps | 1010456  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -0.115   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1010355  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 640      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1940     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 5161     |\n",
      "|    total_timesteps | 1016560  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | 0.0458   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1016459  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=3451.82 +/- 26.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 666      |\n",
      "|    ep_rew_mean     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1950     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 5196     |\n",
      "|    total_timesteps | 1023572  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1023471  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=3422.04 +/- 15.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 706      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1960     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 5238     |\n",
      "|    total_timesteps | 1031811  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | -0.242   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1031710  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1970     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5261     |\n",
      "|    total_timesteps | 1036605  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 0.732    |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -1.63    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1036504  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=1287.79 +/- 618.89\n",
      "Episode length: 364.20 +/- 168.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 364      |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 0.743    |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -0.0473  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 658      |\n",
      "|    ep_rew_mean     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1980     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5286     |\n",
      "|    total_timesteps | 1041533  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -0.248   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1041432  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 630      |\n",
      "|    ep_rew_mean     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1990     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5308     |\n",
      "|    total_timesteps | 1046129  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 0.999    |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | 0.607    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1046028  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 590      |\n",
      "|    ep_rew_mean     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2000     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5326     |\n",
      "|    total_timesteps | 1049743  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.931    |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | 0.426    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049642  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=1569.83 +/- 226.11\n",
      "Episode length: 423.20 +/- 56.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 423      |\n",
      "|    mean_reward     | 1.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.935    |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 583      |\n",
      "|    ep_rew_mean     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2010     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5347     |\n",
      "|    total_timesteps | 1053836  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1053735  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 549      |\n",
      "|    ep_rew_mean     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2020     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5363     |\n",
      "|    total_timesteps | 1057227  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 0.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1057126  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=2796.81 +/- 558.24\n",
      "Episode length: 768.00 +/- 165.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 768      |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 0.878    |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | -0.572   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 516      |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2030     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5388     |\n",
      "|    total_timesteps | 1062067  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 0.619    |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | -0.264   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1061966  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 508      |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2040     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5414     |\n",
      "|    total_timesteps | 1067324  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | 0.58     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1067223  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=2592.53 +/- 608.91\n",
      "Episode length: 701.20 +/- 166.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 701      |\n",
      "|    mean_reward     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 430      |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | -0.278   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 514      |\n",
      "|    ep_rew_mean     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2050     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5452     |\n",
      "|    total_timesteps | 1074942  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 0.746    |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 0.177    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1074841  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=2113.69 +/- 638.17\n",
      "Episode length: 566.80 +/- 169.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 567      |\n",
      "|    mean_reward     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.913    |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -0.128   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 512      |\n",
      "|    ep_rew_mean     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2060     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5493     |\n",
      "|    total_timesteps | 1083039  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 0.732    |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 0.0742   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1082938  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=3475.45 +/- 16.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 2.64     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | 0.516    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1089899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 544      |\n",
      "|    ep_rew_mean     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2070     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5533     |\n",
      "|    total_timesteps | 1091000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.615    |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -0.555   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1090899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 563      |\n",
      "|    ep_rew_mean     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2080     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5566     |\n",
      "|    total_timesteps | 1097851  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 2.04     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 0.631    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1097750  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=3485.36 +/- 4.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 576      |\n",
      "|    ep_rew_mean     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2090     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5596     |\n",
      "|    total_timesteps | 1103762  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 0.529    |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | -0.109   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1103661  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=3530.34 +/- 62.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 0.925    |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | -0.468   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1109899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 613      |\n",
      "|    ep_rew_mean     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2100     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5633     |\n",
      "|    total_timesteps | 1111025  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1110924  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 645      |\n",
      "|    ep_rew_mean     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2110     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5669     |\n",
      "|    total_timesteps | 1118346  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1118245  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=1672.21 +/- 52.24\n",
      "Episode length: 456.40 +/- 11.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 456      |\n",
      "|    mean_reward     | 1.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 0.574    |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -0.858   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 695      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2120     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5711     |\n",
      "|    total_timesteps | 1126681  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | 0.687    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1126580  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=1632.00 +/- 22.34\n",
      "Episode length: 441.00 +/- 5.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 441      |\n",
      "|    mean_reward     | 1.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | 0.983    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 739      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2130     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5757     |\n",
      "|    total_timesteps | 1135949  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 0.692    |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.788    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1135848  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=3298.16 +/- 435.96\n",
      "Episode length: 933.20 +/- 133.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 933      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.683    |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | -0.189   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2140     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5796     |\n",
      "|    total_timesteps | 1143728  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | -0.977   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1143627  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=3527.00 +/- 29.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | 0.994    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2150     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5845     |\n",
      "|    total_timesteps | 1153387  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 0.704    |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | 1.96     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1153286  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=3493.47 +/- 73.49\n",
      "Episode length: 991.80 +/- 16.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 992      |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.398    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 782      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2160     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5885     |\n",
      "|    total_timesteps | 1161215  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 0.0244   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1161114  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=1638.84 +/- 113.97\n",
      "Episode length: 458.40 +/- 29.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 458      |\n",
      "|    mean_reward     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | 1.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2170     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5930     |\n",
      "|    total_timesteps | 1170235  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.922    |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | 2.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1170134  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2180     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5935     |\n",
      "|    total_timesteps | 1171300  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.762    |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -0.142   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1171199  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 676      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2190     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5935     |\n",
      "|    total_timesteps | 1171371  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.757    |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | -0.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1171270  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 634      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2200     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5950     |\n",
      "|    total_timesteps | 1174426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.912    |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1174325  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=3452.12 +/- 7.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 1.31     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 640      |\n",
      "|    ep_rew_mean     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2210     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 5991     |\n",
      "|    total_timesteps | 1182390  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.88     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1182289  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=3343.85 +/- 396.03\n",
      "Episode length: 908.40 +/- 124.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 908      |\n",
      "|    mean_reward     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.633    |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | -0.479   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 638      |\n",
      "|    ep_rew_mean     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2220     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6031     |\n",
      "|    total_timesteps | 1190503  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1190402  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 616      |\n",
      "|    ep_rew_mean     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2230     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6066     |\n",
      "|    total_timesteps | 1197577  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0236   |\n",
      "|    ent_coef_loss   | -0.408   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1197476  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=2470.88 +/- 620.27\n",
      "Episode length: 662.00 +/- 169.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 662      |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 597      |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2240     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6096     |\n",
      "|    total_timesteps | 1203418  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.634    |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | -0.928   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1203317  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=1686.71 +/- 187.04\n",
      "Episode length: 454.60 +/- 45.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0226   |\n",
      "|    ent_coef_loss   | 0.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 576      |\n",
      "|    ep_rew_mean     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2250     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6134     |\n",
      "|    total_timesteps | 1210992  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.718    |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | -0.0945  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1210891  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 577      |\n",
      "|    ep_rew_mean     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2260     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6172     |\n",
      "|    total_timesteps | 1218880  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.586    |\n",
      "|    ent_coef        | 0.0233   |\n",
      "|    ent_coef_loss   | -0.736   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1218779  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=3524.21 +/- 8.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.846    |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 580      |\n",
      "|    ep_rew_mean     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2270     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6219     |\n",
      "|    total_timesteps | 1228251  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.884    |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | 0.165    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1228150  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=2119.92 +/- 485.37\n",
      "Episode length: 563.60 +/- 119.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 564      |\n",
      "|    mean_reward     | 2.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -0.83    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 626      |\n",
      "|    ep_rew_mean     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2280     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6247     |\n",
      "|    total_timesteps | 1233856  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | 0.226    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1233755  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=1829.90 +/- 82.94\n",
      "Episode length: 494.00 +/- 21.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 494      |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | 0.415    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 690      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2290     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6280     |\n",
      "|    total_timesteps | 1240394  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.665    |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | -0.848   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1240293  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 705      |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2300     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6302     |\n",
      "|    total_timesteps | 1244879  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | -0.817   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1244778  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 673      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2310     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6325     |\n",
      "|    total_timesteps | 1249683  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.539    |\n",
      "|    ent_coef        | 0.0215   |\n",
      "|    ent_coef_loss   | 0.198    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1249582  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=1509.01 +/- 377.27\n",
      "Episode length: 422.00 +/- 98.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 422      |\n",
      "|    mean_reward     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.913    |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | 0.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 648      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2320     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6353     |\n",
      "|    total_timesteps | 1255261  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.483    |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1255160  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 616      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2330     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6372     |\n",
      "|    total_timesteps | 1259145  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.787    |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259044  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=1287.32 +/- 33.42\n",
      "Episode length: 363.60 +/- 9.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 364      |\n",
      "|    mean_reward     | 1.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.47     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 0.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 600      |\n",
      "|    ep_rew_mean     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2340     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6394     |\n",
      "|    total_timesteps | 1263383  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.985    |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -0.336   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1263282  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 573      |\n",
      "|    ep_rew_mean     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2350     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6418     |\n",
      "|    total_timesteps | 1268299  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.926    |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 0.818    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1268198  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=3268.75 +/- 424.06\n",
      "Episode length: 911.60 +/- 128.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 912      |\n",
      "|    mean_reward     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | -0.424   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 560      |\n",
      "|    ep_rew_mean     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2360     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6451     |\n",
      "|    total_timesteps | 1274851  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.813    |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | 0.434    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1274750  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=2046.88 +/- 227.61\n",
      "Episode length: 553.20 +/- 57.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 553      |\n",
      "|    mean_reward     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.569    |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | -0.782   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 531      |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2370     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6483     |\n",
      "|    total_timesteps | 1281308  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | 0.615    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1281207  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 543      |\n",
      "|    ep_rew_mean     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2380     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6516     |\n",
      "|    total_timesteps | 1288118  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1288017  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=2774.10 +/- 504.87\n",
      "Episode length: 734.40 +/- 132.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 734      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.569    |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.674   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 548      |\n",
      "|    ep_rew_mean     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2390     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6552     |\n",
      "|    total_timesteps | 1295166  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.546    |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -0.163   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1295065  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=1826.48 +/- 509.29\n",
      "Episode length: 509.40 +/- 116.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 509      |\n",
      "|    mean_reward     | 1.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.692    |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | -0.554   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 573      |\n",
      "|    ep_rew_mean     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2400     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6587     |\n",
      "|    total_timesteps | 1302225  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.935    |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.645   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1302124  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 581      |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2410     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6614     |\n",
      "|    total_timesteps | 1307762  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.978    |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | 0.835    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1307661  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=2838.39 +/- 673.27\n",
      "Episode length: 763.40 +/- 188.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 763      |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.845    |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 0.314    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 587      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2420     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6646     |\n",
      "|    total_timesteps | 1313934  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | 2.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1313833  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=2599.93 +/- 595.53\n",
      "Episode length: 694.40 +/- 161.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 694      |\n",
      "|    mean_reward     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 0.625    |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 614      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2430     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6679     |\n",
      "|    total_timesteps | 1320513  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 2.49     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1320412  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 636      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2440     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6710     |\n",
      "|    total_timesteps | 1326987  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 0.732    |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | -0.285   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1326886  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=3276.09 +/- 479.29\n",
      "Episode length: 925.60 +/- 148.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 926      |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.527    |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 658      |\n",
      "|    ep_rew_mean     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2450     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6746     |\n",
      "|    total_timesteps | 1334123  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.771    |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | 0.228    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1334022  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=3530.62 +/- 2.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.559    |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 0.00435  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1339899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 676      |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2460     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6788     |\n",
      "|    total_timesteps | 1342493  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.564    |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -0.445   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1342392  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=3488.66 +/- 5.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.709    |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | -0.768   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 705      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2470     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6835     |\n",
      "|    total_timesteps | 1351816  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.768    |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | -0.421   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1351715  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=3486.57 +/- 11.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.685    |\n",
      "|    ent_coef        | 0.0194   |\n",
      "|    ent_coef_loss   | -0.368   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 729      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2480     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6881     |\n",
      "|    total_timesteps | 1361026  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.783    |\n",
      "|    ent_coef        | 0.0195   |\n",
      "|    ent_coef_loss   | -0.841   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1360925  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2490     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6917     |\n",
      "|    total_timesteps | 1368303  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0185   |\n",
      "|    ent_coef_loss   | -0.206   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1368202  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=2088.33 +/- 545.34\n",
      "Episode length: 554.40 +/- 140.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 554      |\n",
      "|    mean_reward     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 4.03     |\n",
      "|    ent_coef        | 0.0193   |\n",
      "|    ent_coef_loss   | 0.724    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 724      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2500     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6949     |\n",
      "|    total_timesteps | 1374581  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.623    |\n",
      "|    ent_coef        | 0.0196   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1374480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=2730.78 +/- 741.50\n",
      "Episode length: 776.60 +/- 218.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 777      |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 2.41     |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 0.909    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 741      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2510     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 6985     |\n",
      "|    total_timesteps | 1381860  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0192   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1381759  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=564.32 +/- 633.18\n",
      "Episode length: 180.60 +/- 152.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 181      |\n",
      "|    mean_reward     | 564      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 0.597    |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | -0.444   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2520     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7027     |\n",
      "|    total_timesteps | 1390359  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.872    |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | -0.228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1390258  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2530     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7052     |\n",
      "|    total_timesteps | 1395492  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.676    |\n",
      "|    ent_coef        | 0.0191   |\n",
      "|    ent_coef_loss   | 0.555    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1395391  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=1417.88 +/- 1461.63\n",
      "Episode length: 407.40 +/- 383.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 407      |\n",
      "|    mean_reward     | 1.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.85     |\n",
      "|    ent_coef        | 0.0185   |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 741      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2540     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7080     |\n",
      "|    total_timesteps | 1401093  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.641    |\n",
      "|    ent_coef        | 0.0197   |\n",
      "|    ent_coef_loss   | -0.0745  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1400992  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2550     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7101     |\n",
      "|    total_timesteps | 1405303  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1405202  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=1823.29 +/- 458.50\n",
      "Episode length: 486.00 +/- 110.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 486      |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.019    |\n",
      "|    ent_coef_loss   | 0.0733   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 691      |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2560     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7132     |\n",
      "|    total_timesteps | 1411593  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0189   |\n",
      "|    ent_coef_loss   | -0.268   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1411492  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 670      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2570     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7167     |\n",
      "|    total_timesteps | 1418815  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 0.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1418714  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=2865.68 +/- 773.07\n",
      "Episode length: 814.20 +/- 233.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 814      |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.598    |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 0.288    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 622      |\n",
      "|    ep_rew_mean     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2580     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7190     |\n",
      "|    total_timesteps | 1423276  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.694    |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | 0.0633   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1423175  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 616      |\n",
      "|    ep_rew_mean     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2590     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7223     |\n",
      "|    total_timesteps | 1429931  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.529    |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 0.264    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429830  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=2862.51 +/- 588.14\n",
      "Episode length: 749.20 +/- 144.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 749      |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | -0.339   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 619      |\n",
      "|    ep_rew_mean     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2600     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7256     |\n",
      "|    total_timesteps | 1436525  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 2.62     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.135   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1436424  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=3500.19 +/- 46.83\n",
      "Episode length: 985.40 +/- 29.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 985      |\n",
      "|    mean_reward     | 3.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.676    |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | -0.575   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 610      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2610     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7288     |\n",
      "|    total_timesteps | 1442835  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.482    |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -0.343   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1442734  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 582      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2620     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7316     |\n",
      "|    total_timesteps | 1448606  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.656    |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | -0.872   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1448505  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=2618.09 +/- 747.06\n",
      "Episode length: 725.20 +/- 205.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 725      |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0218   |\n",
      "|    ent_coef_loss   | 0.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 594      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2630     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 7348     |\n",
      "|    total_timesteps | 1454914  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 0.94     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | -0.509   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1454813  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=1918.87 +/- 316.09\n",
      "Episode length: 525.00 +/- 70.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 525      |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.896    |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 604      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2640     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7381     |\n",
      "|    total_timesteps | 1461525  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.876    |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | 1.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1461424  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 626      |\n",
      "|    ep_rew_mean     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2650     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7412     |\n",
      "|    total_timesteps | 1467950  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -0.333   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1467849  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=2771.34 +/- 487.16\n",
      "Episode length: 739.80 +/- 144.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 740      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 0.673    |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 0.186    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 641      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2660     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7451     |\n",
      "|    total_timesteps | 1475721  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 0.533    |\n",
      "|    ent_coef        | 0.0214   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1475620  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=3189.14 +/- 441.04\n",
      "Episode length: 854.00 +/- 132.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 854      |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.922    |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | -0.377   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 635      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2670     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7485     |\n",
      "|    total_timesteps | 1482322  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 0.907    |\n",
      "|    ent_coef        | 0.0216   |\n",
      "|    ent_coef_loss   | -0.909   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1482221  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=3432.43 +/- 394.12\n",
      "Episode length: 940.40 +/- 116.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 940      |\n",
      "|    mean_reward     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | 0.962    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 673      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2680     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7526     |\n",
      "|    total_timesteps | 1490578  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.589    |\n",
      "|    ent_coef        | 0.0218   |\n",
      "|    ent_coef_loss   | -0.519   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1490477  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 682      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2690     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7563     |\n",
      "|    total_timesteps | 1498135  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.7      |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | -0.349   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1498034  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=2002.05 +/- 96.75\n",
      "Episode length: 528.20 +/- 25.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 528      |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 0.438    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 693      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2700     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7602     |\n",
      "|    total_timesteps | 1505867  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | 0.135    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1505766  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=3653.28 +/- 206.47\n",
      "Episode length: 952.00 +/- 62.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 952      |\n",
      "|    mean_reward     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | 0.352    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1509899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 701      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2710     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7637     |\n",
      "|    total_timesteps | 1512930  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.626    |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1512829  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2720     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7671     |\n",
      "|    total_timesteps | 1519874  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.748    |\n",
      "|    ent_coef        | 0.0219   |\n",
      "|    ent_coef_loss   | 1.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1519773  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=2894.18 +/- 739.39\n",
      "Episode length: 769.00 +/- 195.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 769      |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.87     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | 0.311    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 704      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2730     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7699     |\n",
      "|    total_timesteps | 1525274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.771    |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1525173  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=2486.26 +/- 1101.32\n",
      "Episode length: 650.00 +/- 288.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 650      |\n",
      "|    mean_reward     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.575    |\n",
      "|    ent_coef        | 0.022    |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 706      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2740     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7733     |\n",
      "|    total_timesteps | 1532095  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1531994  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=2783.88 +/- 458.06\n",
      "Episode length: 727.20 +/- 117.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 727      |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.777    |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | 0.713    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 721      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2750     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7773     |\n",
      "|    total_timesteps | 1540018  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | -0.0523  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539917  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 706      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2760     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7803     |\n",
      "|    total_timesteps | 1546281  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 0.957    |\n",
      "|    ent_coef        | 0.0239   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1546180  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=2044.84 +/- 79.09\n",
      "Episode length: 539.40 +/- 19.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 539      |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0231   |\n",
      "|    ent_coef_loss   | 0.692    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 701      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2770     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7834     |\n",
      "|    total_timesteps | 1552404  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 162      |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 0.633    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1552303  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 681      |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2780     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7865     |\n",
      "|    total_timesteps | 1558662  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.965    |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -0.191   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1558561  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=2665.56 +/- 797.20\n",
      "Episode length: 695.80 +/- 200.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 696      |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 2.25     |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | 0.977    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 682      |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2790     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7903     |\n",
      "|    total_timesteps | 1566289  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.58     |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | -0.991   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1566188  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=2771.74 +/- 645.05\n",
      "Episode length: 736.60 +/- 184.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 737      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 664      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2800     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7933     |\n",
      "|    total_timesteps | 1572253  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.863    |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | -0.179   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1572152  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 654      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2810     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7962     |\n",
      "|    total_timesteps | 1578283  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | 0.216    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1578182  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=1887.37 +/- 39.37\n",
      "Episode length: 496.80 +/- 9.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 497      |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 2.43     |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 645      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2820     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 7993     |\n",
      "|    total_timesteps | 1584345  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | -0.123   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1584244  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 647      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2830     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8020     |\n",
      "|    total_timesteps | 1589933  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 3.77     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 0.69     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589832  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=2036.94 +/- 368.25\n",
      "Episode length: 524.60 +/- 84.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 525      |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 0.199    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 654      |\n",
      "|    ep_rew_mean     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2840     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8058     |\n",
      "|    total_timesteps | 1597526  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 3.53     |\n",
      "|    ent_coef        | 0.0229   |\n",
      "|    ent_coef_loss   | 0.0311   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1597425  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=2528.61 +/- 631.28\n",
      "Episode length: 662.20 +/- 158.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 662      |\n",
      "|    mean_reward     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 0.989    |\n",
      "|    ent_coef        | 0.0241   |\n",
      "|    ent_coef_loss   | -0.563   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 632      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2850     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8087     |\n",
      "|    total_timesteps | 1603264  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.778    |\n",
      "|    ent_coef        | 0.0232   |\n",
      "|    ent_coef_loss   | 0.186    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1603163  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 628      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2860     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8115     |\n",
      "|    total_timesteps | 1609038  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 2.06     |\n",
      "|    ent_coef        | 0.0239   |\n",
      "|    ent_coef_loss   | -0.864   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1608937  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=3458.16 +/- 192.72\n",
      "Episode length: 935.00 +/- 81.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 935      |\n",
      "|    mean_reward     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.834    |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 637      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2870     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8151     |\n",
      "|    total_timesteps | 1616079  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 2.76     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | 0.00545  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1615978  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=2896.66 +/- 777.68\n",
      "Episode length: 759.40 +/- 204.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 759      |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 2.88     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | -0.212   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 638      |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2880     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8183     |\n",
      "|    total_timesteps | 1622473  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 0.894    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1622372  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 625      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2890     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8214     |\n",
      "|    total_timesteps | 1628802  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.612    |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -0.536   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1628701  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=2299.46 +/- 370.69\n",
      "Episode length: 599.80 +/- 98.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 600      |\n",
      "|    mean_reward     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -0.0421  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 620      |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2900     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8241     |\n",
      "|    total_timesteps | 1634211  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.76     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | 0.419    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1634110  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=3185.38 +/- 604.98\n",
      "Episode length: 849.60 +/- 171.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 850      |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.646    |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | -0.654   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 625      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2910     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8274     |\n",
      "|    total_timesteps | 1640826  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1640725  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 633      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2920     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8308     |\n",
      "|    total_timesteps | 1647616  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 0.63     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1647515  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=3491.22 +/- 226.62\n",
      "Episode length: 960.00 +/- 80.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 960      |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | 0.861    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 643      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2930     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8341     |\n",
      "|    total_timesteps | 1654274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 0.839    |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | -1.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1654173  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=2879.46 +/- 513.88\n",
      "Episode length: 751.80 +/- 142.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 752      |\n",
      "|    mean_reward     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 0.256    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 637      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2940     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8376     |\n",
      "|    total_timesteps | 1661198  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 0.652    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1661097  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 638      |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2950     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8405     |\n",
      "|    total_timesteps | 1667032  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.756    |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | 0.913    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1666931  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=2587.32 +/- 631.77\n",
      "Episode length: 679.40 +/- 173.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 679      |\n",
      "|    mean_reward     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 0.737    |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | -0.494   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 658      |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2960     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8444     |\n",
      "|    total_timesteps | 1674854  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 4        |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | -0.645   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1674753  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=3015.48 +/- 443.05\n",
      "Episode length: 813.80 +/- 128.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 814      |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 651      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2970     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8476     |\n",
      "|    total_timesteps | 1681160  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.86     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | 0.000766 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1681059  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 653      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2980     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8508     |\n",
      "|    total_timesteps | 1687821  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | 0.824    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1687720  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=2791.65 +/- 603.22\n",
      "Episode length: 754.80 +/- 171.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 755      |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 3.41     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | -0.446   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 658      |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2990     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8542     |\n",
      "|    total_timesteps | 1694588  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -1.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1694487  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=3368.68 +/- 353.97\n",
      "Episode length: 908.20 +/- 114.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 908      |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | 0.323    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 674      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3000     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8577     |\n",
      "|    total_timesteps | 1701587  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 0.798    |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | -0.68    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1701486  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 682      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3010     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8614     |\n",
      "|    total_timesteps | 1709064  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | -0.614   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1708963  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=2066.87 +/- 58.33\n",
      "Episode length: 555.00 +/- 18.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 555      |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.691    |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | 0.812    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 690      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3020     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8652     |\n",
      "|    total_timesteps | 1716645  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 0.74     |\n",
      "|    ent_coef        | 0.0245   |\n",
      "|    ent_coef_loss   | -0.922   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1716544  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=2560.35 +/- 590.27\n",
      "Episode length: 673.00 +/- 168.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 673      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0257   |\n",
      "|    ent_coef_loss   | 0.00563  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 686      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3030     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8684     |\n",
      "|    total_timesteps | 1722916  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | 0.163    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1722815  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=3207.23 +/- 586.99\n",
      "Episode length: 865.60 +/- 172.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 866      |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 689      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3040     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8720     |\n",
      "|    total_timesteps | 1730053  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.952    |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 0.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729952  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 709      |\n",
      "|    ep_rew_mean     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3050     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8758     |\n",
      "|    total_timesteps | 1737980  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.689    |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | 0.764    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1737879  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=3001.43 +/- 656.85\n",
      "Episode length: 789.40 +/- 182.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 789      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 0.863    |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | -0.0182  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 711      |\n",
      "|    ep_rew_mean     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3060     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8799     |\n",
      "|    total_timesteps | 1745990  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -0.0283  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1745889  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=2471.16 +/- 652.98\n",
      "Episode length: 648.00 +/- 181.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 648      |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3070     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8841     |\n",
      "|    total_timesteps | 1754597  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 0.685    |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | -0.0597  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1754496  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=3616.34 +/- 18.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 2.52     |\n",
      "|    ent_coef        | 0.0242   |\n",
      "|    ent_coef_loss   | 0.385    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 752      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3080     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8884     |\n",
      "|    total_timesteps | 1762971  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -0.229   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1762870  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 697      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3090     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8890     |\n",
      "|    total_timesteps | 1764327  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1764226  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=3421.17 +/- 463.94\n",
      "Episode length: 929.20 +/- 141.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 929      |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | -0.388   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 698      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3100     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8926     |\n",
      "|    total_timesteps | 1771375  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1771274  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 703      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3110     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 8965     |\n",
      "|    total_timesteps | 1779318  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 0.746    |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | -0.512   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1779217  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=3653.64 +/- 35.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 2.13     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1779899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 716      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3120     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9009     |\n",
      "|    total_timesteps | 1788197  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0255   |\n",
      "|    ent_coef_loss   | 0.523    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1788096  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=3052.96 +/- 875.04\n",
      "Episode length: 804.20 +/- 239.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.0259   |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 745      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3130     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9055     |\n",
      "|    total_timesteps | 1797442  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 0.508    |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -0.532   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1797341  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=2669.08 +/- 682.26\n",
      "Episode length: 690.60 +/- 175.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 691      |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | -0.458   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 752      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3140     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9094     |\n",
      "|    total_timesteps | 1805249  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.758    |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 0.146    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1805148  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=3168.65 +/- 696.04\n",
      "Episode length: 847.40 +/- 194.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 847      |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | 0.748    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 752      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3150     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9135     |\n",
      "|    total_timesteps | 1813217  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0253   |\n",
      "|    ent_coef_loss   | 0.383    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1813116  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 729      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3160     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9162     |\n",
      "|    total_timesteps | 1818898  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 4.38     |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | 0.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1818797  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=2099.78 +/- 805.29\n",
      "Episode length: 558.00 +/- 221.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 558      |\n",
      "|    mean_reward     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | 0.287    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 716      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3170     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9199     |\n",
      "|    total_timesteps | 1826182  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 0.725    |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | -0.283   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1826081  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=3520.35 +/- 515.46\n",
      "Episode length: 927.40 +/- 145.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 927      |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0251   |\n",
      "|    ent_coef_loss   | 0.271    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 714      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3180     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9240     |\n",
      "|    total_timesteps | 1834321  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.63     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -0.611   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1834220  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=3630.77 +/- 29.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | 0.356    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3190     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9280     |\n",
      "|    total_timesteps | 1842250  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0265   |\n",
      "|    ent_coef_loss   | 0.261    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1842149  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1850000, episode_reward=3475.05 +/- 508.50\n",
      "Episode length: 875.60 +/- 132.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 876      |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.627    |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -0.256   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3200     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9327     |\n",
      "|    total_timesteps | 1851805  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 0.746    |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | -0.0633  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1851704  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=3693.60 +/- 15.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | 0.909    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1859899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3210     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9374     |\n",
      "|    total_timesteps | 1861075  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 0.00119  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1860974  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3220     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9413     |\n",
      "|    total_timesteps | 1869156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | -0.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869055  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=3694.98 +/- 24.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 5.81     |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | -0.498   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3230     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9452     |\n",
      "|    total_timesteps | 1876746  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1876645  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=2278.72 +/- 202.74\n",
      "Episode length: 568.40 +/- 42.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 568      |\n",
      "|    mean_reward     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | 0.542    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3240     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9490     |\n",
      "|    total_timesteps | 1884426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | -0.589   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1884325  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=1521.53 +/- 50.28\n",
      "Episode length: 422.00 +/- 9.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 422      |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -0.373   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3250     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9529     |\n",
      "|    total_timesteps | 1892373  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.944    |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | -0.412   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1892272  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=1622.29 +/- 87.34\n",
      "Episode length: 448.80 +/- 15.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 449      |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 0.0127   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 813      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3260     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9568     |\n",
      "|    total_timesteps | 1900178  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 0.846    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1900077  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3270     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9604     |\n",
      "|    total_timesteps | 1907368  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 2.94     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | 0.476    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1907267  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=2372.40 +/- 737.67\n",
      "Episode length: 611.20 +/- 196.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 611      |\n",
      "|    mean_reward     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0258   |\n",
      "|    ent_coef_loss   | -0.118   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3280     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9642     |\n",
      "|    total_timesteps | 1915049  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 1.6      |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | 0.653    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1914948  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=3730.12 +/- 38.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 0.889    |\n",
      "|    ent_coef        | 0.0256   |\n",
      "|    ent_coef_loss   | -0.242   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3290     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9680     |\n",
      "|    total_timesteps | 1922526  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.682    |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1922425  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3300     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9706     |\n",
      "|    total_timesteps | 1927878  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | 0.333    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1927777  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=2468.13 +/- 1146.55\n",
      "Episode length: 655.00 +/- 285.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 655      |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.977    |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3310     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9738     |\n",
      "|    total_timesteps | 1934331  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.919    |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | -0.239   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1934230  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=2227.16 +/- 767.94\n",
      "Episode length: 594.00 +/- 206.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 594      |\n",
      "|    mean_reward     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 2.7      |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | 0.561    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 741      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3320     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9783     |\n",
      "|    total_timesteps | 1943233  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0264   |\n",
      "|    ent_coef_loss   | 0.127    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1943132  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 729      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3330     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9814     |\n",
      "|    total_timesteps | 1949690  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 0.999    |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1949589  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=3078.80 +/- 728.82\n",
      "Episode length: 816.00 +/- 207.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 816      |\n",
      "|    mean_reward     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 0.955    |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3340     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9855     |\n",
      "|    total_timesteps | 1957870  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 0.994    |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | -0.315   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1957769  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=3503.60 +/- 356.38\n",
      "Episode length: 944.60 +/- 110.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 945      |\n",
      "|    mean_reward     | 3.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.67     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 0.0928   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 729      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3350     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9893     |\n",
      "|    total_timesteps | 1965258  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | -0.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1965157  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=3040.68 +/- 723.58\n",
      "Episode length: 799.40 +/- 188.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 799      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 1.95     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 0.433    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 736      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3360     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9935     |\n",
      "|    total_timesteps | 1973755  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | 0.934    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1973654  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=3678.20 +/- 23.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -0.0518  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 737      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3370     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 9972     |\n",
      "|    total_timesteps | 1981038  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | 0.148    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1980937  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3380     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10004    |\n",
      "|    total_timesteps | 1987539  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 2.95     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1987438  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=3627.13 +/- 29.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | 0.763    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 723      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3390     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10041    |\n",
      "|    total_timesteps | 1994857  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1994756  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=2603.16 +/- 845.22\n",
      "Episode length: 692.00 +/- 232.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 692      |\n",
      "|    mean_reward     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.027    |\n",
      "|    ent_coef_loss   | 0.837    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3400     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10073    |\n",
      "|    total_timesteps | 2001201  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -0.0406  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2001100  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 742      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3410     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10109    |\n",
      "|    total_timesteps | 2008536  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 2.65     |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | 0.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2008435  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=3159.74 +/- 840.63\n",
      "Episode length: 821.20 +/- 220.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 821      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | -0.627   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 735      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3420     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10150    |\n",
      "|    total_timesteps | 2016779  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.0271   |\n",
      "|    ent_coef_loss   | 0.0142   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2016678  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=3370.46 +/- 597.88\n",
      "Episode length: 874.00 +/- 161.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 874      |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 0.769    |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | -0.454   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 748      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3430     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10188    |\n",
      "|    total_timesteps | 2024440  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | 0.964    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2024339  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=3493.93 +/- 389.00\n",
      "Episode length: 904.80 +/- 119.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 905      |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | 0.968    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 744      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3440     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10228    |\n",
      "|    total_timesteps | 2032250  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | -0.396   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2032149  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 738      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3450     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10261    |\n",
      "|    total_timesteps | 2039095  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.943    |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 0.0283   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2038994  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=1533.91 +/- 48.60\n",
      "Episode length: 400.40 +/- 11.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 400      |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | 0.028    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3460     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10301    |\n",
      "|    total_timesteps | 2047138  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 0.582    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2047037  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=3705.39 +/- 15.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 0.988    |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | 0.355    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 744      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3470     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10343    |\n",
      "|    total_timesteps | 2055428  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | 0.519    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2055327  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=3382.04 +/- 556.24\n",
      "Episode length: 871.40 +/- 159.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 871      |\n",
      "|    mean_reward     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | -0.913   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 754      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3480     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10381    |\n",
      "|    total_timesteps | 2062917  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 0.506    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2062816  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=3035.80 +/- 832.77\n",
      "Episode length: 779.20 +/- 215.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 779      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.969    |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -0.262   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 755      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3490     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10418    |\n",
      "|    total_timesteps | 2070311  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | 0.295    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2070210  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 763      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3500     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10453    |\n",
      "|    total_timesteps | 2077456  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.927    |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | -0.105   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2077355  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=3322.99 +/- 809.83\n",
      "Episode length: 888.60 +/- 222.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 889      |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | -1.55    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 757      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3510     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10487    |\n",
      "|    total_timesteps | 2084257  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | -0.186   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2084156  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=2966.00 +/- 1023.54\n",
      "Episode length: 776.80 +/- 274.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 777      |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3520     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10525    |\n",
      "|    total_timesteps | 2091834  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | -0.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2091733  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=3709.93 +/- 280.78\n",
      "Episode length: 957.40 +/- 80.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 957      |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | 0.754    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3530     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10571    |\n",
      "|    total_timesteps | 2100970  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | -0.477   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2100869  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 769      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3540     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10612    |\n",
      "|    total_timesteps | 2109154  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | -0.552   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2109053  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=1663.95 +/- 240.44\n",
      "Episode length: 447.20 +/- 65.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 447      |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 0.763    |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | 1.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3550     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10659    |\n",
      "|    total_timesteps | 2118631  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | -0.00232 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2118530  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=3706.06 +/- 22.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0275   |\n",
      "|    ent_coef_loss   | 1.45     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3560     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10701    |\n",
      "|    total_timesteps | 2127046  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.812    |\n",
      "|    ent_coef        | 0.0272   |\n",
      "|    ent_coef_loss   | 0.155    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2126945  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=3654.47 +/- 31.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.894    |\n",
      "|    ent_coef        | 0.0266   |\n",
      "|    ent_coef_loss   | -0.289   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3570     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10739    |\n",
      "|    total_timesteps | 2134588  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 0.99     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | 0.161    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2134487  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=1963.35 +/- 653.99\n",
      "Episode length: 513.60 +/- 173.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 514      |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.771    |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | -0.194   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 797      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3580     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10779    |\n",
      "|    total_timesteps | 2142599  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | -0.0669  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2142498  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=3582.31 +/- 18.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0277   |\n",
      "|    ent_coef_loss   | 0.546    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 806      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3590     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10821    |\n",
      "|    total_timesteps | 2150949  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | 0.541    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2150848  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3600     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10861    |\n",
      "|    total_timesteps | 2159119  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0273   |\n",
      "|    ent_coef_loss   | -0.0238  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2159018  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=3720.29 +/- 48.03\n",
      "Episode length: 991.20 +/- 17.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 991      |\n",
      "|    mean_reward     | 3.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.768    |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | 0.893    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3610     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10904    |\n",
      "|    total_timesteps | 2167582  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0278   |\n",
      "|    ent_coef_loss   | 1.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2167481  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=3740.25 +/- 52.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -0.0403  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2169899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 832      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3620     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10941    |\n",
      "|    total_timesteps | 2175061  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.891    |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2174960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=3584.10 +/- 8.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | 1        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3630     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 10982    |\n",
      "|    total_timesteps | 2183014  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.852    |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | 0.447    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2182913  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=3750.11 +/- 44.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.916    |\n",
      "|    ent_coef        | 0.0269   |\n",
      "|    ent_coef_loss   | -0.822   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2189899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3640     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 11023    |\n",
      "|    total_timesteps | 2191206  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.98     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | -0.903   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2191105  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3650     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 11060    |\n",
      "|    total_timesteps | 2198745  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | 0.804    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2198644  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=2455.81 +/- 984.10\n",
      "Episode length: 640.00 +/- 256.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 640      |\n",
      "|    mean_reward     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.883    |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 0.946    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3660     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 11099    |\n",
      "|    total_timesteps | 2206202  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 6.16     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | -0.229   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2206101  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=3469.94 +/- 608.43\n",
      "Episode length: 914.40 +/- 171.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 914      |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | 1.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3670     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 11151    |\n",
      "|    total_timesteps | 2215240  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 0.216    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2215139  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=3740.89 +/- 148.18\n",
      "Episode length: 968.40 +/- 56.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 968      |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -0.244   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3680     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 11198    |\n",
      "|    total_timesteps | 2223835  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.865    |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -0.623   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2223734  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=2274.09 +/- 880.64\n",
      "Episode length: 576.80 +/- 206.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 577      |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | 1.42     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3690     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 11244    |\n",
      "|    total_timesteps | 2231370  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.73     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -0.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2231269  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3700     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 11284    |\n",
      "|    total_timesteps | 2238425  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.797    |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | -0.286   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2238324  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=3597.64 +/- 485.35\n",
      "Episode length: 918.20 +/- 140.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 918      |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.736    |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -0.587   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3710     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 11338    |\n",
      "|    total_timesteps | 2247615  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.965    |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | 0.0621   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2247514  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=3697.84 +/- 50.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.805    |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | -1.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3720     |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 11390    |\n",
      "|    total_timesteps | 2256047  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2255946  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=3817.16 +/- 44.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -0.118   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2259899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3730     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 11430    |\n",
      "|    total_timesteps | 2262780  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 2.58     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2262679  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=3327.55 +/- 475.77\n",
      "Episode length: 849.60 +/- 131.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 850      |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -0.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3740     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 11481    |\n",
      "|    total_timesteps | 2271522  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 4.94     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -0.742   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2271421  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3750     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 11524    |\n",
      "|    total_timesteps | 2279003  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.559    |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2278902  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=3455.18 +/- 444.64\n",
      "Episode length: 898.60 +/- 125.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 899      |\n",
      "|    mean_reward     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | -1.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3760     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 11580    |\n",
      "|    total_timesteps | 2288412  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 4.07     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | 0.724    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2288311  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=3850.98 +/- 35.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.902    |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | 0.206    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2289899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3770     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 11639    |\n",
      "|    total_timesteps | 2298097  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2297996  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=2856.29 +/- 889.78\n",
      "Episode length: 722.60 +/- 241.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 723      |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.948    |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | -0.949   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3780     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 11680    |\n",
      "|    total_timesteps | 2305524  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -0.143   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2305423  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310000, episode_reward=2935.35 +/- 725.60\n",
      "Episode length: 760.40 +/- 202.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 760      |\n",
      "|    mean_reward     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | -0.602   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3790     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 11729    |\n",
      "|    total_timesteps | 2313433  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | -0.602   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2313332  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=2128.32 +/- 338.53\n",
      "Episode length: 527.20 +/- 79.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 527      |\n",
      "|    mean_reward     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.786    |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | 0.142    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3800     |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 11776    |\n",
      "|    total_timesteps | 2320910  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | 0.346    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2320809  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3810     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 11822    |\n",
      "|    total_timesteps | 2328441  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | 0.236    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2328340  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=1771.76 +/- 221.62\n",
      "Episode length: 445.00 +/- 45.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 445      |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -0.795   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3820     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 11864    |\n",
      "|    total_timesteps | 2335289  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.794    |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | 0.0143   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2335188  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=3857.58 +/- 56.79\n",
      "Episode length: 996.80 +/- 6.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 997      |\n",
      "|    mean_reward     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -0.385   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2339899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3830     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 11907    |\n",
      "|    total_timesteps | 2342390  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.759    |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | -0.135   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2342289  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 778      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3840     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 11953    |\n",
      "|    total_timesteps | 2349284  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2349183  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=1739.85 +/- 174.24\n",
      "Episode length: 439.20 +/- 37.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 439      |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | 0.115    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 773      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3850     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 11995    |\n",
      "|    total_timesteps | 2356348  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 2.56     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -0.473   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2356247  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=2980.33 +/- 489.43\n",
      "Episode length: 780.40 +/- 136.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 780      |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -0.377   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 746      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3860     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 12035    |\n",
      "|    total_timesteps | 2362990  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 5.66     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | -0.547   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2362889  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=2255.15 +/- 660.02\n",
      "Episode length: 554.80 +/- 152.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 555      |\n",
      "|    mean_reward     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | -0.981   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 722      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3870     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 12079    |\n",
      "|    total_timesteps | 2370321  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -0.348   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2370220  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 700      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3880     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 12108    |\n",
      "|    total_timesteps | 2375564  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -0.629   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2375463  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=2318.92 +/- 176.01\n",
      "Episode length: 591.60 +/- 48.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 592      |\n",
      "|    mean_reward     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | 0.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 685      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3890     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 12148    |\n",
      "|    total_timesteps | 2381963  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.65     |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | -0.966   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2381862  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 618      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3900     |\n",
      "|    fps             | 196      |\n",
      "|    time_elapsed    | 12153    |\n",
      "|    total_timesteps | 2382724  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -0.605   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2382623  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=2961.78 +/- 538.91\n",
      "Episode length: 752.60 +/- 150.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 753      |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | 0.284    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 617      |\n",
      "|    ep_rew_mean     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3910     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12200    |\n",
      "|    total_timesteps | 2390189  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -0.097   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2390088  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 620      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3920     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12241    |\n",
      "|    total_timesteps | 2397272  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.913    |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -0.736   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2397171  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=3367.42 +/- 490.52\n",
      "Episode length: 847.80 +/- 126.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 848      |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 2.06     |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | 0.372    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 612      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3930     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12279    |\n",
      "|    total_timesteps | 2403550  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 6.63     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 0.414    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2403449  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 599      |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3940     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12315    |\n",
      "|    total_timesteps | 2409172  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 2.5      |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.836    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409071  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=1610.37 +/- 106.20\n",
      "Episode length: 414.60 +/- 23.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 415      |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -0.368   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 601      |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3950     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12359    |\n",
      "|    total_timesteps | 2416489  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.936    |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2416388  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=3330.11 +/- 429.31\n",
      "Episode length: 844.00 +/- 130.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 844      |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 0.817    |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 0.532    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3960     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12396    |\n",
      "|    total_timesteps | 2423224  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.87     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -0.0692  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2423123  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=3766.99 +/- 144.39\n",
      "Episode length: 957.60 +/- 52.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 958      |\n",
      "|    mean_reward     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.137    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 598      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3970     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12436    |\n",
      "|    total_timesteps | 2430095  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 65.7     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.243   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2429994  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 619      |\n",
      "|    ep_rew_mean     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3980     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12476    |\n",
      "|    total_timesteps | 2437438  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 0.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2437337  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=3814.61 +/- 30.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | 0.133    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 626      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3990     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12523    |\n",
      "|    total_timesteps | 2444556  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | -0.844   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2444455  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2450000, episode_reward=1781.95 +/- 208.85\n",
      "Episode length: 450.20 +/- 49.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 450      |\n",
      "|    mean_reward     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | -0.658   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 689      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4000     |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 12567    |\n",
      "|    total_timesteps | 2451665  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 3.08     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | -0.487   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2451564  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 681      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4010     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12607    |\n",
      "|    total_timesteps | 2458295  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | 0.42     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2458194  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=1681.62 +/- 297.22\n",
      "Episode length: 436.00 +/- 69.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 436      |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.954    |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | 0.472    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4020     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12646    |\n",
      "|    total_timesteps | 2464925  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | 0.499    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2464824  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=1853.46 +/- 87.10\n",
      "Episode length: 462.00 +/- 19.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 462      |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.821    |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 674      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4030     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12682    |\n",
      "|    total_timesteps | 2470936  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 2.6      |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | 0.855    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2470835  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 682      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4040     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12720    |\n",
      "|    total_timesteps | 2477366  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.745    |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | 0.413    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2477265  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=2691.56 +/- 979.56\n",
      "Episode length: 682.40 +/- 259.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 682      |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | 0.469    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 671      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4050     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12760    |\n",
      "|    total_timesteps | 2483623  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | -0.227   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2483522  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=3534.94 +/- 767.14\n",
      "Episode length: 874.60 +/- 195.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 875      |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -0.996   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 675      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4060     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12804    |\n",
      "|    total_timesteps | 2490758  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | -0.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2490657  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 652      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4070     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12832    |\n",
      "|    total_timesteps | 2495319  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.948    |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | -0.535   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2495218  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=1607.12 +/- 51.71\n",
      "Episode length: 412.60 +/- 9.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 413      |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -0.461   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 642      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4080     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12870    |\n",
      "|    total_timesteps | 2501598  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 2.7      |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | 0.425    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2501497  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 632      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4090     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12907    |\n",
      "|    total_timesteps | 2507775  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 2.2      |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | 0.562    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2507674  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=1443.04 +/- 53.90\n",
      "Episode length: 375.40 +/- 12.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 375      |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 3.41     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | 0.741    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 614      |\n",
      "|    ep_rew_mean     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4100     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12941    |\n",
      "|    total_timesteps | 2513098  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2512997  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 610      |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4110     |\n",
      "|    fps             | 194      |\n",
      "|    time_elapsed    | 12979    |\n",
      "|    total_timesteps | 2519289  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | 0.885    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519188  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=2536.48 +/- 839.42\n",
      "Episode length: 629.80 +/- 211.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 630      |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.779    |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 607      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4120     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13019    |\n",
      "|    total_timesteps | 2525576  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2525475  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=2242.67 +/- 527.15\n",
      "Episode length: 549.20 +/- 119.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 549      |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | -0.0889  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 596      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4130     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13050    |\n",
      "|    total_timesteps | 2530534  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.921    |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -0.237   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2530433  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 597      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4140     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13093    |\n",
      "|    total_timesteps | 2537100  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | -0.788   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2536999  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=1702.66 +/- 79.05\n",
      "Episode length: 432.80 +/- 18.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 433      |\n",
      "|    mean_reward     | 1.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 2.76     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.939    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 607      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4150     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13141    |\n",
      "|    total_timesteps | 2544292  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.815    |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2544191  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 588      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4160     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13175    |\n",
      "|    total_timesteps | 2549518  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.436   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2549417  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=2525.98 +/- 785.14\n",
      "Episode length: 624.00 +/- 193.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 624      |\n",
      "|    mean_reward     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | 0.0374   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 607      |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4170     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13217    |\n",
      "|    total_timesteps | 2555981  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | 0.814    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2555880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=3011.57 +/- 895.36\n",
      "Episode length: 755.40 +/- 231.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 755      |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 600      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4180     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13254    |\n",
      "|    total_timesteps | 2561608  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | 0.0842   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2561507  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 592      |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4190     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13288    |\n",
      "|    total_timesteps | 2567010  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 3.93     |\n",
      "|    ent_coef        | 0.0307   |\n",
      "|    ent_coef_loss   | 0.384    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2566909  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=2162.09 +/- 904.66\n",
      "Episode length: 543.40 +/- 228.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 543      |\n",
      "|    mean_reward     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.711    |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | 0.606    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 600      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4200     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13326    |\n",
      "|    total_timesteps | 2573074  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2572973  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 586      |\n",
      "|    ep_rew_mean     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4210     |\n",
      "|    fps             | 193      |\n",
      "|    time_elapsed    | 13356    |\n",
      "|    total_timesteps | 2577905  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.999    |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | 0.325    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2577804  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=1655.20 +/- 46.16\n",
      "Episode length: 419.40 +/- 8.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 419      |\n",
      "|    mean_reward     | 1.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.804    |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.284    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 599      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4220     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13399    |\n",
      "|    total_timesteps | 2585440  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | 0.332    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2585339  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=3212.72 +/- 789.34\n",
      "Episode length: 827.20 +/- 221.25\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 598      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4230     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13425    |\n",
      "|    total_timesteps | 2590304  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -0.765   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2590203  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 603      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4240     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13461    |\n",
      "|    total_timesteps | 2597413  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 2.63     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | -0.132   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2597312  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=3158.34 +/- 701.14\n",
      "Episode length: 799.00 +/- 186.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 799      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.773    |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | 0.803    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 601      |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4250     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13498    |\n",
      "|    total_timesteps | 2604404  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 0.915    |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2604303  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=1891.55 +/- 165.50\n",
      "Episode length: 477.60 +/- 41.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 478      |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 2.36     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | 0.321    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 623      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4260     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13536    |\n",
      "|    total_timesteps | 2611802  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 3.06     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | 0.128    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2611701  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 625      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4270     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13570    |\n",
      "|    total_timesteps | 2618478  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.806    |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -0.0154  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2618377  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=2108.97 +/- 1051.76\n",
      "Episode length: 533.40 +/- 242.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 533      |\n",
      "|    mean_reward     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 633      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4280     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13603    |\n",
      "|    total_timesteps | 2624900  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -0.341   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2624799  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2630000, episode_reward=1739.86 +/- 302.42\n",
      "Episode length: 435.60 +/- 65.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 436      |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 0.774    |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -0.948   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 662      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4290     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13650    |\n",
      "|    total_timesteps | 2633162  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | 0.33     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2633061  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 663      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4300     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13685    |\n",
      "|    total_timesteps | 2639359  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.83     |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | 0.0583   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2639258  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=2400.60 +/- 799.77\n",
      "Episode length: 602.00 +/- 207.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 602      |\n",
      "|    mean_reward     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 699      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4310     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13732    |\n",
      "|    total_timesteps | 2647810  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -0.477   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2647709  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=2662.47 +/- 763.59\n",
      "Episode length: 675.00 +/- 207.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 675      |\n",
      "|    mean_reward     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | 0.316    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 714      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4320     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13783    |\n",
      "|    total_timesteps | 2656812  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2656711  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=3039.56 +/- 598.89\n",
      "Episode length: 765.20 +/- 162.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 765      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | 0.211    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4330     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13821    |\n",
      "|    total_timesteps | 2663732  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 6.78     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -0.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2663631  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=3660.74 +/- 338.59\n",
      "Episode length: 947.20 +/- 105.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 947      |\n",
      "|    mean_reward     | 3.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -1.62    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 747      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4340     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13868    |\n",
      "|    total_timesteps | 2672090  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | -0.811   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2671989  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 747      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4350     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13907    |\n",
      "|    total_timesteps | 2679125  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -0.693   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2679024  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=2028.31 +/- 816.13\n",
      "Episode length: 525.80 +/- 224.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 526      |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.856    |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -0.509   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4360     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13950    |\n",
      "|    total_timesteps | 2686949  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.792    |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | -0.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2686848  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=3022.36 +/- 856.66\n",
      "Episode length: 757.60 +/- 214.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 758      |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.797    |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | -0.254   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 762      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4370     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 13995    |\n",
      "|    total_timesteps | 2694667  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | 0.575    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2694566  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=3696.00 +/- 207.93\n",
      "Episode length: 969.00 +/- 62.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 969      |\n",
      "|    mean_reward     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 0.256    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4380     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14043    |\n",
      "|    total_timesteps | 2702490  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | 0.00747  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2702389  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4390     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14086    |\n",
      "|    total_timesteps | 2709566  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | -0.0576  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2709465  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=2567.10 +/- 863.31\n",
      "Episode length: 651.00 +/- 233.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 651      |\n",
      "|    mean_reward     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 3.54     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 1.31     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4400     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14126    |\n",
      "|    total_timesteps | 2716887  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | 0.0615   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2716786  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=1531.60 +/- 193.36\n",
      "Episode length: 392.60 +/- 35.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 393      |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 5.36     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | 0.0073   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 768      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4410     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14166    |\n",
      "|    total_timesteps | 2724656  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 4.43     |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | -0.933   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2724555  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=2078.94 +/- 882.50\n",
      "Episode length: 533.40 +/- 234.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 533      |\n",
      "|    mean_reward     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 2.17     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -0.208   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 735      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4420     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14195    |\n",
      "|    total_timesteps | 2730303  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | 0.387    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2730202  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 732      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4430     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14231    |\n",
      "|    total_timesteps | 2736947  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.841    |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 0.352    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2736846  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=2852.62 +/- 568.81\n",
      "Episode length: 705.20 +/- 141.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 705      |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | -0.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 716      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4440     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14271    |\n",
      "|    total_timesteps | 2743667  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | -0.389   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2743566  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2750000, episode_reward=2955.78 +/- 462.71\n",
      "Episode length: 722.00 +/- 109.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 722      |\n",
      "|    mean_reward     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -0.774   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 717      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4450     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14314    |\n",
      "|    total_timesteps | 2750836  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -0.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2750735  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4460     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14358    |\n",
      "|    total_timesteps | 2758175  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | -0.923   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2758074  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=3777.09 +/- 337.00\n",
      "Episode length: 956.00 +/- 88.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 956      |\n",
      "|    mean_reward     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.878    |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -0.502   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 708      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4470     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14400    |\n",
      "|    total_timesteps | 2765433  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.574    |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -0.611   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2765332  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=2577.24 +/- 845.96\n",
      "Episode length: 646.00 +/- 227.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 646      |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.501   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 695      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4480     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14434    |\n",
      "|    total_timesteps | 2772033  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | 0.439    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2771932  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 686      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4490     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14465    |\n",
      "|    total_timesteps | 2778160  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.665    |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | 0.317    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2778059  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=3254.81 +/- 962.82\n",
      "Episode length: 806.60 +/- 242.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 807      |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.891    |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 0.994    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 682      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4500     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14501    |\n",
      "|    total_timesteps | 2785069  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.864    |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.253   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2784968  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=3829.30 +/- 42.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.52     |\n",
      "|    ent_coef        | 0.0267   |\n",
      "|    ent_coef_loss   | -0.362   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 669      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4510     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14535    |\n",
      "|    total_timesteps | 2791580  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.582    |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2791479  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4520     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14567    |\n",
      "|    total_timesteps | 2797997  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.701    |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | -0.446   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2797896  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=1728.85 +/- 49.63\n",
      "Episode length: 426.80 +/- 10.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 427      |\n",
      "|    mean_reward     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.613    |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 0.583    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2799899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 693       |\n",
      "|    ep_rew_mean     | 2.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4530      |\n",
      "|    fps             | 192       |\n",
      "|    time_elapsed    | 14609     |\n",
      "|    total_timesteps | 2806199   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -353      |\n",
      "|    critic_loss     | 2.87      |\n",
      "|    ent_coef        | 0.0289    |\n",
      "|    ent_coef_loss   | -0.000948 |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2806098   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=3551.48 +/- 562.98\n",
      "Episode length: 920.60 +/- 158.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 921      |\n",
      "|    mean_reward     | 3.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.607    |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | -0.914   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 689      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4540     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14642    |\n",
      "|    total_timesteps | 2812552  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 0.758    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2812451  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 671      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4550     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14669    |\n",
      "|    total_timesteps | 2817899  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | -0.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2817798  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2820000, episode_reward=3421.01 +/- 565.05\n",
      "Episode length: 869.80 +/- 162.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 870      |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.784    |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | 0.656    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 657      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4560     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14702    |\n",
      "|    total_timesteps | 2823877  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.789    |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | 0.289    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2823776  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 638      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4570     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14729    |\n",
      "|    total_timesteps | 2829190  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.529    |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -0.107   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2829089  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2830000, episode_reward=3167.70 +/- 862.65\n",
      "Episode length: 807.60 +/- 236.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 808      |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.734    |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | -0.929   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 628      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4580     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14759    |\n",
      "|    total_timesteps | 2834803  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.573    |\n",
      "|    ent_coef        | 0.0279   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2834702  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 618      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4590     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14786    |\n",
      "|    total_timesteps | 2839914  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.0294   |\n",
      "|    ent_coef_loss   | -0.746   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2839813  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2840000, episode_reward=2043.95 +/- 723.71\n",
      "Episode length: 511.60 +/- 176.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 512      |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | 0.321    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 614      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4600     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14820    |\n",
      "|    total_timesteps | 2846468  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.885    |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | -0.691   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2846367  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=2649.43 +/- 875.51\n",
      "Episode length: 669.20 +/- 220.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 669      |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 615      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4610     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14854    |\n",
      "|    total_timesteps | 2853071  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.915    |\n",
      "|    ent_coef        | 0.0283   |\n",
      "|    ent_coef_loss   | 0.475    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2852970  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 612      |\n",
      "|    ep_rew_mean     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4620     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14886    |\n",
      "|    total_timesteps | 2859197  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0276   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2859096  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=1172.95 +/- 12.10\n",
      "Episode length: 332.20 +/- 1.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 332      |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 2.65     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | 0.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 592      |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4630     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14918    |\n",
      "|    total_timesteps | 2865427  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | 0.402    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2865326  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 573      |\n",
      "|    ep_rew_mean     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4640     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14941    |\n",
      "|    total_timesteps | 2869878  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.663    |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 0.062    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869777  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=1619.59 +/- 267.01\n",
      "Episode length: 414.20 +/- 58.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 414      |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.684    |\n",
      "|    ent_coef        | 0.0281   |\n",
      "|    ent_coef_loss   | -0.645   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 571      |\n",
      "|    ep_rew_mean     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4650     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 14968    |\n",
      "|    total_timesteps | 2874986  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.989    |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.439   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2874885  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=2502.18 +/- 901.61\n",
      "Episode length: 631.80 +/- 233.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 632      |\n",
      "|    mean_reward     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | 0.283    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 572      |\n",
      "|    ep_rew_mean     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4660     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 15000    |\n",
      "|    total_timesteps | 2881046  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 1.86     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | -0.844   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2880945  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 569      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4670     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 15026    |\n",
      "|    total_timesteps | 2886054  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | 0.013    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2885953  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=1647.55 +/- 87.52\n",
      "Episode length: 431.00 +/- 25.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 431      |\n",
      "|    mean_reward     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 0.338    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 561      |\n",
      "|    ep_rew_mean     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4680     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 15052    |\n",
      "|    total_timesteps | 2890871  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | 0.926    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2890770  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 567      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4690     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 15081    |\n",
      "|    total_timesteps | 2896637  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0307   |\n",
      "|    ent_coef_loss   | 0.501    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2896536  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=2492.25 +/- 727.68\n",
      "Episode length: 607.60 +/- 168.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 608      |\n",
      "|    mean_reward     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 4.04     |\n",
      "|    ent_coef        | 0.0293   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 561      |\n",
      "|    ep_rew_mean     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4700     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 15112    |\n",
      "|    total_timesteps | 2902546  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | 0.93     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2902445  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 558      |\n",
      "|    ep_rew_mean     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4710     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 15145    |\n",
      "|    total_timesteps | 2908864  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | -0.402   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2908763  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2910000, episode_reward=2811.06 +/- 865.52\n",
      "Episode length: 713.80 +/- 237.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 714      |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0295   |\n",
      "|    ent_coef_loss   | 0.882    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 555      |\n",
      "|    ep_rew_mean     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4720     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 15177    |\n",
      "|    total_timesteps | 2914737  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 1.63     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2914636  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=3187.34 +/- 587.58\n",
      "Episode length: 806.20 +/- 152.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | 0.987    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 562      |\n",
      "|    ep_rew_mean     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4730     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15217    |\n",
      "|    total_timesteps | 2921631  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.627    |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.522   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2921530  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 570      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4740     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15248    |\n",
      "|    total_timesteps | 2926874  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.861    |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | -0.0606  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2926773  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=1679.08 +/- 141.75\n",
      "Episode length: 420.00 +/- 30.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 420      |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 0.418    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 590      |\n",
      "|    ep_rew_mean     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4750     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15285    |\n",
      "|    total_timesteps | 2934009  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | 0.542    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2933908  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 588      |\n",
      "|    ep_rew_mean     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4760     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15315    |\n",
      "|    total_timesteps | 2939814  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2939713  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=2182.15 +/- 363.21\n",
      "Episode length: 549.00 +/- 91.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 549      |\n",
      "|    mean_reward     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 0.823    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 611      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4770     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15357    |\n",
      "|    total_timesteps | 2947146  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 0.775    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2947045  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2950000, episode_reward=2473.41 +/- 797.45\n",
      "Episode length: 630.60 +/- 215.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 631      |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.964    |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.403    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 629      |\n",
      "|    ep_rew_mean     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4780     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15392    |\n",
      "|    total_timesteps | 2953787  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | 0.955    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2953686  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2960000, episode_reward=2869.94 +/- 1157.48\n",
      "Episode length: 752.20 +/- 304.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 752      |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 0.672    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 642      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4790     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15432    |\n",
      "|    total_timesteps | 2960860  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.0672   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2960759  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 653      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4800     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15468    |\n",
      "|    total_timesteps | 2967806  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 5.18     |\n",
      "|    ent_coef        | 0.0307   |\n",
      "|    ent_coef_loss   | 0.0926   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2967705  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=3199.88 +/- 613.13\n",
      "Episode length: 900.40 +/- 199.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 900      |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.853    |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | -0.78    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 653      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4810     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15502    |\n",
      "|    total_timesteps | 2974159  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 2.04     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.0761   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2974058  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=1807.61 +/- 205.76\n",
      "Episode length: 445.00 +/- 46.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 445      |\n",
      "|    mean_reward     | 1.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.879    |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4820     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15545    |\n",
      "|    total_timesteps | 2982404  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.8      |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -0.158   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2982303  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=2985.74 +/- 780.42\n",
      "Episode length: 765.00 +/- 216.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 765      |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.879    |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 0.334    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 684      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4830     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15584    |\n",
      "|    total_timesteps | 2990002  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.656    |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.613   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2989901  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 707      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4840     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15623    |\n",
      "|    total_timesteps | 2997528  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 3.4      |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | 0.514    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2997427  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=3264.68 +/- 373.77\n",
      "Episode length: 825.60 +/- 118.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 826      |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -0.201   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 707      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4850     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15661    |\n",
      "|    total_timesteps | 3004722  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.818    |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -0.384   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3004621  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3010000, episode_reward=3210.31 +/- 563.87\n",
      "Episode length: 813.00 +/- 158.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 813      |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 2.41     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.449    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4860     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15701    |\n",
      "|    total_timesteps | 3012304  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 2.04     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | 0.0477   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3012203  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3020000, episode_reward=3246.98 +/- 372.27\n",
      "Episode length: 861.20 +/- 136.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 861      |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.692    |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 0.515    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 732      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4870     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15743    |\n",
      "|    total_timesteps | 3020348  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 2.31     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3020247  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 740      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4880     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15782    |\n",
      "|    total_timesteps | 3027797  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.682    |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 0.551    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3027696  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3030000, episode_reward=3679.79 +/- 29.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | 0.407    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 745      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4890     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15824    |\n",
      "|    total_timesteps | 3035389  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.859    |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3035288  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=3744.26 +/- 151.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.925    |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 759      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4900     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15869    |\n",
      "|    total_timesteps | 3043724  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -1.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3043623  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3050000, episode_reward=3539.81 +/- 2.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.795    |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.273    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4910     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15916    |\n",
      "|    total_timesteps | 3051555  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.684    |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.139   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3051454  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060000, episode_reward=3630.58 +/- 122.87\n",
      "Episode length: 996.20 +/- 7.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 996      |\n",
      "|    mean_reward     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.204    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4920     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 15966    |\n",
      "|    total_timesteps | 3060347  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | 0.408    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3060246  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 782      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4930     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16011    |\n",
      "|    total_timesteps | 3068217  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.962    |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.334   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3068116  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3070000, episode_reward=3492.41 +/- 579.76\n",
      "Episode length: 920.40 +/- 159.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 920      |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.673    |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.453   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4940     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16062    |\n",
      "|    total_timesteps | 3076810  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 0.398    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3076709  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3080000, episode_reward=3385.68 +/- 836.03\n",
      "Episode length: 870.20 +/- 223.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 870      |\n",
      "|    mean_reward     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 0.477    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4950     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16100    |\n",
      "|    total_timesteps | 3083931  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | -0.396   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3083830  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4960     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16127    |\n",
      "|    total_timesteps | 3089325  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.984    |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.417    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3089224  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090000, episode_reward=3697.45 +/- 188.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 0.863    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4970     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16160    |\n",
      "|    total_timesteps | 3095435  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.795    |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.263    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3095334  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3100000, episode_reward=2575.95 +/- 874.08\n",
      "Episode length: 647.80 +/- 236.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 648      |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.959    |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | 0.225    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 736      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4980     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16192    |\n",
      "|    total_timesteps | 3101389  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 4.24     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 0.346    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3101288  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 729      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4990     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16230    |\n",
      "|    total_timesteps | 3108249  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.869    |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 0.465    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3108148  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3110000, episode_reward=2426.69 +/- 754.03\n",
      "Episode length: 581.60 +/- 174.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 582      |\n",
      "|    mean_reward     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.283   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 704      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5000     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16265    |\n",
      "|    total_timesteps | 3114145  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.854    |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | -0.935   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3114044  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 669      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5010     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16292    |\n",
      "|    total_timesteps | 3118429  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 0.73     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3118328  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3120000, episode_reward=2027.18 +/- 390.30\n",
      "Episode length: 491.20 +/- 90.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 491      |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -0.856   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 639      |\n",
      "|    ep_rew_mean     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5020     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16322    |\n",
      "|    total_timesteps | 3124206  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | 0.444    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3124105  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 604      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5030     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16345    |\n",
      "|    total_timesteps | 3128575  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.824    |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.194    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3128474  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3130000, episode_reward=2892.00 +/- 683.19\n",
      "Episode length: 716.80 +/- 185.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 717      |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | 1.62     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 570      |\n",
      "|    ep_rew_mean     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5040     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16372    |\n",
      "|    total_timesteps | 3133798  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 0.386    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3133697  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 561      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5050     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16404    |\n",
      "|    total_timesteps | 3139990  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.967    |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | 0.578    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3139889  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3140000, episode_reward=2638.64 +/- 710.87\n",
      "Episode length: 662.40 +/- 191.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 662      |\n",
      "|    mean_reward     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 0.707    |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 564      |\n",
      "|    ep_rew_mean     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5060     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16434    |\n",
      "|    total_timesteps | 3145700  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -1.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3145599  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3150000, episode_reward=3074.19 +/- 745.15\n",
      "Episode length: 762.60 +/- 195.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 763      |\n",
      "|    mean_reward     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.706    |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 588      |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5070     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16479    |\n",
      "|    total_timesteps | 3154261  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.946    |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | 0.234    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3154160  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 584      |\n",
      "|    ep_rew_mean     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5080     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16507    |\n",
      "|    total_timesteps | 3159796  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 0.252    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3159695  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3160000, episode_reward=2209.74 +/- 577.28\n",
      "Episode length: 543.40 +/- 140.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 543      |\n",
      "|    mean_reward     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.998    |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -0.317   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 574      |\n",
      "|    ep_rew_mean     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5090     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16537    |\n",
      "|    total_timesteps | 3165636  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 1.95     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.114    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3165535  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3170000, episode_reward=3331.74 +/- 698.29\n",
      "Episode length: 847.20 +/- 192.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 847      |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -0.543   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 562      |\n",
      "|    ep_rew_mean     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5100     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16562    |\n",
      "|    total_timesteps | 3170375  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.734    |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.787   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3170274  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 583      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5110     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16594    |\n",
      "|    total_timesteps | 3176728  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.909    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3176627  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3180000, episode_reward=3703.43 +/- 83.91\n",
      "Episode length: 963.80 +/- 45.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 964      |\n",
      "|    mean_reward     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.713    |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.124   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 589      |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5120     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16628    |\n",
      "|    total_timesteps | 3183153  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.868    |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.474   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3183052  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 610      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5130     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16660    |\n",
      "|    total_timesteps | 3189532  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.801    |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3189431  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3190000, episode_reward=1620.50 +/- 127.49\n",
      "Episode length: 404.60 +/- 26.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 405      |\n",
      "|    mean_reward     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.89     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.217   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 616      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5140     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16690    |\n",
      "|    total_timesteps | 3195350  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.587    |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.269   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3195249  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=1923.18 +/- 212.86\n",
      "Episode length: 469.80 +/- 45.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 470      |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.0882  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 617      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5150     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16723    |\n",
      "|    total_timesteps | 3201688  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.697   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3201587  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 614      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5160     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16751    |\n",
      "|    total_timesteps | 3207075  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.457   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3206974  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210000, episode_reward=3038.94 +/- 769.97\n",
      "Episode length: 763.80 +/- 214.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 764      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.676    |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.85    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5170     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16791    |\n",
      "|    total_timesteps | 3214460  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 9.01     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.251   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3214359  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3220000, episode_reward=1924.40 +/- 251.79\n",
      "Episode length: 466.60 +/- 55.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 467      |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 615      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5180     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16827    |\n",
      "|    total_timesteps | 3221270  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.383    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3221169  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 630      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5190     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16865    |\n",
      "|    total_timesteps | 3228661  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.192   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3228560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3230000, episode_reward=2460.03 +/- 702.51\n",
      "Episode length: 616.40 +/- 200.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 616      |\n",
      "|    mean_reward     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 5.38     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.751    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 651      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5200     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16901    |\n",
      "|    total_timesteps | 3235522  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.823    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3235421  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3240000, episode_reward=2254.22 +/- 789.63\n",
      "Episode length: 564.00 +/- 219.21\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 564      |\n",
      "|    mean_reward     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.188    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 646      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5210     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16932    |\n",
      "|    total_timesteps | 3241351  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.711    |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | -0.151   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3241250  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 636      |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5220     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16960    |\n",
      "|    total_timesteps | 3246753  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 0.402    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3246652  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3250000, episode_reward=2569.06 +/- 1071.42\n",
      "Episode length: 650.00 +/- 285.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 650      |\n",
      "|    mean_reward     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 643      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5230     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 16997    |\n",
      "|    total_timesteps | 3253834  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.886    |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.646   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3253733  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3260000, episode_reward=3634.29 +/- 432.08\n",
      "Episode length: 930.40 +/- 126.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 930      |\n",
      "|    mean_reward     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.235    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 659      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5240     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17037    |\n",
      "|    total_timesteps | 3261280  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.812    |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -0.634   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3261179  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 669      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5250     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17075    |\n",
      "|    total_timesteps | 3268610  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.822    |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3268509  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270000, episode_reward=3670.54 +/- 397.33\n",
      "Episode length: 941.00 +/- 118.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 941      |\n",
      "|    mean_reward     | 3.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.00537  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 699      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5260     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17118    |\n",
      "|    total_timesteps | 3276927  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.598   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3276826  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3280000, episode_reward=3654.77 +/- 330.40\n",
      "Episode length: 917.20 +/- 111.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 917      |\n",
      "|    mean_reward     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.0961  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 707      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5270     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17161    |\n",
      "|    total_timesteps | 3285198  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.737    |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.266    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3285097  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3290000, episode_reward=2192.05 +/- 803.37\n",
      "Episode length: 550.60 +/- 225.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 551      |\n",
      "|    mean_reward     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5280     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17205    |\n",
      "|    total_timesteps | 3293791  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.182   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3293690  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300000, episode_reward=2678.91 +/- 382.52\n",
      "Episode length: 651.40 +/- 95.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 651      |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.742    |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -0.436   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5290     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17249    |\n",
      "|    total_timesteps | 3302079  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 2.86     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -0.506   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3301978  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 740      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5300     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17293    |\n",
      "|    total_timesteps | 3309552  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.666    |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.711   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3309451  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3310000, episode_reward=3058.46 +/- 550.86\n",
      "Episode length: 742.80 +/- 125.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 743      |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.665    |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.657   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 5310     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17328    |\n",
      "|    total_timesteps | 3316362  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.845    |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3316261  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3320000, episode_reward=1707.34 +/- 118.47\n",
      "Episode length: 423.40 +/- 27.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 423      |\n",
      "|    mean_reward     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.522    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5320     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17362    |\n",
      "|    total_timesteps | 3322883  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3322782  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5330     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17397    |\n",
      "|    total_timesteps | 3329668  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 2.24     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.628    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3329567  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3330000, episode_reward=2932.95 +/- 780.14\n",
      "Episode length: 747.00 +/- 214.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 747      |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.789    |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.479   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 755      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5340     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17433    |\n",
      "|    total_timesteps | 3336829  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -1.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3336728  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3340000, episode_reward=3478.01 +/- 585.00\n",
      "Episode length: 846.00 +/- 141.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 846      |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 5.22     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 0.208    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5350     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17467    |\n",
      "|    total_timesteps | 3343612  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.748    |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -0.206   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3343511  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3350000, episode_reward=2989.31 +/- 606.64\n",
      "Episode length: 734.40 +/- 167.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 734      |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 0.748    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 737      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5360     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17503    |\n",
      "|    total_timesteps | 3350666  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.933    |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3350565  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 738      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5370     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17544    |\n",
      "|    total_timesteps | 3359016  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 2.53     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -0.131   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3358915  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=2839.23 +/- 680.90\n",
      "Episode length: 687.40 +/- 181.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 687      |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.742    |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -0.737   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5380     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17581    |\n",
      "|    total_timesteps | 3366316  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.984    |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.314    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3366215  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3370000, episode_reward=3507.77 +/- 664.38\n",
      "Episode length: 906.00 +/- 188.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 906      |\n",
      "|    mean_reward     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.288    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 715      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5390     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17618    |\n",
      "|    total_timesteps | 3373540  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.583   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3373439  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3380000, episode_reward=2897.81 +/- 754.52\n",
      "Episode length: 700.40 +/- 193.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 700      |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 2.09     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 0.671    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5400     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17655    |\n",
      "|    total_timesteps | 3380804  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.883    |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3380703  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 726      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5410     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17695    |\n",
      "|    total_timesteps | 3388956  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -0.132   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3388855  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3390000, episode_reward=3717.59 +/- 313.91\n",
      "Episode length: 950.40 +/- 99.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 950      |\n",
      "|    mean_reward     | 3.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -1.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 744      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5420     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17737    |\n",
      "|    total_timesteps | 3397239  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.823    |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.514    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3397138  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3400000, episode_reward=3959.53 +/- 136.93\n",
      "Episode length: 973.60 +/- 39.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 974      |\n",
      "|    mean_reward     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.66     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.149    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3399899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 759      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5430     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17780    |\n",
      "|    total_timesteps | 3405563  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3405462  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3410000, episode_reward=3936.49 +/- 61.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 744      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 5440     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17808    |\n",
      "|    total_timesteps | 3411198  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.584    |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.483    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3411097  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 744      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 5450     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17842    |\n",
      "|    total_timesteps | 3417978  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.201   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3417877  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3420000, episode_reward=655.28 +/- 802.51\n",
      "Episode length: 185.40 +/- 172.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 185      |\n",
      "|    mean_reward     | 655      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.832    |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 0.305    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 754      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5460     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17883    |\n",
      "|    total_timesteps | 3426105  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 0.885    |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.369   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3426004  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3430000, episode_reward=2778.27 +/- 761.50\n",
      "Episode length: 676.40 +/- 183.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 676      |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 0.791    |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.428   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 740      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5470     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17917    |\n",
      "|    total_timesteps | 3433027  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.641    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3432926  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3440000, episode_reward=2113.12 +/- 529.49\n",
      "Episode length: 512.40 +/- 121.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 512      |\n",
      "|    mean_reward     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 748      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5480     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17958    |\n",
      "|    total_timesteps | 3441155  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.331    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3441054  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5490     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 17995    |\n",
      "|    total_timesteps | 3448553  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.701    |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -0.633   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3448452  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3450000, episode_reward=3358.08 +/- 580.79\n",
      "Episode length: 837.20 +/- 163.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 837      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.0963   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5500     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18037    |\n",
      "|    total_timesteps | 3456922  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 0.295    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3456821  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3460000, episode_reward=40.65 +/- 0.58\n",
      "Episode length: 33.40 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 33.4     |\n",
      "|    mean_reward     | 40.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | 0.937    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5510     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18082    |\n",
      "|    total_timesteps | 3466024  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.826    |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.269   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3465923  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3470000, episode_reward=3713.63 +/- 247.13\n",
      "Episode length: 953.40 +/- 93.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 953      |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | 0.101    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5520     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18119    |\n",
      "|    total_timesteps | 3473333  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 1.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3473232  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480000, episode_reward=3903.52 +/- 46.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.811    |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.988    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5530     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18156    |\n",
      "|    total_timesteps | 3480661  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.621    |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.00388  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3480560  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5540     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18192    |\n",
      "|    total_timesteps | 3487673  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.451    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3487572  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3490000, episode_reward=3452.05 +/- 603.84\n",
      "Episode length: 843.60 +/- 150.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 844      |\n",
      "|    mean_reward     | 3.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -0.629   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5550     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18231    |\n",
      "|    total_timesteps | 3495456  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.847    |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.305   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3495355  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500000, episode_reward=3697.84 +/- 504.34\n",
      "Episode length: 916.60 +/- 139.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 917      |\n",
      "|    mean_reward     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | 0.804    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5560     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18273    |\n",
      "|    total_timesteps | 3503667  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.202   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3503566  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3510000, episode_reward=3444.33 +/- 689.55\n",
      "Episode length: 868.80 +/- 190.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 869      |\n",
      "|    mean_reward     | 3.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.644    |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 0.132    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5570     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18311    |\n",
      "|    total_timesteps | 3511170  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.778   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3511069  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 759      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5580     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18340    |\n",
      "|    total_timesteps | 3517048  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.182    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3516947  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=3850.69 +/- 100.71\n",
      "Episode length: 996.20 +/- 7.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 996      |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -0.116   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5590     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18373    |\n",
      "|    total_timesteps | 3523413  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.134    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3523312  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3530000, episode_reward=2478.03 +/- 67.76\n",
      "Episode length: 595.00 +/- 17.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 595      |\n",
      "|    mean_reward     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.038    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 740      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5600     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18411    |\n",
      "|    total_timesteps | 3530971  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.892    |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.182   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3530870  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 727      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5610     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18449    |\n",
      "|    total_timesteps | 3538690  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.114    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3538589  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3540000, episode_reward=3722.20 +/- 399.30\n",
      "Episode length: 902.40 +/- 96.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 902      |\n",
      "|    mean_reward     | 3.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.365    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 734      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 5620     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18490    |\n",
      "|    total_timesteps | 3546772  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.57     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3546671  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3550000, episode_reward=2541.94 +/- 464.14\n",
      "Episode length: 612.60 +/- 108.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 613      |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 3.68     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 1.63     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5630     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18525    |\n",
      "|    total_timesteps | 3553794  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.829    |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.415   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3553693  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3560000, episode_reward=2647.80 +/- 370.32\n",
      "Episode length: 634.40 +/- 85.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 634      |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 0.087    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 727      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5640     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18558    |\n",
      "|    total_timesteps | 3560337  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.574    |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.757   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3560236  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 705      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5650     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18586    |\n",
      "|    total_timesteps | 3565967  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 2.64     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 0.242    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3565866  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570000, episode_reward=1945.08 +/- 122.36\n",
      "Episode length: 496.40 +/- 27.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 496      |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.801    |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 693      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5660     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18621    |\n",
      "|    total_timesteps | 3572974  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.838    |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.115    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3572873  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3580000, episode_reward=2693.19 +/- 353.04\n",
      "Episode length: 653.40 +/- 85.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 653      |\n",
      "|    mean_reward     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.786    |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.0961  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 693      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5670     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18659    |\n",
      "|    total_timesteps | 3580453  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.608    |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.744   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3580352  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 715      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5680     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18699    |\n",
      "|    total_timesteps | 3588502  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.835    |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | 0.0274   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3588401  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3590000, episode_reward=3519.64 +/- 299.28\n",
      "Episode length: 845.40 +/- 72.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 845      |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.327   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 729      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5690     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18738    |\n",
      "|    total_timesteps | 3596343  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.944    |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.796   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3596242  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3600000, episode_reward=2057.56 +/- 342.81\n",
      "Episode length: 506.00 +/- 84.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 506      |\n",
      "|    mean_reward     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.997    |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -0.401   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 715      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5700     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18770    |\n",
      "|    total_timesteps | 3602501  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.672    |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 1.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3602400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3610000, episode_reward=3938.81 +/- 83.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.644    |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.951    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 722      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5710     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18812    |\n",
      "|    total_timesteps | 3610908  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.645    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3610807  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 716      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5720     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18849    |\n",
      "|    total_timesteps | 3618338  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.704    |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.114    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3618237  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3620000, episode_reward=3007.32 +/- 538.02\n",
      "Episode length: 733.00 +/- 120.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 733      |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.504    |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -1.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 727      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5730     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18890    |\n",
      "|    total_timesteps | 3626459  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.374    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3626358  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3630000, episode_reward=1037.64 +/- 538.81\n",
      "Episode length: 268.60 +/- 107.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 269      |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.678    |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.147   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 744      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5740     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 18931    |\n",
      "|    total_timesteps | 3634729  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.062   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3634628  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3640000, episode_reward=2505.71 +/- 159.41\n",
      "Episode length: 614.20 +/- 37.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 614      |\n",
      "|    mean_reward     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | -0.0275  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5750     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 18968    |\n",
      "|    total_timesteps | 3641987  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 0.698    |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.0864   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3641886  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3650000, episode_reward=4086.00 +/- 76.81\n",
      "Episode length: 985.20 +/- 18.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 985      |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.647    |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -0.335   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3649899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5760     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19008    |\n",
      "|    total_timesteps | 3650014  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.929    |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.204   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3649913  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5770     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19042    |\n",
      "|    total_timesteps | 3656885  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.547    |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.725   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3656784  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660000, episode_reward=3978.70 +/- 110.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.772    |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.245   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 763      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5780     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19083    |\n",
      "|    total_timesteps | 3664764  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.568    |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.223   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3664663  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3670000, episode_reward=3103.73 +/- 573.15\n",
      "Episode length: 743.60 +/- 134.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 744      |\n",
      "|    mean_reward     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.903   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5790     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19121    |\n",
      "|    total_timesteps | 3672293  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.178   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3672192  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=3940.27 +/- 80.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.201   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 778      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5800     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19162    |\n",
      "|    total_timesteps | 3680343  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.232   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3680242  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5810     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19201    |\n",
      "|    total_timesteps | 3688302  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.00863 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3688201  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690000, episode_reward=3153.62 +/- 806.89\n",
      "Episode length: 758.80 +/- 199.67\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 759      |\n",
      "|    mean_reward     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.661    |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.722    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5820     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19242    |\n",
      "|    total_timesteps | 3696245  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.987    |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.625    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3696144  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3700000, episode_reward=4019.49 +/- 69.53\n",
      "Episode length: 995.00 +/- 10.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 995      |\n",
      "|    mean_reward     | 4.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.764    |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 766      |\n",
      "|    ep_rew_mean     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5830     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19276    |\n",
      "|    total_timesteps | 3703020  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.969    |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3702919  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5840     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19310    |\n",
      "|    total_timesteps | 3709858  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 0.548    |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -0.137   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3709757  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3710000, episode_reward=3062.42 +/- 236.46\n",
      "Episode length: 724.20 +/- 55.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 724      |\n",
      "|    mean_reward     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.777    |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -0.123   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 747      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5850     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19345    |\n",
      "|    total_timesteps | 3716696  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | 0.245    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3716595  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3720000, episode_reward=2376.38 +/- 264.47\n",
      "Episode length: 567.60 +/- 61.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 568      |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 0.529    |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.254    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5860     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19377    |\n",
      "|    total_timesteps | 3723096  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.542   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3722995  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5870     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19408    |\n",
      "|    total_timesteps | 3729422  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.754    |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.733   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3729321  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3730000, episode_reward=3436.73 +/- 420.97\n",
      "Episode length: 808.40 +/- 96.97\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 808      |\n",
      "|    mean_reward     | 3.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | 0.397    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5880     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19442    |\n",
      "|    total_timesteps | 3735989  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -368     |\n",
      "|    critic_loss     | 0.647    |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 0.321    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3735888  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3740000, episode_reward=2113.02 +/- 150.57\n",
      "Episode length: 509.60 +/- 35.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 510      |\n",
      "|    mean_reward     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 699      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5890     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19473    |\n",
      "|    total_timesteps | 3742206  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.662    |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 0.102    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3742105  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 693      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5900     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19510    |\n",
      "|    total_timesteps | 3749646  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.518    |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3749545  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750000, episode_reward=1861.31 +/- 168.71\n",
      "Episode length: 451.40 +/- 38.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 451      |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.893    |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -0.301   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5910     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19542    |\n",
      "|    total_timesteps | 3755971  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.64     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.0282   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3755870  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3760000, episode_reward=4009.78 +/- 58.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.85     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -0.324   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 669      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5920     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19579    |\n",
      "|    total_timesteps | 3763170  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | 1.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3763069  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 665      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5930     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19610    |\n",
      "|    total_timesteps | 3769493  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.612    |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | 0.478    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3769392  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3770000, episode_reward=3024.46 +/- 265.57\n",
      "Episode length: 714.80 +/- 63.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 715      |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.163   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 672      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5940     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19648    |\n",
      "|    total_timesteps | 3777019  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 0.565    |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.564   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3776918  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3780000, episode_reward=2893.99 +/- 465.30\n",
      "Episode length: 684.60 +/- 108.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 685      |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 0.403    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 668      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5950     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19681    |\n",
      "|    total_timesteps | 3783501  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.901    |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.336    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3783400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3790000, episode_reward=2828.75 +/- 255.06\n",
      "Episode length: 668.40 +/- 57.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 668      |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.76     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 0.242    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 671      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5960     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19715    |\n",
      "|    total_timesteps | 3790211  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.455    |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 0.365    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3790110  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 663      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5970     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19742    |\n",
      "|    total_timesteps | 3795752  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.245    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3795651  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3800000, episode_reward=2586.99 +/- 294.69\n",
      "Episode length: 614.20 +/- 68.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 614      |\n",
      "|    mean_reward     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.787    |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 0.383    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 647      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5980     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19768    |\n",
      "|    total_timesteps | 3800728  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | 0.555    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3800627  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 652      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5990     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19801    |\n",
      "|    total_timesteps | 3807391  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 0.699    |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | -0.037   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3807290  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810000, episode_reward=2517.74 +/- 701.81\n",
      "Episode length: 597.00 +/- 162.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 597      |\n",
      "|    mean_reward     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 0.542    |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | 0.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 640      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6000     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19832    |\n",
      "|    total_timesteps | 3813660  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.564    |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.211   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3813559  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 636      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6010     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19862    |\n",
      "|    total_timesteps | 3819591  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.752    |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 0.688    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3819490  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3820000, episode_reward=3189.62 +/- 692.48\n",
      "Episode length: 760.80 +/- 169.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 761      |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 0.61     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 620      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6020     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19890    |\n",
      "|    total_timesteps | 3825131  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.0219  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3825030  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3830000, episode_reward=3016.76 +/- 60.36\n",
      "Episode length: 708.00 +/- 14.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 708      |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 3.08     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 0.574    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 620      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6030     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19922    |\n",
      "|    total_timesteps | 3831483  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.669    |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | 0.0672   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3831382  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 594      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6040     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19946    |\n",
      "|    total_timesteps | 3836404  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | 0.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3836303  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=3167.87 +/- 611.27\n",
      "Episode length: 745.60 +/- 139.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 746      |\n",
      "|    mean_reward     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.816    |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 581      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6050     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 19973    |\n",
      "|    total_timesteps | 3841616  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.881    |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.828    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3841515  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 580      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6060     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20005    |\n",
      "|    total_timesteps | 3848182  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.797    |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.383   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3848081  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850000, episode_reward=2741.14 +/- 833.01\n",
      "Episode length: 661.20 +/- 198.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 661      |\n",
      "|    mean_reward     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.698    |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | 0.721    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 568      |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6070     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20028    |\n",
      "|    total_timesteps | 3852517  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.975    |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.487    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3852416  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 573      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6080     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20055    |\n",
      "|    total_timesteps | 3858052  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 0.189    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3857951  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3860000, episode_reward=1889.76 +/- 186.35\n",
      "Episode length: 454.60 +/- 42.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 567      |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6090     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20085    |\n",
      "|    total_timesteps | 3864044  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 2.04     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.413    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3863943  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870000, episode_reward=1837.76 +/- 1276.49\n",
      "Episode length: 451.60 +/- 280.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 452      |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 0.0812   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 564      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6100     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20116    |\n",
      "|    total_timesteps | 3870069  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 1.6      |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3869968  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 550      |\n",
      "|    ep_rew_mean     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6110     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20139    |\n",
      "|    total_timesteps | 3874614  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.631    |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3874513  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 545      |\n",
      "|    ep_rew_mean     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6120     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20163    |\n",
      "|    total_timesteps | 3879588  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.756    |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -0.508   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3879487  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3880000, episode_reward=1543.76 +/- 8.71\n",
      "Episode length: 369.80 +/- 2.14\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 370      |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.827    |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.633    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 529      |\n",
      "|    ep_rew_mean     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6130     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20187    |\n",
      "|    total_timesteps | 3884399  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.661    |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 0.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3884298  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 523      |\n",
      "|    ep_rew_mean     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6140     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20209    |\n",
      "|    total_timesteps | 3888717  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.696    |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3888616  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890000, episode_reward=1675.54 +/- 129.26\n",
      "Episode length: 408.60 +/- 29.06\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 409      |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 0.701    |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 0.301    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 517      |\n",
      "|    ep_rew_mean     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6150     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20232    |\n",
      "|    total_timesteps | 3893340  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.569    |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.481   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3893239  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 497      |\n",
      "|    ep_rew_mean     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6160     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20255    |\n",
      "|    total_timesteps | 3897909  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 0.399    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3897808  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900000, episode_reward=1796.03 +/- 237.24\n",
      "Episode length: 431.00 +/- 54.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 431      |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.763    |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 498      |\n",
      "|    ep_rew_mean     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6170     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20277    |\n",
      "|    total_timesteps | 3902327  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.646    |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.285   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3902226  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 484      |\n",
      "|    ep_rew_mean     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6180     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20298    |\n",
      "|    total_timesteps | 3906501  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.578    |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.0108  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3906400  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3910000, episode_reward=1613.75 +/- 247.33\n",
      "Episode length: 380.00 +/- 54.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 380      |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.566    |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.639   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 469      |\n",
      "|    ep_rew_mean     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6190     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20320    |\n",
      "|    total_timesteps | 3910937  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 0.839    |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.145    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3910836  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 452      |\n",
      "|    ep_rew_mean     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6200     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20342    |\n",
      "|    total_timesteps | 3915282  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.804    |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -0.238   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3915181  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3920000, episode_reward=2623.96 +/- 163.07\n",
      "Episode length: 623.40 +/- 37.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 623      |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.0558   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 465      |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6210     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20371    |\n",
      "|    total_timesteps | 3921110  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.697    |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.199   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3921009  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 466      |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6220     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20396    |\n",
      "|    total_timesteps | 3926156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 0.76     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -0.438   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3926055  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3930000, episode_reward=1596.16 +/- 94.55\n",
      "Episode length: 384.80 +/- 20.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 385      |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.62     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -0.968   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 465      |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6230     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20420    |\n",
      "|    total_timesteps | 3930897  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 0.679    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3930796  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 479      |\n",
      "|    ep_rew_mean     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 6240     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20448    |\n",
      "|    total_timesteps | 3936612  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.591    |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.163   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3936511  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3940000, episode_reward=2799.38 +/- 191.59\n",
      "Episode length: 666.00 +/- 43.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 666      |\n",
      "|    mean_reward     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.469    |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -0.75    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 479      |\n",
      "|    ep_rew_mean     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6250     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20472    |\n",
      "|    total_timesteps | 3941269  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.824    |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 0.104    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3941168  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 490      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6260     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20500    |\n",
      "|    total_timesteps | 3946958  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3946857  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3950000, episode_reward=2093.63 +/- 437.61\n",
      "Episode length: 491.60 +/- 97.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 492      |\n",
      "|    mean_reward     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.357    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 508      |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6270     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20531    |\n",
      "|    total_timesteps | 3953152  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.0471  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3953051  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 523      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6280     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20559    |\n",
      "|    total_timesteps | 3958801  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 0.812    |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.369   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3958700  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960000, episode_reward=2419.31 +/- 923.31\n",
      "Episode length: 574.80 +/- 227.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 575      |\n",
      "|    mean_reward     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 2.5      |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.71     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 538      |\n",
      "|    ep_rew_mean     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6290     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20590    |\n",
      "|    total_timesteps | 3964748  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.756    |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.282   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3964647  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 534      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6300     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20609    |\n",
      "|    total_timesteps | 3968709  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.595    |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.0571   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3968608  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3970000, episode_reward=1608.03 +/- 99.31\n",
      "Episode length: 387.40 +/- 22.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 387      |\n",
      "|    mean_reward     | 1.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 535      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6310     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20639    |\n",
      "|    total_timesteps | 3974620  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.88     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.251    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3974519  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3980000, episode_reward=2549.03 +/- 609.92\n",
      "Episode length: 594.60 +/- 141.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 595      |\n",
      "|    mean_reward     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.251    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 539      |\n",
      "|    ep_rew_mean     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6320     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20666    |\n",
      "|    total_timesteps | 3980041  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.518    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3979940  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 537      |\n",
      "|    ep_rew_mean     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6330     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20689    |\n",
      "|    total_timesteps | 3984608  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.967    |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.186   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3984507  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 520      |\n",
      "|    ep_rew_mean     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6340     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20709    |\n",
      "|    total_timesteps | 3988597  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.585    |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.118    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3988496  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990000, episode_reward=1184.72 +/- 54.29\n",
      "Episode length: 284.40 +/- 10.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 284      |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.679    |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 0.159    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 523      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6350     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20734    |\n",
      "|    total_timesteps | 3993582  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 0.892    |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.648   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3993481  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 511      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6360     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20756    |\n",
      "|    total_timesteps | 3998027  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.572    |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.0644  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3997926  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000000, episode_reward=1798.94 +/- 253.63\n",
      "Episode length: 420.20 +/- 58.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 420      |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.772    |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.611    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 511      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6370     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20787    |\n",
      "|    total_timesteps | 4004268  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.368   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4004167  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4010000, episode_reward=2597.96 +/- 633.84\n",
      "Episode length: 604.00 +/- 147.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 604      |\n",
      "|    mean_reward     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.592    |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.0881  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 524      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6380     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20822    |\n",
      "|    total_timesteps | 4011154  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4011053  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 524      |\n",
      "|    ep_rew_mean     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6390     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20852    |\n",
      "|    total_timesteps | 4017134  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 4.13     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.472   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4017033  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4020000, episode_reward=2036.79 +/- 244.25\n",
      "Episode length: 473.20 +/- 58.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 473      |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.818    |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 0.643    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 529      |\n",
      "|    ep_rew_mean     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6400     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20874    |\n",
      "|    total_timesteps | 4021591  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.0322  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4021490  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 529      |\n",
      "|    ep_rew_mean     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6410     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20904    |\n",
      "|    total_timesteps | 4027559  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4027458  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4030000, episode_reward=1979.82 +/- 451.06\n",
      "Episode length: 462.20 +/- 101.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 462      |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.915    |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | 0.185    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 534      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6420     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20935    |\n",
      "|    total_timesteps | 4033398  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -0.105   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4033297  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 547      |\n",
      "|    ep_rew_mean     | 2.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6430     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 20964    |\n",
      "|    total_timesteps | 4039330  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.882    |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 0.276    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4039229  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4040000, episode_reward=2413.04 +/- 228.17\n",
      "Episode length: 561.40 +/- 54.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 561      |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.598    |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.519   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 577      |\n",
      "|    ep_rew_mean     | 2.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6440     |\n",
      "|    fps             | 192      |\n",
      "|    time_elapsed    | 21001    |\n",
      "|    total_timesteps | 4046346  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.805    |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.0689  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4046245  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050000, episode_reward=1735.54 +/- 517.65\n",
      "Episode length: 406.20 +/- 115.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 406      |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.785    |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.248    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 594      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6450     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21179    |\n",
      "|    total_timesteps | 4052979  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 0.841    |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.892    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4052878  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6460     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21206    |\n",
      "|    total_timesteps | 4058182  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.381    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4058081  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4060000, episode_reward=1977.18 +/- 1030.92\n",
      "Episode length: 473.60 +/- 263.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 474      |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 0.286    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 597      |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6470     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21237    |\n",
      "|    total_timesteps | 4063934  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.74     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4063833  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 582      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6480     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21274    |\n",
      "|    total_timesteps | 4069306  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.925    |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | -0.544   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4069205  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4070000, episode_reward=1436.49 +/- 72.88\n",
      "Episode length: 342.20 +/- 16.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 342      |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.855    |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | -0.518   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 598      |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6490     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21319    |\n",
      "|    total_timesteps | 4076884  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -0.185   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4076783  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4080000, episode_reward=2998.14 +/- 942.32\n",
      "Episode length: 716.20 +/- 237.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 716      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 2.89     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.722    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 611      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6500     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21350    |\n",
      "|    total_timesteps | 4082740  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.573    |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -0.926   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4082639  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 601      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6510     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21376    |\n",
      "|    total_timesteps | 4087625  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.809    |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 0.582    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4087524  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4090000, episode_reward=1249.97 +/- 14.60\n",
      "Episode length: 296.00 +/- 4.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 296      |\n",
      "|    mean_reward     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.861    |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | 1.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 595      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6520     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21403    |\n",
      "|    total_timesteps | 4092874  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.923    |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4092773  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 591      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6530     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21431    |\n",
      "|    total_timesteps | 4098476  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.781    |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.574    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4098375  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4100000, episode_reward=1957.12 +/- 188.80\n",
      "Episode length: 453.40 +/- 45.69\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 453      |\n",
      "|    mean_reward     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 0.887    |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | 0.362    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 579      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6540     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21461    |\n",
      "|    total_timesteps | 4104242  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.84     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4104141  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110000, episode_reward=2791.76 +/- 638.57\n",
      "Episode length: 652.20 +/- 140.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 652      |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.656    |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | -0.187   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 571      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6550     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21494    |\n",
      "|    total_timesteps | 4110087  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.69     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.104   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4109986  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 570      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6560     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21521    |\n",
      "|    total_timesteps | 4115177  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 0.988    |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | -0.365   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4115076  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4120000, episode_reward=1523.77 +/- 446.10\n",
      "Episode length: 359.60 +/- 99.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 360      |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 91.8     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -1.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 565      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6570     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21549    |\n",
      "|    total_timesteps | 4120396  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 2.04     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -0.291   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4120295  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 564      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6580     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21577    |\n",
      "|    total_timesteps | 4125693  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.959    |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.314    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4125592  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 530      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6590     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21599    |\n",
      "|    total_timesteps | 4129867  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | -0.623   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4129766  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4130000, episode_reward=2146.55 +/- 656.69\n",
      "Episode length: 497.60 +/- 146.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 498      |\n",
      "|    mean_reward     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.912    |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -0.472   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 529      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6600     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21632    |\n",
      "|    total_timesteps | 4135643  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.45     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | 0.899    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4135542  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4140000, episode_reward=2562.15 +/- 109.80\n",
      "Episode length: 593.60 +/- 25.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 594      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.987    |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.663    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 535      |\n",
      "|    ep_rew_mean     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6610     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21665    |\n",
      "|    total_timesteps | 4141162  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.977    |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -0.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4141061  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 538      |\n",
      "|    ep_rew_mean     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6620     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21694    |\n",
      "|    total_timesteps | 4146667  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 2.31     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.0914  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4146566  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4150000, episode_reward=2236.52 +/- 311.88\n",
      "Episode length: 524.40 +/- 66.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 524      |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.744    |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.494    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 529      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6630     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21719    |\n",
      "|    total_timesteps | 4151419  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 4.28     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | 0.085    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4151318  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 527      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6640     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21747    |\n",
      "|    total_timesteps | 4156917  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.9      |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | -0.809   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4156816  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4160000, episode_reward=2996.01 +/- 511.30\n",
      "Episode length: 702.00 +/- 122.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 702      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.777    |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | -0.417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 527      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6650     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21778    |\n",
      "|    total_timesteps | 4162786  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.842   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4162685  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 544      |\n",
      "|    ep_rew_mean     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6660     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21813    |\n",
      "|    total_timesteps | 4169577  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | -0.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4169476  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4170000, episode_reward=2919.75 +/- 813.83\n",
      "Episode length: 680.20 +/- 188.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 680      |\n",
      "|    mean_reward     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -368     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | 0.684    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 546      |\n",
      "|    ep_rew_mean     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6670     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21842    |\n",
      "|    total_timesteps | 4175031  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.857    |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | -0.706   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4174930  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4180000, episode_reward=3234.59 +/- 647.93\n",
      "Episode length: 759.60 +/- 159.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 760      |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 0.97     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 0.0914   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 545      |\n",
      "|    ep_rew_mean     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6680     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21869    |\n",
      "|    total_timesteps | 4180192  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | 0.918    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4180091  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 563      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6690     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21900    |\n",
      "|    total_timesteps | 4186154  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 0.251    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4186053  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4190000, episode_reward=3433.96 +/- 504.38\n",
      "Episode length: 797.80 +/- 123.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 798      |\n",
      "|    mean_reward     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | 1.46     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 557      |\n",
      "|    ep_rew_mean     | 2.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6700     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21928    |\n",
      "|    total_timesteps | 4191324  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | 0.268    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4191223  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 547      |\n",
      "|    ep_rew_mean     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6710     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21952    |\n",
      "|    total_timesteps | 4195880  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | -0.162   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4195779  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200000, episode_reward=1688.32 +/- 277.09\n",
      "Episode length: 396.20 +/- 60.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 396      |\n",
      "|    mean_reward     | 1.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 4.87     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | 0.395    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 536      |\n",
      "|    ep_rew_mean     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6720     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 21975    |\n",
      "|    total_timesteps | 4200278  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.724    |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | -0.909   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4200177  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 544      |\n",
      "|    ep_rew_mean     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6730     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22003    |\n",
      "|    total_timesteps | 4205819  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | 1.66     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4205718  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4210000, episode_reward=1987.04 +/- 234.33\n",
      "Episode length: 464.80 +/- 49.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 465      |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 4.23     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.911    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 548      |\n",
      "|    ep_rew_mean     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6740     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22035    |\n",
      "|    total_timesteps | 4211671  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | 0.469    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4211570  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 544      |\n",
      "|    ep_rew_mean     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6750     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22064    |\n",
      "|    total_timesteps | 4217227  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.725    |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | 0.455    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4217126  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4220000, episode_reward=2160.30 +/- 98.31\n",
      "Episode length: 506.40 +/- 22.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 506      |\n",
      "|    mean_reward     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 2.21     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | -0.00232 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 518      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6760     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22086    |\n",
      "|    total_timesteps | 4221347  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.0633   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4221246  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 510      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6770     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22110    |\n",
      "|    total_timesteps | 4226019  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.961    |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 0.865    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4225918  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4230000, episode_reward=1598.08 +/- 86.95\n",
      "Episode length: 370.20 +/- 20.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 370      |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.729    |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | 0.753    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 510      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6780     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22137    |\n",
      "|    total_timesteps | 4231213  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.85     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.0943  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4231112  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 494      |\n",
      "|    ep_rew_mean     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6790     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22160    |\n",
      "|    total_timesteps | 4235590  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -0.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4235489  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4240000, episode_reward=2042.64 +/- 419.26\n",
      "Episode length: 478.20 +/- 95.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 478      |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | 0.417    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 492      |\n",
      "|    ep_rew_mean     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6800     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22186    |\n",
      "|    total_timesteps | 4240482  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 0.981    |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4240381  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 485      |\n",
      "|    ep_rew_mean     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6810     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22206    |\n",
      "|    total_timesteps | 4244420  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -0.636   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4244319  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 479      |\n",
      "|    ep_rew_mean     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6820     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22225    |\n",
      "|    total_timesteps | 4248173  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0416   |\n",
      "|    ent_coef_loss   | -0.384   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4248072  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4250000, episode_reward=2891.24 +/- 586.69\n",
      "Episode length: 699.00 +/- 167.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 699      |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.935    |\n",
      "|    ent_coef        | 0.0407   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 467      |\n",
      "|    ep_rew_mean     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6830     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22249    |\n",
      "|    total_timesteps | 4252541  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.894    |\n",
      "|    ent_coef        | 0.0407   |\n",
      "|    ent_coef_loss   | -0.0424  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4252440  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 460      |\n",
      "|    ep_rew_mean     | 1.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6840     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22277    |\n",
      "|    total_timesteps | 4257715  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 0.847    |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | 0.664    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4257614  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4260000, episode_reward=1681.75 +/- 257.39\n",
      "Episode length: 393.60 +/- 62.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 394      |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.978    |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | -0.378   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 451      |\n",
      "|    ep_rew_mean     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6850     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22301    |\n",
      "|    total_timesteps | 4262327  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4262226  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 470      |\n",
      "|    ep_rew_mean     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6860     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22334    |\n",
      "|    total_timesteps | 4268393  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.859    |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | 0.0246   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4268292  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4270000, episode_reward=2338.62 +/- 435.49\n",
      "Episode length: 544.00 +/- 98.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 544      |\n",
      "|    mean_reward     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 0.81     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.132   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 472      |\n",
      "|    ep_rew_mean     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 6870     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22359    |\n",
      "|    total_timesteps | 4273221  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.289    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4273120  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 473      |\n",
      "|    ep_rew_mean     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6880     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22387    |\n",
      "|    total_timesteps | 4278526  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.392    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4278425  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4280000, episode_reward=2511.41 +/- 754.11\n",
      "Episode length: 581.80 +/- 168.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 582      |\n",
      "|    mean_reward     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.68     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | 0.636    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 486      |\n",
      "|    ep_rew_mean     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6890     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22423    |\n",
      "|    total_timesteps | 4284227  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.973    |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | 0.0461   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4284126  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 495      |\n",
      "|    ep_rew_mean     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6900     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22453    |\n",
      "|    total_timesteps | 4289973  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.835    |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | -0.103   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4289872  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4290000, episode_reward=3933.52 +/- 70.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | 0.881    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 499      |\n",
      "|    ep_rew_mean     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6910     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22476    |\n",
      "|    total_timesteps | 4294283  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.855    |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | 0.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4294182  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 508      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6920     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22503    |\n",
      "|    total_timesteps | 4299001  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -0.0222  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4298900  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4300000, episode_reward=2218.89 +/- 404.66\n",
      "Episode length: 516.00 +/- 95.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 516      |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.808    |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 513      |\n",
      "|    ep_rew_mean     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6930     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22531    |\n",
      "|    total_timesteps | 4303879  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.419    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4303778  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 510      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6940     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22556    |\n",
      "|    total_timesteps | 4308715  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.979    |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.0322  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4308614  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4310000, episode_reward=2760.80 +/- 590.78\n",
      "Episode length: 642.20 +/- 133.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 642      |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | -0.404   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 514      |\n",
      "|    ep_rew_mean     | 2.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6950     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22583    |\n",
      "|    total_timesteps | 4313769  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 0.73     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | -0.559   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4313668  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6960     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22608    |\n",
      "|    total_timesteps | 4318443  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 8.81     |\n",
      "|    ent_coef        | 0.0411   |\n",
      "|    ent_coef_loss   | 0.038    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4318342  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320000, episode_reward=2409.38 +/- 258.68\n",
      "Episode length: 559.00 +/- 58.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 559      |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 0.837    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 510      |\n",
      "|    ep_rew_mean     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6970     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22638    |\n",
      "|    total_timesteps | 4324253  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.87     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 0.459    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4324152  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 506      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6980     |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 22665    |\n",
      "|    total_timesteps | 4329145  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.846    |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | 0.0966   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4329044  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4330000, episode_reward=2710.05 +/- 314.56\n",
      "Episode length: 632.40 +/- 71.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 632      |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.716    |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 503      |\n",
      "|    ep_rew_mean     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6990     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 22695    |\n",
      "|    total_timesteps | 4334561  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4334460  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4340000, episode_reward=1839.80 +/- 427.24\n",
      "Episode length: 442.20 +/- 94.44\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 442      |\n",
      "|    mean_reward     | 1.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 506      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7000     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 22729    |\n",
      "|    total_timesteps | 4340535  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | -1.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4340434  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 519      |\n",
      "|    ep_rew_mean     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7010     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 22762    |\n",
      "|    total_timesteps | 4346216  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.703    |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | -0.731   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4346115  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4350000, episode_reward=2317.42 +/- 394.34\n",
      "Episode length: 554.80 +/- 89.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 555      |\n",
      "|    mean_reward     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.781    |\n",
      "|    ent_coef        | 0.0413   |\n",
      "|    ent_coef_loss   | -0.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 526      |\n",
      "|    ep_rew_mean     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7020     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 22790    |\n",
      "|    total_timesteps | 4351560  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0419   |\n",
      "|    ent_coef_loss   | 0.636    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4351459  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 549      |\n",
      "|    ep_rew_mean     | 2.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7030     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 22827    |\n",
      "|    total_timesteps | 4358788  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | 0.225    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4358687  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4360000, episode_reward=2245.80 +/- 210.40\n",
      "Episode length: 527.40 +/- 48.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 527      |\n",
      "|    mean_reward     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.881    |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | 0.163    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 579      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7040     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 22868    |\n",
      "|    total_timesteps | 4366661  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.713    |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -0.368   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4366560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4370000, episode_reward=3605.25 +/- 583.97\n",
      "Episode length: 919.40 +/- 161.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 919      |\n",
      "|    mean_reward     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.918    |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | -0.347   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 612      |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7050     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 22912    |\n",
      "|    total_timesteps | 4374958  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.728    |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | -0.285   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4374857  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380000, episode_reward=3880.34 +/- 173.80\n",
      "Episode length: 969.40 +/- 61.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 969      |\n",
      "|    mean_reward     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0411   |\n",
      "|    ent_coef_loss   | -0.407   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 621      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7060     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 22943    |\n",
      "|    total_timesteps | 4380558  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0424   |\n",
      "|    ent_coef_loss   | -0.016   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4380457  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 622      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7070     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 22975    |\n",
      "|    total_timesteps | 4386418  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 0.227    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4386317  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4390000, episode_reward=2174.34 +/- 778.32\n",
      "Episode length: 517.20 +/- 171.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 517      |\n",
      "|    mean_reward     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0433   |\n",
      "|    ent_coef_loss   | 0.0516   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 651      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7080     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23015    |\n",
      "|    total_timesteps | 4394238  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | -0.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4394137  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4400000, episode_reward=3330.08 +/- 624.92\n",
      "Episode length: 816.60 +/- 169.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 817      |\n",
      "|    mean_reward     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 2.25     |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | 0.248    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 666      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7090     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23052    |\n",
      "|    total_timesteps | 4401139  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 0.84     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | -0.463   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4401038  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 675      |\n",
      "|    ep_rew_mean     | 2.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7100     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23088    |\n",
      "|    total_timesteps | 4408081  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0429   |\n",
      "|    ent_coef_loss   | -0.679   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4407980  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410000, episode_reward=3537.37 +/- 414.02\n",
      "Episode length: 852.20 +/- 130.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 852      |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | 0.302    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 680      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7110     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23125    |\n",
      "|    total_timesteps | 4414230  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.824    |\n",
      "|    ent_coef        | 0.0419   |\n",
      "|    ent_coef_loss   | -0.484   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4414129  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7120     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23151    |\n",
      "|    total_timesteps | 4419255  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | -0.543   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4419154  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4420000, episode_reward=2167.30 +/- 43.30\n",
      "Episode length: 506.20 +/- 10.23\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 506      |\n",
      "|    mean_reward     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.961    |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -1.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 662      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7130     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23181    |\n",
      "|    total_timesteps | 4424947  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0415   |\n",
      "|    ent_coef_loss   | 0.279    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4424846  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4430000, episode_reward=2720.50 +/- 681.56\n",
      "Episode length: 631.20 +/- 165.43\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 631      |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.748    |\n",
      "|    ent_coef        | 0.0427   |\n",
      "|    ent_coef_loss   | 0.327    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 643      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7140     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23213    |\n",
      "|    total_timesteps | 4430960  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.935    |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | 0.215    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4430859  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 619      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7150     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23244    |\n",
      "|    total_timesteps | 4436855  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 0.894    |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | 0.169    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4436754  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4440000, episode_reward=3676.00 +/- 812.91\n",
      "Episode length: 895.80 +/- 208.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 896      |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 2.21     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | 0.599    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 618      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7160     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23277    |\n",
      "|    total_timesteps | 4442354  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.659    |\n",
      "|    ent_coef        | 0.0433   |\n",
      "|    ent_coef_loss   | -0.222   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4442253  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 608      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7170     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23304    |\n",
      "|    total_timesteps | 4447182  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.86     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | -0.114   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4447081  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4450000, episode_reward=2680.55 +/- 726.57\n",
      "Episode length: 621.20 +/- 166.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 621      |\n",
      "|    mean_reward     | 2.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -0.0676  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 586      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7180     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23334    |\n",
      "|    total_timesteps | 4452818  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | 0.682    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4452717  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 585      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7190     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23370    |\n",
      "|    total_timesteps | 4459655  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0407   |\n",
      "|    ent_coef_loss   | -0.946   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4459554  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4460000, episode_reward=2645.17 +/- 708.25\n",
      "Episode length: 606.00 +/- 158.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 606      |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    ent_coef        | 0.0413   |\n",
      "|    ent_coef_loss   | 0.118    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 573      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7200     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23401    |\n",
      "|    total_timesteps | 4465345  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.582    |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -0.254   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4465244  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470000, episode_reward=2243.40 +/- 88.72\n",
      "Episode length: 516.60 +/- 21.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 517      |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 4.08     |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | 0.738    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 560      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7210     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23428    |\n",
      "|    total_timesteps | 4470270  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0424   |\n",
      "|    ent_coef_loss   | 0.0114   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4470169  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 568      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7220     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23458    |\n",
      "|    total_timesteps | 4476086  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 0.795    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4475985  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4480000, episode_reward=2728.10 +/- 315.50\n",
      "Episode length: 627.20 +/- 71.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 627      |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | 0.132    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 563      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7230     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23486    |\n",
      "|    total_timesteps | 4481266  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.689    |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -0.674   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4481165  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 551      |\n",
      "|    ep_rew_mean     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7240     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23512    |\n",
      "|    total_timesteps | 4486019  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.797    |\n",
      "|    ent_coef        | 0.0436   |\n",
      "|    ent_coef_loss   | -0.324   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4485918  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4490000, episode_reward=3885.11 +/- 590.13\n",
      "Episode length: 905.80 +/- 142.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 906      |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -0.874   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 543      |\n",
      "|    ep_rew_mean     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7250     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23540    |\n",
      "|    total_timesteps | 4491169  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 2.77     |\n",
      "|    ent_coef        | 0.0411   |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4491068  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 548      |\n",
      "|    ep_rew_mean     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7260     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23571    |\n",
      "|    total_timesteps | 4497114  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0415   |\n",
      "|    ent_coef_loss   | -0.114   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4497013  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500000, episode_reward=1885.21 +/- 118.09\n",
      "Episode length: 438.60 +/- 26.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 439      |\n",
      "|    mean_reward     | 1.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.929    |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 560      |\n",
      "|    ep_rew_mean     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7270     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23605    |\n",
      "|    total_timesteps | 4503228  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 0.921    |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | 0.309    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4503127  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 569      |\n",
      "|    ep_rew_mean     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7280     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23641    |\n",
      "|    total_timesteps | 4509729  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | 0.464    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4509628  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4510000, episode_reward=2053.44 +/- 54.38\n",
      "Episode length: 472.80 +/- 12.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 473      |\n",
      "|    mean_reward     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0435   |\n",
      "|    ent_coef_loss   | 0.0515   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 556      |\n",
      "|    ep_rew_mean     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7290     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23672    |\n",
      "|    total_timesteps | 4515239  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0428   |\n",
      "|    ent_coef_loss   | -0.665   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4515138  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4520000, episode_reward=1942.03 +/- 235.15\n",
      "Episode length: 445.60 +/- 52.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 446      |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0433   |\n",
      "|    ent_coef_loss   | 0.871    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 556      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7300     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23705    |\n",
      "|    total_timesteps | 4520975  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4520874  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 572      |\n",
      "|    ep_rew_mean     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7310     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23739    |\n",
      "|    total_timesteps | 4527496  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.914    |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | 0.615    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4527395  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4530000, episode_reward=3542.45 +/- 364.39\n",
      "Episode length: 819.60 +/- 95.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | 0.564    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 574      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7320     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23772    |\n",
      "|    total_timesteps | 4533487  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.822    |\n",
      "|    ent_coef        | 0.0446   |\n",
      "|    ent_coef_loss   | -0.482   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4533386  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 581      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7330     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23804    |\n",
      "|    total_timesteps | 4539386  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.769    |\n",
      "|    ent_coef        | 0.0425   |\n",
      "|    ent_coef_loss   | -0.245   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4539285  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4540000, episode_reward=3550.34 +/- 638.22\n",
      "Episode length: 819.40 +/- 155.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 819      |\n",
      "|    mean_reward     | 3.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0428   |\n",
      "|    ent_coef_loss   | 0.0988   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 601      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7340     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23842    |\n",
      "|    total_timesteps | 4546093  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.776    |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | -0.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4545992  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4550000, episode_reward=2069.34 +/- 184.65\n",
      "Episode length: 483.40 +/- 41.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 483      |\n",
      "|    mean_reward     | 2.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | 0.695    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 618      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7350     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23879    |\n",
      "|    total_timesteps | 4552960  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.725    |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | 0.754    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4552859  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 611      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7360     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23906    |\n",
      "|    total_timesteps | 4558199  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 0.398    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4558098  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4560000, episode_reward=1884.30 +/- 43.31\n",
      "Episode length: 441.60 +/- 11.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 442      |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.899    |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 609      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7370     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23938    |\n",
      "|    total_timesteps | 4564122  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.8      |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4564021  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4570000, episode_reward=1817.55 +/- 430.26\n",
      "Episode length: 431.60 +/- 82.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 432      |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -368     |\n",
      "|    critic_loss     | 6.72     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | 0.309    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 612      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7380     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23974    |\n",
      "|    total_timesteps | 4570896  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.729    |\n",
      "|    ent_coef        | 0.0429   |\n",
      "|    ent_coef_loss   | 0.425    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4570795  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 604      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7390     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 23999    |\n",
      "|    total_timesteps | 4575607  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0438   |\n",
      "|    ent_coef_loss   | 0.981    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4575506  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4580000, episode_reward=2344.22 +/- 761.13\n",
      "Episode length: 546.80 +/- 171.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 547      |\n",
      "|    mean_reward     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.631    |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | -0.106   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 601      |\n",
      "|    ep_rew_mean     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7400     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24029    |\n",
      "|    total_timesteps | 4581066  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | -0.347   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4580965  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 585      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7410     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24058    |\n",
      "|    total_timesteps | 4586029  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | -0.745   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4585928  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4590000, episode_reward=2083.20 +/- 348.39\n",
      "Episode length: 481.60 +/- 78.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 482      |\n",
      "|    mean_reward     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.817    |\n",
      "|    ent_coef        | 0.044    |\n",
      "|    ent_coef_loss   | -0.511   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 576      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7420     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24088    |\n",
      "|    total_timesteps | 4591059  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 3.27     |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | 0.0328   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4590958  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 578      |\n",
      "|    ep_rew_mean     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7430     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24124    |\n",
      "|    total_timesteps | 4597164  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0433   |\n",
      "|    ent_coef_loss   | -0.201   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4597063  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4600000, episode_reward=1879.96 +/- 136.95\n",
      "Episode length: 435.40 +/- 28.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 435      |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.903    |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | -0.286   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 577      |\n",
      "|    ep_rew_mean     | 2.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7440     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24163    |\n",
      "|    total_timesteps | 4603781  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0447   |\n",
      "|    ent_coef_loss   | -0.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4603680  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4610000, episode_reward=2698.62 +/- 612.07\n",
      "Episode length: 619.40 +/- 138.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 619      |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.694    |\n",
      "|    ent_coef        | 0.0451   |\n",
      "|    ent_coef_loss   | 0.303    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 577      |\n",
      "|    ep_rew_mean     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7450     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24203    |\n",
      "|    total_timesteps | 4610628  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 0.675    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4610527  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 587      |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7460     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24237    |\n",
      "|    total_timesteps | 4616898  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4616797  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620000, episode_reward=1882.59 +/- 162.06\n",
      "Episode length: 440.00 +/- 39.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 440      |\n",
      "|    mean_reward     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.902    |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | -0.135   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 589      |\n",
      "|    ep_rew_mean     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7470     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24274    |\n",
      "|    total_timesteps | 4623046  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 0.519    |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | -0.365   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4622945  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4630000, episode_reward=2259.79 +/- 601.68\n",
      "Episode length: 525.60 +/- 134.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 526      |\n",
      "|    mean_reward     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 592      |\n",
      "|    ep_rew_mean     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7480     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24316    |\n",
      "|    total_timesteps | 4630134  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | -0.263   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4630033  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 618      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7490     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24355    |\n",
      "|    total_timesteps | 4637390  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 0.961    |\n",
      "|    ent_coef        | 0.0441   |\n",
      "|    ent_coef_loss   | -0.355   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4637289  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4640000, episode_reward=1938.05 +/- 404.49\n",
      "Episode length: 443.80 +/- 89.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 444      |\n",
      "|    mean_reward     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 0.224    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 634      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7500     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24394    |\n",
      "|    total_timesteps | 4644463  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4644362  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4650000, episode_reward=4111.21 +/- 63.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.83     |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | -0.416   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4649899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 649      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7510     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24432    |\n",
      "|    total_timesteps | 4650937  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -0.0961  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4650836  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 664      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7520     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24470    |\n",
      "|    total_timesteps | 4657494  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | 0.0443   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4657393  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4660000, episode_reward=3642.40 +/- 508.15\n",
      "Episode length: 846.20 +/- 119.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 846      |\n",
      "|    mean_reward     | 3.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.638    |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.127    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 667      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7530     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24504    |\n",
      "|    total_timesteps | 4663845  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.612    |\n",
      "|    ent_coef        | 0.0438   |\n",
      "|    ent_coef_loss   | -0.0587  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4663744  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4670000, episode_reward=3225.74 +/- 301.41\n",
      "Episode length: 743.00 +/- 70.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 743      |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.657    |\n",
      "|    ent_coef        | 0.0442   |\n",
      "|    ent_coef_loss   | -0.697   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 686      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7540     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24550    |\n",
      "|    total_timesteps | 4672381  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0467   |\n",
      "|    ent_coef_loss   | -0.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4672280  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 691      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7550     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24588    |\n",
      "|    total_timesteps | 4679709  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.814    |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | 0.0305   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4679608  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680000, episode_reward=3312.88 +/- 1016.24\n",
      "Episode length: 793.00 +/- 253.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 793      |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0433   |\n",
      "|    ent_coef_loss   | 0.551    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 692      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7560     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24623    |\n",
      "|    total_timesteps | 4686097  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 3.3      |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 0.041    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4685996  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4690000, episode_reward=2771.63 +/- 707.65\n",
      "Episode length: 635.60 +/- 160.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 636      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0464   |\n",
      "|    ent_coef_loss   | 0.647    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 686      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7570     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24652    |\n",
      "|    total_timesteps | 4691596  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.208    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4691495  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 672      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7580     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24682    |\n",
      "|    total_timesteps | 4697322  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.812    |\n",
      "|    ent_coef        | 0.0461   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4697221  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4700000, episode_reward=1791.68 +/- 261.80\n",
      "Episode length: 414.20 +/- 57.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 414      |\n",
      "|    mean_reward     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 0.874    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 646      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7590     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24708    |\n",
      "|    total_timesteps | 4702017  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.661    |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | -0.727   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4701916  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 635      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7600     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24739    |\n",
      "|    total_timesteps | 4707957  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.931    |\n",
      "|    ent_coef        | 0.0459   |\n",
      "|    ent_coef_loss   | -0.367   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4707856  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710000, episode_reward=3952.69 +/- 315.56\n",
      "Episode length: 930.20 +/- 85.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 930      |\n",
      "|    mean_reward     | 3.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 0.792    |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | -0.229   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 640      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7610     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24799    |\n",
      "|    total_timesteps | 4714981  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4714880  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4720000, episode_reward=2732.31 +/- 862.05\n",
      "Episode length: 626.00 +/- 192.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 626      |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.901    |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | -0.462   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 642      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7620     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24836    |\n",
      "|    total_timesteps | 4721721  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0459   |\n",
      "|    ent_coef_loss   | -0.475   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4721620  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 639      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7630     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24868    |\n",
      "|    total_timesteps | 4727778  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.924    |\n",
      "|    ent_coef        | 0.0457   |\n",
      "|    ent_coef_loss   | 0.626    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4727677  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4730000, episode_reward=4112.28 +/- 64.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0479   |\n",
      "|    ent_coef_loss   | -0.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4729899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 602      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7640     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24894    |\n",
      "|    total_timesteps | 4732551  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.894    |\n",
      "|    ent_coef        | 0.0456   |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4732450  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 586      |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7650     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24924    |\n",
      "|    total_timesteps | 4738325  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0463   |\n",
      "|    ent_coef_loss   | 0.489    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4738224  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4740000, episode_reward=2154.70 +/- 121.15\n",
      "Episode length: 502.80 +/- 26.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 503      |\n",
      "|    mean_reward     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.855    |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | 0.298    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 592      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7660     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24962    |\n",
      "|    total_timesteps | 4745294  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0487   |\n",
      "|    ent_coef_loss   | 0.262    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4745193  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4750000, episode_reward=3290.98 +/- 1078.77\n",
      "Episode length: 783.60 +/- 265.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 784      |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.658    |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | 0.106    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 599      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7670     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 24995    |\n",
      "|    total_timesteps | 4751499  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -0.405   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4751398  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 607      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7680     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25029    |\n",
      "|    total_timesteps | 4758002  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 1.19     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | -0.0747  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4757901  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4760000, episode_reward=2137.93 +/- 376.39\n",
      "Episode length: 499.00 +/- 84.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 499      |\n",
      "|    mean_reward     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 0.808    |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | -0.906   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 611      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7690     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25056    |\n",
      "|    total_timesteps | 4763110  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 2.59     |\n",
      "|    ent_coef        | 0.0462   |\n",
      "|    ent_coef_loss   | 0.582    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4763009  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 599      |\n",
      "|    ep_rew_mean     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7700     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25082    |\n",
      "|    total_timesteps | 4767879  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.735    |\n",
      "|    ent_coef        | 0.0469   |\n",
      "|    ent_coef_loss   | 0.0151   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4767778  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4770000, episode_reward=2214.75 +/- 397.49\n",
      "Episode length: 511.60 +/- 88.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 512      |\n",
      "|    mean_reward     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0498   |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 592      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7710     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25115    |\n",
      "|    total_timesteps | 4774204  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 0.87     |\n",
      "|    ent_coef        | 0.0481   |\n",
      "|    ent_coef_loss   | 0.0467   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4774103  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 581      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7720     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25144    |\n",
      "|    total_timesteps | 4779803  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | -0.504   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4779702  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4780000, episode_reward=3136.62 +/- 290.38\n",
      "Episode length: 729.80 +/- 66.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 730      |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0474   |\n",
      "|    ent_coef_loss   | 0.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 583      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7730     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25178    |\n",
      "|    total_timesteps | 4786076  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 1.06     |\n",
      "|    ent_coef        | 0.0464   |\n",
      "|    ent_coef_loss   | -0.00697 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4785975  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4790000, episode_reward=3551.42 +/- 159.61\n",
      "Episode length: 815.60 +/- 35.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 816      |\n",
      "|    mean_reward     | 3.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.0451   |\n",
      "|    ent_coef_loss   | -0.751   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 614      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7740     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25219    |\n",
      "|    total_timesteps | 4793910  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.953    |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | 0.725    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4793809  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800000, episode_reward=3556.05 +/- 725.41\n",
      "Episode length: 831.20 +/- 170.13\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 831      |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -0.271   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 629      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7750     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25259    |\n",
      "|    total_timesteps | 4801204  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0462   |\n",
      "|    ent_coef_loss   | -0.487   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4801103  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 629      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7760     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25295    |\n",
      "|    total_timesteps | 4808204  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0451   |\n",
      "|    ent_coef_loss   | -0.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4808103  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4810000, episode_reward=2335.31 +/- 475.05\n",
      "Episode length: 542.80 +/- 107.15\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 543      |\n",
      "|    mean_reward     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 0.795    |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | -0.255   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 639      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7770     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25333    |\n",
      "|    total_timesteps | 4815426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 0.802    |\n",
      "|    ent_coef        | 0.0447   |\n",
      "|    ent_coef_loss   | 0.412    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4815325  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4820000, episode_reward=2695.82 +/- 439.68\n",
      "Episode length: 621.40 +/- 101.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 621      |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.86     |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 636      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7780     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25366    |\n",
      "|    total_timesteps | 4821621  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | 0.311    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4821520  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 666      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7790     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25408    |\n",
      "|    total_timesteps | 4829661  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 0.928    |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | -0.171   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4829560  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4830000, episode_reward=3199.15 +/- 947.66\n",
      "Episode length: 730.00 +/- 213.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 730      |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.999    |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 0.033    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 687      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7800     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25444    |\n",
      "|    total_timesteps | 4836535  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 0.84     |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | -0.321   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4836434  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4840000, episode_reward=3430.01 +/- 80.55\n",
      "Episode length: 792.20 +/- 19.84\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 792      |\n",
      "|    mean_reward     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0449   |\n",
      "|    ent_coef_loss   | 0.972    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 678      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7810     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25472    |\n",
      "|    total_timesteps | 4842009  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.981    |\n",
      "|    ent_coef        | 0.0449   |\n",
      "|    ent_coef_loss   | -0.246   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4841908  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 696      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7820     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25509    |\n",
      "|    total_timesteps | 4849368  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.991    |\n",
      "|    ent_coef        | 0.0459   |\n",
      "|    ent_coef_loss   | 0.856    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4849267  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4850000, episode_reward=4089.42 +/- 112.57\n",
      "Episode length: 972.60 +/- 33.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 973      |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.984    |\n",
      "|    ent_coef        | 0.0457   |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 704      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7830     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25545    |\n",
      "|    total_timesteps | 4856428  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | 0.557    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4856327  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4860000, episode_reward=3872.88 +/- 402.63\n",
      "Episode length: 914.00 +/- 108.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 914      |\n",
      "|    mean_reward     | 3.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.704    |\n",
      "|    ent_coef        | 0.0435   |\n",
      "|    ent_coef_loss   | -0.716   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 696      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7840     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25582    |\n",
      "|    total_timesteps | 4863534  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | -0.788   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4863433  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4870000, episode_reward=2210.06 +/- 250.33\n",
      "Episode length: 516.20 +/- 55.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 516      |\n",
      "|    mean_reward     | 2.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.854    |\n",
      "|    ent_coef        | 0.0453   |\n",
      "|    ent_coef_loss   | 0.45     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 700      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 7850     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25620    |\n",
      "|    total_timesteps | 4871159  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | -0.755   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4871058  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 707      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7860     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25659    |\n",
      "|    total_timesteps | 4878918  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 0.773    |\n",
      "|    ent_coef        | 0.0447   |\n",
      "|    ent_coef_loss   | 0.121    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4878817  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4880000, episode_reward=4069.55 +/- 135.90\n",
      "Episode length: 969.00 +/- 39.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 969      |\n",
      "|    mean_reward     | 4.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.96     |\n",
      "|    ent_coef        | 0.0442   |\n",
      "|    ent_coef_loss   | -0.098   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 714      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7870     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25700    |\n",
      "|    total_timesteps | 4886864  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 0.636    |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4886763  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4890000, episode_reward=4150.39 +/- 32.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.805    |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4889899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 730      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7880     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25740    |\n",
      "|    total_timesteps | 4894647  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 2.1      |\n",
      "|    ent_coef        | 0.0449   |\n",
      "|    ent_coef_loss   | 0.832    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4894546  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4900000, episode_reward=2765.97 +/- 299.15\n",
      "Episode length: 646.80 +/- 72.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 647      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 0.955    |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 730      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7890     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25780    |\n",
      "|    total_timesteps | 4902621  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 0.874    |\n",
      "|    ent_coef        | 0.0453   |\n",
      "|    ent_coef_loss   | -0.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4902520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4910000, episode_reward=2379.18 +/- 444.57\n",
      "Episode length: 553.20 +/- 102.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 553      |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 0.899    |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -0.838   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 748      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7900     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25824    |\n",
      "|    total_timesteps | 4911331  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4911230  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 773      |\n",
      "|    ep_rew_mean     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7910     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25864    |\n",
      "|    total_timesteps | 4919303  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 1.28     |\n",
      "|    ent_coef        | 0.0435   |\n",
      "|    ent_coef_loss   | 0.456    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4919202  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4920000, episode_reward=3690.58 +/- 920.56\n",
      "Episode length: 887.20 +/- 225.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 887      |\n",
      "|    mean_reward     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.996    |\n",
      "|    ent_coef        | 0.0425   |\n",
      "|    ent_coef_loss   | 0.752    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 773      |\n",
      "|    ep_rew_mean     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 7920     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25902    |\n",
      "|    total_timesteps | 4926705  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 0.845    |\n",
      "|    ent_coef        | 0.0442   |\n",
      "|    ent_coef_loss   | -0.0126  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4926604  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4930000, episode_reward=3563.43 +/- 767.74\n",
      "Episode length: 834.20 +/- 180.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 834      |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | -0.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7930     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25937    |\n",
      "|    total_timesteps | 4933567  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | -0.048   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4933466  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4940000, episode_reward=4138.78 +/- 7.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.598    |\n",
      "|    ent_coef        | 0.0446   |\n",
      "|    ent_coef_loss   | -0.189   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 769      |\n",
      "|    ep_rew_mean     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7940     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 25972    |\n",
      "|    total_timesteps | 4940477  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 0.733    |\n",
      "|    ent_coef        | 0.0441   |\n",
      "|    ent_coef_loss   | 0.503    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4940376  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 763      |\n",
      "|    ep_rew_mean     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7950     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 26007    |\n",
      "|    total_timesteps | 4947489  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 3.84     |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | 0.686    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4947388  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4950000, episode_reward=3889.45 +/- 414.37\n",
      "Episode length: 913.80 +/- 105.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 914      |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0453   |\n",
      "|    ent_coef_loss   | 0.609    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7960     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 26048    |\n",
      "|    total_timesteps | 4955299  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 0.659    |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | 0.404    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4955198  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4960000, episode_reward=4140.58 +/- 8.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0449   |\n",
      "|    ent_coef_loss   | 1.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7970     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 26089    |\n",
      "|    total_timesteps | 4963341  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 0.848    |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | 0.362    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4963240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4970000, episode_reward=4173.37 +/- 13.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0438   |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4969899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7980     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 26126    |\n",
      "|    total_timesteps | 4970452  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 0.754    |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -0.0293  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4970351  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 7990     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 26166    |\n",
      "|    total_timesteps | 4978385  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | -0.117   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4978284  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4980000, episode_reward=4169.86 +/- 16.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 0.572    |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | -0.861   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 745      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 8000     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 26204    |\n",
      "|    total_timesteps | 4985852  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0443   |\n",
      "|    ent_coef_loss   | 0.745    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4985751  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4990000, episode_reward=1604.30 +/- 178.38\n",
      "Episode length: 378.80 +/- 41.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 379      |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | 0.628    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 8010     |\n",
      "|    fps             | 190      |\n",
      "|    time_elapsed    | 26248    |\n",
      "|    total_timesteps | 4994413  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 0.898    |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -1.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4994312  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000000, episode_reward=4113.46 +/- 4.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 0.893    |\n",
      "|    ent_coef        | 0.0442   |\n",
      "|    ent_coef_loss   | -0.269   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4999899  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "sac_model.learn(total_timesteps=5e6, log_interval=10, callback=callback)\n",
    "sac_model.save(f\"logs/expert/{env_id}-sac/model\")\n",
    "sac_model.save_replay_buffer(f\"logs/expert/{env_id}-sac/buffer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.6     |\n",
      "|    ep_rew_mean     | -21.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 826      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.9    |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.805    |\n",
      "|    ent_coef_loss   | -2.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 725      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.1     |\n",
      "|    ep_rew_mean     | -32      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 1722     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -27.5    |\n",
      "|    critic_loss     | 3.14     |\n",
      "|    ent_coef        | 0.618    |\n",
      "|    ent_coef_loss   | -5.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1621     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 117      |\n",
      "|    ep_rew_mean     | -47.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 3516     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.3    |\n",
      "|    critic_loss     | 5.71     |\n",
      "|    ent_coef        | 0.37     |\n",
      "|    ent_coef_loss   | -11.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3415     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 123      |\n",
      "|    ep_rew_mean     | -54.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 150      |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 4921     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.4    |\n",
      "|    critic_loss     | 7.58     |\n",
      "|    ent_coef        | 0.25     |\n",
      "|    ent_coef_loss   | -14      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4820     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 147      |\n",
      "|    ep_rew_mean     | -69.2    |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 7328     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.4    |\n",
      "|    critic_loss     | 5.84     |\n",
      "|    ent_coef        | 0.13     |\n",
      "|    ent_coef_loss   | -17.3    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7227     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-101.91 +/- 228.29\n",
      "Episode length: 265.60 +/- 367.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 266      |\n",
      "|    mean_reward     | -102     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.3    |\n",
      "|    critic_loss     | 4.88     |\n",
      "|    ent_coef        | 0.0635   |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 178      |\n",
      "|    ep_rew_mean     | -79.4    |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 10651    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.5    |\n",
      "|    critic_loss     | 4.44     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | -15.1    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 10550    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 212      |\n",
      "|    ep_rew_mean     | -81.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 70       |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 94       |\n",
      "|    total_timesteps | 14841    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.5    |\n",
      "|    critic_loss     | 3.67     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -5.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 14740    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 232      |\n",
      "|    ep_rew_mean     | -68.9    |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 18595    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.7    |\n",
      "|    critic_loss     | 4.16     |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 18494    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=165.59 +/- 169.34\n",
      "Episode length: 645.20 +/- 435.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 645      |\n",
      "|    mean_reward     | 166      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.8    |\n",
      "|    critic_loss     | 3.69     |\n",
      "|    ent_coef        | 0.0167   |\n",
      "|    ent_coef_loss   | -0.0756  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 259      |\n",
      "|    ep_rew_mean     | -53.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 147      |\n",
      "|    total_timesteps | 23320    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.7    |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0177   |\n",
      "|    ent_coef_loss   | -0.949   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 23219    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 270      |\n",
      "|    ep_rew_mean     | -44.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 27007    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.9    |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0186   |\n",
      "|    ent_coef_loss   | 0.529    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 26906    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=50.57 +/- 31.69\n",
      "Episode length: 314.20 +/- 351.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 314      |\n",
      "|    mean_reward     | 50.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.2    |\n",
      "|    critic_loss     | 2.97     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 325      |\n",
      "|    ep_rew_mean     | -30.5    |\n",
      "| time/              |          |\n",
      "|    episodes        | 110      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 209      |\n",
      "|    total_timesteps | 33304    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.8    |\n",
      "|    critic_loss     | 2.79     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.0852  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 33203    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 353      |\n",
      "|    ep_rew_mean     | -14.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 232      |\n",
      "|    total_timesteps | 37030    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.1    |\n",
      "|    critic_loss     | 2.54     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | -0.472   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 36929    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=228.85 +/- 192.62\n",
      "Episode length: 616.80 +/- 469.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 617      |\n",
      "|    mean_reward     | 229      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.1    |\n",
      "|    critic_loss     | 3.17     |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | 0.456    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 370      |\n",
      "|    ep_rew_mean     | 4.77     |\n",
      "| time/              |          |\n",
      "|    episodes        | 130      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 254      |\n",
      "|    total_timesteps | 40474    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.2    |\n",
      "|    critic_loss     | 2.94     |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.291   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 40373    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 421      |\n",
      "|    ep_rew_mean     | 35       |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 46986    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.8    |\n",
      "|    critic_loss     | 2.65     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 0.333    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 46885    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=399.54 +/- 281.83\n",
      "Episode length: 657.20 +/- 419.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 657      |\n",
      "|    mean_reward     | 400      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.9    |\n",
      "|    critic_loss     | 2.17     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | 0.671    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 449      |\n",
      "|    ep_rew_mean     | 73.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 150      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 327      |\n",
      "|    total_timesteps | 52225    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.2    |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | 0.472    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 52124    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 480      |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 367      |\n",
      "|    total_timesteps | 58607    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | 0.958    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 58506    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=620.12 +/- 53.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 620      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -25.9    |\n",
      "|    critic_loss     | 2.43     |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 0.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 510      |\n",
      "|    ep_rew_mean     | 176      |\n",
      "| time/              |          |\n",
      "|    episodes        | 170      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 413      |\n",
      "|    total_timesteps | 65874    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.7    |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0237   |\n",
      "|    ent_coef_loss   | 0.345    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 65773    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=534.76 +/- 295.33\n",
      "Episode length: 802.20 +/- 395.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 802      |\n",
      "|    mean_reward     | 535      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -31.9    |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 520      |\n",
      "|    ep_rew_mean     | 205      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 443      |\n",
      "|    total_timesteps | 70580    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -32.7    |\n",
      "|    critic_loss     | 2.14     |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | -0.0312  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 70479    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 557      |\n",
      "|    ep_rew_mean     | 244      |\n",
      "| time/              |          |\n",
      "|    episodes        | 190      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 495      |\n",
      "|    total_timesteps | 79063    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -36.8    |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 0.0181   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 78962    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=332.01 +/- 365.91\n",
      "Episode length: 452.20 +/- 447.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 452      |\n",
      "|    mean_reward     | 332      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.3    |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | -0.211   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 577      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 530      |\n",
      "|    total_timesteps | 84675    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39      |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0243   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 84574    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=837.32 +/- 43.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 837      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -40.2    |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -0.702   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 595      |\n",
      "|    ep_rew_mean     | 328      |\n",
      "| time/              |          |\n",
      "|    episodes        | 210      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 581      |\n",
      "|    total_timesteps | 92845    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.6    |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | 0.0889   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 92744    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=860.84 +/- 40.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 861      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -43.6    |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -0.254   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 637      |\n",
      "|    ep_rew_mean     | 373      |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 631      |\n",
      "|    total_timesteps | 100734   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -43.7    |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | 0.882    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 100633   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=837.21 +/- 50.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 837      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.5    |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | 0.325    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 703      |\n",
      "|    ep_rew_mean     | 442      |\n",
      "| time/              |          |\n",
      "|    episodes        | 230      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 694      |\n",
      "|    total_timesteps | 110734   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -46.4    |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 110633   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 698      |\n",
      "|    ep_rew_mean     | 468      |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 730      |\n",
      "|    total_timesteps | 116809   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -49.1    |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -0.933   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 116708   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=814.57 +/- 64.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 815      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -50.1    |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0238   |\n",
      "|    ent_coef_loss   | -0.712   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 119899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 737      |\n",
      "|    ep_rew_mean     | 516      |\n",
      "| time/              |          |\n",
      "|    episodes        | 250      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 788      |\n",
      "|    total_timesteps | 125933   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -51.4    |\n",
      "|    critic_loss     | 1.12     |\n",
      "|    ent_coef        | 0.0228   |\n",
      "|    ent_coef_loss   | -0.434   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 125832   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=665.83 +/- 328.72\n",
      "Episode length: 804.40 +/- 391.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 666      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -52.6    |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0227   |\n",
      "|    ent_coef_loss   | -0.402   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 129899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 748      |\n",
      "|    ep_rew_mean     | 532      |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 835      |\n",
      "|    total_timesteps | 133424   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.5    |\n",
      "|    critic_loss     | 0.818    |\n",
      "|    ent_coef        | 0.0223   |\n",
      "|    ent_coef_loss   | -1.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 133323   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=888.92 +/- 23.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 889      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -54.1    |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | -0.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 139899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 574      |\n",
      "| time/              |          |\n",
      "|    episodes        | 270      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 897      |\n",
      "|    total_timesteps | 143424   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -55.1    |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0224   |\n",
      "|    ent_coef_loss   | -0.106   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 143323   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=677.28 +/- 281.50\n",
      "Episode length: 843.80 +/- 312.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 844      |\n",
      "|    mean_reward     | 677      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.3    |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 0.196    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 598      |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 943      |\n",
      "|    total_timesteps | 150598   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -56.5    |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0222   |\n",
      "|    ent_coef_loss   | 0.286    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 150497   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 612      |\n",
      "| time/              |          |\n",
      "|    episodes        | 290      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 985      |\n",
      "|    total_timesteps | 157797   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.5    |\n",
      "|    critic_loss     | 0.918    |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -0.0536  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 157696   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=549.01 +/- 381.43\n",
      "Episode length: 633.40 +/- 449.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 633      |\n",
      "|    mean_reward     | 549      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.3    |\n",
      "|    critic_loss     | 0.955    |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | -0.982   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 628      |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 1028     |\n",
      "|    total_timesteps | 164986   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -57.3    |\n",
      "|    critic_loss     | 0.96     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 164885   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=748.32 +/- 365.98\n",
      "Episode length: 804.00 +/- 392.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 748      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.8    |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 626      |\n",
      "| time/              |          |\n",
      "|    episodes        | 310      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 1073     |\n",
      "|    total_timesteps | 172290   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.9    |\n",
      "|    critic_loss     | 1.47     |\n",
      "|    ent_coef        | 0.021    |\n",
      "|    ent_coef_loss   | 0.747    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 172189   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 629      |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 1116     |\n",
      "|    total_timesteps | 179426   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.4    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | 0.432    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179325   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=814.03 +/- 227.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 814      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -60.5    |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0211   |\n",
      "|    ent_coef_loss   | 0.823    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 620      |\n",
      "| time/              |          |\n",
      "|    episodes        | 330      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 1172     |\n",
      "|    total_timesteps | 188608   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.7    |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | -0.396   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 188507   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=775.93 +/- 179.36\n",
      "Episode length: 877.80 +/- 244.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 878      |\n",
      "|    mean_reward     | 776      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.7    |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 655      |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1233     |\n",
      "|    total_timesteps | 198608   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.6    |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0217   |\n",
      "|    ent_coef_loss   | -0.748   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 198507   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=790.56 +/- 294.37\n",
      "Episode length: 833.20 +/- 333.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 833      |\n",
      "|    mean_reward     | 791      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.7    |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0208   |\n",
      "|    ent_coef_loss   | 0.738    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 199899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 813      |\n",
      "|    ep_rew_mean     | 654      |\n",
      "| time/              |          |\n",
      "|    episodes        | 350      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1286     |\n",
      "|    total_timesteps | 207252   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.7    |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    ent_coef        | 0.0212   |\n",
      "|    ent_coef_loss   | 0.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 207151   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=909.23 +/- 58.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 909      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -60.9    |\n",
      "|    critic_loss     | 0.997    |\n",
      "|    ent_coef        | 0.0203   |\n",
      "|    ent_coef_loss   | -0.476   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 209899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 663      |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1330     |\n",
      "|    total_timesteps | 214361   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -62.9    |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.945    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 214260   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=635.21 +/- 282.71\n",
      "Episode length: 818.20 +/- 363.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 818      |\n",
      "|    mean_reward     | 635      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.3    |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0209   |\n",
      "|    ent_coef_loss   | -0.245   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 651      |\n",
      "| time/              |          |\n",
      "|    episodes        | 370      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1385     |\n",
      "|    total_timesteps | 223487   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.6    |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | 0.166    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 223386   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=947.12 +/- 65.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 947      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -64.3    |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.623    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 229899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 659      |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1430     |\n",
      "|    total_timesteps | 230803   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.9    |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0198   |\n",
      "|    ent_coef_loss   | -0.154   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 230702   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 667      |\n",
      "| time/              |          |\n",
      "|    episodes        | 390      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1475     |\n",
      "|    total_timesteps | 238265   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -66.8    |\n",
      "|    critic_loss     | 0.697    |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 0.221    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 238164   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=1036.11 +/- 154.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.3    |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.02     |\n",
      "|    ent_coef_loss   | 0.688    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 239899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 667      |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1514     |\n",
      "|    total_timesteps | 244586   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.2    |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | -0.293   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 244485   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=673.75 +/- 269.70\n",
      "Episode length: 619.80 +/- 328.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 620      |\n",
      "|    mean_reward     | 674      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.6    |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 249899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 679      |\n",
      "| time/              |          |\n",
      "|    episodes        | 410      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1559     |\n",
      "|    total_timesteps | 252087   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.3    |\n",
      "|    critic_loss     | 1.22     |\n",
      "|    ent_coef        | 0.0207   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 251986   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=932.41 +/- 372.94\n",
      "Episode length: 841.60 +/- 316.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 842      |\n",
      "|    mean_reward     | 932      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -67.6    |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 0.066    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 694      |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1609     |\n",
      "|    total_timesteps | 260351   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.6    |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 0.608    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 260250   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 702      |\n",
      "| time/              |          |\n",
      "|    episodes        | 430      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 1664     |\n",
      "|    total_timesteps | 269583   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -67.8    |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0201   |\n",
      "|    ent_coef_loss   | -0.987   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269482   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=836.19 +/- 422.40\n",
      "Episode length: 826.60 +/- 346.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 836      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -66.8    |\n",
      "|    critic_loss     | 2.07     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -0.522   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 718      |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1719     |\n",
      "|    total_timesteps | 278641   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -68.4    |\n",
      "|    critic_loss     | 1.2      |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.939   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 278540   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=1046.75 +/- 364.28\n",
      "Episode length: 868.80 +/- 262.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 869      |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69      |\n",
      "|    critic_loss     | 2.25     |\n",
      "|    ent_coef        | 0.0202   |\n",
      "|    ent_coef_loss   | 0.347    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 279899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 730      |\n",
      "| time/              |          |\n",
      "|    episodes        | 450      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1761     |\n",
      "|    total_timesteps | 285618   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69      |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0199   |\n",
      "|    ent_coef_loss   | 0.0859   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 285517   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=1242.62 +/- 257.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -70.9    |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0205   |\n",
      "|    ent_coef_loss   | 0.305    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 289899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 757      |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1810     |\n",
      "|    total_timesteps | 293533   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -71      |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | -0.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 293432   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=624.84 +/- 396.95\n",
      "Episode length: 516.00 +/- 410.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 516      |\n",
      "|    mean_reward     | 625      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -71.7    |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0204   |\n",
      "|    ent_coef_loss   | 0.771    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 781      |\n",
      "| time/              |          |\n",
      "|    episodes        | 470      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1864     |\n",
      "|    total_timesteps | 302563   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -72.4    |\n",
      "|    critic_loss     | 2.07     |\n",
      "|    ent_coef        | 0.0213   |\n",
      "|    ent_coef_loss   | 0.0852   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 302462   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=936.97 +/- 235.97\n",
      "Episode length: 561.20 +/- 222.38\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 561      |\n",
      "|    mean_reward     | 937      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -73.6    |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | 0.414    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 805      |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1915     |\n",
      "|    total_timesteps | 311082   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -75.6    |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.023    |\n",
      "|    ent_coef_loss   | 0.565    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 310981   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 834      |\n",
      "| time/              |          |\n",
      "|    episodes        | 490      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 1953     |\n",
      "|    total_timesteps | 317467   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -75.3    |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0225   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 317366   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1045.82 +/- 570.19\n",
      "Episode length: 804.40 +/- 391.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 1.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -74.2    |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 881      |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2004     |\n",
      "|    total_timesteps | 325941   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.2    |\n",
      "|    critic_loss     | 3.16     |\n",
      "|    ent_coef        | 0.025    |\n",
      "|    ent_coef_loss   | 0.421    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 325840   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=1530.58 +/- 668.05\n",
      "Episode length: 851.60 +/- 296.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 852      |\n",
      "|    mean_reward     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -77.8    |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0248   |\n",
      "|    ent_coef_loss   | 0.162    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 329899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 922      |\n",
      "| time/              |          |\n",
      "|    episodes        | 510      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2052     |\n",
      "|    total_timesteps | 333897   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.1    |\n",
      "|    critic_loss     | 3.54     |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | 0.174    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 333796   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=1559.38 +/- 439.21\n",
      "Episode length: 916.40 +/- 167.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 916      |\n",
      "|    mean_reward     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -81.2    |\n",
      "|    critic_loss     | 3.06     |\n",
      "|    ent_coef        | 0.0244   |\n",
      "|    ent_coef_loss   | 0.373    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 339899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 983      |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2105     |\n",
      "|    total_timesteps | 342676   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.1    |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0254   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 342575   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 530      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2146     |\n",
      "|    total_timesteps | 349617   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -84.4    |\n",
      "|    critic_loss     | 2.7      |\n",
      "|    ent_coef        | 0.024    |\n",
      "|    ent_coef_loss   | -0.192   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349516   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=1771.06 +/- 206.85\n",
      "Episode length: 937.20 +/- 78.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 937      |\n",
      "|    mean_reward     | 1.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.4    |\n",
      "|    critic_loss     | 4.9      |\n",
      "|    ent_coef        | 0.0235   |\n",
      "|    ent_coef_loss   | 0.72     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 794      |\n",
      "|    ep_rew_mean     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2197     |\n",
      "|    total_timesteps | 358081   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -85.2    |\n",
      "|    critic_loss     | 2.93     |\n",
      "|    ent_coef        | 0.026    |\n",
      "|    ent_coef_loss   | 0.384    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 357980   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=1594.24 +/- 487.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -88.2    |\n",
      "|    critic_loss     | 2.91     |\n",
      "|    ent_coef        | 0.0247   |\n",
      "|    ent_coef_loss   | -0.666   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 550      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 2243     |\n",
      "|    total_timesteps | 365638   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -91.8    |\n",
      "|    critic_loss     | 4.47     |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | 0.231    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 365537   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=1092.81 +/- 708.19\n",
      "Episode length: 768.80 +/- 375.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 769      |\n",
      "|    mean_reward     | 1.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -89.7    |\n",
      "|    critic_loss     | 2.46     |\n",
      "|    ent_coef        | 0.0252   |\n",
      "|    ent_coef_loss   | -0.429   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 1.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2295     |\n",
      "|    total_timesteps | 374258   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -93.3    |\n",
      "|    critic_loss     | 3.56     |\n",
      "|    ent_coef        | 0.0263   |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 374157   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=1237.38 +/- 740.44\n",
      "Episode length: 783.60 +/- 370.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 784      |\n",
      "|    mean_reward     | 1.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.2    |\n",
      "|    critic_loss     | 3.42     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | -0.596   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 379899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 570      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2343     |\n",
      "|    total_timesteps | 382099   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -96.7    |\n",
      "|    critic_loss     | 3.54     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | -0.755   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 381998   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=1802.85 +/- 479.53\n",
      "Episode length: 992.00 +/- 16.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 992      |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -99.1    |\n",
      "|    critic_loss     | 4.29     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 389899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 1.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 580      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2397     |\n",
      "|    total_timesteps | 391034   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -99.3    |\n",
      "|    critic_loss     | 4.98     |\n",
      "|    ent_coef        | 0.0288   |\n",
      "|    ent_coef_loss   | -1.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 390933   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 1.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 590      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2446     |\n",
      "|    total_timesteps | 399338   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -108     |\n",
      "|    critic_loss     | 5        |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -0.0224  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399237   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1716.29 +/- 566.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -102     |\n",
      "|    critic_loss     | 6.73     |\n",
      "|    ent_coef        | 0.0304   |\n",
      "|    ent_coef_loss   | 1.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 600      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2499     |\n",
      "|    total_timesteps | 408071   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -106     |\n",
      "|    critic_loss     | 3.53     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.207    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 407970   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=1520.36 +/- 779.14\n",
      "Episode length: 751.40 +/- 305.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 751      |\n",
      "|    mean_reward     | 1.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -103     |\n",
      "|    critic_loss     | 4.78     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.0133   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 409899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 610      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2559     |\n",
      "|    total_timesteps | 418071   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 4.97     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 1.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 417970   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=1300.18 +/- 613.63\n",
      "Episode length: 645.80 +/- 312.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 646      |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -101     |\n",
      "|    critic_loss     | 9.44     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.0378   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 419899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 1.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 620      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2612     |\n",
      "|    total_timesteps | 426905   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -114     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 426804   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=1995.84 +/- 619.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -108     |\n",
      "|    critic_loss     | 4.41     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.834   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 429899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 1.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 630      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2669     |\n",
      "|    total_timesteps | 436287   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -108     |\n",
      "|    critic_loss     | 3.79     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.588   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 436186   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=1173.41 +/- 333.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -118     |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.248   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 640      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2723     |\n",
      "|    total_timesteps | 445324   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -117     |\n",
      "|    critic_loss     | 4.13     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 445223   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=2375.30 +/- 264.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -121     |\n",
      "|    critic_loss     | 5.52     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.671    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 449899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 897      |\n",
      "|    ep_rew_mean     | 1.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 650      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2784     |\n",
      "|    total_timesteps | 455324   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -116     |\n",
      "|    critic_loss     | 4.81     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -0.243   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 455223   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=1063.87 +/- 555.42\n",
      "Episode length: 748.20 +/- 362.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 748      |\n",
      "|    mean_reward     | 1.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -118     |\n",
      "|    critic_loss     | 3.32     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | 0.847    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 459899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 660      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2833     |\n",
      "|    total_timesteps | 463450   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -124     |\n",
      "|    critic_loss     | 5.34     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.308    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 463349   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=2542.20 +/- 88.50\n",
      "Episode length: 976.80 +/- 46.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 977      |\n",
      "|    mean_reward     | 2.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -124     |\n",
      "|    critic_loss     | 5.44     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 469899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 1.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 670      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2874     |\n",
      "|    total_timesteps | 470178   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -128     |\n",
      "|    critic_loss     | 5.46     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -0.561   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 470077   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 1.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 680      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2922     |\n",
      "|    total_timesteps | 478375   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -131     |\n",
      "|    critic_loss     | 4.44     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | 0.892    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 478274   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=1679.48 +/- 261.93\n",
      "Episode length: 814.00 +/- 228.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 814      |\n",
      "|    mean_reward     | 1.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -124     |\n",
      "|    critic_loss     | 15.2     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.402    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 479899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 1.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 690      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 2979     |\n",
      "|    total_timesteps | 487906   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -126     |\n",
      "|    critic_loss     | 4        |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.473   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 487805   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=2702.34 +/- 71.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -130     |\n",
      "|    critic_loss     | 8.61     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 2.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 489899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 1.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 700      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3021     |\n",
      "|    total_timesteps | 494891   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -131     |\n",
      "|    critic_loss     | 5.16     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.946    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 494790   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=2618.93 +/- 83.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -128     |\n",
      "|    critic_loss     | 7.42     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.584    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 710      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3081     |\n",
      "|    total_timesteps | 504805   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -133     |\n",
      "|    critic_loss     | 5.42     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.0202  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 504704   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=1582.10 +/- 505.26\n",
      "Episode length: 978.00 +/- 44.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 978      |\n",
      "|    mean_reward     | 1.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -139     |\n",
      "|    critic_loss     | 6.84     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.0775  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 509899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 720      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3132     |\n",
      "|    total_timesteps | 513255   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 8.62     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.443    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 513154   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=2532.45 +/- 71.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -136     |\n",
      "|    critic_loss     | 4.08     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 519899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 1.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 730      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3192     |\n",
      "|    total_timesteps | 523255   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 6.74     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.148   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 523154   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=1868.61 +/- 924.27\n",
      "Episode length: 954.60 +/- 90.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 955      |\n",
      "|    mean_reward     | 1.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 4.54     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 0.837    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 529899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 740      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3246     |\n",
      "|    total_timesteps | 532142   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -142     |\n",
      "|    critic_loss     | 9.09     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 0.897    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 532041   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=2759.01 +/- 271.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 5.29     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | 0.464    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 539899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 1.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 750      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3302     |\n",
      "|    total_timesteps | 541351   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -143     |\n",
      "|    critic_loss     | 6.3      |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 541250   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 760      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3349     |\n",
      "|    total_timesteps | 549359   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 5.97     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549258   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=2413.57 +/- 571.40\n",
      "Episode length: 901.80 +/- 196.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 902      |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -141     |\n",
      "|    critic_loss     | 5.01     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 880      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 770      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3402     |\n",
      "|    total_timesteps | 558183   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -146     |\n",
      "|    critic_loss     | 9.56     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.529    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 558082   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=1924.18 +/- 833.76\n",
      "Episode length: 979.60 +/- 40.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 980      |\n",
      "|    mean_reward     | 1.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.935    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 559899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 2.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 780      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3457     |\n",
      "|    total_timesteps | 567244   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 4.87     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.307   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 567143   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=2491.58 +/- 937.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -145     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.529   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 569899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 790      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3499     |\n",
      "|    total_timesteps | 574115   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -152     |\n",
      "|    critic_loss     | 4.94     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 574014   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=2347.43 +/- 682.41\n",
      "Episode length: 911.00 +/- 178.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 911      |\n",
      "|    mean_reward     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -144     |\n",
      "|    critic_loss     | 8.19     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.973    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 579899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 800      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3559     |\n",
      "|    total_timesteps | 584115   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -152     |\n",
      "|    critic_loss     | 7.12     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.158   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 584014   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=1747.08 +/- 775.52\n",
      "Episode length: 722.40 +/- 342.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 722      |\n",
      "|    mean_reward     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -151     |\n",
      "|    critic_loss     | 8.52     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.755    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 589899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 810      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3603     |\n",
      "|    total_timesteps | 591374   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -150     |\n",
      "|    critic_loss     | 5.36     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.0292  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 591273   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=2019.67 +/- 1183.46\n",
      "Episode length: 701.60 +/- 396.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 702      |\n",
      "|    mean_reward     | 2.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -155     |\n",
      "|    critic_loss     | 11.6     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.616    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 599899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 820      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3657     |\n",
      "|    total_timesteps | 600438   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -154     |\n",
      "|    critic_loss     | 5.6      |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -1.55    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 600337   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 830      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3713     |\n",
      "|    total_timesteps | 609812   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -164     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.793    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609711   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=2449.39 +/- 649.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -158     |\n",
      "|    critic_loss     | 9.67     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 840      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3771     |\n",
      "|    total_timesteps | 619536   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -162     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619435   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=3250.31 +/- 61.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.944   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 850      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3824     |\n",
      "|    total_timesteps | 628239   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -157     |\n",
      "|    critic_loss     | 5.12     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 628138   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=3031.97 +/- 82.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -163     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.671    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 629899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 860      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3871     |\n",
      "|    total_timesteps | 635912   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 8.51     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.861   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 635811   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=2363.25 +/- 1145.92\n",
      "Episode length: 820.40 +/- 359.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -155     |\n",
      "|    critic_loss     | 7.58     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.031   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 639899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 870      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3927     |\n",
      "|    total_timesteps | 645166   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 11       |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 645065   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=2334.76 +/- 883.28\n",
      "Episode length: 769.60 +/- 298.71\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 770      |\n",
      "|    mean_reward     | 2.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 7.64     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.267    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 649899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 880      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3987     |\n",
      "|    total_timesteps | 655166   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -164     |\n",
      "|    critic_loss     | 7.29     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.175   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 655065   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=2637.44 +/- 741.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 7.08     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 0.773    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 659899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 885      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 890      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4032     |\n",
      "|    total_timesteps | 662644   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -165     |\n",
      "|    critic_loss     | 6.92     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.0746   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 662543   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=2050.90 +/- 1149.03\n",
      "Episode length: 638.40 +/- 363.57\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 638      |\n",
      "|    mean_reward     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -173     |\n",
      "|    critic_loss     | 7.82     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.656   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 2.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 900      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4079     |\n",
      "|    total_timesteps | 670450   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 6.98     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -0.468   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 670349   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 910      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4129     |\n",
      "|    total_timesteps | 678962   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 9.2      |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.716   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 678861   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=2766.69 +/- 671.56\n",
      "Episode length: 853.80 +/- 214.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 854      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 8.07     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.303   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 679899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 920      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4175     |\n",
      "|    total_timesteps | 686680   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 8.54     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 686579   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=2903.51 +/- 725.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -169     |\n",
      "|    critic_loss     | 8.01     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.0953  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 689899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 930      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4235     |\n",
      "|    total_timesteps | 696680   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 8.19     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -0.399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 696579   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=2757.14 +/- 1078.33\n",
      "Episode length: 840.40 +/- 319.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 840      |\n",
      "|    mean_reward     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -173     |\n",
      "|    critic_loss     | 6.43     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.0634  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 940      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4291     |\n",
      "|    total_timesteps | 706007   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 9.95     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 705906   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=2723.99 +/- 1277.46\n",
      "Episode length: 812.60 +/- 374.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 813      |\n",
      "|    mean_reward     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.637    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 709899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 950      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4344     |\n",
      "|    total_timesteps | 714807   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -175     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.704   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 714706   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=3372.93 +/- 120.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 7.25     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -1.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 719899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 2.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 960      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4392     |\n",
      "|    total_timesteps | 722719   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 7.88     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.223    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 722618   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=2674.37 +/- 1185.47\n",
      "Episode length: 822.80 +/- 354.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 823      |\n",
      "|    mean_reward     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -179     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.395   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 729899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 970      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4452     |\n",
      "|    total_timesteps | 732719   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -183     |\n",
      "|    critic_loss     | 8.96     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.244   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 732618   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=3298.05 +/- 226.87\n",
      "Episode length: 970.80 +/- 58.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 971      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -186     |\n",
      "|    critic_loss     | 9.7      |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 1.61     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 739899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 980      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4507     |\n",
      "|    total_timesteps | 741807   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -172     |\n",
      "|    critic_loss     | 5.74     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.521   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 741706   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=2640.41 +/- 734.19\n",
      "Episode length: 915.80 +/- 168.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 916      |\n",
      "|    mean_reward     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -194     |\n",
      "|    critic_loss     | 21.7     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 749899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 990      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4559     |\n",
      "|    total_timesteps | 750518   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.339    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 750417   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 2.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1000     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4603     |\n",
      "|    total_timesteps | 757999   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 8.28     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 757898   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=3421.78 +/- 39.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -183     |\n",
      "|    critic_loss     | 9.4      |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 759899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 890      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1010     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4663     |\n",
      "|    total_timesteps | 767999   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 9.1      |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.801   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 767898   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=3396.51 +/- 37.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -184     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 769899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 900      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1020     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4716     |\n",
      "|    total_timesteps | 776695   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -189     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.914    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 776594   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=2389.58 +/- 1226.13\n",
      "Episode length: 836.60 +/- 310.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 837      |\n",
      "|    mean_reward     | 2.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -186     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.296    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 779899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1030     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4771     |\n",
      "|    total_timesteps | 785900   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -183     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.296   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 785799   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=3029.40 +/- 966.68\n",
      "Episode length: 865.60 +/- 268.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 866      |\n",
      "|    mean_reward     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -192     |\n",
      "|    critic_loss     | 9.6      |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.124   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 789899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1040     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4817     |\n",
      "|    total_timesteps | 793473   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.266   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 793372   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=2791.49 +/- 657.41\n",
      "Episode length: 934.60 +/- 130.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 935      |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -207     |\n",
      "|    critic_loss     | 6.15     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.649   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1050     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4877     |\n",
      "|    total_timesteps | 803473   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 23.2     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 803372   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=3115.79 +/- 510.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -191     |\n",
      "|    critic_loss     | 9.67     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.768    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 809899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1060     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4932     |\n",
      "|    total_timesteps | 812658   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -199     |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | -0.558   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 812557   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=1850.40 +/- 1655.19\n",
      "Episode length: 770.00 +/- 298.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 770      |\n",
      "|    mean_reward     | 1.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 6.71     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -1.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 819899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1070     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 4982     |\n",
      "|    total_timesteps | 820938   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -202     |\n",
      "|    critic_loss     | 9.26     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.475   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 820837   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1080     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5029     |\n",
      "|    total_timesteps | 828915   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -203     |\n",
      "|    critic_loss     | 9.18     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.184    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 828814   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=3606.73 +/- 52.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -194     |\n",
      "|    critic_loss     | 7.4      |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.183    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 829899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1090     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5082     |\n",
      "|    total_timesteps | 837741   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -199     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.175   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 837640   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=2627.39 +/- 1382.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 6.44     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -2.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 839899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1100     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5126     |\n",
      "|    total_timesteps | 844951   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -208     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.358    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 844850   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=2947.99 +/- 997.30\n",
      "Episode length: 863.80 +/- 272.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 864      |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -200     |\n",
      "|    critic_loss     | 19       |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.349   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 849899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1110     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5168     |\n",
      "|    total_timesteps | 851915   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -206     |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.399    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 851814   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=3595.78 +/- 130.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -207     |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | 0.476    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 859899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 840      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1120     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5220     |\n",
      "|    total_timesteps | 860665   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -201     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 860564   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=2032.21 +/- 1343.51\n",
      "Episode length: 735.60 +/- 360.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 736      |\n",
      "|    mean_reward     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -209     |\n",
      "|    critic_loss     | 8.58     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 869899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1130     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5280     |\n",
      "|    total_timesteps | 870665   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -203     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.399    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 870564   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 862      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1140     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5333     |\n",
      "|    total_timesteps | 879697   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -205     |\n",
      "|    critic_loss     | 8.67     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -0.178   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879596   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=3022.98 +/- 583.70\n",
      "Episode length: 869.20 +/- 171.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 869      |\n",
      "|    mean_reward     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -210     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.0936   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1150     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5387     |\n",
      "|    total_timesteps | 888775   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -215     |\n",
      "|    critic_loss     | 7.06     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.677    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 888674   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=1971.82 +/- 1302.07\n",
      "Episode length: 816.60 +/- 366.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 817      |\n",
      "|    mean_reward     | 1.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 889899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1160     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5435     |\n",
      "|    total_timesteps | 896745   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 6.96     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 0.273    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 896644   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=3296.50 +/- 792.86\n",
      "Episode length: 900.40 +/- 199.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 900      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -224     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 840      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1170     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5485     |\n",
      "|    total_timesteps | 904907   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 19.9     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.241   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 904806   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=2310.64 +/- 1169.00\n",
      "Episode length: 931.20 +/- 137.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 931      |\n",
      "|    mean_reward     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 13.4     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.426    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 909899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1180     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5545     |\n",
      "|    total_timesteps | 914907   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -214     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.992    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 914806   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=2568.97 +/- 1824.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -212     |\n",
      "|    critic_loss     | 8.86     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -0.678   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 919899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1190     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 5601     |\n",
      "|    total_timesteps | 924175   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 924074   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=2563.77 +/- 878.60\n",
      "Episode length: 814.60 +/- 241.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 815      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 22       |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 929899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1200     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5656     |\n",
      "|    total_timesteps | 933310   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -215     |\n",
      "|    critic_loss     | 15       |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.277    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 933209   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=2790.12 +/- 1155.61\n",
      "Episode length: 887.20 +/- 225.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 887      |\n",
      "|    mean_reward     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 0.189    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 939899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1210     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5705     |\n",
      "|    total_timesteps | 941497   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 9.11     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 941396   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=2745.33 +/- 1103.26\n",
      "Episode length: 840.60 +/- 318.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 841      |\n",
      "|    mean_reward     | 2.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -0.293   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 949899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 907      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1220     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5764     |\n",
      "|    total_timesteps | 951354   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.0873   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 951253   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1230     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5812     |\n",
      "|    total_timesteps | 959553   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 8.49     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.419    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959452   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=1431.12 +/- 1106.54\n",
      "Episode length: 724.20 +/- 338.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 724      |\n",
      "|    mean_reward     | 1.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -222     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | -0.418   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1240     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5857     |\n",
      "|    total_timesteps | 967115   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 967014   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=3281.88 +/- 659.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 18.3     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | 0.358    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 969899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1250     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5909     |\n",
      "|    total_timesteps | 975703   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -216     |\n",
      "|    critic_loss     | 9.31     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 0.848    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 975602   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=2781.90 +/- 1011.87\n",
      "Episode length: 921.20 +/- 157.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 921      |\n",
      "|    mean_reward     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -217     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 0.217    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 979899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1260     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5950     |\n",
      "|    total_timesteps | 982447   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -222     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | -0.771   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 982346   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=2907.40 +/- 1002.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.0081  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 989899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1270     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 5997     |\n",
      "|    total_timesteps | 990210   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -218     |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -1.4     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 990109   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 2.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1280     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6050     |\n",
      "|    total_timesteps | 999104   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -225     |\n",
      "|    critic_loss     | 6.75     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.196   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999003   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=3652.30 +/- 83.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 8.53     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 0.832    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1290     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6099     |\n",
      "|    total_timesteps | 1007254  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -0.727   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1007153  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=2201.93 +/- 959.98\n",
      "Episode length: 870.80 +/- 188.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 871      |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -225     |\n",
      "|    critic_loss     | 7.63     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1300     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6152     |\n",
      "|    total_timesteps | 1016121  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -228     |\n",
      "|    critic_loss     | 9.74     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.9      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1016020  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=3126.47 +/- 1253.87\n",
      "Episode length: 835.60 +/- 328.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 836      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -230     |\n",
      "|    critic_loss     | 7.59     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.531    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1310     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6214     |\n",
      "|    total_timesteps | 1026121  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -229     |\n",
      "|    critic_loss     | 9.01     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1026020  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=2444.46 +/- 1388.57\n",
      "Episode length: 859.80 +/- 280.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 2.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 22.4     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.181   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1320     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6264     |\n",
      "|    total_timesteps | 1034193  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 13.6     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.238   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1034092  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=2517.30 +/- 1152.45\n",
      "Episode length: 672.20 +/- 291.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 672      |\n",
      "|    mean_reward     | 2.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.147    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 2.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1330     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 6317     |\n",
      "|    total_timesteps | 1042496  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -229     |\n",
      "|    critic_loss     | 8.1      |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 2.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1042395  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=3472.82 +/- 607.85\n",
      "Episode length: 919.60 +/- 160.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 920      |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -221     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -0.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1340     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6371     |\n",
      "|    total_timesteps | 1050969  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -236     |\n",
      "|    critic_loss     | 22.8     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.617    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1050868  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=2080.48 +/- 1559.12\n",
      "Episode length: 700.00 +/- 377.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 700      |\n",
      "|    mean_reward     | 2.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -227     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -0.642   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1350     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6430     |\n",
      "|    total_timesteps | 1060136  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -234     |\n",
      "|    critic_loss     | 5.99     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.263   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1060035  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1360     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6488     |\n",
      "|    total_timesteps | 1069463  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -240     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.501    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069362  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=2062.94 +/- 1330.74\n",
      "Episode length: 702.40 +/- 367.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 702      |\n",
      "|    mean_reward     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -238     |\n",
      "|    critic_loss     | 9.63     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.61     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1370     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6529     |\n",
      "|    total_timesteps | 1076323  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -235     |\n",
      "|    critic_loss     | 7.12     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.812   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1076222  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=3143.17 +/- 1350.97\n",
      "Episode length: 823.60 +/- 352.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -235     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.667   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1380     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6581     |\n",
      "|    total_timesteps | 1084974  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -233     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.788   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1084873  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=3886.55 +/- 142.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -244     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.538   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1089899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1390     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6637     |\n",
      "|    total_timesteps | 1094255  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 0.558    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1094154  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=3094.73 +/- 1041.57\n",
      "Episode length: 863.80 +/- 272.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 864      |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -243     |\n",
      "|    critic_loss     | 9.08     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1400     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6688     |\n",
      "|    total_timesteps | 1102633  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -243     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1102532  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=2809.93 +/- 1225.59\n",
      "Episode length: 896.60 +/- 206.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 897      |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -236     |\n",
      "|    critic_loss     | 8.06     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.505   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1410     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6742     |\n",
      "|    total_timesteps | 1111584  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -231     |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.356    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1111483  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=2807.10 +/- 1238.79\n",
      "Episode length: 894.20 +/- 211.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 894      |\n",
      "|    mean_reward     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -241     |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.888    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1420     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6793     |\n",
      "|    total_timesteps | 1120010  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -236     |\n",
      "|    critic_loss     | 17.7     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.658    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119909  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1430     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6849     |\n",
      "|    total_timesteps | 1129618  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129517  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=3177.11 +/- 1359.46\n",
      "Episode length: 824.20 +/- 351.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -244     |\n",
      "|    critic_loss     | 8.87     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.00847  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1440     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6900     |\n",
      "|    total_timesteps | 1138073  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -241     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.456   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1137972  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=3530.18 +/- 500.75\n",
      "Episode length: 936.20 +/- 127.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 936      |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 25       |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.145   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1450     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 6940     |\n",
      "|    total_timesteps | 1144644  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -244     |\n",
      "|    critic_loss     | 20.5     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.177    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1144543  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=2866.66 +/- 1135.86\n",
      "Episode length: 881.40 +/- 237.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 881      |\n",
      "|    mean_reward     | 2.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -249     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.0529   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1460     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 7000     |\n",
      "|    total_timesteps | 1154566  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -245     |\n",
      "|    critic_loss     | 8.17     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -0.415   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1154465  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=2244.86 +/- 1404.41\n",
      "Episode length: 582.20 +/- 353.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 582      |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -243     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.393   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1470     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 7049     |\n",
      "|    total_timesteps | 1162889  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -253     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | 0.839    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1162788  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=3013.95 +/- 1026.40\n",
      "Episode length: 883.40 +/- 233.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 883      |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -247     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -0.142   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1480     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 7104     |\n",
      "|    total_timesteps | 1171999  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -0.858   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1171898  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=3572.42 +/- 635.99\n",
      "Episode length: 913.00 +/- 174.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 913      |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 16.8     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -0.235   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1490     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 7156     |\n",
      "|    total_timesteps | 1180580  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 9.47     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 0.245    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1180479  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1500     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7202     |\n",
      "|    total_timesteps | 1188621  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -253     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.927   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1188520  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=1398.87 +/- 561.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 19.5     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.511   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1510     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7250     |\n",
      "|    total_timesteps | 1196446  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.353    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1196345  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=3802.45 +/- 245.40\n",
      "Episode length: 980.80 +/- 38.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 981      |\n",
      "|    mean_reward     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 31.4     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | -1.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1520     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7309     |\n",
      "|    total_timesteps | 1206295  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 1.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1206194  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=3740.05 +/- 239.20\n",
      "Episode length: 987.40 +/- 25.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 987      |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -249     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -0.408   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1530     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7363     |\n",
      "|    total_timesteps | 1215158  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -257     |\n",
      "|    critic_loss     | 9.68     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -1.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1215057  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=3322.53 +/- 1193.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -247     |\n",
      "|    critic_loss     | 8.49     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 0.0747   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1540     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7417     |\n",
      "|    total_timesteps | 1224101  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -259     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 1        |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1224000  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=3311.16 +/- 1085.65\n",
      "Episode length: 827.20 +/- 264.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -262     |\n",
      "|    critic_loss     | 6.66     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | 0.189    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 892      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1550     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7475     |\n",
      "|    total_timesteps | 1233892  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1233791  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=3482.51 +/- 519.92\n",
      "Episode length: 897.80 +/- 126.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 898      |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 14.3     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | 0.963    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1560     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7525     |\n",
      "|    total_timesteps | 1242104  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1242003  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=3919.03 +/- 108.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -255     |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.858    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1249899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1570     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7573     |\n",
      "|    total_timesteps | 1250123  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -269     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -0.275   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1250022  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 876      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1580     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7628     |\n",
      "|    total_timesteps | 1259560  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -266     |\n",
      "|    critic_loss     | 24.6     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.106    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259459  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=2040.35 +/- 1355.49\n",
      "Episode length: 671.80 +/- 352.61\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 672      |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 6.07     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.265   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1590     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7679     |\n",
      "|    total_timesteps | 1268044  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -271     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.798   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1267943  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=2732.32 +/- 1004.50\n",
      "Episode length: 703.40 +/- 259.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 703      |\n",
      "|    mean_reward     | 2.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -271     |\n",
      "|    critic_loss     | 14       |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | 0.378    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1600     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7729     |\n",
      "|    total_timesteps | 1276338  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -266     |\n",
      "|    critic_loss     | 37       |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1276237  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=3777.18 +/- 272.27\n",
      "Episode length: 965.40 +/- 69.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 965      |\n",
      "|    mean_reward     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | 0.365    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1610     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7785     |\n",
      "|    total_timesteps | 1285785  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -273     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.424   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1285684  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=3672.21 +/- 570.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -278     |\n",
      "|    critic_loss     | 73.1     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1620     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7835     |\n",
      "|    total_timesteps | 1294051  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 25.7     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | -0.799   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1293950  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=3358.36 +/- 1199.38\n",
      "Episode length: 850.60 +/- 298.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 851      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -275     |\n",
      "|    critic_loss     | 8.96     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.699    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1630     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7884     |\n",
      "|    total_timesteps | 1302048  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -276     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1301947  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1640     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7924     |\n",
      "|    total_timesteps | 1308883  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 8.11     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.979   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1308782  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=3786.33 +/- 502.08\n",
      "Episode length: 942.80 +/- 114.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 943      |\n",
      "|    mean_reward     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 24       |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | -0.407   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 2.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1650     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7978     |\n",
      "|    total_timesteps | 1318032  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.331   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1317931  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=3852.84 +/- 406.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -277     |\n",
      "|    critic_loss     | 6.25     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1660     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8033     |\n",
      "|    total_timesteps | 1327152  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -274     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1327051  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=2852.20 +/- 1535.80\n",
      "Episode length: 810.00 +/- 380.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 810      |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1670     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8087     |\n",
      "|    total_timesteps | 1336059  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -0.608   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1335958  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=2853.58 +/- 993.35\n",
      "Episode length: 713.60 +/- 248.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 714      |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -1.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1680     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8143     |\n",
      "|    total_timesteps | 1345494  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 9.32     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.657   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1345393  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=3482.61 +/- 887.28\n",
      "Episode length: 882.00 +/- 217.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 882      |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.708   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1690     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8182     |\n",
      "|    total_timesteps | 1351983  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -285     |\n",
      "|    critic_loss     | 18.4     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.899    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1351882  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1700     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8220     |\n",
      "|    total_timesteps | 1358465  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.395   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1358364  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=3300.09 +/- 1132.59\n",
      "Episode length: 964.40 +/- 71.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 964      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -283     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.761   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1710     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8271     |\n",
      "|    total_timesteps | 1366834  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 20.9     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1366733  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=3641.57 +/- 931.80\n",
      "Episode length: 886.80 +/- 226.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 887      |\n",
      "|    mean_reward     | 3.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 22.1     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.316    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1720     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8321     |\n",
      "|    total_timesteps | 1375217  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -288     |\n",
      "|    critic_loss     | 13.9     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.489    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1375116  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=-534.12 +/- 826.89\n",
      "Episode length: 860.80 +/- 278.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 861      |\n",
      "|    mean_reward     | -534     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | 0.599    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 2.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1730     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8363     |\n",
      "|    total_timesteps | 1382027  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 38.9     |\n",
      "|    ent_coef        | 0.0649   |\n",
      "|    ent_coef_loss   | 2.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1381926  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 2.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1740     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8401     |\n",
      "|    total_timesteps | 1388475  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0505   |\n",
      "|    ent_coef_loss   | 0.116    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1388374  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=-171.95 +/- 548.72\n",
      "Episode length: 816.80 +/- 366.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 817      |\n",
      "|    mean_reward     | -172     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 27.4     |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | 2.67     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 796      |\n",
      "|    ep_rew_mean     | 2.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1750     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8456     |\n",
      "|    total_timesteps | 1397590  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 16.2     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.191    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1397489  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=1803.26 +/- 773.45\n",
      "Episode length: 496.40 +/- 254.91\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 496      |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 31.8     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.0121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1760     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8498     |\n",
      "|    total_timesteps | 1404682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 29.2     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1404581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=3471.44 +/- 822.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 786      |\n",
      "|    ep_rew_mean     | 1.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1770     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8558     |\n",
      "|    total_timesteps | 1414682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.131   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1414581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=2839.93 +/- 1692.76\n",
      "Episode length: 937.40 +/- 125.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 937      |\n",
      "|    mean_reward     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 17       |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 1.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1780     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8610     |\n",
      "|    total_timesteps | 1423182  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 13.9     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.312   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1423081  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=2996.22 +/- 721.55\n",
      "Episode length: 754.40 +/- 188.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 754      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1790     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8661     |\n",
      "|    total_timesteps | 1431739  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1431638  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=3371.77 +/- 1470.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 0.981    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 2.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1800     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8721     |\n",
      "|    total_timesteps | 1441739  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 47.1     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 0.326    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1441638  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 1.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1810     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8759     |\n",
      "|    total_timesteps | 1448293  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 18.5     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.887   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1448192  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=2767.53 +/- 1389.70\n",
      "Episode length: 880.80 +/- 187.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 881      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.899   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1820     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8812     |\n",
      "|    total_timesteps | 1457085  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 89.5     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | 0.758    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1456984  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=4138.79 +/- 499.07\n",
      "Episode length: 954.40 +/- 91.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 954      |\n",
      "|    mean_reward     | 4.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -292     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.872   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1459899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 836      |\n",
      "|    ep_rew_mean     | 2.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1830     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8864     |\n",
      "|    total_timesteps | 1465584  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -291     |\n",
      "|    critic_loss     | 23.3     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.207   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1465483  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=3611.16 +/- 1288.53\n",
      "Episode length: 846.80 +/- 306.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 847      |\n",
      "|    mean_reward     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 10       |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.648    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1840     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8915     |\n",
      "|    total_timesteps | 1474096  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 21.2     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.862   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1473995  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=3716.95 +/- 844.86\n",
      "Episode length: 901.80 +/- 196.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 902      |\n",
      "|    mean_reward     | 3.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 19       |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1850     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 8969     |\n",
      "|    total_timesteps | 1483115  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.413   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1483014  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 840      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1860     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9002     |\n",
      "|    total_timesteps | 1488723  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.557    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1488622  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=2659.81 +/- 1885.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.555   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1870     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9045     |\n",
      "|    total_timesteps | 1495816  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 9.11     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.286    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1495715  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=4101.09 +/- 101.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 63.7     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.453    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1880     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9086     |\n",
      "|    total_timesteps | 1502645  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 41.9     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 0.809    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1502544  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=4180.35 +/- 113.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 32.4     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -0.599   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1509899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1890     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9137     |\n",
      "|    total_timesteps | 1511057  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 0.876    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1510956  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=2223.62 +/- 1684.10\n",
      "Episode length: 885.60 +/- 228.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 886      |\n",
      "|    mean_reward     | 2.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 10       |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.409   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 2.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1900     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9196     |\n",
      "|    total_timesteps | 1520893  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -284     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | 0.087    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1520792  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1910     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9226     |\n",
      "|    total_timesteps | 1526001  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 24.5     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.317   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1525900  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=3943.31 +/- 300.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 19.7     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.695    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1920     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9273     |\n",
      "|    total_timesteps | 1533767  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 1.72     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1533666  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=1980.61 +/- 1789.95\n",
      "Episode length: 628.00 +/- 455.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 628      |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 18       |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.876    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1930     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9322     |\n",
      "|    total_timesteps | 1542000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 34.3     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 1.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1541899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1940     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9368     |\n",
      "|    total_timesteps | 1549858  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 23.6     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549757  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=3708.70 +/- 689.10\n",
      "Episode length: 967.40 +/- 65.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 967      |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 1.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 741      |\n",
      "|    ep_rew_mean     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1950     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9412     |\n",
      "|    total_timesteps | 1557172  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1557071  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=3514.88 +/- 1586.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.433    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1960     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9458     |\n",
      "|    total_timesteps | 1564734  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 18.3     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | -0.231   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1564633  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=1602.71 +/- 1735.62\n",
      "Episode length: 402.00 +/- 413.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 402      |\n",
      "|    mean_reward     | 1.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 44.9     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 2.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 753      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1970     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9496     |\n",
      "|    total_timesteps | 1571110  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 26.5     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1571009  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1980     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9546     |\n",
      "|    total_timesteps | 1579771  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.0365   |\n",
      "|    ent_coef_loss   | 0.208    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579670  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=726.66 +/- 556.64\n",
      "Episode length: 515.40 +/- 405.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 515      |\n",
      "|    mean_reward     | 727      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 18.5     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | 0.436    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 2.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1990     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9590     |\n",
      "|    total_timesteps | 1587024  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 16       |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.224   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1586923  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=3293.07 +/- 1034.27\n",
      "Episode length: 791.80 +/- 264.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 792      |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 18.2     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.0647   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 2.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2000     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9632     |\n",
      "|    total_timesteps | 1594016  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1593915  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=3133.37 +/- 1790.89\n",
      "Episode length: 938.00 +/- 124.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 938      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 9.05     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 769      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2010     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9685     |\n",
      "|    total_timesteps | 1602854  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1602753  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=2259.30 +/- 1173.08\n",
      "Episode length: 800.60 +/- 284.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 801      |\n",
      "|    mean_reward     | 2.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 20.3     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.0881   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2020     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9739     |\n",
      "|    total_timesteps | 1611822  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 16.9     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.00702  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1611721  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 769      |\n",
      "|    ep_rew_mean     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2030     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9780     |\n",
      "|    total_timesteps | 1618949  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 9.44     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1618848  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=4044.19 +/- 107.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | 0.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 2.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2040     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9829     |\n",
      "|    total_timesteps | 1627083  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 87.7     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.707    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1626982  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1630000, episode_reward=4266.50 +/- 132.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | -0.109   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1629899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 2.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2050     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9889     |\n",
      "|    total_timesteps | 1637056  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -0.994   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1636955  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=2834.31 +/- 1105.74\n",
      "Episode length: 684.80 +/- 263.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 685      |\n",
      "|    mean_reward     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.99    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2060     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9945     |\n",
      "|    total_timesteps | 1646349  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -0.761   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1646248  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=3603.95 +/- 1010.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.622   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 2.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2070     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9997     |\n",
      "|    total_timesteps | 1654948  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.506    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1654847  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=2654.35 +/- 1907.50\n",
      "Episode length: 637.80 +/- 446.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 638      |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 29       |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | 0.0249   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2080     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10050    |\n",
      "|    total_timesteps | 1663954  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 66.6     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1663853  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1670000, episode_reward=3690.26 +/- 1107.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 2.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2090     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10102    |\n",
      "|    total_timesteps | 1672513  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 6.88     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | 0.734    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1672412  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=4110.08 +/- 158.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 23.3     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2100     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10149    |\n",
      "|    total_timesteps | 1680296  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 15.8     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | -0.157   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1680195  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2110     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10200    |\n",
      "|    total_timesteps | 1688968  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 0.769    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1688867  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=3355.18 +/- 853.15\n",
      "Episode length: 894.40 +/- 203.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 894      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 63.6     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2120     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10249    |\n",
      "|    total_timesteps | 1697096  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 14.3     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.0446  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1696995  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=3308.18 +/- 1260.67\n",
      "Episode length: 942.20 +/- 115.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 942      |\n",
      "|    mean_reward     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | 0.758    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2130     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10297    |\n",
      "|    total_timesteps | 1704966  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 16.2     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | 0.431    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1704865  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=3088.95 +/- 1314.45\n",
      "Episode length: 881.80 +/- 236.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 882      |\n",
      "|    mean_reward     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 22.2     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.629    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2140     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10339    |\n",
      "|    total_timesteps | 1711965  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 75.3     |\n",
      "|    ent_coef        | 0.0371   |\n",
      "|    ent_coef_loss   | 0.591    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1711864  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2150     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10386    |\n",
      "|    total_timesteps | 1719912  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0373   |\n",
      "|    ent_coef_loss   | -0.884   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719811  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=3838.01 +/- 915.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 14       |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2160     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10443    |\n",
      "|    total_timesteps | 1729439  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729338  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=2456.31 +/- 1652.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 30.9     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | 0.554    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2170     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10503    |\n",
      "|    total_timesteps | 1739335  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -294     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | -0.729   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739234  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=2358.41 +/- 1618.26\n",
      "Episode length: 802.60 +/- 394.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 803      |\n",
      "|    mean_reward     | 2.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | -0.933   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2180     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10548    |\n",
      "|    total_timesteps | 1746760  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 14       |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1746659  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=4236.68 +/- 115.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | -0.688   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2190     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10601    |\n",
      "|    total_timesteps | 1755509  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    ent_coef        | 0.037    |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1755408  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=3233.95 +/- 1319.69\n",
      "Episode length: 892.20 +/- 215.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 892      |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | -0.357   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 2.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2200     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10648    |\n",
      "|    total_timesteps | 1763295  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 20.8     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | -0.716   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1763194  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1770000, episode_reward=3163.87 +/- 1180.81\n",
      "Episode length: 903.40 +/- 193.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 903      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | 0.804    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 832      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2210     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10701    |\n",
      "|    total_timesteps | 1772150  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | -0.419   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1772049  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=3887.47 +/- 633.37\n",
      "Episode length: 931.20 +/- 137.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 931      |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | -0.102   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2220     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10753    |\n",
      "|    total_timesteps | 1780579  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 0.107    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1780478  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2230     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10804    |\n",
      "|    total_timesteps | 1789383  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | 0.227    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789282  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=4220.06 +/- 131.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 21.9     |\n",
      "|    ent_coef        | 0.0377   |\n",
      "|    ent_coef_loss   | -0.597   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2240     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10860    |\n",
      "|    total_timesteps | 1798695  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 22.3     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | 0.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1798594  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1800000, episode_reward=3997.04 +/- 341.00\n",
      "Episode length: 958.60 +/- 82.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 959      |\n",
      "|    mean_reward     | 4e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.189   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 882      |\n",
      "|    ep_rew_mean     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2250     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10917    |\n",
      "|    total_timesteps | 1808144  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 73.6     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | 0.136    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1808043  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=3443.02 +/- 741.92\n",
      "Episode length: 912.20 +/- 175.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 912      |\n",
      "|    mean_reward     | 3.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 15.2     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | -2.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2260     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10966    |\n",
      "|    total_timesteps | 1816344  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 9.5      |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | -0.108   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1816243  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=3572.93 +/- 1082.08\n",
      "Episode length: 834.40 +/- 235.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 834      |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | 0.923    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 850      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2270     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11015    |\n",
      "|    total_timesteps | 1824374  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -0.745   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1824273  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=2577.75 +/- 1832.97\n",
      "Episode length: 907.40 +/- 185.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 907      |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | 0.0937   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2280     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11071    |\n",
      "|    total_timesteps | 1833671  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | -0.107   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1833570  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=1802.33 +/- 1026.71\n",
      "Episode length: 666.60 +/- 292.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 667      |\n",
      "|    mean_reward     | 1.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 20.3     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -0.69    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2290     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11130    |\n",
      "|    total_timesteps | 1843598  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 23.9     |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.315    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1843497  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1850000, episode_reward=1981.18 +/- 1958.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 1.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -0.445   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2300     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11181    |\n",
      "|    total_timesteps | 1852002  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -0.417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1851901  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=2707.14 +/- 1778.83\n",
      "Episode length: 803.40 +/- 295.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 803      |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | -0.207   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 896      |\n",
      "|    ep_rew_mean     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2310     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11239    |\n",
      "|    total_timesteps | 1861753  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 2.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1861652  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=3207.76 +/- 1621.87\n",
      "Episode length: 743.60 +/- 367.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 744      |\n",
      "|    mean_reward     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 30.4     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | 0.678    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 894      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2320     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11289    |\n",
      "|    total_timesteps | 1870017  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | 0.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869916  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2330     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11331    |\n",
      "|    total_timesteps | 1877193  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 16.3     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -0.0795  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1877092  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1880000, episode_reward=3932.03 +/- 1003.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 16.4     |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | 0.317    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2340     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11373    |\n",
      "|    total_timesteps | 1884132  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | 0.151    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1884031  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=2623.07 +/- 1527.27\n",
      "Episode length: 611.60 +/- 342.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 612      |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 72.6     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | -0.0447  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2350     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11425    |\n",
      "|    total_timesteps | 1892887  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | 0.167    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1892786  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=3693.35 +/- 787.12\n",
      "Episode length: 856.40 +/- 199.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 856      |\n",
      "|    mean_reward     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 22       |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.721   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2360     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11474    |\n",
      "|    total_timesteps | 1900883  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    ent_coef        | 0.0384   |\n",
      "|    ent_coef_loss   | -1.74    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1900782  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=3674.45 +/- 1094.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 34.4     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.954   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2370     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11533    |\n",
      "|    total_timesteps | 1910693  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 32.2     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1910592  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=3733.13 +/- 1041.01\n",
      "Episode length: 968.60 +/- 62.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 969      |\n",
      "|    mean_reward     | 3.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | 0.141    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2380     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11593    |\n",
      "|    total_timesteps | 1920693  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | -1.46    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1920592  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2390     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11638    |\n",
      "|    total_timesteps | 1928444  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.185   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1928343  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=4115.64 +/- 516.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | 0.145    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2400     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11694    |\n",
      "|    total_timesteps | 1937747  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 12.1     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | -0.495   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1937646  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=3123.11 +/- 1614.41\n",
      "Episode length: 719.20 +/- 369.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 719      |\n",
      "|    mean_reward     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 66.6     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | -0.422   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2410     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11744    |\n",
      "|    total_timesteps | 1946105  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1946004  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=2892.90 +/- 1736.65\n",
      "Episode length: 679.20 +/- 393.16\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 679      |\n",
      "|    mean_reward     | 2.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 46.3     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 1.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2420     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11797    |\n",
      "|    total_timesteps | 1954922  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 25.7     |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.719    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1954821  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=3014.75 +/- 1066.83\n",
      "Episode length: 701.00 +/- 251.98\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 701      |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 13.6     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | 0.166    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2430     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11843    |\n",
      "|    total_timesteps | 1962640  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1962539  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=3739.48 +/- 1144.83\n",
      "Episode length: 872.20 +/- 255.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 872      |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | 0.316    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2440     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11891    |\n",
      "|    total_timesteps | 1970537  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 27.1     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | -0.664   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1970436  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2450     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11936    |\n",
      "|    total_timesteps | 1978294  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 59.4     |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | -0.409   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1978193  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=3560.93 +/- 1081.60\n",
      "Episode length: 803.60 +/- 243.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 16.1     |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.0963   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 852      |\n",
      "|    ep_rew_mean     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2460     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11983    |\n",
      "|    total_timesteps | 1986120  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 18.2     |\n",
      "|    ent_coef        | 0.0388   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1986019  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1990000, episode_reward=3471.61 +/- 1281.42\n",
      "Episode length: 850.40 +/- 299.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 850      |\n",
      "|    mean_reward     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.665   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 852      |\n",
      "|    ep_rew_mean     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2470     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12042    |\n",
      "|    total_timesteps | 1995930  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 19.5     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1995829  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=4122.33 +/- 547.71\n",
      "Episode length: 961.00 +/- 78.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 961      |\n",
      "|    mean_reward     | 4.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 24.1     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | 0.813    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2480     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12084    |\n",
      "|    total_timesteps | 2002882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0391   |\n",
      "|    ent_coef_loss   | 0.487    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2002781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=2630.16 +/- 1369.56\n",
      "Episode length: 793.00 +/- 266.42\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 793      |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 16.4     |\n",
      "|    ent_coef        | 0.0389   |\n",
      "|    ent_coef_loss   | 0.338    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 833      |\n",
      "|    ep_rew_mean     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2490     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12137    |\n",
      "|    total_timesteps | 2011733  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 16.2     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | -0.772   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2011632  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 800      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2500     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12172    |\n",
      "|    total_timesteps | 2017697  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 14.2     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2017596  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=3837.24 +/- 1166.81\n",
      "Episode length: 868.20 +/- 263.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 868      |\n",
      "|    mean_reward     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -300     |\n",
      "|    critic_loss     | 40.8     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | 0.642    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 789      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2510     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12216    |\n",
      "|    total_timesteps | 2024985  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2024884  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=3178.08 +/- 1559.28\n",
      "Episode length: 741.60 +/- 353.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 742      |\n",
      "|    mean_reward     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 17.6     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.0727   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2520     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12261    |\n",
      "|    total_timesteps | 2032411  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -314     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.121    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2032310  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=3507.57 +/- 822.62\n",
      "Episode length: 853.20 +/- 180.08\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | 0.0374   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 782      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2530     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12312    |\n",
      "|    total_timesteps | 2040860  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 15       |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | 0.422    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2040759  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2540     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12357    |\n",
      "|    total_timesteps | 2048546  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 22.7     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2048445  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=3494.38 +/- 1508.35\n",
      "Episode length: 785.40 +/- 328.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 785      |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0407   |\n",
      "|    ent_coef_loss   | -0.153   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 778      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2550     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12402    |\n",
      "|    total_timesteps | 2056138  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 20.9     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | 0.202    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2056037  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=3810.61 +/- 1209.59\n",
      "Episode length: 866.40 +/- 266.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 866      |\n",
      "|    mean_reward     | 3.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | 0.217    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2560     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12459    |\n",
      "|    total_timesteps | 2065649  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0396   |\n",
      "|    ent_coef_loss   | -0.138   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2065548  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=3226.37 +/- 1487.62\n",
      "Episode length: 732.80 +/- 327.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 733      |\n",
      "|    mean_reward     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -305     |\n",
      "|    critic_loss     | 15.2     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.779   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2570     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12505    |\n",
      "|    total_timesteps | 2073342  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 18.7     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2073241  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=2862.57 +/- 1939.52\n",
      "Episode length: 650.40 +/- 430.33\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 650      |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 48.4     |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | -0.0815  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2580     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12562    |\n",
      "|    total_timesteps | 2082811  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | 0.952    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2082710  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=3420.70 +/- 1432.04\n",
      "Episode length: 776.00 +/- 325.51\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 776      |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 63.2     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | 0.756    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2590     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12621    |\n",
      "|    total_timesteps | 2092716  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -307     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | 1.83     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2092615  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 817       |\n",
      "|    ep_rew_mean     | 3.25e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2600      |\n",
      "|    fps             | 165       |\n",
      "|    time_elapsed    | 12660     |\n",
      "|    total_timesteps | 2099426   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -318      |\n",
      "|    critic_loss     | 30.8      |\n",
      "|    ent_coef        | 0.0411    |\n",
      "|    ent_coef_loss   | -0.000713 |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2099325   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=2930.46 +/- 1979.70\n",
      "Episode length: 653.40 +/- 428.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 653      |\n",
      "|    mean_reward     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 60       |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | 0.551    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2610     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12704    |\n",
      "|    total_timesteps | 2106709  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 20.4     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 0.328    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2106608  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=3601.80 +/- 1608.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | 0.118    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2620     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12749    |\n",
      "|    total_timesteps | 2114107  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.0416   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2114006  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=1987.98 +/- 1574.32\n",
      "Episode length: 604.80 +/- 402.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 605      |\n",
      "|    mean_reward     | 1.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -306     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.0425   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2630     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12803    |\n",
      "|    total_timesteps | 2123118  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 38.4     |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | -0.494   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2123017  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=4025.07 +/- 1039.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -312     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | -1.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2640     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12858    |\n",
      "|    total_timesteps | 2132262  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 31.5     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | 0.668    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2132161  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2650     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12901    |\n",
      "|    total_timesteps | 2139581  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -0.0347  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139480  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=4034.37 +/- 773.15\n",
      "Episode length: 897.80 +/- 153.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 898      |\n",
      "|    mean_reward     | 4.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.0429   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2660     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 12949    |\n",
      "|    total_timesteps | 2147532  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 16.5     |\n",
      "|    ent_coef        | 0.0411   |\n",
      "|    ent_coef_loss   | -0.567   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2147431  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=3581.58 +/- 1097.16\n",
      "Episode length: 812.00 +/- 239.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 812      |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 26.3     |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | 0.308    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2670     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13008    |\n",
      "|    total_timesteps | 2157442  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 24.8     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | -0.629   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2157341  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=2565.43 +/- 1748.04\n",
      "Episode length: 582.00 +/- 380.26\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 582      |\n",
      "|    mean_reward     | 2.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 47.7     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | 0.533    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2680     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13051    |\n",
      "|    total_timesteps | 2164622  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 33.9     |\n",
      "|    ent_coef        | 0.0427   |\n",
      "|    ent_coef_loss   | 0.494    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2164521  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=4154.55 +/- 771.84\n",
      "Episode length: 914.60 +/- 170.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 915      |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 56.4     |\n",
      "|    ent_coef        | 0.0422   |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2690     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13095    |\n",
      "|    total_timesteps | 2171885  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2171784  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2180000, episode_reward=4050.37 +/- 621.67\n",
      "Episode length: 897.20 +/- 111.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 897      |\n",
      "|    mean_reward     | 4.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | 0.287    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2700     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13154    |\n",
      "|    total_timesteps | 2181736  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0428   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2181635  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2710     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13197    |\n",
      "|    total_timesteps | 2189085  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 47.6     |\n",
      "|    ent_coef        | 0.0427   |\n",
      "|    ent_coef_loss   | -0.983   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2188984  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=3350.69 +/- 1427.78\n",
      "Episode length: 872.80 +/- 254.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 873      |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0438   |\n",
      "|    ent_coef_loss   | 1.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2720     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13242    |\n",
      "|    total_timesteps | 2196493  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -0.596   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2196392  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=4468.63 +/- 110.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 45.2     |\n",
      "|    ent_coef        | 0.0441   |\n",
      "|    ent_coef_loss   | -0.507   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2199899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2730     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13294    |\n",
      "|    total_timesteps | 2205168  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 22       |\n",
      "|    ent_coef        | 0.0429   |\n",
      "|    ent_coef_loss   | -0.827   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2205067  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=2766.67 +/- 1413.26\n",
      "Episode length: 627.00 +/- 316.54\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 627      |\n",
      "|    mean_reward     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 803      |\n",
      "|    ep_rew_mean     | 3.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2740     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13338    |\n",
      "|    total_timesteps | 2212572  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | 0.174    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2212471  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=3774.37 +/- 1602.62\n",
      "Episode length: 830.20 +/- 339.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 830      |\n",
      "|    mean_reward     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 46.1     |\n",
      "|    ent_coef        | 0.045    |\n",
      "|    ent_coef_loss   | 0.524    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 3.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2750     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13394    |\n",
      "|    total_timesteps | 2221884  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | 0.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2221783  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2760     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13420    |\n",
      "|    total_timesteps | 2226274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 27.2     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2226173  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=4369.71 +/- 215.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | -0.282   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 766      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2770     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13468    |\n",
      "|    total_timesteps | 2234040  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 68.5     |\n",
      "|    ent_coef        | 0.0451   |\n",
      "|    ent_coef_loss   | 0.407    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2233939  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=3846.00 +/- 1281.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 33.1     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 0.711    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2780     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13518    |\n",
      "|    total_timesteps | 2242233  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 20.4     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | 0.846    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2242132  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=4542.66 +/- 58.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 36.8     |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | -0.763   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2249899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 788      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2790     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13569    |\n",
      "|    total_timesteps | 2250700  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 42.9     |\n",
      "|    ent_coef        | 0.0446   |\n",
      "|    ent_coef_loss   | 0.908    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2250599  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2800     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13617    |\n",
      "|    total_timesteps | 2258799  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 18.9     |\n",
      "|    ent_coef        | 0.0449   |\n",
      "|    ent_coef_loss   | 0.236    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2258698  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=3134.40 +/- 1689.14\n",
      "Episode length: 878.40 +/- 243.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 878      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 38.2     |\n",
      "|    ent_coef        | 0.0449   |\n",
      "|    ent_coef_loss   | -0.501   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2810     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13674    |\n",
      "|    total_timesteps | 2268331  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.0446   |\n",
      "|    ent_coef_loss   | -0.458   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2268230  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=3415.15 +/- 1439.58\n",
      "Episode length: 773.00 +/- 325.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 773      |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 15.5     |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2820     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13722    |\n",
      "|    total_timesteps | 2276383  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -316     |\n",
      "|    critic_loss     | 25.9     |\n",
      "|    ent_coef        | 0.0452   |\n",
      "|    ent_coef_loss   | 1.64     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2276282  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=3156.52 +/- 1183.53\n",
      "Episode length: 845.60 +/- 308.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 846      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | 0.194    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2830     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13766    |\n",
      "|    total_timesteps | 2283636  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 22.3     |\n",
      "|    ent_coef        | 0.0453   |\n",
      "|    ent_coef_loss   | 1.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2283535  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=3434.51 +/- 1166.83\n",
      "Episode length: 755.40 +/- 258.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 755      |\n",
      "|    mean_reward     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 21.6     |\n",
      "|    ent_coef        | 0.0451   |\n",
      "|    ent_coef_loss   | -0.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2840     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13817    |\n",
      "|    total_timesteps | 2292109  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 19.3     |\n",
      "|    ent_coef        | 0.0459   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2292008  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2850     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13855    |\n",
      "|    total_timesteps | 2298603  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 21.3     |\n",
      "|    ent_coef        | 0.0462   |\n",
      "|    ent_coef_loss   | 0.584    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2298502  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=3724.46 +/- 1642.26\n",
      "Episode length: 822.40 +/- 355.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 822      |\n",
      "|    mean_reward     | 3.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 58.9     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -0.0946  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2860     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13901    |\n",
      "|    total_timesteps | 2306169  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 30.5     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | 0.618    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2306068  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2310000, episode_reward=2465.29 +/- 1583.36\n",
      "Episode length: 744.20 +/- 315.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 744      |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    ent_coef        | 0.0464   |\n",
      "|    ent_coef_loss   | 0.0747   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 3.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2870     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13951    |\n",
      "|    total_timesteps | 2314575  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 30.6     |\n",
      "|    ent_coef        | 0.0454   |\n",
      "|    ent_coef_loss   | 0.91     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2314474  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2320000, episode_reward=3696.63 +/- 1187.50\n",
      "Episode length: 812.80 +/- 242.93\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 813      |\n",
      "|    mean_reward     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.0457   |\n",
      "|    ent_coef_loss   | 0.0913   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2880     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 13986    |\n",
      "|    total_timesteps | 2320289  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 21.9     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2320188  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 762      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2890     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14024    |\n",
      "|    total_timesteps | 2326882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 52.1     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2326781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=3530.77 +/- 1486.47\n",
      "Episode length: 779.40 +/- 322.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 779      |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | -0.384   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2900     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14084    |\n",
      "|    total_timesteps | 2336882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 15.9     |\n",
      "|    ent_coef        | 0.0463   |\n",
      "|    ent_coef_loss   | -0.992   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2336781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=4042.02 +/- 1091.34\n",
      "Episode length: 887.20 +/- 225.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 887      |\n",
      "|    mean_reward     | 4.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 25.4     |\n",
      "|    ent_coef        | 0.0462   |\n",
      "|    ent_coef_loss   | 0.0865   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2910     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14130    |\n",
      "|    total_timesteps | 2344384  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 40.7     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2344283  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=2246.29 +/- 2233.04\n",
      "Episode length: 695.40 +/- 399.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 695      |\n",
      "|    mean_reward     | 2.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | 0.386    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2920     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14182    |\n",
      "|    total_timesteps | 2353105  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -317     |\n",
      "|    critic_loss     | 69.6     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | -0.562   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2353004  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 3.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2930     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14221    |\n",
      "|    total_timesteps | 2359761  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 22.7     |\n",
      "|    ent_coef        | 0.0473   |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359660  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2360000, episode_reward=3417.75 +/- 1467.45\n",
      "Episode length: 752.00 +/- 304.59\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 752      |\n",
      "|    mean_reward     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 30.1     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 0.992    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2940     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14277    |\n",
      "|    total_timesteps | 2369126  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 95.2     |\n",
      "|    ent_coef        | 0.0469   |\n",
      "|    ent_coef_loss   | 0.621    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2369025  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=4214.05 +/- 612.92\n",
      "Episode length: 901.20 +/- 148.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 901      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 20.6     |\n",
      "|    ent_coef        | 0.048    |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 3.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2950     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14325    |\n",
      "|    total_timesteps | 2377086  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.0479   |\n",
      "|    ent_coef_loss   | -0.0972  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2376985  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=4679.78 +/- 99.19\n",
      "Episode length: 988.80 +/- 22.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 989      |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 28.3     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | -0.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2379899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 790      |\n",
      "|    ep_rew_mean     | 3.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2960     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14374    |\n",
      "|    total_timesteps | 2385161  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 44.4     |\n",
      "|    ent_coef        | 0.0479   |\n",
      "|    ent_coef_loss   | -0.828   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2385060  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=3352.59 +/- 1317.63\n",
      "Episode length: 880.80 +/- 159.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 881      |\n",
      "|    mean_reward     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 20.2     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -0.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2970     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14426    |\n",
      "|    total_timesteps | 2393753  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 70.7     |\n",
      "|    ent_coef        | 0.0476   |\n",
      "|    ent_coef_loss   | 0.0919   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2393652  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=3735.70 +/- 2008.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | -0.559   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2980     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14471    |\n",
      "|    total_timesteps | 2401204  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 19.5     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | 0.832    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2401103  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2990     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14518    |\n",
      "|    total_timesteps | 2409257  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -319     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409156  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=4486.04 +/- 323.44\n",
      "Episode length: 970.60 +/- 58.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 971      |\n",
      "|    mean_reward     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 22.4     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | 1.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3000     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14566    |\n",
      "|    total_timesteps | 2417258  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 23.6     |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | 0.283    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2417157  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2420000, episode_reward=4172.96 +/- 915.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3010     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14616    |\n",
      "|    total_timesteps | 2425462  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0468   |\n",
      "|    ent_coef_loss   | -0.447   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2425361  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=2405.43 +/- 1820.00\n",
      "Episode length: 538.80 +/- 389.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 539      |\n",
      "|    mean_reward     | 2.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -324     |\n",
      "|    critic_loss     | 57.6     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | 0.722    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3020     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14668    |\n",
      "|    total_timesteps | 2434096  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | -0.0645  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2433995  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=2196.66 +/- 1992.51\n",
      "Episode length: 480.20 +/- 427.05\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 480      |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    ent_coef        | 0.0474   |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3030     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14724    |\n",
      "|    total_timesteps | 2443567  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    ent_coef        | 0.0487   |\n",
      "|    ent_coef_loss   | -0.979   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2443466  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2450000, episode_reward=4667.49 +/- 64.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 14.5     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | -1.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3040     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14764    |\n",
      "|    total_timesteps | 2450235  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -318     |\n",
      "|    critic_loss     | 29.8     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2450134  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3050     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14810    |\n",
      "|    total_timesteps | 2458134  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 46.2     |\n",
      "|    ent_coef        | 0.0474   |\n",
      "|    ent_coef_loss   | 0.85     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2458033  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2460000, episode_reward=4285.70 +/- 1025.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    ent_coef        | 0.0475   |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3060     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14856    |\n",
      "|    total_timesteps | 2465570  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 32.8     |\n",
      "|    ent_coef        | 0.048    |\n",
      "|    ent_coef_loss   | -0.762   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2465469  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=4384.76 +/- 566.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.0486   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3070     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14904    |\n",
      "|    total_timesteps | 2473624  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 32.3     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | -0.437   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2473523  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=4173.34 +/- 1270.75\n",
      "Episode length: 866.80 +/- 266.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 867      |\n",
      "|    mean_reward     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 41.6     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 0.00924  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3080     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 14959    |\n",
      "|    total_timesteps | 2482817  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.0472   |\n",
      "|    ent_coef_loss   | 1.3      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2482716  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=3633.33 +/- 1177.81\n",
      "Episode length: 901.60 +/- 196.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 902      |\n",
      "|    mean_reward     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 90.4     |\n",
      "|    ent_coef        | 0.0475   |\n",
      "|    ent_coef_loss   | 0.937    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3090     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15006    |\n",
      "|    total_timesteps | 2490477  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | -0.435   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2490376  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 782      |\n",
      "|    ep_rew_mean     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3100     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15035    |\n",
      "|    total_timesteps | 2495496  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 40.7     |\n",
      "|    ent_coef        | 0.0491   |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2495395  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=3833.53 +/- 1722.66\n",
      "Episode length: 819.40 +/- 361.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 819      |\n",
      "|    mean_reward     | 3.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 22.5     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | -0.209   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3110     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15090    |\n",
      "|    total_timesteps | 2504577  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -323     |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.0488   |\n",
      "|    ent_coef_loss   | 0.307    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2504476  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=3676.15 +/- 1587.49\n",
      "Episode length: 761.80 +/- 309.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 762      |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 16.9     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | -0.772   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3120     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15133    |\n",
      "|    total_timesteps | 2511783  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 46       |\n",
      "|    ent_coef        | 0.0488   |\n",
      "|    ent_coef_loss   | 0.00404  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2511682  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 764      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3130     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15181    |\n",
      "|    total_timesteps | 2519975  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 59.2     |\n",
      "|    ent_coef        | 0.0487   |\n",
      "|    ent_coef_loss   | 0.494    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519874  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=2646.25 +/- 1504.12\n",
      "Episode length: 565.40 +/- 300.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 565      |\n",
      "|    mean_reward     | 2.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    ent_coef        | 0.0486   |\n",
      "|    ent_coef_loss   | -0.379   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3140     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15225    |\n",
      "|    total_timesteps | 2527368  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 19.4     |\n",
      "|    ent_coef        | 0.05     |\n",
      "|    ent_coef_loss   | 0.00295  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2527267  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=4180.41 +/- 747.15\n",
      "Episode length: 890.60 +/- 159.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 891      |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 27.9     |\n",
      "|    ent_coef        | 0.0503   |\n",
      "|    ent_coef_loss   | 0.461    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3150     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15273    |\n",
      "|    total_timesteps | 2535334  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.05     |\n",
      "|    ent_coef_loss   | 0.896    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2535233  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=3481.59 +/- 1265.91\n",
      "Episode length: 770.80 +/- 259.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 771      |\n",
      "|    mean_reward     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 48.5     |\n",
      "|    ent_coef        | 0.05     |\n",
      "|    ent_coef_loss   | 0.324    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3160     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15321    |\n",
      "|    total_timesteps | 2543285  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 20       |\n",
      "|    ent_coef        | 0.0499   |\n",
      "|    ent_coef_loss   | 1.75     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2543184  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=4711.19 +/- 153.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 25.2     |\n",
      "|    ent_coef        | 0.0501   |\n",
      "|    ent_coef_loss   | -0.22    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2549899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3170     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15367    |\n",
      "|    total_timesteps | 2550804  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 32.3     |\n",
      "|    ent_coef        | 0.0513   |\n",
      "|    ent_coef_loss   | -0.883   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2550703  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3180     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15414    |\n",
      "|    total_timesteps | 2558837  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 22.8     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | 0.171    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2558736  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2560000, episode_reward=3390.70 +/- 1399.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 27.4     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | -0.316   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3190     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15463    |\n",
      "|    total_timesteps | 2566994  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 53.7     |\n",
      "|    ent_coef        | 0.0506   |\n",
      "|    ent_coef_loss   | -0.548   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2566893  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=3524.06 +/- 1902.91\n",
      "Episode length: 912.40 +/- 175.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 912      |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 41.4     |\n",
      "|    ent_coef        | 0.0504   |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3200     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15516    |\n",
      "|    total_timesteps | 2575862  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.0505   |\n",
      "|    ent_coef_loss   | -0.932   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2575761  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=3581.97 +/- 1598.36\n",
      "Episode length: 909.40 +/- 181.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 909      |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 51.8     |\n",
      "|    ent_coef        | 0.0502   |\n",
      "|    ent_coef_loss   | -0.225   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 773      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3210     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 15553    |\n",
      "|    total_timesteps | 2581867  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 26.3     |\n",
      "|    ent_coef        | 0.0509   |\n",
      "|    ent_coef_loss   | -0.0412  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2581766  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3220     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15597    |\n",
      "|    total_timesteps | 2589313  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    ent_coef        | 0.0505   |\n",
      "|    ent_coef_loss   | 0.161    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2589212  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=3050.39 +/- 1811.71\n",
      "Episode length: 812.20 +/- 255.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 812      |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 18.5     |\n",
      "|    ent_coef        | 0.0506   |\n",
      "|    ent_coef_loss   | 0.555    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 2.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3230     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15638    |\n",
      "|    total_timesteps | 2596035  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 19.3     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2595934  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=4838.94 +/- 45.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 51.2     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | 0.571    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2599899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 752      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3240     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15677    |\n",
      "|    total_timesteps | 2602554  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 25.2     |\n",
      "|    ent_coef        | 0.0501   |\n",
      "|    ent_coef_loss   | -0.317   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2602453  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=3711.37 +/- 933.13\n",
      "Episode length: 850.00 +/- 190.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 850      |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 13.8     |\n",
      "|    ent_coef        | 0.0507   |\n",
      "|    ent_coef_loss   | 0.255    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 747      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3250     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15723    |\n",
      "|    total_timesteps | 2610048  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 27.3     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | -2.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609947  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 736      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3260     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15763    |\n",
      "|    total_timesteps | 2616887  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 43.1     |\n",
      "|    ent_coef        | 0.0515   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2616786  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=3497.70 +/- 1780.28\n",
      "Episode length: 794.40 +/- 372.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 794      |\n",
      "|    mean_reward     | 3.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -328     |\n",
      "|    critic_loss     | 92.4     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | 0.483    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 738      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3270     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15809    |\n",
      "|    total_timesteps | 2624614  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.0514   |\n",
      "|    ent_coef_loss   | -0.945   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2624513  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2630000, episode_reward=3629.91 +/- 1616.52\n",
      "Episode length: 742.20 +/- 316.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 742      |\n",
      "|    mean_reward     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 23.2     |\n",
      "|    ent_coef        | 0.0507   |\n",
      "|    ent_coef_loss   | -0.541   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3280     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15864    |\n",
      "|    total_timesteps | 2633729  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 56.4     |\n",
      "|    ent_coef        | 0.0501   |\n",
      "|    ent_coef_loss   | -0.241   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2633628  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=4267.80 +/- 1089.44\n",
      "Episode length: 893.00 +/- 214.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 893      |\n",
      "|    mean_reward     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    ent_coef        | 0.0521   |\n",
      "|    ent_coef_loss   | -0.621   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 748      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3290     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15913    |\n",
      "|    total_timesteps | 2641842  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 21       |\n",
      "|    ent_coef        | 0.0532   |\n",
      "|    ent_coef_loss   | -0.485   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2641741  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 716      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3300     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15946    |\n",
      "|    total_timesteps | 2647446  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 32.1     |\n",
      "|    ent_coef        | 0.0523   |\n",
      "|    ent_coef_loss   | 0.357    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2647345  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=3157.47 +/- 2031.56\n",
      "Episode length: 833.00 +/- 334.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 833      |\n",
      "|    mean_reward     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 15.3     |\n",
      "|    ent_coef        | 0.0529   |\n",
      "|    ent_coef_loss   | -0.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3310     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 15980    |\n",
      "|    total_timesteps | 2653086  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 39.2     |\n",
      "|    ent_coef        | 0.0527   |\n",
      "|    ent_coef_loss   | -0.0205  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2652985  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=4695.96 +/- 159.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 73.8     |\n",
      "|    ent_coef        | 0.0528   |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3320     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16026    |\n",
      "|    total_timesteps | 2660614  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 38       |\n",
      "|    ent_coef        | 0.0524   |\n",
      "|    ent_coef_loss   | 0.71     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2660513  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 728      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3330     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16074    |\n",
      "|    total_timesteps | 2668873  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 43.7     |\n",
      "|    ent_coef        | 0.0534   |\n",
      "|    ent_coef_loss   | 0.349    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2668772  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=2090.28 +/- 1104.95\n",
      "Episode length: 584.40 +/- 271.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 584      |\n",
      "|    mean_reward     | 2.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0519   |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3340     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16129    |\n",
      "|    total_timesteps | 2678168  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 24.4     |\n",
      "|    ent_coef        | 0.0532   |\n",
      "|    ent_coef_loss   | 0.688    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2678067  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=4403.84 +/- 712.64\n",
      "Episode length: 926.40 +/- 147.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 926      |\n",
      "|    mean_reward     | 4.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.0524   |\n",
      "|    ent_coef_loss   | 0.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 3.12e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3350     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16172    |\n",
      "|    total_timesteps | 2685150  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 21.9     |\n",
      "|    ent_coef        | 0.0528   |\n",
      "|    ent_coef_loss   | -0.532   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2685049  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=3776.88 +/- 1521.08\n",
      "Episode length: 962.60 +/- 74.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 963      |\n",
      "|    mean_reward     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 228      |\n",
      "|    ent_coef        | 0.0532   |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3360     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16221    |\n",
      "|    total_timesteps | 2693394  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 36.9     |\n",
      "|    ent_coef        | 0.0537   |\n",
      "|    ent_coef_loss   | -1.5     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2693293  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 3.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3370     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16248    |\n",
      "|    total_timesteps | 2697912  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 115      |\n",
      "|    ent_coef        | 0.0533   |\n",
      "|    ent_coef_loss   | 0.914    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2697811  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=3692.74 +/- 1344.17\n",
      "Episode length: 859.80 +/- 280.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 62.8     |\n",
      "|    ent_coef        | 0.053    |\n",
      "|    ent_coef_loss   | -0.654   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 717      |\n",
      "|    ep_rew_mean     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3380     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16293    |\n",
      "|    total_timesteps | 2705426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | 0.71     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2705325  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=3133.02 +/- 1933.33\n",
      "Episode length: 677.40 +/- 409.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 677      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 30.2     |\n",
      "|    ent_coef        | 0.0552   |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 715      |\n",
      "|    ep_rew_mean     | 2.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3390     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16341    |\n",
      "|    total_timesteps | 2713359  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 69.5     |\n",
      "|    ent_coef        | 0.0545   |\n",
      "|    ent_coef_loss   | 0.559    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2713258  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 3400     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16379    |\n",
      "|    total_timesteps | 2719940  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.986   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719839  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=4664.00 +/- 406.99\n",
      "Episode length: 959.00 +/- 82.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 959      |\n",
      "|    mean_reward     | 4.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 75.8     |\n",
      "|    ent_coef        | 0.0543   |\n",
      "|    ent_coef_loss   | 0.126    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 735      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3410     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16419    |\n",
      "|    total_timesteps | 2726560  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 30.7     |\n",
      "|    ent_coef        | 0.0544   |\n",
      "|    ent_coef_loss   | -0.589   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2726459  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=3681.56 +/- 1511.80\n",
      "Episode length: 931.40 +/- 94.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 931      |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.0543   |\n",
      "|    ent_coef_loss   | 0.47     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3420     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16477    |\n",
      "|    total_timesteps | 2736185  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 54.2     |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | 0.739    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2736084  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=2285.50 +/- 1754.89\n",
      "Episode length: 854.40 +/- 191.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 854      |\n",
      "|    mean_reward     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.054    |\n",
      "|    ent_coef_loss   | -0.496   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 765      |\n",
      "|    ep_rew_mean     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3430     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16532    |\n",
      "|    total_timesteps | 2745334  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 47.6     |\n",
      "|    ent_coef        | 0.0544   |\n",
      "|    ent_coef_loss   | 0.809    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2745233  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2750000, episode_reward=4614.07 +/- 372.36\n",
      "Episode length: 956.20 +/- 87.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 956      |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 29.2     |\n",
      "|    ent_coef        | 0.0541   |\n",
      "|    ent_coef_loss   | -0.836   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 754      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3440     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16582    |\n",
      "|    total_timesteps | 2753576  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 25.3     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2753475  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 732      |\n",
      "|    ep_rew_mean     | 3.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3450     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16610    |\n",
      "|    total_timesteps | 2758343  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 18.1     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | -0.172   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2758242  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=4816.59 +/- 182.03\n",
      "Episode length: 985.60 +/- 28.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 986      |\n",
      "|    mean_reward     | 4.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 29       |\n",
      "|    ent_coef        | 0.0551   |\n",
      "|    ent_coef_loss   | -0.691   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 3.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3460     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16660    |\n",
      "|    total_timesteps | 2766682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    ent_coef        | 0.0551   |\n",
      "|    ent_coef_loss   | -0.349   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2766581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=3379.22 +/- 1996.07\n",
      "Episode length: 687.20 +/- 386.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 687      |\n",
      "|    mean_reward     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    ent_coef        | 0.0545   |\n",
      "|    ent_coef_loss   | 0.341    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 3.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3470     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16697    |\n",
      "|    total_timesteps | 2772786  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 37       |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | -0.141   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2772685  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=2629.72 +/- 2076.08\n",
      "Episode length: 654.20 +/- 430.77\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 654      |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    ent_coef        | 0.0546   |\n",
      "|    ent_coef_loss   | -0.0707  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 3.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3480     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16748    |\n",
      "|    total_timesteps | 2781404  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 42.3     |\n",
      "|    ent_coef        | 0.0553   |\n",
      "|    ent_coef_loss   | 0.512    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2781303  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3490     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16790    |\n",
      "|    total_timesteps | 2788500  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.0581   |\n",
      "|    ent_coef_loss   | -0.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2788399  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=2967.10 +/- 1357.33\n",
      "Episode length: 928.00 +/- 104.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 928      |\n",
      "|    mean_reward     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 24.3     |\n",
      "|    ent_coef        | 0.0579   |\n",
      "|    ent_coef_loss   | -0.407   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 749      |\n",
      "|    ep_rew_mean     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3500     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16828    |\n",
      "|    total_timesteps | 2794870  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 28.7     |\n",
      "|    ent_coef        | 0.0557   |\n",
      "|    ent_coef_loss   | 0.146    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2794769  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 730      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3510     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16856    |\n",
      "|    total_timesteps | 2799571  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 61.6     |\n",
      "|    ent_coef        | 0.0563   |\n",
      "|    ent_coef_loss   | 1.79     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2799470  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=2285.57 +/- 1757.33\n",
      "Episode length: 798.60 +/- 260.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 799      |\n",
      "|    mean_reward     | 2.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 62.1     |\n",
      "|    ent_coef        | 0.0568   |\n",
      "|    ent_coef_loss   | 0.792    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 712      |\n",
      "|    ep_rew_mean     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3520     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16903    |\n",
      "|    total_timesteps | 2807343  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    ent_coef        | 0.0577   |\n",
      "|    ent_coef_loss   | -0.488   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2807242  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=4473.12 +/- 795.75\n",
      "Episode length: 925.80 +/- 148.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 926      |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 40.5     |\n",
      "|    ent_coef        | 0.0576   |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 687      |\n",
      "|    ep_rew_mean     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3530     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16944    |\n",
      "|    total_timesteps | 2814075  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 29       |\n",
      "|    ent_coef        | 0.059    |\n",
      "|    ent_coef_loss   | 0.242    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2813974  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2820000, episode_reward=2201.91 +/- 1868.09\n",
      "Episode length: 697.20 +/- 374.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 697      |\n",
      "|    mean_reward     | 2.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 66.9     |\n",
      "|    ent_coef        | 0.0584   |\n",
      "|    ent_coef_loss   | 0.741    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 673      |\n",
      "|    ep_rew_mean     | 2.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3540     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 16984    |\n",
      "|    total_timesteps | 2820832  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 46.1     |\n",
      "|    ent_coef        | 0.0581   |\n",
      "|    ent_coef_loss   | 0.518    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2820731  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 685      |\n",
      "|    ep_rew_mean     | 2.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3550     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17019    |\n",
      "|    total_timesteps | 2826818  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 20.4     |\n",
      "|    ent_coef        | 0.0582   |\n",
      "|    ent_coef_loss   | 0.00143  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2826717  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2830000, episode_reward=2993.40 +/- 1545.63\n",
      "Episode length: 805.60 +/- 177.64\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 2.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0574   |\n",
      "|    ent_coef_loss   | -0.472   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 690      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3560     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17073    |\n",
      "|    total_timesteps | 2835722  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0595   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2835621  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2840000, episode_reward=4375.03 +/- 790.62\n",
      "Episode length: 880.20 +/- 148.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 880      |\n",
      "|    mean_reward     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 61.6     |\n",
      "|    ent_coef        | 0.0596   |\n",
      "|    ent_coef_loss   | -0.124   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 704      |\n",
      "|    ep_rew_mean     | 2.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3570     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17118    |\n",
      "|    total_timesteps | 2843167  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 30.8     |\n",
      "|    ent_coef        | 0.0598   |\n",
      "|    ent_coef_loss   | 0.085    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2843066  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 682      |\n",
      "|    ep_rew_mean     | 2.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3580     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17156    |\n",
      "|    total_timesteps | 2849606  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 55.9     |\n",
      "|    ent_coef        | 0.0575   |\n",
      "|    ent_coef_loss   | -0.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2849505  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=3904.82 +/- 982.24\n",
      "Episode length: 826.60 +/- 224.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 3.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 33.8     |\n",
      "|    ent_coef        | 0.0578   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 685      |\n",
      "|    ep_rew_mean     | 2.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3590     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17200    |\n",
      "|    total_timesteps | 2857002  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 103      |\n",
      "|    ent_coef        | 0.0573   |\n",
      "|    ent_coef_loss   | -1.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2856901  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=4390.91 +/- 1051.96\n",
      "Episode length: 962.80 +/- 74.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 963      |\n",
      "|    mean_reward     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 49.5     |\n",
      "|    ent_coef        | 0.06     |\n",
      "|    ent_coef_loss   | 0.558    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 688      |\n",
      "|    ep_rew_mean     | 2.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3600     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17241    |\n",
      "|    total_timesteps | 2863713  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 24.2     |\n",
      "|    ent_coef        | 0.0591   |\n",
      "|    ent_coef_loss   | -0.379   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2863612  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=4233.80 +/- 1015.80\n",
      "Episode length: 854.40 +/- 192.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 854      |\n",
      "|    mean_reward     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 27.6     |\n",
      "|    ent_coef        | 0.0567   |\n",
      "|    ent_coef_loss   | -0.442   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 714      |\n",
      "|    ep_rew_mean     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3610     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17285    |\n",
      "|    total_timesteps | 2870990  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 18.8     |\n",
      "|    ent_coef        | 0.0592   |\n",
      "|    ent_coef_loss   | -0.116   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2870889  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 713      |\n",
      "|    ep_rew_mean     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3620     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17330    |\n",
      "|    total_timesteps | 2878628  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    ent_coef        | 0.0578   |\n",
      "|    ent_coef_loss   | 0.914    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2878527  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=4327.55 +/- 1217.92\n",
      "Episode length: 875.40 +/- 249.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 875      |\n",
      "|    mean_reward     | 4.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 36.7     |\n",
      "|    ent_coef        | 0.0585   |\n",
      "|    ent_coef_loss   | -0.405   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 725      |\n",
      "|    ep_rew_mean     | 3.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3630     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17377    |\n",
      "|    total_timesteps | 2886531  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.0579   |\n",
      "|    ent_coef_loss   | -0.878   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2886430  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=4604.43 +/- 753.29\n",
      "Episode length: 930.20 +/- 139.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 930      |\n",
      "|    mean_reward     | 4.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 277      |\n",
      "|    ent_coef        | 0.0586   |\n",
      "|    ent_coef_loss   | 0.804    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 738      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3640     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17426    |\n",
      "|    total_timesteps | 2894672  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.0587   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2894571  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=3248.99 +/- 2147.88\n",
      "Episode length: 838.80 +/- 313.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 839      |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 35.6     |\n",
      "|    ent_coef        | 0.0581   |\n",
      "|    ent_coef_loss   | -1.47    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 3.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3650     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17473    |\n",
      "|    total_timesteps | 2902449  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 26.5     |\n",
      "|    ent_coef        | 0.0577   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2902348  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2910000, episode_reward=3540.48 +/- 1274.87\n",
      "Episode length: 867.40 +/- 171.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 867      |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 66.6     |\n",
      "|    ent_coef        | 0.0591   |\n",
      "|    ent_coef_loss   | 0.0607   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 751      |\n",
      "|    ep_rew_mean     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3660     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17524    |\n",
      "|    total_timesteps | 2910839  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 74.2     |\n",
      "|    ent_coef        | 0.0579   |\n",
      "|    ent_coef_loss   | -0.792   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2910738  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 750      |\n",
      "|    ep_rew_mean     | 3.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3670     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17567    |\n",
      "|    total_timesteps | 2918146  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 50.7     |\n",
      "|    ent_coef        | 0.0594   |\n",
      "|    ent_coef_loss   | 0.0161   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2918045  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=4001.32 +/- 1470.34\n",
      "Episode length: 852.80 +/- 294.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | 4e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0588   |\n",
      "|    ent_coef_loss   | -0.686   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 778      |\n",
      "|    ep_rew_mean     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3680     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17622    |\n",
      "|    total_timesteps | 2927388  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -337     |\n",
      "|    critic_loss     | 39.1     |\n",
      "|    ent_coef        | 0.0593   |\n",
      "|    ent_coef_loss   | 0.756    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2927287  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=3999.37 +/- 1903.26\n",
      "Episode length: 810.00 +/- 380.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 810      |\n",
      "|    mean_reward     | 4e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 21       |\n",
      "|    ent_coef        | 0.0578   |\n",
      "|    ent_coef_loss   | -0.815   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3690     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17665    |\n",
      "|    total_timesteps | 2934455  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 26.4     |\n",
      "|    ent_coef        | 0.0597   |\n",
      "|    ent_coef_loss   | 0.399    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2934354  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=4272.09 +/- 856.95\n",
      "Episode length: 921.80 +/- 156.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 922      |\n",
      "|    mean_reward     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 39.2     |\n",
      "|    ent_coef        | 0.0588   |\n",
      "|    ent_coef_loss   | 0.818    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3700     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17711    |\n",
      "|    total_timesteps | 2942083  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 51.1     |\n",
      "|    ent_coef        | 0.0592   |\n",
      "|    ent_coef_loss   | -0.239   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2941982  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2950000, episode_reward=5177.34 +/- 153.08\n",
      "Episode length: 985.80 +/- 28.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 986      |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 43.8     |\n",
      "|    ent_coef        | 0.0589   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2949899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3710     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17764    |\n",
      "|    total_timesteps | 2950793  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 23.4     |\n",
      "|    ent_coef        | 0.0588   |\n",
      "|    ent_coef_loss   | -0.063   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2950692  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3720     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17813    |\n",
      "|    total_timesteps | 2959156  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 15.9     |\n",
      "|    ent_coef        | 0.0601   |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2959055  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2960000, episode_reward=4810.81 +/- 248.49\n",
      "Episode length: 971.40 +/- 57.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 971      |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 28.8     |\n",
      "|    ent_coef        | 0.0582   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3730     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17867    |\n",
      "|    total_timesteps | 2968202  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 28.9     |\n",
      "|    ent_coef        | 0.0593   |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2968101  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=4952.67 +/- 119.27\n",
      "Episode length: 997.20 +/- 5.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 997      |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 33.8     |\n",
      "|    ent_coef        | 0.0607   |\n",
      "|    ent_coef_loss   | 0.0655   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 808      |\n",
      "|    ep_rew_mean     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3740     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17911    |\n",
      "|    total_timesteps | 2975426  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.059    |\n",
      "|    ent_coef_loss   | 0.275    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2975325  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=3287.80 +/- 1961.53\n",
      "Episode length: 644.40 +/- 365.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 644      |\n",
      "|    mean_reward     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 31.8     |\n",
      "|    ent_coef        | 0.0599   |\n",
      "|    ent_coef_loss   | 0.526    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3750     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 17960    |\n",
      "|    total_timesteps | 2983595  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 77.8     |\n",
      "|    ent_coef        | 0.0608   |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2983494  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=4901.77 +/- 114.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 102      |\n",
      "|    ent_coef        | 0.0594   |\n",
      "|    ent_coef_loss   | 0.468    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3760     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18016    |\n",
      "|    total_timesteps | 2992864  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 18       |\n",
      "|    ent_coef        | 0.0608   |\n",
      "|    ent_coef_loss   | -0.0306  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2992763  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=4843.41 +/- 194.50\n",
      "Episode length: 973.60 +/- 52.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 974      |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 87.2     |\n",
      "|    ent_coef        | 0.0619   |\n",
      "|    ent_coef_loss   | -0.378   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3770     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18070    |\n",
      "|    total_timesteps | 3001810  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 31.7     |\n",
      "|    ent_coef        | 0.0616   |\n",
      "|    ent_coef_loss   | 0.645    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3001709  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3010000, episode_reward=2043.77 +/- 2237.47\n",
      "Episode length: 671.20 +/- 325.87\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 671      |\n",
      "|    mean_reward     | 2.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 42.6     |\n",
      "|    ent_coef        | 0.0624   |\n",
      "|    ent_coef_loss   | -0.0856  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3780     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18121    |\n",
      "|    total_timesteps | 3010259  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 23.4     |\n",
      "|    ent_coef        | 0.0625   |\n",
      "|    ent_coef_loss   | -0.0941  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3010158  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3790     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18174    |\n",
      "|    total_timesteps | 3019344  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 37.6     |\n",
      "|    ent_coef        | 0.0633   |\n",
      "|    ent_coef_loss   | 0.435    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019243  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3020000, episode_reward=3820.43 +/- 1392.39\n",
      "Episode length: 779.20 +/- 276.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 779      |\n",
      "|    mean_reward     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    ent_coef        | 0.0622   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3800     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18217    |\n",
      "|    total_timesteps | 3026558  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 44.8     |\n",
      "|    ent_coef        | 0.0623   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3026457  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3030000, episode_reward=3245.50 +/- 2067.05\n",
      "Episode length: 819.40 +/- 325.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 819      |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 67       |\n",
      "|    ent_coef        | 0.0617   |\n",
      "|    ent_coef_loss   | -0.61    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3810     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18263    |\n",
      "|    total_timesteps | 3034221  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0623   |\n",
      "|    ent_coef_loss   | 0.227    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3034120  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=3486.59 +/- 1967.73\n",
      "Episode length: 690.20 +/- 384.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 690      |\n",
      "|    mean_reward     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 47       |\n",
      "|    ent_coef        | 0.0618   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 844      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3820     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18319    |\n",
      "|    total_timesteps | 3043508  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 29.8     |\n",
      "|    ent_coef        | 0.0618   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3043407  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3050000, episode_reward=5002.74 +/- 75.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 39.7     |\n",
      "|    ent_coef        | 0.0621   |\n",
      "|    ent_coef_loss   | -0.0404  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3830     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18366    |\n",
      "|    total_timesteps | 3051221  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0615   |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3051120  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 838      |\n",
      "|    ep_rew_mean     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3840     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18412    |\n",
      "|    total_timesteps | 3059181  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 84.5     |\n",
      "|    ent_coef        | 0.0625   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3059080  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060000, episode_reward=3785.41 +/- 1585.26\n",
      "Episode length: 775.80 +/- 286.72\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 776      |\n",
      "|    mean_reward     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 20.9     |\n",
      "|    ent_coef        | 0.0617   |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3850     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18450    |\n",
      "|    total_timesteps | 3065478  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 25.7     |\n",
      "|    ent_coef        | 0.0627   |\n",
      "|    ent_coef_loss   | -1.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3065377  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3070000, episode_reward=3642.57 +/- 1558.28\n",
      "Episode length: 703.60 +/- 289.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 704      |\n",
      "|    mean_reward     | 3.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    ent_coef        | 0.0607   |\n",
      "|    ent_coef_loss   | -0.711   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 791      |\n",
      "|    ep_rew_mean     | 3.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3860     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18489    |\n",
      "|    total_timesteps | 3071980  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 21.9     |\n",
      "|    ent_coef        | 0.0628   |\n",
      "|    ent_coef_loss   | -0.141   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3071879  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 781      |\n",
      "|    ep_rew_mean     | 3.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3870     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18536    |\n",
      "|    total_timesteps | 3079921  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 20.3     |\n",
      "|    ent_coef        | 0.061    |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3079820  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3080000, episode_reward=4687.20 +/- 1127.69\n",
      "Episode length: 894.00 +/- 212.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 894      |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    ent_coef        | 0.061    |\n",
      "|    ent_coef_loss   | 0.341    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3880     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18583    |\n",
      "|    total_timesteps | 3087625  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 61.1     |\n",
      "|    ent_coef        | 0.061    |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3087524  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090000, episode_reward=4145.46 +/- 1128.89\n",
      "Episode length: 863.00 +/- 199.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 863      |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 31.2     |\n",
      "|    ent_coef        | 0.0626   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 757      |\n",
      "|    ep_rew_mean     | 3.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3890     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18627    |\n",
      "|    total_timesteps | 3095031  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.0617   |\n",
      "|    ent_coef_loss   | 0.0496   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3094930  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3100000, episode_reward=4647.77 +/- 721.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 50.1     |\n",
      "|    ent_coef        | 0.062    |\n",
      "|    ent_coef_loss   | 0.164    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3900     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18677    |\n",
      "|    total_timesteps | 3103240  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 36.4     |\n",
      "|    ent_coef        | 0.0616   |\n",
      "|    ent_coef_loss   | -0.449   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3103139  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3110000, episode_reward=4206.73 +/- 1819.45\n",
      "Episode length: 832.00 +/- 336.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 832      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 25.4     |\n",
      "|    ent_coef        | 0.0628   |\n",
      "|    ent_coef_loss   | -0.374   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3910     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18728    |\n",
      "|    total_timesteps | 3111792  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    ent_coef        | 0.0638   |\n",
      "|    ent_coef_loss   | -1.84    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3111691  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3120000, episode_reward=4605.37 +/- 894.35\n",
      "Episode length: 876.20 +/- 157.35\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 876      |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 53.3     |\n",
      "|    ent_coef        | 0.0626   |\n",
      "|    ent_coef_loss   | -0.224   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 783      |\n",
      "|    ep_rew_mean     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3920     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18788    |\n",
      "|    total_timesteps | 3121775  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 34.9     |\n",
      "|    ent_coef        | 0.0634   |\n",
      "|    ent_coef_loss   | 0.121    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3121674  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3930     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18826    |\n",
      "|    total_timesteps | 3128319  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 70.8     |\n",
      "|    ent_coef        | 0.0633   |\n",
      "|    ent_coef_loss   | 0.799    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3128218  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3130000, episode_reward=5246.08 +/- 179.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 61.7     |\n",
      "|    ent_coef        | 0.0623   |\n",
      "|    ent_coef_loss   | 0.214    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3129899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 767      |\n",
      "|    ep_rew_mean     | 3.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3940     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18872    |\n",
      "|    total_timesteps | 3135843  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 46.8     |\n",
      "|    ent_coef        | 0.0616   |\n",
      "|    ent_coef_loss   | -1.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3135742  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3140000, episode_reward=4694.54 +/- 482.53\n",
      "Episode length: 952.00 +/- 96.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 952      |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 53.3     |\n",
      "|    ent_coef        | 0.0633   |\n",
      "|    ent_coef_loss   | 0.764    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 797      |\n",
      "|    ep_rew_mean     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3950     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18928    |\n",
      "|    total_timesteps | 3145160  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 52.3     |\n",
      "|    ent_coef        | 0.0634   |\n",
      "|    ent_coef_loss   | -1.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3145059  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3150000, episode_reward=3363.58 +/- 2231.82\n",
      "Episode length: 893.40 +/- 213.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 893      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 36.8     |\n",
      "|    ent_coef        | 0.0622   |\n",
      "|    ent_coef_loss   | 0.728    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 3960     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 18986    |\n",
      "|    total_timesteps | 3154808  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 23       |\n",
      "|    ent_coef        | 0.0624   |\n",
      "|    ent_coef_loss   | -0.627   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3154707  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3160000, episode_reward=4050.84 +/- 1816.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 39.7     |\n",
      "|    ent_coef        | 0.0621   |\n",
      "|    ent_coef_loss   | 0.88     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3970     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19032    |\n",
      "|    total_timesteps | 3162367  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    ent_coef        | 0.0626   |\n",
      "|    ent_coef_loss   | 0.0721   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3162266  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3170000, episode_reward=2856.75 +/- 2189.99\n",
      "Episode length: 561.20 +/- 406.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 561      |\n",
      "|    mean_reward     | 2.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0639   |\n",
      "|    ent_coef_loss   | -1.08    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3980     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19087    |\n",
      "|    total_timesteps | 3171682  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 26.6     |\n",
      "|    ent_coef        | 0.0634   |\n",
      "|    ent_coef_loss   | 0.435    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3171581  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3180000, episode_reward=4093.35 +/- 2098.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    ent_coef        | 0.0637   |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 3.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3990     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19144    |\n",
      "|    total_timesteps | 3181106  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 93.2     |\n",
      "|    ent_coef        | 0.0647   |\n",
      "|    ent_coef_loss   | 0.848    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3181005  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 867      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4000     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19195    |\n",
      "|    total_timesteps | 3189933  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 41.9     |\n",
      "|    ent_coef        | 0.065    |\n",
      "|    ent_coef_loss   | 0.207    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3189832  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3190000, episode_reward=4964.46 +/- 379.12\n",
      "Episode length: 976.20 +/- 47.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 976      |\n",
      "|    mean_reward     | 4.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 28.8     |\n",
      "|    ent_coef        | 0.0649   |\n",
      "|    ent_coef_loss   | -0.726   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4010     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19250    |\n",
      "|    total_timesteps | 3198950  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0637   |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3198849  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=4531.43 +/- 1493.49\n",
      "Episode length: 860.40 +/- 279.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 4.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 65.5     |\n",
      "|    ent_coef        | 0.0652   |\n",
      "|    ent_coef_loss   | 0.906    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 3.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4020     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19296    |\n",
      "|    total_timesteps | 3206638  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 123      |\n",
      "|    ent_coef        | 0.0651   |\n",
      "|    ent_coef_loss   | 2.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3206537  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210000, episode_reward=5315.21 +/- 98.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 36.4     |\n",
      "|    ent_coef        | 0.0646   |\n",
      "|    ent_coef_loss   | -0.945   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3209899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4030     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19341    |\n",
      "|    total_timesteps | 3214020  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 20.8     |\n",
      "|    ent_coef        | 0.0671   |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3213919  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3220000, episode_reward=5198.78 +/- 135.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 25.2     |\n",
      "|    ent_coef        | 0.0652   |\n",
      "|    ent_coef_loss   | -0.504   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4040     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19380    |\n",
      "|    total_timesteps | 3220483  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 38.6     |\n",
      "|    ent_coef        | 0.0657   |\n",
      "|    ent_coef_loss   | -0.525   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3220382  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4050     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19424    |\n",
      "|    total_timesteps | 3227877  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 73.8     |\n",
      "|    ent_coef        | 0.0654   |\n",
      "|    ent_coef_loss   | 0.511    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3227776  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3230000, episode_reward=5254.48 +/- 213.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0648   |\n",
      "|    ent_coef_loss   | 0.105    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 3.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4060     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19481    |\n",
      "|    total_timesteps | 3237441  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 107      |\n",
      "|    ent_coef        | 0.0647   |\n",
      "|    ent_coef_loss   | 0.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3237340  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3240000, episode_reward=5227.33 +/- 115.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 41       |\n",
      "|    ent_coef        | 0.065    |\n",
      "|    ent_coef_loss   | -0.207   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 3.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4070     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19529    |\n",
      "|    total_timesteps | 3245363  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 56.8     |\n",
      "|    ent_coef        | 0.0639   |\n",
      "|    ent_coef_loss   | -0.00401 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3245262  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3250000, episode_reward=5200.08 +/- 175.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 42.7     |\n",
      "|    ent_coef        | 0.0662   |\n",
      "|    ent_coef_loss   | -0.326   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 815      |\n",
      "|    ep_rew_mean     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4080     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19576    |\n",
      "|    total_timesteps | 3253226  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 47.1     |\n",
      "|    ent_coef        | 0.0662   |\n",
      "|    ent_coef_loss   | -0.313   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3253125  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3260000, episode_reward=5183.23 +/- 204.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 30       |\n",
      "|    ent_coef        | 0.0659   |\n",
      "|    ent_coef_loss   | -0.277   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 816      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4090     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19633    |\n",
      "|    total_timesteps | 3262687  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 56.6     |\n",
      "|    ent_coef        | 0.0666   |\n",
      "|    ent_coef_loss   | 0.588    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3262586  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270000, episode_reward=3748.85 +/- 1667.30\n",
      "Episode length: 807.20 +/- 287.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 807      |\n",
      "|    mean_reward     | 3.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 23.7     |\n",
      "|    ent_coef        | 0.0667   |\n",
      "|    ent_coef_loss   | 0.0236   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 820      |\n",
      "|    ep_rew_mean     | 4.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4100     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19689    |\n",
      "|    total_timesteps | 3271978  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -374     |\n",
      "|    critic_loss     | 22       |\n",
      "|    ent_coef        | 0.0677   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3271877  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3280000, episode_reward=3754.41 +/- 1729.91\n",
      "Episode length: 843.20 +/- 313.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 843      |\n",
      "|    mean_reward     | 3.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 27.1     |\n",
      "|    ent_coef        | 0.068    |\n",
      "|    ent_coef_loss   | 0.0313   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 4e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4110     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19737    |\n",
      "|    total_timesteps | 3280003  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 49.1     |\n",
      "|    ent_coef        | 0.068    |\n",
      "|    ent_coef_loss   | 0.299    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3279902  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 826      |\n",
      "|    ep_rew_mean     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4120     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19791    |\n",
      "|    total_timesteps | 3289205  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 31.1     |\n",
      "|    ent_coef        | 0.0679   |\n",
      "|    ent_coef_loss   | 0.831    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3289104  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3290000, episode_reward=4235.38 +/- 1740.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 40.4     |\n",
      "|    ent_coef        | 0.068    |\n",
      "|    ent_coef_loss   | 0.656    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4130     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19846    |\n",
      "|    total_timesteps | 3298329  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 72.1     |\n",
      "|    ent_coef        | 0.0726   |\n",
      "|    ent_coef_loss   | -0.108   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3298228  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300000, episode_reward=4220.01 +/- 1065.39\n",
      "Episode length: 819.80 +/- 202.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -382     |\n",
      "|    critic_loss     | 38.4     |\n",
      "|    ent_coef        | 0.0752   |\n",
      "|    ent_coef_loss   | 2.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4140     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19897    |\n",
      "|    total_timesteps | 3306880  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -389     |\n",
      "|    critic_loss     | 46.7     |\n",
      "|    ent_coef        | 0.1      |\n",
      "|    ent_coef_loss   | -0.658   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3306779  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3310000, episode_reward=-2065.99 +/- 198.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | -2.07e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 3310000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -506      |\n",
      "|    critic_loss     | 215       |\n",
      "|    ent_coef        | 0.243     |\n",
      "|    ent_coef_loss   | 5.89      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3309899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4150     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 19943    |\n",
      "|    total_timesteps | 3314487  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -824     |\n",
      "|    critic_loss     | 623      |\n",
      "|    ent_coef        | 0.416    |\n",
      "|    ent_coef_loss   | -0.717   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3314386  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3320000, episode_reward=-619.97 +/- 801.09\n",
      "Episode length: 412.60 +/- 479.63\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 413       |\n",
      "|    mean_reward     | -620      |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 3320000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 709       |\n",
      "|    ent_coef        | 0.459     |\n",
      "|    ent_coef_loss   | 0.261     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3319899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 831       |\n",
      "|    ep_rew_mean     | 3.17e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4160      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 19980     |\n",
      "|    total_timesteps | 3320552   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 1.92e+03  |\n",
      "|    ent_coef        | 0.469     |\n",
      "|    ent_coef_loss   | 0.185     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3320451   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 784       |\n",
      "|    ep_rew_mean     | 2.72e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4170      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 19998     |\n",
      "|    total_timesteps | 3323715   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.35e+03 |\n",
      "|    critic_loss     | 664       |\n",
      "|    ent_coef        | 0.509     |\n",
      "|    ent_coef_loss   | 0.225     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3323614   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 756       |\n",
      "|    ep_rew_mean     | 2.27e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4180      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 20028     |\n",
      "|    total_timesteps | 3328805   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.54e+03 |\n",
      "|    critic_loss     | 3.8e+03   |\n",
      "|    ent_coef        | 0.496     |\n",
      "|    ent_coef_loss   | -0.151    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3328704   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3330000, episode_reward=-382.53 +/- 454.41\n",
      "Episode length: 429.20 +/- 467.74\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 429       |\n",
      "|    mean_reward     | -383      |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 3330000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.56e+03 |\n",
      "|    critic_loss     | 726       |\n",
      "|    ent_coef        | 0.432     |\n",
      "|    ent_coef_loss   | 0.306     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3329899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 713       |\n",
      "|    ep_rew_mean     | 1.75e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4190      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 20060     |\n",
      "|    total_timesteps | 3334021   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.48e+03 |\n",
      "|    critic_loss     | 555       |\n",
      "|    ent_coef        | 0.293     |\n",
      "|    ent_coef_loss   | -0.215    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3333920   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 674      |\n",
      "|    ep_rew_mean     | 1.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4200     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20091    |\n",
      "|    total_timesteps | 3339401  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3e+03 |\n",
      "|    critic_loss     | 301      |\n",
      "|    ent_coef        | 0.204    |\n",
      "|    ent_coef_loss   | -0.513   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3339300  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3340000, episode_reward=-17.21 +/- 199.94\n",
      "Episode length: 640.00 +/- 441.01\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 640       |\n",
      "|    mean_reward     | -17.2     |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 3340000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.28e+03 |\n",
      "|    critic_loss     | 533       |\n",
      "|    ent_coef        | 0.205     |\n",
      "|    ent_coef_loss   | -0.284    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3339899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 676       |\n",
      "|    ep_rew_mean     | 818       |\n",
      "| time/              |           |\n",
      "|    episodes        | 4210      |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 20140     |\n",
      "|    total_timesteps | 3347582   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 236       |\n",
      "|    ent_coef        | 0.167     |\n",
      "|    ent_coef_loss   | 0.98      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3347481   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3350000, episode_reward=-276.10 +/- 379.61\n",
      "Episode length: 616.20 +/- 470.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 616      |\n",
      "|    mean_reward     | -276     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -980     |\n",
      "|    critic_loss     | 363      |\n",
      "|    ent_coef        | 0.165    |\n",
      "|    ent_coef_loss   | 0.704    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 649      |\n",
      "|    ep_rew_mean     | 303      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4220     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20180    |\n",
      "|    total_timesteps | 3354075  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -905     |\n",
      "|    critic_loss     | 618      |\n",
      "|    ent_coef        | 0.163    |\n",
      "|    ent_coef_loss   | 0.0708   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3353974  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 612      |\n",
      "|    ep_rew_mean     | -152     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4230     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20212    |\n",
      "|    total_timesteps | 3359574  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -803     |\n",
      "|    critic_loss     | 232      |\n",
      "|    ent_coef        | 0.137    |\n",
      "|    ent_coef_loss   | -0.608   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3359473  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=-270.59 +/- 640.22\n",
      "Episode length: 455.00 +/- 445.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 455      |\n",
      "|    mean_reward     | -271     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -784     |\n",
      "|    critic_loss     | 605      |\n",
      "|    ent_coef        | 0.138    |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 617      |\n",
      "|    ep_rew_mean     | -553     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4240     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20265    |\n",
      "|    total_timesteps | 3368588  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -658     |\n",
      "|    critic_loss     | 92.4     |\n",
      "|    ent_coef        | 0.112    |\n",
      "|    ent_coef_loss   | 0.584    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3368487  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3370000, episode_reward=-364.86 +/- 468.16\n",
      "Episode length: 811.00 +/- 378.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 811      |\n",
      "|    mean_reward     | -365     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -633     |\n",
      "|    critic_loss     | 128      |\n",
      "|    ent_coef        | 0.11     |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 613      |\n",
      "|    ep_rew_mean     | -453     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4250     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20309    |\n",
      "|    total_timesteps | 3375763  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -599     |\n",
      "|    critic_loss     | 134      |\n",
      "|    ent_coef        | 0.0985   |\n",
      "|    ent_coef_loss   | 0.214    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3375662  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3380000, episode_reward=-257.78 +/- 458.65\n",
      "Episode length: 803.80 +/- 392.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | -258     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -538     |\n",
      "|    critic_loss     | 86.6     |\n",
      "|    ent_coef        | 0.0917   |\n",
      "|    ent_coef_loss   | -0.214   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 633      |\n",
      "|    ep_rew_mean     | -373     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4260     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20358    |\n",
      "|    total_timesteps | 3383849  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 89.8     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 0.392    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3383748  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3390000, episode_reward=-466.55 +/- 494.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | -467     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 129      |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.00137  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 667      |\n",
      "|    ep_rew_mean     | -317     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4270     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20399    |\n",
      "|    total_timesteps | 3390434  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -486     |\n",
      "|    critic_loss     | 250      |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | -0.604   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3390333  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 671      |\n",
      "|    ep_rew_mean     | -246     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4280     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20431    |\n",
      "|    total_timesteps | 3395918  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -477     |\n",
      "|    critic_loss     | 431      |\n",
      "|    ent_coef        | 0.0826   |\n",
      "|    ent_coef_loss   | -0.874   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3395817  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3400000, episode_reward=109.28 +/- 404.56\n",
      "Episode length: 837.80 +/- 324.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 838      |\n",
      "|    mean_reward     | 109      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -462     |\n",
      "|    critic_loss     | 78.4     |\n",
      "|    ent_coef        | 0.0814   |\n",
      "|    ent_coef_loss   | 0.0185   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 686      |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4290     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20472    |\n",
      "|    total_timesteps | 3402639  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -453     |\n",
      "|    critic_loss     | 52.7     |\n",
      "|    ent_coef        | 0.0788   |\n",
      "|    ent_coef_loss   | 0.00694  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3402538  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 684      |\n",
      "|    ep_rew_mean     | -154     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4300     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20502    |\n",
      "|    total_timesteps | 3407789  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 51.4     |\n",
      "|    ent_coef        | 0.079    |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3407688  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3410000, episode_reward=1030.91 +/- 799.43\n",
      "Episode length: 839.00 +/- 322.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 839      |\n",
      "|    mean_reward     | 1.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -422     |\n",
      "|    critic_loss     | 52.6     |\n",
      "|    ent_coef        | 0.0792   |\n",
      "|    ent_coef_loss   | -0.709   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 677      |\n",
      "|    ep_rew_mean     | -52.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4310     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20547    |\n",
      "|    total_timesteps | 3415258  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -414     |\n",
      "|    critic_loss     | 71.6     |\n",
      "|    ent_coef        | 0.0805   |\n",
      "|    ent_coef_loss   | 0.0269   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3415157  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3420000, episode_reward=2115.87 +/- 1921.85\n",
      "Episode length: 749.00 +/- 256.10\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 749      |\n",
      "|    mean_reward     | 2.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -396     |\n",
      "|    critic_loss     | 36       |\n",
      "|    ent_coef        | 0.0806   |\n",
      "|    ent_coef_loss   | 0.367    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 695      |\n",
      "|    ep_rew_mean     | 57.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4320     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20597    |\n",
      "|    total_timesteps | 3423574  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -403     |\n",
      "|    critic_loss     | 239      |\n",
      "|    ent_coef        | 0.0807   |\n",
      "|    ent_coef_loss   | -0.298   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3423473  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 685      |\n",
      "|    ep_rew_mean     | 102      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4330     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20624    |\n",
      "|    total_timesteps | 3428054  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -392     |\n",
      "|    critic_loss     | 32       |\n",
      "|    ent_coef        | 0.0808   |\n",
      "|    ent_coef_loss   | 0.81     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3427953  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3430000, episode_reward=1461.76 +/- 1961.22\n",
      "Episode length: 896.20 +/- 207.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 896      |\n",
      "|    mean_reward     | 1.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -393     |\n",
      "|    critic_loss     | 71.5     |\n",
      "|    ent_coef        | 0.0825   |\n",
      "|    ent_coef_loss   | 0.77     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 672      |\n",
      "|    ep_rew_mean     | 209      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4340     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20670    |\n",
      "|    total_timesteps | 3435760  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -386     |\n",
      "|    critic_loss     | 49.7     |\n",
      "|    ent_coef        | 0.0818   |\n",
      "|    ent_coef_loss   | 0.604    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3435659  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3440000, episode_reward=1024.67 +/- 2124.94\n",
      "Episode length: 673.20 +/- 400.70\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 673      |\n",
      "|    mean_reward     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -385     |\n",
      "|    critic_loss     | 45.5     |\n",
      "|    ent_coef        | 0.0812   |\n",
      "|    ent_coef_loss   | 1.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 665      |\n",
      "|    ep_rew_mean     | 317      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4350     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20709    |\n",
      "|    total_timesteps | 3442236  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -374     |\n",
      "|    critic_loss     | 50.8     |\n",
      "|    ent_coef        | 0.0842   |\n",
      "|    ent_coef_loss   | -0.663   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3442135  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 651      |\n",
      "|    ep_rew_mean     | 462      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4360     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20749    |\n",
      "|    total_timesteps | 3448989  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -377     |\n",
      "|    critic_loss     | 64.3     |\n",
      "|    ent_coef        | 0.0832   |\n",
      "|    ent_coef_loss   | 0.674    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3448888  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3450000, episode_reward=2557.41 +/- 2484.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -376     |\n",
      "|    critic_loss     | 50.4     |\n",
      "|    ent_coef        | 0.0833   |\n",
      "|    ent_coef_loss   | -0.527   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 674      |\n",
      "|    ep_rew_mean     | 655      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4370     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20803    |\n",
      "|    total_timesteps | 3457882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 97.5     |\n",
      "|    ent_coef        | 0.0811   |\n",
      "|    ent_coef_loss   | -0.515   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3457781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3460000, episode_reward=945.08 +/- 1256.67\n",
      "Episode length: 401.40 +/- 380.02\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 401      |\n",
      "|    mean_reward     | 945      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 46.4     |\n",
      "|    ent_coef        | 0.0811   |\n",
      "|    ent_coef_loss   | -0.678   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 688      |\n",
      "|    ep_rew_mean     | 928      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4380     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20844    |\n",
      "|    total_timesteps | 3464752  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 71.1     |\n",
      "|    ent_coef        | 0.0812   |\n",
      "|    ent_coef_loss   | -0.619   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3464651  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3470000, episode_reward=2897.50 +/- 1617.09\n",
      "Episode length: 643.00 +/- 354.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 643      |\n",
      "|    mean_reward     | 2.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 43.8     |\n",
      "|    ent_coef        | 0.0817   |\n",
      "|    ent_coef_loss   | -0.337   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 710      |\n",
      "|    ep_rew_mean     | 1.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4390     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20897    |\n",
      "|    total_timesteps | 3473597  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 32.6     |\n",
      "|    ent_coef        | 0.0819   |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3473496  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 721      |\n",
      "|    ep_rew_mean     | 1.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4400     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20934    |\n",
      "|    total_timesteps | 3479876  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 38.4     |\n",
      "|    ent_coef        | 0.0821   |\n",
      "|    ent_coef_loss   | -0.653   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3479775  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480000, episode_reward=3248.74 +/- 2317.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 42.4     |\n",
      "|    ent_coef        | 0.0824   |\n",
      "|    ent_coef_loss   | -0.701   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 731      |\n",
      "|    ep_rew_mean     | 1.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4410     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 20985    |\n",
      "|    total_timesteps | 3488366  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 184      |\n",
      "|    ent_coef        | 0.0833   |\n",
      "|    ent_coef_loss   | 0.94     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3488265  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3490000, episode_reward=4300.17 +/- 1251.97\n",
      "Episode length: 882.40 +/- 235.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 882      |\n",
      "|    mean_reward     | 4.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 49.7     |\n",
      "|    ent_coef        | 0.0818   |\n",
      "|    ent_coef_loss   | 0.724    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 733      |\n",
      "|    ep_rew_mean     | 1.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4420     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21036    |\n",
      "|    total_timesteps | 3496877  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 52.1     |\n",
      "|    ent_coef        | 0.0835   |\n",
      "|    ent_coef_loss   | 0.646    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3496776  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500000, episode_reward=2707.92 +/- 1432.01\n",
      "Episode length: 559.20 +/- 279.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 559      |\n",
      "|    mean_reward     | 2.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 50       |\n",
      "|    ent_coef        | 0.0837   |\n",
      "|    ent_coef_loss   | -0.518   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 763      |\n",
      "|    ep_rew_mean     | 2.03e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4430     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21081    |\n",
      "|    total_timesteps | 3504396  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 41.4     |\n",
      "|    ent_coef        | 0.0847   |\n",
      "|    ent_coef_loss   | -0.566   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3504295  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3510000, episode_reward=2551.24 +/- 2500.19\n",
      "Episode length: 972.40 +/- 55.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 972      |\n",
      "|    mean_reward     | 2.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 46.3     |\n",
      "|    ent_coef        | 0.0832   |\n",
      "|    ent_coef_loss   | -0.903   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4440     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21135    |\n",
      "|    total_timesteps | 3513341  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 77.7     |\n",
      "|    ent_coef        | 0.0843   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3513240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=4294.65 +/- 1918.64\n",
      "Episode length: 820.20 +/- 359.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 50.1     |\n",
      "|    ent_coef        | 0.0834   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4450     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21190    |\n",
      "|    total_timesteps | 3522459  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 66.6     |\n",
      "|    ent_coef        | 0.0849   |\n",
      "|    ent_coef_loss   | -0.137   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3522358  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3530000, episode_reward=2576.93 +/- 2307.08\n",
      "Episode length: 747.00 +/- 312.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 747      |\n",
      "|    mean_reward     | 2.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 31.7     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | 0.242    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 824      |\n",
      "|    ep_rew_mean     | 2.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4460     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21243    |\n",
      "|    total_timesteps | 3531389  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 40.5     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | -0.517   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3531288  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 802      |\n",
      "|    ep_rew_mean     | 2.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4470     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21282    |\n",
      "|    total_timesteps | 3538041  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 55.2     |\n",
      "|    ent_coef        | 0.0849   |\n",
      "|    ent_coef_loss   | -0.249   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3537940  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3540000, episode_reward=1859.60 +/- 1359.53\n",
      "Episode length: 547.00 +/- 304.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 547      |\n",
      "|    mean_reward     | 1.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 145      |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | 0.695    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4480     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21323    |\n",
      "|    total_timesteps | 3544881  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 38.2     |\n",
      "|    ent_coef        | 0.0853   |\n",
      "|    ent_coef_loss   | 0.334    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3544780  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3550000, episode_reward=4010.96 +/- 1978.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 58.2     |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | 0.413    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4490     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21368    |\n",
      "|    total_timesteps | 3552286  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.0852   |\n",
      "|    ent_coef_loss   | 0.734    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3552185  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3560000, episode_reward=4220.81 +/- 1278.72\n",
      "Episode length: 807.40 +/- 236.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 807      |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 44       |\n",
      "|    ent_coef        | 0.0849   |\n",
      "|    ent_coef_loss   | 0.348    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4500     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21420    |\n",
      "|    total_timesteps | 3560956  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 137      |\n",
      "|    ent_coef        | 0.0846   |\n",
      "|    ent_coef_loss   | -0.267   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3560855  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 793      |\n",
      "|    ep_rew_mean     | 3.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4510     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21460    |\n",
      "|    total_timesteps | 3567700  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 184      |\n",
      "|    ent_coef        | 0.0833   |\n",
      "|    ent_coef_loss   | 1.37     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3567599  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570000, episode_reward=4129.09 +/- 1833.29\n",
      "Episode length: 822.40 +/- 355.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 822      |\n",
      "|    mean_reward     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -334     |\n",
      "|    critic_loss     | 53.8     |\n",
      "|    ent_coef        | 0.084    |\n",
      "|    ent_coef_loss   | -0.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 3.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4520     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21506    |\n",
      "|    total_timesteps | 3575380  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 44.4     |\n",
      "|    ent_coef        | 0.0843   |\n",
      "|    ent_coef_loss   | 0.267    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3575279  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3580000, episode_reward=4564.01 +/- 1251.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 68.5     |\n",
      "|    ent_coef        | 0.084    |\n",
      "|    ent_coef_loss   | -0.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4530     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21556    |\n",
      "|    total_timesteps | 3583567  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 139      |\n",
      "|    ent_coef        | 0.0829   |\n",
      "|    ent_coef_loss   | -0.0707  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3583466  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3590000, episode_reward=4983.52 +/- 747.37\n",
      "Episode length: 936.20 +/- 127.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 936      |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 36.7     |\n",
      "|    ent_coef        | 0.0844   |\n",
      "|    ent_coef_loss   | -0.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 787      |\n",
      "|    ep_rew_mean     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4540     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21607    |\n",
      "|    total_timesteps | 3592074  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 50       |\n",
      "|    ent_coef        | 0.0856   |\n",
      "|    ent_coef_loss   | 0.738    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3591973  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3600000, episode_reward=4678.47 +/- 1226.18\n",
      "Episode length: 883.20 +/- 233.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 883      |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 64       |\n",
      "|    ent_coef        | 0.0851   |\n",
      "|    ent_coef_loss   | 0.741    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4550     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21665    |\n",
      "|    total_timesteps | 3601701  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3601600  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3610000, episode_reward=4460.07 +/- 1074.88\n",
      "Episode length: 851.40 +/- 186.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 851      |\n",
      "|    mean_reward     | 4.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 34.7     |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | 0.353    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4560     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21723    |\n",
      "|    total_timesteps | 3611333  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 47.6     |\n",
      "|    ent_coef        | 0.0847   |\n",
      "|    ent_coef_loss   | -0.021   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3611232  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3620000, episode_reward=5107.94 +/- 193.81\n",
      "Episode length: 978.60 +/- 42.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 979      |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 81.1     |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | 0.662    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 821      |\n",
      "|    ep_rew_mean     | 3.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4570     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21776    |\n",
      "|    total_timesteps | 3620188  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 32.8     |\n",
      "|    ent_coef        | 0.0842   |\n",
      "|    ent_coef_loss   | -0.914   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3620087  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 3.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4580     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21821    |\n",
      "|    total_timesteps | 3627360  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 161      |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | 0.332    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3627259  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3630000, episode_reward=3438.25 +/- 2479.78\n",
      "Episode length: 635.40 +/- 446.55\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 635      |\n",
      "|    mean_reward     | 3.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 46.8     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | 0.668    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4590     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21865    |\n",
      "|    total_timesteps | 3634457  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 36.8     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | 0.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3634356  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3640000, episode_reward=4212.47 +/- 1873.71\n",
      "Episode length: 822.40 +/- 355.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 822      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 45.2     |\n",
      "|    ent_coef        | 0.0853   |\n",
      "|    ent_coef_loss   | -0.757   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4600     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21920    |\n",
      "|    total_timesteps | 3643188  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 44.1     |\n",
      "|    ent_coef        | 0.0872   |\n",
      "|    ent_coef_loss   | -0.644   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3643087  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3650000, episode_reward=2350.39 +/- 2045.54\n",
      "Episode length: 455.60 +/- 370.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 456      |\n",
      "|    mean_reward     | 2.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 51.7     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 3.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4610     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 21967    |\n",
      "|    total_timesteps | 3650652  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.0837   |\n",
      "|    ent_coef_loss   | -0.414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3650551  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 3.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4620     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22014    |\n",
      "|    total_timesteps | 3658322  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.0833   |\n",
      "|    ent_coef_loss   | 0.444    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3658221  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660000, episode_reward=4175.41 +/- 1501.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 76       |\n",
      "|    ent_coef        | 0.0843   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4630     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22072    |\n",
      "|    total_timesteps | 3667655  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 77.5     |\n",
      "|    ent_coef        | 0.0869   |\n",
      "|    ent_coef_loss   | 0.209    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3667554  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3670000, episode_reward=3917.75 +/- 1502.73\n",
      "Episode length: 831.80 +/- 258.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 832      |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -343     |\n",
      "|    critic_loss     | 41.5     |\n",
      "|    ent_coef        | 0.0877   |\n",
      "|    ent_coef_loss   | -0.502   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4640     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22116    |\n",
      "|    total_timesteps | 3674880  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 118      |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | 0.413    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3674779  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=5222.48 +/- 353.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 63.8     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 0.542    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 817      |\n",
      "|    ep_rew_mean     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4650     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22168    |\n",
      "|    total_timesteps | 3683443  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 63.7     |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | 0.995    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3683342  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690000, episode_reward=4408.32 +/- 2175.79\n",
      "Episode length: 806.00 +/- 388.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 71.4     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | 0.276    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4660     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22218    |\n",
      "|    total_timesteps | 3691717  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 52.6     |\n",
      "|    ent_coef        | 0.0877   |\n",
      "|    ent_coef_loss   | -0.666   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3691616  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4670     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22254    |\n",
      "|    total_timesteps | 3697882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 39.5     |\n",
      "|    ent_coef        | 0.0891   |\n",
      "|    ent_coef_loss   | -0.0171  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3697781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3700000, episode_reward=3761.42 +/- 2236.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 86.4     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | -0.413   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4680     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22294    |\n",
      "|    total_timesteps | 3704573  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.0885   |\n",
      "|    ent_coef_loss   | -0.842   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3704472  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3710000, episode_reward=4517.51 +/- 1461.24\n",
      "Episode length: 825.40 +/- 260.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 825      |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 34.8     |\n",
      "|    ent_coef        | 0.0897   |\n",
      "|    ent_coef_loss   | 0.464    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 779      |\n",
      "|    ep_rew_mean     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 4690     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22341    |\n",
      "|    total_timesteps | 3712344  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 39       |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | 0.526    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3712243  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3720000, episode_reward=4671.72 +/- 1634.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 29.3     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | -0.483   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4700     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22388    |\n",
      "|    total_timesteps | 3720169  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 43.1     |\n",
      "|    ent_coef        | 0.0896   |\n",
      "|    ent_coef_loss   | 0.0839   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3720068  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4710     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22435    |\n",
      "|    total_timesteps | 3728082  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 64.3     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | 0.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3727981  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3730000, episode_reward=5300.94 +/- 352.27\n",
      "Episode length: 977.80 +/- 44.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 978      |\n",
      "|    mean_reward     | 5.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | 0.0838   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4720     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22478    |\n",
      "|    total_timesteps | 3735277  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 68.3     |\n",
      "|    ent_coef        | 0.0912   |\n",
      "|    ent_coef_loss   | 1.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3735176  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3740000, episode_reward=3300.92 +/- 1862.28\n",
      "Episode length: 618.20 +/- 329.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 618      |\n",
      "|    mean_reward     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -370     |\n",
      "|    critic_loss     | 70.6     |\n",
      "|    ent_coef        | 0.0914   |\n",
      "|    ent_coef_loss   | 0.224    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 752      |\n",
      "|    ep_rew_mean     | 3.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4730     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22523    |\n",
      "|    total_timesteps | 3742869  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 44       |\n",
      "|    ent_coef        | 0.0918   |\n",
      "|    ent_coef_loss   | -0.317   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3742768  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750000, episode_reward=5368.28 +/- 122.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 34.7     |\n",
      "|    ent_coef        | 0.0942   |\n",
      "|    ent_coef_loss   | -0.383   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3749899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4740     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22579    |\n",
      "|    total_timesteps | 3752118  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -380     |\n",
      "|    critic_loss     | 197      |\n",
      "|    ent_coef        | 0.0936   |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3752017  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3760000, episode_reward=2560.72 +/- 2227.31\n",
      "Episode length: 505.40 +/- 413.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 505      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    ent_coef        | 0.0944   |\n",
      "|    ent_coef_loss   | -0.235   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 774      |\n",
      "|    ep_rew_mean     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4750     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22631    |\n",
      "|    total_timesteps | 3760854  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    ent_coef        | 0.0934   |\n",
      "|    ent_coef_loss   | -0.464   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3760753  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 730      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4760     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22653    |\n",
      "|    total_timesteps | 3764719  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 34.6     |\n",
      "|    ent_coef        | 0.0912   |\n",
      "|    ent_coef_loss   | -1.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3764618  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3770000, episode_reward=2244.18 +/- 1986.79\n",
      "Episode length: 557.40 +/- 430.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 557      |\n",
      "|    mean_reward     | 2.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 92.8     |\n",
      "|    ent_coef        | 0.0891   |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 732      |\n",
      "|    ep_rew_mean     | 3.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4770     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22692    |\n",
      "|    total_timesteps | 3771110  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 104      |\n",
      "|    ent_coef        | 0.0886   |\n",
      "|    ent_coef_loss   | 0.647    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3771009  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3780000, episode_reward=3764.09 +/- 2054.64\n",
      "Episode length: 863.60 +/- 272.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 864      |\n",
      "|    mean_reward     | 3.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 48.6     |\n",
      "|    ent_coef        | 0.0879   |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4780     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22748    |\n",
      "|    total_timesteps | 3780590  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0887   |\n",
      "|    ent_coef_loss   | -0.184   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3780489  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 761      |\n",
      "|    ep_rew_mean     | 3.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4790     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22794    |\n",
      "|    total_timesteps | 3788442  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 36.1     |\n",
      "|    ent_coef        | 0.0882   |\n",
      "|    ent_coef_loss   | -0.886   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3788341  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3790000, episode_reward=5175.86 +/- 530.48\n",
      "Episode length: 954.40 +/- 91.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 954      |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 43.9     |\n",
      "|    ent_coef        | 0.0862   |\n",
      "|    ent_coef_loss   | 0.361    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 760      |\n",
      "|    ep_rew_mean     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4800     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22841    |\n",
      "|    total_timesteps | 3796141  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 108      |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | 1.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3796040  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3800000, episode_reward=4755.78 +/- 1090.93\n",
      "Episode length: 905.40 +/- 189.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 905      |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 46.2     |\n",
      "|    ent_coef        | 0.088    |\n",
      "|    ent_coef_loss   | 0.337    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 3.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4810     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22901    |\n",
      "|    total_timesteps | 3806103  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 47.9     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | 0.165    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3806002  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810000, episode_reward=4661.12 +/- 1671.40\n",
      "Episode length: 856.60 +/- 286.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 857      |\n",
      "|    mean_reward     | 4.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -368     |\n",
      "|    critic_loss     | 26.5     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | -0.187   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 786      |\n",
      "|    ep_rew_mean     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4820     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 22948    |\n",
      "|    total_timesteps | 3813848  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 31.4     |\n",
      "|    ent_coef        | 0.0869   |\n",
      "|    ent_coef_loss   | -0.507   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3813747  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3820000, episode_reward=5377.82 +/- 346.74\n",
      "Episode length: 989.60 +/- 20.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 990      |\n",
      "|    mean_reward     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 158      |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -0.336   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3819899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 806      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4830     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23006    |\n",
      "|    total_timesteps | 3823474  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 44.2     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | -0.161   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3823373  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3830000, episode_reward=1544.03 +/- 1890.53\n",
      "Episode length: 483.60 +/- 426.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 484      |\n",
      "|    mean_reward     | 1.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4840     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23057    |\n",
      "|    total_timesteps | 3832038  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 53.3     |\n",
      "|    ent_coef        | 0.0869   |\n",
      "|    ent_coef_loss   | 0.233    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3831937  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 771      |\n",
      "|    ep_rew_mean     | 3.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4850     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23091    |\n",
      "|    total_timesteps | 3837911  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3837810  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=3728.30 +/- 2333.76\n",
      "Episode length: 905.80 +/- 188.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 906      |\n",
      "|    mean_reward     | 3.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 37.9     |\n",
      "|    ent_coef        | 0.0847   |\n",
      "|    ent_coef_loss   | -0.603   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4860     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23139    |\n",
      "|    total_timesteps | 3845871  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 112      |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -0.609   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3845770  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850000, episode_reward=4296.77 +/- 1626.12\n",
      "Episode length: 860.40 +/- 279.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 4.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 43.1     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 842      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4870     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23196    |\n",
      "|    total_timesteps | 3855292  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 35.8     |\n",
      "|    ent_coef        | 0.0876   |\n",
      "|    ent_coef_loss   | 0.565    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3855191  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3860000, episode_reward=4580.11 +/- 1174.13\n",
      "Episode length: 885.80 +/- 228.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 886      |\n",
      "|    mean_reward     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 37.1     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | -0.872   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4880     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23250    |\n",
      "|    total_timesteps | 3864259  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 36.9     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | 0.912    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3864158  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870000, episode_reward=5410.08 +/- 203.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 88.4     |\n",
      "|    ent_coef        | 0.0874   |\n",
      "|    ent_coef_loss   | 0.481    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3869899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 853      |\n",
      "|    ep_rew_mean     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4890     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23306    |\n",
      "|    total_timesteps | 3873703  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 66.3     |\n",
      "|    ent_coef        | 0.085    |\n",
      "|    ent_coef_loss   | 0.471    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3873602  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3880000, episode_reward=4611.16 +/- 1526.63\n",
      "Episode length: 860.20 +/- 279.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 860      |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -348     |\n",
      "|    critic_loss     | 35       |\n",
      "|    ent_coef        | 0.0857   |\n",
      "|    ent_coef_loss   | -0.251   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 851      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4900     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23352    |\n",
      "|    total_timesteps | 3881229  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 54.1     |\n",
      "|    ent_coef        | 0.0843   |\n",
      "|    ent_coef_loss   | -0.211   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3881128  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 837      |\n",
      "|    ep_rew_mean     | 3.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4910     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23402    |\n",
      "|    total_timesteps | 3889838  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 41.5     |\n",
      "|    ent_coef        | 0.0868   |\n",
      "|    ent_coef_loss   | -0.509   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3889737  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890000, episode_reward=4502.96 +/- 1722.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 63.2     |\n",
      "|    ent_coef        | 0.086    |\n",
      "|    ent_coef_loss   | 0.0673   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4920     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23451    |\n",
      "|    total_timesteps | 3897994  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 46.5     |\n",
      "|    ent_coef        | 0.0871   |\n",
      "|    ent_coef_loss   | 0.257    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3897893  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900000, episode_reward=5114.73 +/- 660.02\n",
      "Episode length: 937.60 +/- 124.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 938      |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 47.1     |\n",
      "|    ent_coef        | 0.0856   |\n",
      "|    ent_coef_loss   | 0.236    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4930     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23508    |\n",
      "|    total_timesteps | 3907402  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 42.1     |\n",
      "|    ent_coef        | 0.0874   |\n",
      "|    ent_coef_loss   | -0.551   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3907301  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3910000, episode_reward=5433.94 +/- 46.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 46.8     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | -0.138   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3909899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4940     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23568    |\n",
      "|    total_timesteps | 3917402  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -346     |\n",
      "|    critic_loss     | 43.3     |\n",
      "|    ent_coef        | 0.0885   |\n",
      "|    ent_coef_loss   | 0.871    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3917301  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3920000, episode_reward=4182.61 +/- 1991.71\n",
      "Episode length: 815.20 +/- 369.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 815      |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 54.5     |\n",
      "|    ent_coef        | 0.0876   |\n",
      "|    ent_coef_loss   | 0.0538   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 887      |\n",
      "|    ep_rew_mean     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4950     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23623    |\n",
      "|    total_timesteps | 3926617  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 59.9     |\n",
      "|    ent_coef        | 0.0887   |\n",
      "|    ent_coef_loss   | -0.136   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3926516  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3930000, episode_reward=5520.89 +/- 120.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 65.2     |\n",
      "|    ent_coef        | 0.0879   |\n",
      "|    ent_coef_loss   | 0.468    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3929899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 891      |\n",
      "|    ep_rew_mean     | 4.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4960     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23673    |\n",
      "|    total_timesteps | 3935005  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 31.4     |\n",
      "|    ent_coef        | 0.0868   |\n",
      "|    ent_coef_loss   | -0.471   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3934904  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3940000, episode_reward=5277.85 +/- 525.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 46.8     |\n",
      "|    ent_coef        | 0.0885   |\n",
      "|    ent_coef_loss   | 0.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4970     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23722    |\n",
      "|    total_timesteps | 3942982  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 133      |\n",
      "|    ent_coef        | 0.089    |\n",
      "|    ent_coef_loss   | -0.228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3942881  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3950000, episode_reward=5202.07 +/- 892.75\n",
      "Episode length: 926.40 +/- 147.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 926      |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 56.3     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | -0.0354  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 4.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4980     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23769    |\n",
      "|    total_timesteps | 3950845  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 42.4     |\n",
      "|    ent_coef        | 0.0902   |\n",
      "|    ent_coef_loss   | 0.349    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3950744  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4990     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23820    |\n",
      "|    total_timesteps | 3959622  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 44.3     |\n",
      "|    ent_coef        | 0.0888   |\n",
      "|    ent_coef_loss   | -0.285   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3959521  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960000, episode_reward=4340.35 +/- 2595.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 37.9     |\n",
      "|    ent_coef        | 0.0877   |\n",
      "|    ent_coef_loss   | -0.536   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 4.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5000     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23871    |\n",
      "|    total_timesteps | 3967999  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 52.4     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 0.355    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3967898  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3970000, episode_reward=4723.65 +/- 1046.99\n",
      "Episode length: 858.20 +/- 199.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 858      |\n",
      "|    mean_reward     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -340     |\n",
      "|    critic_loss     | 55.8     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 1.7      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 4.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5010     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23915    |\n",
      "|    total_timesteps | 3975273  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 58.5     |\n",
      "|    ent_coef        | 0.089    |\n",
      "|    ent_coef_loss   | 0.129    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3975172  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3980000, episode_reward=3403.07 +/- 2454.90\n",
      "Episode length: 611.60 +/- 430.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 612      |\n",
      "|    mean_reward     | 3.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | -0.449   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 4.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5020     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 23968    |\n",
      "|    total_timesteps | 3984296  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 29.9     |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | -0.997   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3984195  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990000, episode_reward=5575.40 +/- 112.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | -0.436   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3989899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5030     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24006    |\n",
      "|    total_timesteps | 3990493  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 54.3     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | -0.0863  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3990392  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 825      |\n",
      "|    ep_rew_mean     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5040     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24061    |\n",
      "|    total_timesteps | 3999853  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 34.9     |\n",
      "|    ent_coef        | 0.0899   |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3999752  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000000, episode_reward=4938.17 +/- 1396.60\n",
      "Episode length: 878.40 +/- 243.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 878      |\n",
      "|    mean_reward     | 4.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -356     |\n",
      "|    critic_loss     | 122      |\n",
      "|    ent_coef        | 0.0904   |\n",
      "|    ent_coef_loss   | -0.541   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 813      |\n",
      "|    ep_rew_mean     | 4.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5050     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24109    |\n",
      "|    total_timesteps | 4007890  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -347     |\n",
      "|    critic_loss     | 33.3     |\n",
      "|    ent_coef        | 0.0902   |\n",
      "|    ent_coef_loss   | 0.0192   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4007789  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4010000, episode_reward=5450.10 +/- 65.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -349     |\n",
      "|    critic_loss     | 52.8     |\n",
      "|    ent_coef        | 0.0906   |\n",
      "|    ent_coef_loss   | 0.339    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 819      |\n",
      "|    ep_rew_mean     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5060     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24163    |\n",
      "|    total_timesteps | 4016913  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 46       |\n",
      "|    ent_coef        | 0.0913   |\n",
      "|    ent_coef_loss   | 1.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4016812  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4020000, episode_reward=4706.09 +/- 1490.20\n",
      "Episode length: 874.40 +/- 251.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 874      |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 29.3     |\n",
      "|    ent_coef        | 0.093    |\n",
      "|    ent_coef_loss   | 0.521    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5070     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24217    |\n",
      "|    total_timesteps | 4025976  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 48.8     |\n",
      "|    ent_coef        | 0.092    |\n",
      "|    ent_coef_loss   | -0.192   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4025875  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4030000, episode_reward=5009.61 +/- 954.09\n",
      "Episode length: 919.40 +/- 161.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 919      |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 69.3     |\n",
      "|    ent_coef        | 0.0905   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 830      |\n",
      "|    ep_rew_mean     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5080     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24265    |\n",
      "|    total_timesteps | 4033882  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 32       |\n",
      "|    ent_coef        | 0.0909   |\n",
      "|    ent_coef_loss   | -0.691   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4033781  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4040000, episode_reward=3197.51 +/- 2014.13\n",
      "Episode length: 700.60 +/- 375.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 701      |\n",
      "|    mean_reward     | 3.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 25.3     |\n",
      "|    ent_coef        | 0.0903   |\n",
      "|    ent_coef_loss   | 0.406    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5090     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24315    |\n",
      "|    total_timesteps | 4042313  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 36.6     |\n",
      "|    ent_coef        | 0.0919   |\n",
      "|    ent_coef_loss   | -0.454   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4042212  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050000, episode_reward=5639.18 +/- 121.34\n",
      "Episode length: 998.60 +/- 2.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 999      |\n",
      "|    mean_reward     | 5.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 41.3     |\n",
      "|    ent_coef        | 0.0918   |\n",
      "|    ent_coef_loss   | 1.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4049899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 834      |\n",
      "|    ep_rew_mean     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5100     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24370    |\n",
      "|    total_timesteps | 4051353  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 38.9     |\n",
      "|    ent_coef        | 0.0918   |\n",
      "|    ent_coef_loss   | 0.427    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4051252  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4060000, episode_reward=4945.23 +/- 1186.36\n",
      "Episode length: 897.20 +/- 205.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 897      |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 52.2     |\n",
      "|    ent_coef        | 0.0932   |\n",
      "|    ent_coef_loss   | -0.874   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5110     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24427    |\n",
      "|    total_timesteps | 4060866  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 98.6     |\n",
      "|    ent_coef        | 0.0931   |\n",
      "|    ent_coef_loss   | -0.732   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4060765  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4070000, episode_reward=3600.16 +/- 2241.79\n",
      "Episode length: 805.80 +/- 388.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 47.5     |\n",
      "|    ent_coef        | 0.0921   |\n",
      "|    ent_coef_loss   | 0.0543   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 4.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5120     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24482    |\n",
      "|    total_timesteps | 4070097  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 53.1     |\n",
      "|    ent_coef        | 0.0921   |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4069996  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 883      |\n",
      "|    ep_rew_mean     | 4.44e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5130     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24533    |\n",
      "|    total_timesteps | 4078803  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 30.5     |\n",
      "|    ent_coef        | 0.0924   |\n",
      "|    ent_coef_loss   | -0.167   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4078702  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4080000, episode_reward=5674.43 +/- 100.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -383     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0932   |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4079899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5140     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24581    |\n",
      "|    total_timesteps | 4086754  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -364     |\n",
      "|    critic_loss     | 27       |\n",
      "|    ent_coef        | 0.0943   |\n",
      "|    ent_coef_loss   | 0.559    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4086653  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4090000, episode_reward=5611.69 +/- 72.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -379     |\n",
      "|    critic_loss     | 27.9     |\n",
      "|    ent_coef        | 0.0938   |\n",
      "|    ent_coef_loss   | -1.57    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 889      |\n",
      "|    ep_rew_mean     | 4.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5150     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24641    |\n",
      "|    total_timesteps | 4096754  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 40.6     |\n",
      "|    ent_coef        | 0.0956   |\n",
      "|    ent_coef_loss   | -0.411   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4096653  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4100000, episode_reward=2560.07 +/- 2607.91\n",
      "Episode length: 705.60 +/- 361.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 706      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -373     |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    ent_coef        | 0.0944   |\n",
      "|    ent_coef_loss   | -0.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5160     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24686    |\n",
      "|    total_timesteps | 4104370  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 44.7     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | 1.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4104269  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110000, episode_reward=4016.89 +/- 1593.24\n",
      "Episode length: 711.20 +/- 265.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 711      |\n",
      "|    mean_reward     | 4.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    ent_coef        | 0.0937   |\n",
      "|    ent_coef_loss   | 0.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5170     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24740    |\n",
      "|    total_timesteps | 4113439  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 252      |\n",
      "|    ent_coef        | 0.0928   |\n",
      "|    ent_coef_loss   | 0.00525  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4113338  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4120000, episode_reward=4506.73 +/- 1711.13\n",
      "Episode length: 959.40 +/- 81.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 959      |\n",
      "|    mean_reward     | 4.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -375     |\n",
      "|    critic_loss     | 37.2     |\n",
      "|    ent_coef        | 0.0931   |\n",
      "|    ent_coef_loss   | 0.647    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 893      |\n",
      "|    ep_rew_mean     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5180     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24798    |\n",
      "|    total_timesteps | 4123138  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 63.8     |\n",
      "|    ent_coef        | 0.0952   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4123037  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4130000, episode_reward=5722.72 +/- 86.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -376     |\n",
      "|    critic_loss     | 286      |\n",
      "|    ent_coef        | 0.0953   |\n",
      "|    ent_coef_loss   | -0.302   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4129899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 899      |\n",
      "|    ep_rew_mean     | 4.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5190     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24853    |\n",
      "|    total_timesteps | 4132184  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -374     |\n",
      "|    critic_loss     | 69.9     |\n",
      "|    ent_coef        | 0.095    |\n",
      "|    ent_coef_loss   | 0.616    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4132083  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 872      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5200     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24890    |\n",
      "|    total_timesteps | 4138540  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 32.9     |\n",
      "|    ent_coef        | 0.0941   |\n",
      "|    ent_coef_loss   | -0.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4138439  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4140000, episode_reward=4792.32 +/- 1702.76\n",
      "Episode length: 854.40 +/- 291.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 854      |\n",
      "|    mean_reward     | 4.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.0923   |\n",
      "|    ent_coef_loss   | -0.156   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 852      |\n",
      "|    ep_rew_mean     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5210     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24935    |\n",
      "|    total_timesteps | 4146061  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -375     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    ent_coef        | 0.0949   |\n",
      "|    ent_coef_loss   | 0.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4145960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4150000, episode_reward=5553.20 +/- 139.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 56.1     |\n",
      "|    ent_coef        | 0.095    |\n",
      "|    ent_coef_loss   | 0.12     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5220     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 24996    |\n",
      "|    total_timesteps | 4156061  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 29.3     |\n",
      "|    ent_coef        | 0.0938   |\n",
      "|    ent_coef_loss   | 0.416    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4155960  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4160000, episode_reward=4838.31 +/- 1481.15\n",
      "Episode length: 872.80 +/- 254.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 873      |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 43.5     |\n",
      "|    ent_coef        | 0.0932   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5230     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25047    |\n",
      "|    total_timesteps | 4164559  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    ent_coef        | 0.0941   |\n",
      "|    ent_coef_loss   | -0.466   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4164458  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4170000, episode_reward=5653.29 +/- 139.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -360     |\n",
      "|    critic_loss     | 44.8     |\n",
      "|    ent_coef        | 0.0945   |\n",
      "|    ent_coef_loss   | -0.392   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5240     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25094    |\n",
      "|    total_timesteps | 4172341  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.0938   |\n",
      "|    ent_coef_loss   | 0.532    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4172240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4180000, episode_reward=4689.25 +/- 2013.06\n",
      "Episode length: 827.20 +/- 345.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -350     |\n",
      "|    critic_loss     | 62       |\n",
      "|    ent_coef        | 0.0921   |\n",
      "|    ent_coef_loss   | 1.64     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5250     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25154    |\n",
      "|    total_timesteps | 4182341  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 54.4     |\n",
      "|    ent_coef        | 0.0917   |\n",
      "|    ent_coef_loss   | -1.7     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4182240  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4190000, episode_reward=5619.35 +/- 129.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 39.9     |\n",
      "|    ent_coef        | 0.0919   |\n",
      "|    ent_coef_loss   | -0.498   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5260     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25201    |\n",
      "|    total_timesteps | 4190013  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 53.8     |\n",
      "|    ent_coef        | 0.0919   |\n",
      "|    ent_coef_loss   | 0.664    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4189912  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200000, episode_reward=4888.40 +/- 1311.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    ent_coef        | 0.0911   |\n",
      "|    ent_coef_loss   | 0.719    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5270     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25261    |\n",
      "|    total_timesteps | 4200013  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 44.6     |\n",
      "|    ent_coef        | 0.0912   |\n",
      "|    ent_coef_loss   | -0.937   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4199912  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5280     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25307    |\n",
      "|    total_timesteps | 4207880  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 35.5     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4207779  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4210000, episode_reward=5678.28 +/- 94.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 58.6     |\n",
      "|    ent_coef        | 0.091    |\n",
      "|    ent_coef_loss   | 0.292    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5290     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25349    |\n",
      "|    total_timesteps | 4214912  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 87.4     |\n",
      "|    ent_coef        | 0.0923   |\n",
      "|    ent_coef_loss   | -0.538   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4214811  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4220000, episode_reward=4946.70 +/- 934.56\n",
      "Episode length: 957.40 +/- 85.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 957      |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.0928   |\n",
      "|    ent_coef_loss   | 0.106    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5300     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25404    |\n",
      "|    total_timesteps | 4224004  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 38.5     |\n",
      "|    ent_coef        | 0.0928   |\n",
      "|    ent_coef_loss   | 0.463    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4223903  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4230000, episode_reward=5645.13 +/- 136.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -370     |\n",
      "|    critic_loss     | 31.2     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 4.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5310     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25464    |\n",
      "|    total_timesteps | 4233898  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -355     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0938   |\n",
      "|    ent_coef_loss   | 0.41     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4233797  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4240000, episode_reward=5250.03 +/- 781.60\n",
      "Episode length: 938.20 +/- 123.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 938      |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 38       |\n",
      "|    ent_coef        | 0.0909   |\n",
      "|    ent_coef_loss   | 0.454    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5320     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25523    |\n",
      "|    total_timesteps | 4243898  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 52.8     |\n",
      "|    ent_coef        | 0.0934   |\n",
      "|    ent_coef_loss   | 1.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4243797  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4250000, episode_reward=4577.27 +/- 2280.06\n",
      "Episode length: 802.80 +/- 394.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 803      |\n",
      "|    mean_reward     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 39.6     |\n",
      "|    ent_coef        | 0.0953   |\n",
      "|    ent_coef_loss   | 0.395    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5330     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25579    |\n",
      "|    total_timesteps | 4253174  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -383     |\n",
      "|    critic_loss     | 38.3     |\n",
      "|    ent_coef        | 0.094    |\n",
      "|    ent_coef_loss   | -0.618   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4253073  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4260000, episode_reward=4748.06 +/- 2104.75\n",
      "Episode length: 823.60 +/- 352.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 41       |\n",
      "|    ent_coef        | 0.0945   |\n",
      "|    ent_coef_loss   | 0.905    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 886      |\n",
      "|    ep_rew_mean     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5340     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25625    |\n",
      "|    total_timesteps | 4260927  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 56.1     |\n",
      "|    ent_coef        | 0.0968   |\n",
      "|    ent_coef_loss   | 0.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4260826  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 4.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5350     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25677    |\n",
      "|    total_timesteps | 4269785  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 96       |\n",
      "|    ent_coef        | 0.0971   |\n",
      "|    ent_coef_loss   | 0.874    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269684  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4270000, episode_reward=3528.09 +/- 2563.88\n",
      "Episode length: 841.80 +/- 316.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 842      |\n",
      "|    mean_reward     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 31.8     |\n",
      "|    ent_coef        | 0.0966   |\n",
      "|    ent_coef_loss   | -1.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 875      |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5360     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25723    |\n",
      "|    total_timesteps | 4277465  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 39.5     |\n",
      "|    ent_coef        | 0.098    |\n",
      "|    ent_coef_loss   | 0.449    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4277364  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4280000, episode_reward=2496.15 +/- 1612.71\n",
      "Episode length: 462.20 +/- 278.46\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 462      |\n",
      "|    mean_reward     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -365     |\n",
      "|    critic_loss     | 26.4     |\n",
      "|    ent_coef        | 0.0965   |\n",
      "|    ent_coef_loss   | 0.279    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 847      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5370     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25766    |\n",
      "|    total_timesteps | 4284728  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -368     |\n",
      "|    critic_loss     | 73.3     |\n",
      "|    ent_coef        | 0.0968   |\n",
      "|    ent_coef_loss   | -0.696   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4284627  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4290000, episode_reward=4484.19 +/- 2243.22\n",
      "Episode length: 802.40 +/- 395.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 802      |\n",
      "|    mean_reward     | 4.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 44.7     |\n",
      "|    ent_coef        | 0.0942   |\n",
      "|    ent_coef_loss   | -0.631   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5380     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25819    |\n",
      "|    total_timesteps | 4293436  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 34       |\n",
      "|    ent_coef        | 0.0961   |\n",
      "|    ent_coef_loss   | -1.07    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4293335  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4300000, episode_reward=5610.35 +/- 98.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 53.4     |\n",
      "|    ent_coef        | 0.096    |\n",
      "|    ent_coef_loss   | 0.358    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 863      |\n",
      "|    ep_rew_mean     | 4.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5390     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25866    |\n",
      "|    total_timesteps | 4301188  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 31.7     |\n",
      "|    ent_coef        | 0.0962   |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4301087  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 859      |\n",
      "|    ep_rew_mean     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5400     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25916    |\n",
      "|    total_timesteps | 4309865  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 40.6     |\n",
      "|    ent_coef        | 0.096    |\n",
      "|    ent_coef_loss   | -0.248   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4309764  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4310000, episode_reward=5782.85 +/- 97.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0955   |\n",
      "|    ent_coef_loss   | -0.514   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4309899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5410     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 25966    |\n",
      "|    total_timesteps | 4318215  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -361     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0975   |\n",
      "|    ent_coef_loss   | -0.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4318114  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320000, episode_reward=4657.99 +/- 2203.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 27.5     |\n",
      "|    ent_coef        | 0.096    |\n",
      "|    ent_coef_loss   | -0.0003  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5420     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26018    |\n",
      "|    total_timesteps | 4326745  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -376     |\n",
      "|    critic_loss     | 36.2     |\n",
      "|    ent_coef        | 0.0964   |\n",
      "|    ent_coef_loss   | 0.234    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4326644  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4330000, episode_reward=4341.75 +/- 2621.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 255      |\n",
      "|    ent_coef        | 0.096    |\n",
      "|    ent_coef_loss   | 0.279    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5430     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26074    |\n",
      "|    total_timesteps | 4336070  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -371     |\n",
      "|    critic_loss     | 31       |\n",
      "|    ent_coef        | 0.0962   |\n",
      "|    ent_coef_loss   | -0.595   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4335969  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4340000, episode_reward=5659.32 +/- 64.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -385     |\n",
      "|    critic_loss     | 47.8     |\n",
      "|    ent_coef        | 0.0964   |\n",
      "|    ent_coef_loss   | 0.352    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 4.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5440     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26131    |\n",
      "|    total_timesteps | 4345478  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -378     |\n",
      "|    critic_loss     | 51.8     |\n",
      "|    ent_coef        | 0.0974   |\n",
      "|    ent_coef_loss   | 0.846    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4345377  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4350000, episode_reward=5324.89 +/- 667.07\n",
      "Episode length: 939.00 +/- 122.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 939      |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -380     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    ent_coef        | 0.0963   |\n",
      "|    ent_coef_loss   | -0.444   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5450     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26183    |\n",
      "|    total_timesteps | 4354251  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -385     |\n",
      "|    critic_loss     | 70.8     |\n",
      "|    ent_coef        | 0.098    |\n",
      "|    ent_coef_loss   | -0.323   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4354150  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4360000, episode_reward=4996.83 +/- 1241.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -389     |\n",
      "|    critic_loss     | 37.1     |\n",
      "|    ent_coef        | 0.0966   |\n",
      "|    ent_coef_loss   | 0.353    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 4.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5460     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26239    |\n",
      "|    total_timesteps | 4363558  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -386     |\n",
      "|    critic_loss     | 29.1     |\n",
      "|    ent_coef        | 0.0941   |\n",
      "|    ent_coef_loss   | -1.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4363457  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4370000, episode_reward=5668.97 +/- 52.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -386     |\n",
      "|    critic_loss     | 36.3     |\n",
      "|    ent_coef        | 0.0926   |\n",
      "|    ent_coef_loss   | -0.0872  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 4.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5470     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26281    |\n",
      "|    total_timesteps | 4370388  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -391     |\n",
      "|    critic_loss     | 48.5     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | -0.428   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4370287  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5480     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26331    |\n",
      "|    total_timesteps | 4379040  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -398     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0936   |\n",
      "|    ent_coef_loss   | -0.817   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4378939  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380000, episode_reward=5319.99 +/- 943.71\n",
      "Episode length: 923.40 +/- 153.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 923      |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -394     |\n",
      "|    critic_loss     | 41.1     |\n",
      "|    ent_coef        | 0.093    |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 4.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5490     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26385    |\n",
      "|    total_timesteps | 4387970  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -386     |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    ent_coef        | 0.0904   |\n",
      "|    ent_coef_loss   | -0.00818 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4387869  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4390000, episode_reward=5570.01 +/- 505.16\n",
      "Episode length: 957.20 +/- 85.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 957      |\n",
      "|    mean_reward     | 5.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -380     |\n",
      "|    critic_loss     | 59.1     |\n",
      "|    ent_coef        | 0.0914   |\n",
      "|    ent_coef_loss   | 0.392    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 868      |\n",
      "|    ep_rew_mean     | 4.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5500     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26437    |\n",
      "|    total_timesteps | 4396657  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -381     |\n",
      "|    critic_loss     | 43       |\n",
      "|    ent_coef        | 0.0917   |\n",
      "|    ent_coef_loss   | 0.722    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4396556  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4400000, episode_reward=4881.70 +/- 1406.95\n",
      "Episode length: 874.60 +/- 250.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 875      |\n",
      "|    mean_reward     | 4.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -395     |\n",
      "|    critic_loss     | 40.2     |\n",
      "|    ent_coef        | 0.0907   |\n",
      "|    ent_coef_loss   | -0.205   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5510     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26489    |\n",
      "|    total_timesteps | 4405171  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -392     |\n",
      "|    critic_loss     | 61.6     |\n",
      "|    ent_coef        | 0.0924   |\n",
      "|    ent_coef_loss   | 0.423    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4405070  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410000, episode_reward=4734.94 +/- 1308.11\n",
      "Episode length: 842.80 +/- 214.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 843      |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -389     |\n",
      "|    critic_loss     | 50.2     |\n",
      "|    ent_coef        | 0.0896   |\n",
      "|    ent_coef_loss   | -0.613   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 884      |\n",
      "|    ep_rew_mean     | 4.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5520     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26548    |\n",
      "|    total_timesteps | 4415171  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -388     |\n",
      "|    critic_loss     | 59       |\n",
      "|    ent_coef        | 0.0897   |\n",
      "|    ent_coef_loss   | -0.00567 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4415070  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4420000, episode_reward=5312.09 +/- 899.55\n",
      "Episode length: 923.40 +/- 153.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 923      |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -388     |\n",
      "|    critic_loss     | 137      |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | 0.0626   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5530     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26587    |\n",
      "|    total_timesteps | 4421596  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -399     |\n",
      "|    critic_loss     | 437      |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4421495  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 845      |\n",
      "|    ep_rew_mean     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5540     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26638    |\n",
      "|    total_timesteps | 4429978  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -402     |\n",
      "|    critic_loss     | 85.6     |\n",
      "|    ent_coef        | 0.088    |\n",
      "|    ent_coef_loss   | -0.419   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4429877  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4430000, episode_reward=5653.74 +/- 72.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -396     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.088    |\n",
      "|    ent_coef_loss   | -1.8     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 857      |\n",
      "|    ep_rew_mean     | 4.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5550     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26698    |\n",
      "|    total_timesteps | 4439978  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -404     |\n",
      "|    critic_loss     | 46.4     |\n",
      "|    ent_coef        | 0.0856   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4439877  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4440000, episode_reward=5831.57 +/- 133.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 42.8     |\n",
      "|    ent_coef        | 0.0857   |\n",
      "|    ent_coef_loss   | -0.0682  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4439899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 861      |\n",
      "|    ep_rew_mean     | 4.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5560     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26756    |\n",
      "|    total_timesteps | 4449628  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -394     |\n",
      "|    critic_loss     | 50.4     |\n",
      "|    ent_coef        | 0.0855   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4449527  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4450000, episode_reward=5405.60 +/- 513.89\n",
      "Episode length: 952.80 +/- 94.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 953      |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -404     |\n",
      "|    critic_loss     | 87.5     |\n",
      "|    ent_coef        | 0.0862   |\n",
      "|    ent_coef_loss   | 0.479    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 881      |\n",
      "|    ep_rew_mean     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5570     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26809    |\n",
      "|    total_timesteps | 4458520  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 33.6     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -0.109   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4458419  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4460000, episode_reward=4805.27 +/- 1780.59\n",
      "Episode length: 993.40 +/- 13.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 993      |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -408     |\n",
      "|    critic_loss     | 87.4     |\n",
      "|    ent_coef        | 0.0842   |\n",
      "|    ent_coef_loss   | 0.533    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 877      |\n",
      "|    ep_rew_mean     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5580     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26858    |\n",
      "|    total_timesteps | 4466706  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -397     |\n",
      "|    critic_loss     | 43.6     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -0.449   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4466605  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470000, episode_reward=4645.29 +/- 1340.07\n",
      "Episode length: 826.40 +/- 232.31\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 826      |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 31.6     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -0.136   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 855      |\n",
      "|    ep_rew_mean     | 4.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5590     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26899    |\n",
      "|    total_timesteps | 4473500  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 47.7     |\n",
      "|    ent_coef        | 0.0857   |\n",
      "|    ent_coef_loss   | -0.576   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4473399  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4480000, episode_reward=4035.19 +/- 2100.65\n",
      "Episode length: 709.00 +/- 363.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 709      |\n",
      "|    mean_reward     | 4.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -410     |\n",
      "|    critic_loss     | 56.3     |\n",
      "|    ent_coef        | 0.0842   |\n",
      "|    ent_coef_loss   | -0.149   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 856      |\n",
      "|    ep_rew_mean     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5600     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 26951    |\n",
      "|    total_timesteps | 4482213  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    ent_coef        | 0.0851   |\n",
      "|    ent_coef_loss   | -0.579   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4482112  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4490000, episode_reward=4517.43 +/- 2222.15\n",
      "Episode length: 808.40 +/- 383.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 808      |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -410     |\n",
      "|    critic_loss     | 19.9     |\n",
      "|    ent_coef        | 0.0862   |\n",
      "|    ent_coef_loss   | -0.0943  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 858      |\n",
      "|    ep_rew_mean     | 4.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5610     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27004    |\n",
      "|    total_timesteps | 4490921  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | -0.266   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4490820  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5620     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27058    |\n",
      "|    total_timesteps | 4499938  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 48.8     |\n",
      "|    ent_coef        | 0.0848   |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4499837  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500000, episode_reward=4937.23 +/- 1599.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 45.5     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | -0.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5630     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27109    |\n",
      "|    total_timesteps | 4508453  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -414     |\n",
      "|    critic_loss     | 25.8     |\n",
      "|    ent_coef        | 0.0837   |\n",
      "|    ent_coef_loss   | 0.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4508352  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4510000, episode_reward=4465.94 +/- 2704.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 80.5     |\n",
      "|    ent_coef        | 0.086    |\n",
      "|    ent_coef_loss   | 1.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 874      |\n",
      "|    ep_rew_mean     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5640     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27164    |\n",
      "|    total_timesteps | 4517334  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -419     |\n",
      "|    critic_loss     | 95.6     |\n",
      "|    ent_coef        | 0.0845   |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4517233  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4520000, episode_reward=3362.36 +/- 2998.65\n",
      "Episode length: 980.40 +/- 39.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 980      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -419     |\n",
      "|    critic_loss     | 45.7     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -0.616   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5650     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27224    |\n",
      "|    total_timesteps | 4527298  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 37       |\n",
      "|    ent_coef        | 0.084    |\n",
      "|    ent_coef_loss   | 0.524    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4527197  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4530000, episode_reward=5680.73 +/- 111.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.517    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 866      |\n",
      "|    ep_rew_mean     | 4.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 5660     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27278    |\n",
      "|    total_timesteps | 4536271  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 25.6     |\n",
      "|    ent_coef        | 0.0862   |\n",
      "|    ent_coef_loss   | -0.892   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4536170  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4540000, episode_reward=4767.49 +/- 1466.97\n",
      "Episode length: 940.40 +/- 119.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 940      |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 62       |\n",
      "|    ent_coef        | 0.0864   |\n",
      "|    ent_coef_loss   | 0.495    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 865      |\n",
      "|    ep_rew_mean     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5670     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27331    |\n",
      "|    total_timesteps | 4545066  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 42.2     |\n",
      "|    ent_coef        | 0.0879   |\n",
      "|    ent_coef_loss   | -0.319   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4544965  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4550000, episode_reward=4496.74 +/- 2241.29\n",
      "Episode length: 803.60 +/- 392.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 4.5e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    ent_coef        | 0.0855   |\n",
      "|    ent_coef_loss   | -0.58    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 846      |\n",
      "|    ep_rew_mean     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5680     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27368    |\n",
      "|    total_timesteps | 4551293  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -422     |\n",
      "|    critic_loss     | 34.9     |\n",
      "|    ent_coef        | 0.0874   |\n",
      "|    ent_coef_loss   | -0.553   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4551192  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 4.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5690     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27409    |\n",
      "|    total_timesteps | 4558282  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 36.7     |\n",
      "|    ent_coef        | 0.0882   |\n",
      "|    ent_coef_loss   | 0.354    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4558181  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4560000, episode_reward=4300.82 +/- 1904.92\n",
      "Episode length: 843.60 +/- 312.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 844      |\n",
      "|    mean_reward     | 4.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 398      |\n",
      "|    ent_coef        | 0.0888   |\n",
      "|    ent_coef_loss   | -0.268   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 839      |\n",
      "|    ep_rew_mean     | 4.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5700     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27456    |\n",
      "|    total_timesteps | 4566084  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 40.1     |\n",
      "|    ent_coef        | 0.088    |\n",
      "|    ent_coef_loss   | -0.387   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4565983  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4570000, episode_reward=4535.27 +/- 2582.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 46.2     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | 0.271    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 829      |\n",
      "|    ep_rew_mean     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5710     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27503    |\n",
      "|    total_timesteps | 4573859  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.0867   |\n",
      "|    ent_coef_loss   | 0.198    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4573758  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4580000, episode_reward=4694.87 +/- 1337.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 47       |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | 0.734    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 3.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5720     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27545    |\n",
      "|    total_timesteps | 4580685  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 141      |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | 0.196    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4580584  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5730     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27593    |\n",
      "|    total_timesteps | 4588917  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -422     |\n",
      "|    critic_loss     | 48.7     |\n",
      "|    ent_coef        | 0.085    |\n",
      "|    ent_coef_loss   | 0.828    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4588816  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4590000, episode_reward=3939.49 +/- 2366.26\n",
      "Episode length: 689.40 +/- 386.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 689      |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 36.3     |\n",
      "|    ent_coef        | 0.0853   |\n",
      "|    ent_coef_loss   | -0.373   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 812      |\n",
      "|    ep_rew_mean     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5740     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27650    |\n",
      "|    total_timesteps | 4598531  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 31.3     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -0.708   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4598430  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4600000, episode_reward=5809.90 +/- 113.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    ent_coef        | 0.0854   |\n",
      "|    ent_coef_loss   | -0.969   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 775      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5750     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27688    |\n",
      "|    total_timesteps | 4604797  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 24.7     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.307    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4604696  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4610000, episode_reward=4152.57 +/- 2023.81\n",
      "Episode length: 730.00 +/- 346.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 730      |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 53.4     |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 776      |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5760     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27742    |\n",
      "|    total_timesteps | 4613829  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 39       |\n",
      "|    ent_coef        | 0.085    |\n",
      "|    ent_coef_loss   | -0.399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4613728  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620000, episode_reward=4745.04 +/- 1515.90\n",
      "Episode length: 945.80 +/- 108.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 946      |\n",
      "|    mean_reward     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 35.2     |\n",
      "|    ent_coef        | 0.0868   |\n",
      "|    ent_coef_loss   | -0.221   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 756      |\n",
      "|    ep_rew_mean     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5770     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27784    |\n",
      "|    total_timesteps | 4620671  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 26.9     |\n",
      "|    ent_coef        | 0.0867   |\n",
      "|    ent_coef_loss   | -0.835   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4620570  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5780     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27837    |\n",
      "|    total_timesteps | 4629740  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 87.8     |\n",
      "|    ent_coef        | 0.0859   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4629639  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4630000, episode_reward=5306.98 +/- 885.84\n",
      "Episode length: 928.20 +/- 143.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 928      |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 68.7     |\n",
      "|    ent_coef        | 0.0861   |\n",
      "|    ent_coef_loss   | -0.67    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 780      |\n",
      "|    ep_rew_mean     | 3.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5790     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27877    |\n",
      "|    total_timesteps | 4636313  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -416     |\n",
      "|    critic_loss     | 227      |\n",
      "|    ent_coef        | 0.0855   |\n",
      "|    ent_coef_loss   | -0.0474  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4636212  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4640000, episode_reward=3883.60 +/- 2384.34\n",
      "Episode length: 677.00 +/- 410.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 677      |\n",
      "|    mean_reward     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 36.4     |\n",
      "|    ent_coef        | 0.0852   |\n",
      "|    ent_coef_loss   | 0.328    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 784      |\n",
      "|    ep_rew_mean     | 3.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5800     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27925    |\n",
      "|    total_timesteps | 4644497  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 37.6     |\n",
      "|    ent_coef        | 0.0877   |\n",
      "|    ent_coef_loss   | -0.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4644396  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4650000, episode_reward=5729.87 +/- 159.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 47.2     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | -0.686   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 799      |\n",
      "|    ep_rew_mean     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5810     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 27981    |\n",
      "|    total_timesteps | 4653740  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 31.8     |\n",
      "|    ent_coef        | 0.089    |\n",
      "|    ent_coef_loss   | 0.853    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4653639  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4660000, episode_reward=4558.00 +/- 1846.57\n",
      "Episode length: 805.20 +/- 314.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 805      |\n",
      "|    mean_reward     | 4.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 56.9     |\n",
      "|    ent_coef        | 0.0879   |\n",
      "|    ent_coef_loss   | 2.07     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 795      |\n",
      "|    ep_rew_mean     | 3.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5820     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28020    |\n",
      "|    total_timesteps | 4660208  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 37.7     |\n",
      "|    ent_coef        | 0.0882   |\n",
      "|    ent_coef_loss   | -0.194   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4660107  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4670000, episode_reward=4759.31 +/- 1867.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 38.7     |\n",
      "|    ent_coef        | 0.0881   |\n",
      "|    ent_coef_loss   | 0.0889   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5830     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28079    |\n",
      "|    total_timesteps | 4670000  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 805      |\n",
      "|    ep_rew_mean     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5840     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28132    |\n",
      "|    total_timesteps | 4679025  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 23.5     |\n",
      "|    ent_coef        | 0.0868   |\n",
      "|    ent_coef_loss   | 0.0677   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4678924  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680000, episode_reward=3895.41 +/- 2144.59\n",
      "Episode length: 697.80 +/- 373.11\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 698      |\n",
      "|    mean_reward     | 3.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 65.9     |\n",
      "|    ent_coef        | 0.0872   |\n",
      "|    ent_coef_loss   | -0.186   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 831      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5850     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28185    |\n",
      "|    total_timesteps | 4687883  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 43.2     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | -0.152   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4687782  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4690000, episode_reward=3963.29 +/- 2182.47\n",
      "Episode length: 718.00 +/- 385.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 718      |\n",
      "|    mean_reward     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 295      |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | 0.0273   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 827      |\n",
      "|    ep_rew_mean     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5860     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28236    |\n",
      "|    total_timesteps | 4696486  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 65.3     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.161    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4696385  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4700000, episode_reward=2980.55 +/- 3483.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 2.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -437     |\n",
      "|    critic_loss     | 29       |\n",
      "|    ent_coef        | 0.0858   |\n",
      "|    ent_coef_loss   | -0.199   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 854      |\n",
      "|    ep_rew_mean     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5870     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28294    |\n",
      "|    total_timesteps | 4706036  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 35.8     |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -0.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4705935  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710000, episode_reward=2620.45 +/- 2617.60\n",
      "Episode length: 792.40 +/- 330.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 792      |\n",
      "|    mean_reward     | 2.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 30.1     |\n",
      "|    ent_coef        | 0.085    |\n",
      "|    ent_coef_loss   | -0.872   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 843      |\n",
      "|    ep_rew_mean     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5880     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28342    |\n",
      "|    total_timesteps | 4714027  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 30       |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | 0.161    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4713926  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4720000, episode_reward=3129.21 +/- 3272.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 29.5     |\n",
      "|    ent_coef        | 0.0864   |\n",
      "|    ent_coef_loss   | -0.671   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5890     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28397    |\n",
      "|    total_timesteps | 4723207  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -438     |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -0.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4723106  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4730000, episode_reward=2586.02 +/- 2571.18\n",
      "Episode length: 683.80 +/- 404.28\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 684      |\n",
      "|    mean_reward     | 2.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 43.4     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | -0.399   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 869      |\n",
      "|    ep_rew_mean     | 3.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5900     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28446    |\n",
      "|    total_timesteps | 4731446  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 35.3     |\n",
      "|    ent_coef        | 0.0881   |\n",
      "|    ent_coef_loss   | 0.641    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4731345  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 841      |\n",
      "|    ep_rew_mean     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5910     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28484    |\n",
      "|    total_timesteps | 4737859  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -432     |\n",
      "|    critic_loss     | 26.1     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | 0.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4737758  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4740000, episode_reward=687.72 +/- 2754.81\n",
      "Episode length: 806.40 +/- 387.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 806      |\n",
      "|    mean_reward     | 688      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 43.4     |\n",
      "|    ent_coef        | 0.0878   |\n",
      "|    ent_coef_loss   | -0.358   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 849      |\n",
      "|    ep_rew_mean     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5920     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28527    |\n",
      "|    total_timesteps | 4745114  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.0865   |\n",
      "|    ent_coef_loss   | -0.836   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4745013  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4750000, episode_reward=4706.40 +/- 2080.69\n",
      "Episode length: 824.40 +/- 351.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 42       |\n",
      "|    ent_coef        | 0.0888   |\n",
      "|    ent_coef_loss   | -0.675   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 3.38e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5930     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28564    |\n",
      "|    total_timesteps | 4751099  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 63       |\n",
      "|    ent_coef        | 0.0881   |\n",
      "|    ent_coef_loss   | 0.115    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4750998  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 804      |\n",
      "|    ep_rew_mean     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5940     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28613    |\n",
      "|    total_timesteps | 4759463  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 35.9     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | -0.827   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4759362  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4760000, episode_reward=5304.14 +/- 1021.59\n",
      "Episode length: 916.40 +/- 167.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 916      |\n",
      "|    mean_reward     | 5.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 67.8     |\n",
      "|    ent_coef        | 0.0887   |\n",
      "|    ent_coef_loss   | -0.324   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 801      |\n",
      "|    ep_rew_mean     | 3.49e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5950     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28664    |\n",
      "|    total_timesteps | 4768013  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 87.3     |\n",
      "|    ent_coef        | 0.0886   |\n",
      "|    ent_coef_loss   | 0.595    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4767912  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4770000, episode_reward=5626.98 +/- 118.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 23.9     |\n",
      "|    ent_coef        | 0.0884   |\n",
      "|    ent_coef_loss   | -1.52    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 798      |\n",
      "|    ep_rew_mean     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5960     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28714    |\n",
      "|    total_timesteps | 4776274  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 61.7     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | 1.47     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4776173  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4780000, episode_reward=5739.96 +/- 118.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -438     |\n",
      "|    critic_loss     | 49.1     |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 792      |\n",
      "|    ep_rew_mean     | 3.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5970     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28768    |\n",
      "|    total_timesteps | 4785268  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -437     |\n",
      "|    critic_loss     | 33.2     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | -0.393   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4785167  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4790000, episode_reward=5610.98 +/- 145.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -432     |\n",
      "|    critic_loss     | 66.3     |\n",
      "|    ent_coef        | 0.0884   |\n",
      "|    ent_coef_loss   | 0.336    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 770      |\n",
      "|    ep_rew_mean     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5980     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28803    |\n",
      "|    total_timesteps | 4791006  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 67.6     |\n",
      "|    ent_coef        | 0.0887   |\n",
      "|    ent_coef_loss   | -0.406   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4790905  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 763      |\n",
      "|    ep_rew_mean     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5990     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28853    |\n",
      "|    total_timesteps | 4799490  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 48.4     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | 0.0611   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4799389  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800000, episode_reward=3801.66 +/- 2409.76\n",
      "Episode length: 672.40 +/- 405.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 672      |\n",
      "|    mean_reward     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 34.2     |\n",
      "|    ent_coef        | 0.0896   |\n",
      "|    ent_coef_loss   | -0.191   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 758      |\n",
      "|    ep_rew_mean     | 3.65e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6000     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28899    |\n",
      "|    total_timesteps | 4807215  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 61.5     |\n",
      "|    ent_coef        | 0.0891   |\n",
      "|    ent_coef_loss   | 1.36     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4807114  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4810000, episode_reward=4960.28 +/- 1523.31\n",
      "Episode length: 872.00 +/- 256.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 872      |\n",
      "|    mean_reward     | 4.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    ent_coef        | 0.0878   |\n",
      "|    ent_coef_loss   | 0.294    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6010     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28950    |\n",
      "|    total_timesteps | 4815532  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 30.8     |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | -0.49    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4815431  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4820000, episode_reward=5785.24 +/- 65.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 50.6     |\n",
      "|    ent_coef        | 0.0883   |\n",
      "|    ent_coef_loss   | 0.456    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 777      |\n",
      "|    ep_rew_mean     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6020     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 28995    |\n",
      "|    total_timesteps | 4822844  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 37.4     |\n",
      "|    ent_coef        | 0.0885   |\n",
      "|    ent_coef_loss   | -0.968   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4822743  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4830000, episode_reward=5659.66 +/- 194.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 49.5     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 810      |\n",
      "|    ep_rew_mean     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6030     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29052    |\n",
      "|    total_timesteps | 4832120  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 31.9     |\n",
      "|    ent_coef        | 0.09     |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4832019  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4840000, episode_reward=3827.06 +/- 2441.77\n",
      "Episode length: 816.20 +/- 367.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 816      |\n",
      "|    mean_reward     | 3.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 21.5     |\n",
      "|    ent_coef        | 0.0884   |\n",
      "|    ent_coef_loss   | 0.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 814      |\n",
      "|    ep_rew_mean     | 4.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6040     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29104    |\n",
      "|    total_timesteps | 4840894  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 37.5     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | -0.638   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4840793  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 818      |\n",
      "|    ep_rew_mean     | 4.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6050     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29156    |\n",
      "|    total_timesteps | 4849849  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 49.4     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | 0.131    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4849748  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4850000, episode_reward=4898.70 +/- 1830.50\n",
      "Episode length: 846.60 +/- 306.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 847      |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 164      |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | -0.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 809      |\n",
      "|    ep_rew_mean     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6060     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29200    |\n",
      "|    total_timesteps | 4857189  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 459      |\n",
      "|    ent_coef        | 0.0904   |\n",
      "|    ent_coef_loss   | 0.991    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4857088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4860000, episode_reward=4898.87 +/- 1710.04\n",
      "Episode length: 852.00 +/- 296.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 852      |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 74.5     |\n",
      "|    ent_coef        | 0.0892   |\n",
      "|    ent_coef_loss   | 0.627    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 785      |\n",
      "|    ep_rew_mean     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6070     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29240    |\n",
      "|    total_timesteps | 4863788  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 32.6     |\n",
      "|    ent_coef        | 0.0908   |\n",
      "|    ent_coef_loss   | 0.054    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4863687  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4870000, episode_reward=5825.41 +/- 59.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 40.9     |\n",
      "|    ent_coef        | 0.0909   |\n",
      "|    ent_coef_loss   | -1.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 811      |\n",
      "|    ep_rew_mean     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6080     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29290    |\n",
      "|    total_timesteps | 4872100  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -416     |\n",
      "|    critic_loss     | 22.2     |\n",
      "|    ent_coef        | 0.0884   |\n",
      "|    ent_coef_loss   | 0.239    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4871999  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4880000, episode_reward=5796.86 +/- 109.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 32.3     |\n",
      "|    ent_coef        | 0.0905   |\n",
      "|    ent_coef_loss   | 0.00703  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 807      |\n",
      "|    ep_rew_mean     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6090     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29340    |\n",
      "|    total_timesteps | 4880173  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 39.8     |\n",
      "|    ent_coef        | 0.0906   |\n",
      "|    ent_coef_loss   | -1.43    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4880072  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6100     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29394    |\n",
      "|    total_timesteps | 4889495  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 39.1     |\n",
      "|    ent_coef        | 0.0933   |\n",
      "|    ent_coef_loss   | -0.903   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4889394  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4890000, episode_reward=5784.38 +/- 136.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 51.3     |\n",
      "|    ent_coef        | 0.0925   |\n",
      "|    ent_coef_loss   | 0.473    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6110     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29446    |\n",
      "|    total_timesteps | 4898303  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 37       |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | -0.0241  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4898202  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4900000, episode_reward=4757.74 +/- 1179.47\n",
      "Episode length: 833.00 +/- 193.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 833      |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 41.8     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | 0.433    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 835      |\n",
      "|    ep_rew_mean     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6120     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29494    |\n",
      "|    total_timesteps | 4906365  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 29.6     |\n",
      "|    ent_coef        | 0.0897   |\n",
      "|    ent_coef_loss   | -0.285   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4906264  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4910000, episode_reward=3563.41 +/- 2034.68\n",
      "Episode length: 633.00 +/- 348.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 633      |\n",
      "|    mean_reward     | 3.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 60.9     |\n",
      "|    ent_coef        | 0.0911   |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 822      |\n",
      "|    ep_rew_mean     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6130     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29541    |\n",
      "|    total_timesteps | 4914349  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 37.3     |\n",
      "|    ent_coef        | 0.092    |\n",
      "|    ent_coef_loss   | 0.0778   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4914248  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4920000, episode_reward=5235.24 +/- 867.32\n",
      "Episode length: 928.60 +/- 142.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 929      |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -419     |\n",
      "|    critic_loss     | 52.9     |\n",
      "|    ent_coef        | 0.0916   |\n",
      "|    ent_coef_loss   | -0.434   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 823      |\n",
      "|    ep_rew_mean     | 4.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6140     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29594    |\n",
      "|    total_timesteps | 4923189  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 29.4     |\n",
      "|    ent_coef        | 0.0926   |\n",
      "|    ent_coef_loss   | -0.635   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4923088  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4930000, episode_reward=3146.29 +/- 3114.27\n",
      "Episode length: 804.20 +/- 391.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 804      |\n",
      "|    mean_reward     | 3.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -416     |\n",
      "|    critic_loss     | 22.1     |\n",
      "|    ent_coef        | 0.0905   |\n",
      "|    ent_coef_loss   | -0.329   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 828      |\n",
      "|    ep_rew_mean     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6150     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29650    |\n",
      "|    total_timesteps | 4932648  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 32.2     |\n",
      "|    ent_coef        | 0.0906   |\n",
      "|    ent_coef_loss   | -0.579   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4932547  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4940000, episode_reward=5764.24 +/- 83.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 50.9     |\n",
      "|    ent_coef        | 0.0898   |\n",
      "|    ent_coef_loss   | 0.658    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 848      |\n",
      "|    ep_rew_mean     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6160     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29706    |\n",
      "|    total_timesteps | 4941946  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 43.6     |\n",
      "|    ent_coef        | 0.0904   |\n",
      "|    ent_coef_loss   | 0.643    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4941845  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4950000, episode_reward=5690.82 +/- 108.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 28.5     |\n",
      "|    ent_coef        | 0.0914   |\n",
      "|    ent_coef_loss   | -0.716   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 870      |\n",
      "|    ep_rew_mean     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6170     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29758    |\n",
      "|    total_timesteps | 4950812  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 33.7     |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | -0.242   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4950711  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 864      |\n",
      "|    ep_rew_mean     | 4.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6180     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29802    |\n",
      "|    total_timesteps | 4958484  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -408     |\n",
      "|    critic_loss     | 38.5     |\n",
      "|    ent_coef        | 0.0894   |\n",
      "|    ent_coef_loss   | 0.301    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4958383  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4960000, episode_reward=5832.43 +/- 97.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 32.1     |\n",
      "|    ent_coef        | 0.0902   |\n",
      "|    ent_coef_loss   | 0.558    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4959899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 4.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 6190     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29856    |\n",
      "|    total_timesteps | 4967520  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 26.8     |\n",
      "|    ent_coef        | 0.0889   |\n",
      "|    ent_coef_loss   | 0.516    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4967419  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4970000, episode_reward=5726.31 +/- 50.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -422     |\n",
      "|    critic_loss     | 79.9     |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | -0.873   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 871      |\n",
      "|    ep_rew_mean     | 4.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6200     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29910    |\n",
      "|    total_timesteps | 4976625  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 49       |\n",
      "|    ent_coef        | 0.0873   |\n",
      "|    ent_coef_loss   | -0.288   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4976524  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4980000, episode_reward=5055.23 +/- 1547.19\n",
      "Episode length: 875.40 +/- 249.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 875      |\n",
      "|    mean_reward     | 5.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 29.7     |\n",
      "|    ent_coef        | 0.0895   |\n",
      "|    ent_coef_loss   | -0.0167  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 873      |\n",
      "|    ep_rew_mean     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6210     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 29964    |\n",
      "|    total_timesteps | 4985588  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 35.1     |\n",
      "|    ent_coef        | 0.0866   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4985487  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4990000, episode_reward=5873.83 +/- 81.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 30       |\n",
      "|    ent_coef        | 0.09     |\n",
      "|    ent_coef_loss   | 0.223    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4989899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 879      |\n",
      "|    ep_rew_mean     | 4.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 6220     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 30016    |\n",
      "|    total_timesteps | 4994288  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -419     |\n",
      "|    critic_loss     | 59       |\n",
      "|    ent_coef        | 0.0875   |\n",
      "|    ent_coef_loss   | 0.819    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4994187  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000000, episode_reward=5803.81 +/- 79.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 41.6     |\n",
      "|    ent_coef        | 0.0893   |\n",
      "|    ent_coef_loss   | 0.205    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4999899  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ant-v4\n",
    "env_id = 'Ant-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "sac_model = SAC(\"MlpPolicy\", env, verbose=10)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1e6, save_path=f'logs/{env_id}')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'logs/{env_id}/best_model',\n",
    "                             log_path=f'logs/{env_id}/results', eval_freq=10000)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "sac_model.learn(total_timesteps=5e6, log_interval=10, callback=callback)\n",
    "sac_model.save(f\"logs/expert/{env_id}-sac/model5e6\")\n",
    "sac_model.save_replay_buffer(f\"logs/expert/{env_id}-sac/buffer5e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=10000, episode_reward=408.27 +/- 53.35\n",
      "Episode length: 207.80 +/- 22.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 208      |\n",
      "|    mean_reward     | 408      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69.2    |\n",
      "|    critic_loss     | 8.6      |\n",
      "|    ent_coef        | 0.0685   |\n",
      "|    ent_coef_loss   | -6.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=399.41 +/- 104.15\n",
      "Episode length: 277.40 +/- 50.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 277      |\n",
      "|    mean_reward     | 399      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -94.2    |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=490.37 +/- 252.29\n",
      "Episode length: 458.40 +/- 240.78\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 458      |\n",
      "|    mean_reward     | 490      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -103     |\n",
      "|    critic_loss     | 13.7     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=40000, episode_reward=356.83 +/- 53.57\n",
      "Episode length: 194.60 +/- 25.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 195      |\n",
      "|    mean_reward     | 357      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -113     |\n",
      "|    critic_loss     | 13.4     |\n",
      "|    ent_coef        | 0.0398   |\n",
      "|    ent_coef_loss   | 0.251    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=469.70 +/- 268.04\n",
      "Episode length: 555.40 +/- 273.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 555      |\n",
      "|    mean_reward     | 470      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -114     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.0369   |\n",
      "|    ent_coef_loss   | -0.794   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=446.23 +/- 24.68\n",
      "Episode length: 241.20 +/- 29.17\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 241      |\n",
      "|    mean_reward     | 446      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -114     |\n",
      "|    critic_loss     | 8.74     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=414.28 +/- 48.80\n",
      "Episode length: 231.60 +/- 32.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 232      |\n",
      "|    mean_reward     | 414      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -115     |\n",
      "|    critic_loss     | 7.38     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.569    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=442.93 +/- 133.99\n",
      "Episode length: 200.20 +/- 38.01\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | 443      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 7.41     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 1.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=635.34 +/- 111.12\n",
      "Episode length: 300.00 +/- 54.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 300      |\n",
      "|    mean_reward     | 635      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 90000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 7.1      |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | -0.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89899    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=100000, episode_reward=426.65 +/- 26.27\n",
      "Episode length: 238.20 +/- 14.47\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 238      |\n",
      "|    mean_reward     | 427      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -116     |\n",
      "|    critic_loss     | 8.55     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 2.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99899    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=715.57 +/- 142.91\n",
      "Episode length: 283.20 +/- 42.56\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 283      |\n",
      "|    mean_reward     | 716      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 110000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -119     |\n",
      "|    critic_loss     | 8.78     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.989    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=120000, episode_reward=621.91 +/- 56.61\n",
      "Episode length: 256.00 +/- 42.30\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 256      |\n",
      "|    mean_reward     | 622      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -125     |\n",
      "|    critic_loss     | 8.52     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 119899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=515.15 +/- 79.61\n",
      "Episode length: 212.20 +/- 20.96\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 212      |\n",
      "|    mean_reward     | 515      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -126     |\n",
      "|    critic_loss     | 8.18     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | -0.228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 129899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=512.45 +/- 87.56\n",
      "Episode length: 223.20 +/- 31.66\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 223      |\n",
      "|    mean_reward     | 512      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -129     |\n",
      "|    critic_loss     | 7.84     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | -0.824   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 139899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=854.85 +/- 450.56\n",
      "Episode length: 519.20 +/- 228.04\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 519      |\n",
      "|    mean_reward     | 855      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 8.1      |\n",
      "|    ent_coef        | 0.039    |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=160000, episode_reward=690.69 +/- 67.98\n",
      "Episode length: 265.20 +/- 30.89\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 265      |\n",
      "|    mean_reward     | 691      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -134     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | -0.0493  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=482.58 +/- 338.21\n",
      "Episode length: 228.40 +/- 116.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 228      |\n",
      "|    mean_reward     | 483      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -137     |\n",
      "|    critic_loss     | 11.7     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.527    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=750.04 +/- 284.56\n",
      "Episode length: 301.00 +/- 80.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 301      |\n",
      "|    mean_reward     | 750      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 180000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -140     |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.0409   |\n",
      "|    ent_coef_loss   | -0.861   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=714.61 +/- 340.60\n",
      "Episode length: 283.00 +/- 106.99\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 283      |\n",
      "|    mean_reward     | 715      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 190000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -147     |\n",
      "|    critic_loss     | 8.91     |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | -0.314   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=875.77 +/- 67.07\n",
      "Episode length: 308.80 +/- 16.81\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 309      |\n",
      "|    mean_reward     | 876      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 200000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -149     |\n",
      "|    critic_loss     | 9.35     |\n",
      "|    ent_coef        | 0.0473   |\n",
      "|    ent_coef_loss   | -0.179   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 199899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=210000, episode_reward=1122.77 +/- 543.00\n",
      "Episode length: 418.80 +/- 146.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 419      |\n",
      "|    mean_reward     | 1.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 210000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -157     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 209899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=220000, episode_reward=2060.36 +/- 769.33\n",
      "Episode length: 660.40 +/- 230.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 660      |\n",
      "|    mean_reward     | 2.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -158     |\n",
      "|    critic_loss     | 8.94     |\n",
      "|    ent_coef        | 0.0479   |\n",
      "|    ent_coef_loss   | 1.08     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=230000, episode_reward=1214.06 +/- 198.09\n",
      "Episode length: 390.60 +/- 48.53\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 391      |\n",
      "|    mean_reward     | 1.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -161     |\n",
      "|    critic_loss     | 12.2     |\n",
      "|    ent_coef        | 0.0496   |\n",
      "|    ent_coef_loss   | 0.424    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 229899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=1736.82 +/- 400.21\n",
      "Episode length: 521.00 +/- 105.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 521      |\n",
      "|    mean_reward     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -166     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0501   |\n",
      "|    ent_coef_loss   | -0.677   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 239899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=1444.84 +/- 769.48\n",
      "Episode length: 438.40 +/- 208.19\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 438      |\n",
      "|    mean_reward     | 1.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -171     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.0493   |\n",
      "|    ent_coef_loss   | 1.23     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 249899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=3430.55 +/- 19.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 260000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -178     |\n",
      "|    critic_loss     | 9.93     |\n",
      "|    ent_coef        | 0.0517   |\n",
      "|    ent_coef_loss   | -0.428   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=270000, episode_reward=1225.30 +/- 972.31\n",
      "Episode length: 411.80 +/- 294.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 412      |\n",
      "|    mean_reward     | 1.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 270000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -182     |\n",
      "|    critic_loss     | 12.2     |\n",
      "|    ent_coef        | 0.0521   |\n",
      "|    ent_coef_loss   | 0.638    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=3390.18 +/- 41.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 280000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -189     |\n",
      "|    critic_loss     | 9.63     |\n",
      "|    ent_coef        | 0.0529   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 279899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=2562.49 +/- 1069.88\n",
      "Episode length: 761.80 +/- 299.27\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 762      |\n",
      "|    mean_reward     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 290000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -200     |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | 0.749    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 289899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=2449.80 +/- 1231.23\n",
      "Episode length: 726.20 +/- 335.37\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 726      |\n",
      "|    mean_reward     | 2.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -201     |\n",
      "|    critic_loss     | 16       |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | -0.374   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=3524.42 +/- 34.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -206     |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    ent_coef        | 0.0557   |\n",
      "|    ent_coef_loss   | -0.406   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=320000, episode_reward=3010.03 +/- 605.71\n",
      "Episode length: 868.00 +/- 166.90\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 868      |\n",
      "|    mean_reward     | 3.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -211     |\n",
      "|    critic_loss     | 9.78     |\n",
      "|    ent_coef        | 0.0561   |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=960.67 +/- 275.88\n",
      "Episode length: 314.60 +/- 77.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 315      |\n",
      "|    mean_reward     | 961      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -212     |\n",
      "|    critic_loss     | 15       |\n",
      "|    ent_coef        | 0.0549   |\n",
      "|    ent_coef_loss   | 0.332    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 329899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=3362.16 +/- 419.34\n",
      "Episode length: 910.20 +/- 114.18\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 910      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -220     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0562   |\n",
      "|    ent_coef_loss   | 0.131    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 339899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=3540.57 +/- 69.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 350000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -215     |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.0553   |\n",
      "|    ent_coef_loss   | -0.611   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=360000, episode_reward=2849.77 +/- 1188.28\n",
      "Episode length: 792.60 +/- 308.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 793      |\n",
      "|    mean_reward     | 2.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 360000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    ent_coef        | 0.0555   |\n",
      "|    ent_coef_loss   | 0.118    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=3679.63 +/- 25.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 370000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -226     |\n",
      "|    critic_loss     | 9.3      |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | 1.13     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=380000, episode_reward=3707.91 +/- 25.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 380000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -231     |\n",
      "|    critic_loss     | 11.8     |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 379899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=390000, episode_reward=3823.44 +/- 32.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -245     |\n",
      "|    critic_loss     | 9.05     |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | -0.162   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 389899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=400000, episode_reward=3765.66 +/- 98.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -229     |\n",
      "|    critic_loss     | 9.98     |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | -0.304   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=3598.67 +/- 15.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.6e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -246     |\n",
      "|    critic_loss     | 10.8     |\n",
      "|    ent_coef        | 0.0528   |\n",
      "|    ent_coef_loss   | 0.356    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 409899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=3794.83 +/- 14.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -250     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.0542   |\n",
      "|    ent_coef_loss   | -1.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 419899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=3784.91 +/- 30.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 430000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -259     |\n",
      "|    critic_loss     | 9.56     |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | -0.475   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 429899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=3921.59 +/- 35.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 440000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -249     |\n",
      "|    critic_loss     | 6.95     |\n",
      "|    ent_coef        | 0.0549   |\n",
      "|    ent_coef_loss   | 0.719    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=450000, episode_reward=3788.99 +/- 58.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 450000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -261     |\n",
      "|    critic_loss     | 8.45     |\n",
      "|    ent_coef        | 0.0552   |\n",
      "|    ent_coef_loss   | 0.742    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 449899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=3807.14 +/- 88.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 460000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 7.74     |\n",
      "|    ent_coef        | 0.0541   |\n",
      "|    ent_coef_loss   | 0.0269   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 459899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=3812.52 +/- 69.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 470000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -271     |\n",
      "|    critic_loss     | 9.56     |\n",
      "|    ent_coef        | 0.0577   |\n",
      "|    ent_coef_loss   | 0.934    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 469899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=3457.70 +/- 781.48\n",
      "Episode length: 906.00 +/- 188.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 906      |\n",
      "|    mean_reward     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -268     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.0587   |\n",
      "|    ent_coef_loss   | -0.0502  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 479899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=3854.13 +/- 53.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 490000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -264     |\n",
      "|    critic_loss     | 11.6     |\n",
      "|    ent_coef        | 0.0547   |\n",
      "|    ent_coef_loss   | -0.589   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 489899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=3857.03 +/- 72.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -270     |\n",
      "|    critic_loss     | 15.6     |\n",
      "|    ent_coef        | 0.0566   |\n",
      "|    ent_coef_loss   | 0.492    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=3681.98 +/- 151.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 510000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -272     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0567   |\n",
      "|    ent_coef_loss   | -0.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 509899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=3890.88 +/- 23.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 520000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -269     |\n",
      "|    critic_loss     | 12.2     |\n",
      "|    ent_coef        | 0.0566   |\n",
      "|    ent_coef_loss   | -0.418   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 519899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=3858.91 +/- 21.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 530000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -278     |\n",
      "|    critic_loss     | 7.45     |\n",
      "|    ent_coef        | 0.0562   |\n",
      "|    ent_coef_loss   | -0.653   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 529899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=3841.96 +/- 10.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 540000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -280     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.057    |\n",
      "|    ent_coef_loss   | -0.88    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 539899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=3779.72 +/- 44.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 550000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    ent_coef        | 0.0585   |\n",
      "|    ent_coef_loss   | 0.246    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=3880.91 +/- 58.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.0582   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 559899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=3882.41 +/- 54.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 570000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -285     |\n",
      "|    critic_loss     | 7.29     |\n",
      "|    ent_coef        | 0.0555   |\n",
      "|    ent_coef_loss   | 0.368    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 569899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=3880.59 +/- 43.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 580000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 9.35     |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | 0.482    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 579899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=3927.54 +/- 71.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 590000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -289     |\n",
      "|    critic_loss     | 9.14     |\n",
      "|    ent_coef        | 0.0552   |\n",
      "|    ent_coef_loss   | 1.65     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 589899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=600000, episode_reward=3976.94 +/- 16.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 600000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -279     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0536   |\n",
      "|    ent_coef_loss   | -0.671   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 599899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=610000, episode_reward=4012.27 +/- 65.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 610000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 7.22     |\n",
      "|    ent_coef        | 0.0549   |\n",
      "|    ent_coef_loss   | -0.446   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=620000, episode_reward=3961.03 +/- 37.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 620000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -290     |\n",
      "|    critic_loss     | 7.36     |\n",
      "|    ent_coef        | 0.0541   |\n",
      "|    ent_coef_loss   | -0.0989  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=3924.82 +/- 70.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 630000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -298     |\n",
      "|    critic_loss     | 15.9     |\n",
      "|    ent_coef        | 0.056    |\n",
      "|    ent_coef_loss   | 1.94     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 629899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=3854.13 +/- 15.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 640000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.0539   |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 639899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=3926.85 +/- 61.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 650000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 8.39     |\n",
      "|    ent_coef        | 0.0571   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 649899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=3967.35 +/- 94.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 660000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -295     |\n",
      "|    critic_loss     | 7.72     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | -0.501   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 659899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=3765.07 +/- 521.36\n",
      "Episode length: 940.60 +/- 118.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 941      |\n",
      "|    mean_reward     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 670000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -301     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.0534   |\n",
      "|    ent_coef_loss   | -0.377   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=4009.96 +/- 76.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 680000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -296     |\n",
      "|    critic_loss     | 13.1     |\n",
      "|    ent_coef        | 0.0548   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 679899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=3937.87 +/- 26.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 690000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -297     |\n",
      "|    critic_loss     | 9.31     |\n",
      "|    ent_coef        | 0.0539   |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 689899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=4057.15 +/- 21.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 700000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 12.9     |\n",
      "|    ent_coef        | 0.0538   |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=710000, episode_reward=4079.59 +/- 28.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 710000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 8.06     |\n",
      "|    ent_coef        | 0.0543   |\n",
      "|    ent_coef_loss   | 0.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 709899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=720000, episode_reward=4097.65 +/- 23.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 720000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 6.57     |\n",
      "|    ent_coef        | 0.0537   |\n",
      "|    ent_coef_loss   | -0.959   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 719899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=730000, episode_reward=3952.66 +/- 24.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 3.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 730000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -303     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.0526   |\n",
      "|    ent_coef_loss   | 0.299    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 729899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=3542.31 +/- 1147.49\n",
      "Episode length: 869.40 +/- 261.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 869      |\n",
      "|    mean_reward     | 3.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 740000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -308     |\n",
      "|    critic_loss     | 6.12     |\n",
      "|    ent_coef        | 0.0528   |\n",
      "|    ent_coef_loss   | -0.678   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 739899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=4122.15 +/- 56.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 750000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -310     |\n",
      "|    critic_loss     | 7.49     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | 0.103    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 749899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=760000, episode_reward=4094.15 +/- 51.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 760000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 9.28     |\n",
      "|    ent_coef        | 0.0517   |\n",
      "|    ent_coef_loss   | 0.231    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 759899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=4113.74 +/- 41.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 770000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 9.11     |\n",
      "|    ent_coef        | 0.0519   |\n",
      "|    ent_coef_loss   | 0.0729   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 769899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=4136.93 +/- 26.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 780000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 7.77     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | 0.495    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 779899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=790000, episode_reward=4107.96 +/- 20.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 790000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -311     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.0485   |\n",
      "|    ent_coef_loss   | 1.94     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 789899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=4058.88 +/- 79.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 800000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -313     |\n",
      "|    critic_loss     | 5.16     |\n",
      "|    ent_coef        | 0.0498   |\n",
      "|    ent_coef_loss   | -0.446   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=4176.67 +/- 69.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 810000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 16.8     |\n",
      "|    ent_coef        | 0.0489   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 809899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=820000, episode_reward=4024.43 +/- 16.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 820000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -321     |\n",
      "|    critic_loss     | 7.08     |\n",
      "|    ent_coef        | 0.0492   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 819899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=4137.73 +/- 103.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 830000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -326     |\n",
      "|    critic_loss     | 7.6      |\n",
      "|    ent_coef        | 0.0493   |\n",
      "|    ent_coef_loss   | -0.929   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 829899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=4125.03 +/- 22.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 840000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -322     |\n",
      "|    critic_loss     | 6.6      |\n",
      "|    ent_coef        | 0.0495   |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 839899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=4103.09 +/- 14.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 850000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 5        |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | 0.205    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 849899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=4125.55 +/- 13.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 860000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -325     |\n",
      "|    critic_loss     | 7.89     |\n",
      "|    ent_coef        | 0.0476   |\n",
      "|    ent_coef_loss   | -0.463   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 859899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=4179.23 +/- 24.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 870000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -332     |\n",
      "|    critic_loss     | 14.1     |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | -0.533   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 869899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=880000, episode_reward=4189.38 +/- 17.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 880000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 7.48     |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=890000, episode_reward=4213.61 +/- 19.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 890000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 7.91     |\n",
      "|    ent_coef        | 0.048    |\n",
      "|    ent_coef_loss   | -1.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 889899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=900000, episode_reward=4089.97 +/- 15.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 900000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 5.26     |\n",
      "|    ent_coef        | 0.047    |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=4070.50 +/- 25.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 910000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 4.76     |\n",
      "|    ent_coef        | 0.0487   |\n",
      "|    ent_coef_loss   | -0.287   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 909899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=4216.11 +/- 41.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 920000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 5.53     |\n",
      "|    ent_coef        | 0.0489   |\n",
      "|    ent_coef_loss   | -0.205   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 919899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=930000, episode_reward=4242.63 +/- 8.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 930000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -330     |\n",
      "|    critic_loss     | 8.17     |\n",
      "|    ent_coef        | 0.0485   |\n",
      "|    ent_coef_loss   | -2.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 929899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=940000, episode_reward=4099.18 +/- 26.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 940000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 0.256    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 939899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=4132.09 +/- 44.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 950000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -331     |\n",
      "|    critic_loss     | 8.37     |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 949899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=4201.02 +/- 12.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 960000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 5.72     |\n",
      "|    ent_coef        | 0.0478   |\n",
      "|    ent_coef_loss   | 0.288    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=4195.53 +/- 26.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 970000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    ent_coef        | 0.0471   |\n",
      "|    ent_coef_loss   | -0.736   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 969899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=4228.62 +/- 28.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 980000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -339     |\n",
      "|    critic_loss     | 5.01     |\n",
      "|    ent_coef        | 0.0486   |\n",
      "|    ent_coef_loss   | -0.742   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 979899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=4224.25 +/- 12.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 990000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -338     |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    ent_coef        | 0.0475   |\n",
      "|    ent_coef_loss   | 0.767    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 989899   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=4275.06 +/- 13.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -335     |\n",
      "|    critic_loss     | 11.5     |\n",
      "|    ent_coef        | 0.0469   |\n",
      "|    ent_coef_loss   | 1.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999899   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1010000, episode_reward=4208.59 +/- 31.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -351     |\n",
      "|    critic_loss     | 6.66     |\n",
      "|    ent_coef        | 0.0477   |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1009899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=4151.75 +/- 41.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -342     |\n",
      "|    critic_loss     | 6.71     |\n",
      "|    ent_coef        | 0.0456   |\n",
      "|    ent_coef_loss   | -0.145   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=4256.00 +/- 52.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -354     |\n",
      "|    critic_loss     | 5.35     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 0.685    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=4230.50 +/- 45.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -353     |\n",
      "|    critic_loss     | 7.99     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | -2.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1039899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=4171.50 +/- 39.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 4.61     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 0.396    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=4284.03 +/- 17.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -352     |\n",
      "|    critic_loss     | 7.92     |\n",
      "|    ent_coef        | 0.0452   |\n",
      "|    ent_coef_loss   | 0.489    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1059899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1070000, episode_reward=4250.48 +/- 41.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 5.4      |\n",
      "|    ent_coef        | 0.0447   |\n",
      "|    ent_coef_loss   | -0.238   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=4241.09 +/- 24.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 5.42     |\n",
      "|    ent_coef        | 0.0429   |\n",
      "|    ent_coef_loss   | 1.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1079899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=4152.80 +/- 26.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 4.35     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1089899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=4275.62 +/- 56.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -362     |\n",
      "|    critic_loss     | 4.66     |\n",
      "|    ent_coef        | 0.0413   |\n",
      "|    ent_coef_loss   | 1.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=4219.86 +/- 25.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -367     |\n",
      "|    critic_loss     | 4.13     |\n",
      "|    ent_coef        | 0.0421   |\n",
      "|    ent_coef_loss   | -0.934   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1109899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=4276.29 +/- 8.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -363     |\n",
      "|    critic_loss     | 7.05     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | -0.604   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=4205.60 +/- 26.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -372     |\n",
      "|    critic_loss     | 5.53     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | 0.317    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=4093.62 +/- 11.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -369     |\n",
      "|    critic_loss     | 4.27     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | 0.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1139899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=4239.08 +/- 48.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -376     |\n",
      "|    critic_loss     | 3.9      |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1149899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=4208.41 +/- 38.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -382     |\n",
      "|    critic_loss     | 7.07     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | 0.0584   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1159899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=4316.35 +/- 21.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -383     |\n",
      "|    critic_loss     | 3.3      |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | -0.975   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1169899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1180000, episode_reward=3459.55 +/- 1534.72\n",
      "Episode length: 830.80 +/- 338.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 831      |\n",
      "|    mean_reward     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -384     |\n",
      "|    critic_loss     | 3.54     |\n",
      "|    ent_coef        | 0.0382   |\n",
      "|    ent_coef_loss   | 0.134    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1179899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=4224.59 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -382     |\n",
      "|    critic_loss     | 3.69     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1189899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=4157.94 +/- 21.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -387     |\n",
      "|    critic_loss     | 2.36     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.812    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1199899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=2632.82 +/- 1920.99\n",
      "Episode length: 646.60 +/- 433.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 647      |\n",
      "|    mean_reward     | 2.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -388     |\n",
      "|    critic_loss     | 2.74     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | -0.438   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1209899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=4261.47 +/- 31.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -388     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.255    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1219899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=4271.86 +/- 12.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -392     |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.487   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1229899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=4354.03 +/- 18.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -398     |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 2.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1250000, episode_reward=4342.11 +/- 18.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -390     |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 0.556    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1249899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=4235.24 +/- 105.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -396     |\n",
      "|    critic_loss     | 2.71     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -0.42    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=4310.55 +/- 33.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -399     |\n",
      "|    critic_loss     | 3.52     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.514   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1269899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=4386.33 +/- 17.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -394     |\n",
      "|    critic_loss     | 3.17     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.561   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1279899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1290000, episode_reward=4353.83 +/- 47.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -398     |\n",
      "|    critic_loss     | 3.6      |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | 0.993    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1289899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=4412.15 +/- 27.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -402     |\n",
      "|    critic_loss     | 2.91     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.155    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1299899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1310000, episode_reward=3921.28 +/- 696.27\n",
      "Episode length: 926.20 +/- 147.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 926      |\n",
      "|    mean_reward     | 3.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -398     |\n",
      "|    critic_loss     | 2.42     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1309899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=4441.93 +/- 33.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -403     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.208   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1319899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1330000, episode_reward=4388.87 +/- 18.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | -0.45    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1329899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=4344.66 +/- 31.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -403     |\n",
      "|    critic_loss     | 2.11     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -1.63    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1339899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=4124.85 +/- 660.95\n",
      "Episode length: 935.80 +/- 128.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 936      |\n",
      "|    mean_reward     | 4.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -1.35    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1349899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=3391.38 +/- 1612.75\n",
      "Episode length: 820.60 +/- 358.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 821      |\n",
      "|    mean_reward     | 3.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -406     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.322   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1359899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=4348.10 +/- 50.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 2.49     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 2.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1369899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=0.02 +/- 0.05\n",
      "Episode length: 9.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | 0.0238   |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 3.31     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 1.05     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1379899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=4468.98 +/- 31.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 0.942    |\n",
      "|    ent_coef        | 0.0297   |\n",
      "|    ent_coef_loss   | -0.739   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1389899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1400000, episode_reward=4477.53 +/- 24.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -412     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.028    |\n",
      "|    ent_coef_loss   | -0.252   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1399899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1410000, episode_reward=4550.61 +/- 82.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -413     |\n",
      "|    critic_loss     | 1.82     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1409899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1420000, episode_reward=4607.08 +/- 30.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -410     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -0.0365  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1419899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1430000, episode_reward=4551.19 +/- 27.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.55e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -407     |\n",
      "|    critic_loss     | 8.99     |\n",
      "|    ent_coef        | 0.0286   |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=4494.48 +/- 46.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -413     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -0.952   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1439899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=3574.99 +/- 1718.78\n",
      "Episode length: 816.80 +/- 366.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 817      |\n",
      "|    mean_reward     | 3.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -410     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 1.49     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1449899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1460000, episode_reward=4335.29 +/- 121.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -413     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -1.53    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1459899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1470000, episode_reward=4411.98 +/- 54.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -409     |\n",
      "|    critic_loss     | 65.7     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.0793  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1469899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1480000, episode_reward=3577.09 +/- 1699.87\n",
      "Episode length: 818.20 +/- 363.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 818      |\n",
      "|    mean_reward     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 2.16     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1479899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1490000, episode_reward=4581.62 +/- 10.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | -0.828   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1489899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=3891.70 +/- 1234.57\n",
      "Episode length: 875.20 +/- 249.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 875      |\n",
      "|    mean_reward     | 3.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 3.31     |\n",
      "|    ent_coef        | 0.029    |\n",
      "|    ent_coef_loss   | -0.384   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1510000, episode_reward=4387.85 +/- 25.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    ent_coef        | 0.0305   |\n",
      "|    ent_coef_loss   | -2.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1509899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1520000, episode_reward=4522.30 +/- 50.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 2.58     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 1.18     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1519899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1530000, episode_reward=4448.15 +/- 37.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 2.52     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.355   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1529899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1540000, episode_reward=4375.87 +/- 96.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -417     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 2.34     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1550000, episode_reward=3039.53 +/- 1874.17\n",
      "Episode length: 693.40 +/- 386.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 693      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 2.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1560000, episode_reward=4542.89 +/- 23.35\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.54e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 1.14     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 0.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1570000, episode_reward=4407.56 +/- 101.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 2.74     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 0.677    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1569899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1580000, episode_reward=4452.51 +/- 113.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.735   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1590000, episode_reward=4473.64 +/- 33.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 1.6      |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1600000, episode_reward=4586.31 +/- 44.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -418     |\n",
      "|    critic_loss     | 4.65     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | -1.51    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1599899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1610000, episode_reward=3368.35 +/- 1347.72\n",
      "Episode length: 776.00 +/- 277.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 776      |\n",
      "|    mean_reward     | 3.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 4.26     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1609899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1620000, episode_reward=4640.51 +/- 53.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -421     |\n",
      "|    critic_loss     | 3.48     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 0.615    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1619899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1630000, episode_reward=4578.99 +/- 31.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.58e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -420     |\n",
      "|    critic_loss     | 2.27     |\n",
      "|    ent_coef        | 0.0302   |\n",
      "|    ent_coef_loss   | -1.72    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1629899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1640000, episode_reward=4526.13 +/- 33.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -0.579   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1639899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1650000, episode_reward=3679.55 +/- 1746.41\n",
      "Episode length: 818.00 +/- 364.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 818      |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 0.876    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1649899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1660000, episode_reward=4645.01 +/- 21.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -427     |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -0.137   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1659899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1670000, episode_reward=4634.99 +/- 56.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -425     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 1.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1669899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1680000, episode_reward=4422.90 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | 0.373    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1679899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1690000, episode_reward=4590.05 +/- 18.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 0.97     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | -2.9     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1689899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1700000, episode_reward=4053.81 +/- 1028.09\n",
      "Episode length: 897.40 +/- 205.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 897      |\n",
      "|    mean_reward     | 4.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1699899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1710000, episode_reward=4205.10 +/- 588.09\n",
      "Episode length: 942.60 +/- 114.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 943      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 1.95     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | 2.74     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1720000, episode_reward=4519.28 +/- 18.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -428     |\n",
      "|    critic_loss     | 21.8     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.361   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1730000, episode_reward=4627.86 +/- 28.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -429     |\n",
      "|    critic_loss     | 6.17     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1740000, episode_reward=4486.53 +/- 26.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 2.01     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -1.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=4634.98 +/- 11.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -432     |\n",
      "|    critic_loss     | 2.72     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | 1.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1749899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1760000, episode_reward=4650.89 +/- 9.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -426     |\n",
      "|    critic_loss     | 1.69     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 0.872    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1759899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1770000, episode_reward=3736.09 +/- 1783.75\n",
      "Episode length: 823.60 +/- 352.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 3.11     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.275   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1769899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1780000, episode_reward=4633.45 +/- 30.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -437     |\n",
      "|    critic_loss     | 1.21     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.215   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1779899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1790000, episode_reward=4672.79 +/- 55.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.67e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.227    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1800000, episode_reward=4649.06 +/- 21.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -0.329   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1799899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1810000, episode_reward=3003.71 +/- 1836.82\n",
      "Episode length: 693.20 +/- 375.86\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 693      |\n",
      "|    mean_reward     | 3e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.233   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1809899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1820000, episode_reward=4651.41 +/- 65.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.65e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 2.14     |\n",
      "|    ent_coef        | 0.0298   |\n",
      "|    ent_coef_loss   | -0.441   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1819899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1830000, episode_reward=4513.40 +/- 368.00\n",
      "Episode length: 965.20 +/- 69.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 965      |\n",
      "|    mean_reward     | 4.51e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -434     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | -0.202   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1829899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1840000, episode_reward=4717.83 +/- 22.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -0.306   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1839899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1850000, episode_reward=4611.62 +/- 24.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -434     |\n",
      "|    critic_loss     | 4.24     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -0.527   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1849899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1860000, episode_reward=4629.46 +/- 64.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.309    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1859899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1870000, episode_reward=4737.74 +/- 7.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -433     |\n",
      "|    critic_loss     | 2.69     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.666   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1880000, episode_reward=4594.34 +/- 172.88\n",
      "Episode length: 970.20 +/- 36.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 970      |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -434     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.624    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1879899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1890000, episode_reward=4693.14 +/- 51.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.0412   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1889899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1900000, episode_reward=4730.35 +/- 24.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 2.36     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.0803  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1910000, episode_reward=4482.73 +/- 418.64\n",
      "Episode length: 958.80 +/- 82.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 959      |\n",
      "|    mean_reward     | 4.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -432     |\n",
      "|    critic_loss     | 3.14     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 1.86     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1909899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1920000, episode_reward=3050.81 +/- 2116.59\n",
      "Episode length: 670.60 +/- 405.36\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 671      |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -436     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.815   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1930000, episode_reward=3805.46 +/- 1768.54\n",
      "Episode length: 830.40 +/- 339.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 830      |\n",
      "|    mean_reward     | 3.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 2.73     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | 2.04     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1929899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1940000, episode_reward=4736.31 +/- 12.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.575   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1939899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1950000, episode_reward=4706.76 +/- 36.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1949899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1960000, episode_reward=4615.45 +/- 96.93\n",
      "Episode length: 991.20 +/- 17.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 991      |\n",
      "|    mean_reward     | 4.62e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 1.44     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -0.284   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1959899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1970000, episode_reward=4698.95 +/- 15.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.7e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -443     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.362   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1969899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1980000, episode_reward=4770.34 +/- 11.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 2.1      |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.925   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1979899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1990000, episode_reward=4585.55 +/- 21.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -442     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.00122 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2000000, episode_reward=4689.76 +/- 21.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -434     |\n",
      "|    critic_loss     | 3.57     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | -1.38    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1999899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2010000, episode_reward=4681.37 +/- 32.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -444     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 0.236    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2009899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2020000, episode_reward=4680.34 +/- 93.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -435     |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | 0.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2019899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2030000, episode_reward=4723.62 +/- 21.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -440     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.859    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2029899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2040000, episode_reward=4694.20 +/- 20.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -443     |\n",
      "|    critic_loss     | 1.9      |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 1.94     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2039899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2050000, episode_reward=4684.23 +/- 11.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -438     |\n",
      "|    critic_loss     | 3        |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.27     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2049899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2060000, episode_reward=4078.65 +/- 1265.45\n",
      "Episode length: 880.20 +/- 239.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 880      |\n",
      "|    mean_reward     | 4.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -445     |\n",
      "|    critic_loss     | 1.05     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.679   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2059899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2070000, episode_reward=3663.15 +/- 1682.70\n",
      "Episode length: 801.40 +/- 325.29\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 801      |\n",
      "|    mean_reward     | 3.66e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -443     |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | 0.0738   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2069899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2080000, episode_reward=4605.27 +/- 16.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -439     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.376    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2079899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2090000, episode_reward=2950.48 +/- 2150.61\n",
      "Episode length: 658.40 +/- 420.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 658      |\n",
      "|    mean_reward     | 2.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -441     |\n",
      "|    critic_loss     | 1.55     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | 0.609    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2089899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2100000, episode_reward=1948.92 +/- 2207.44\n",
      "Episode length: 452.80 +/- 446.82\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 453      |\n",
      "|    mean_reward     | 1.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.103    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2099899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2110000, episode_reward=4630.94 +/- 6.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.63e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | -2.6     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2109899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2120000, episode_reward=4455.20 +/- 42.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -446     |\n",
      "|    critic_loss     | 17       |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.644    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2119899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2130000, episode_reward=3804.87 +/- 1798.17\n",
      "Episode length: 819.80 +/- 360.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 3.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -446     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | 0.603    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2129899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2140000, episode_reward=4754.51 +/- 9.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -441     |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0358   |\n",
      "|    ent_coef_loss   | 0.579    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2150000, episode_reward=4742.96 +/- 38.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -448     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | -0.0187  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2149899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2160000, episode_reward=4732.66 +/- 33.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -445     |\n",
      "|    critic_loss     | 1.25     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | -0.758   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2159899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2170000, episode_reward=4781.78 +/- 21.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -0.129   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2169899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2180000, episode_reward=4569.57 +/- 378.43\n",
      "Episode length: 965.60 +/- 68.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 966      |\n",
      "|    mean_reward     | 4.57e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -451     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -0.271   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2179899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2190000, episode_reward=4763.87 +/- 12.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -451     |\n",
      "|    critic_loss     | 1.88     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -2.09    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2189899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2200000, episode_reward=4715.51 +/- 67.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.72e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 2.98     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 2.68     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2199899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2210000, episode_reward=4759.05 +/- 49.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -448     |\n",
      "|    critic_loss     | 3.68     |\n",
      "|    ent_coef        | 0.038    |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2209899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2220000, episode_reward=4734.06 +/- 46.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -445     |\n",
      "|    critic_loss     | 2.97     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | 1.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2219899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2230000, episode_reward=4757.93 +/- 28.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 3.27     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.233    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2229899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2240000, episode_reward=4726.23 +/- 15.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.73e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | 2.54     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2239899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2250000, episode_reward=4742.24 +/- 13.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -449     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -1.95    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2249899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2260000, episode_reward=4759.73 +/- 55.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -447     |\n",
      "|    critic_loss     | 2.88     |\n",
      "|    ent_coef        | 0.0352   |\n",
      "|    ent_coef_loss   | 0.283    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2259899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2270000, episode_reward=4774.02 +/- 26.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -455     |\n",
      "|    critic_loss     | 2.73     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2269899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2280000, episode_reward=4015.33 +/- 1547.31\n",
      "Episode length: 852.60 +/- 294.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | 4.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 2.73     |\n",
      "|    ent_coef        | 0.0348   |\n",
      "|    ent_coef_loss   | -0.355   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2279899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2290000, episode_reward=4753.33 +/- 21.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 3.04     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.96     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2289899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2300000, episode_reward=4798.04 +/- 17.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -453     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0359   |\n",
      "|    ent_coef_loss   | 2.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2310000, episode_reward=4816.94 +/- 11.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -455     |\n",
      "|    critic_loss     | 2.8      |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -1.02    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2309899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2320000, episode_reward=4811.92 +/- 52.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -454     |\n",
      "|    critic_loss     | 1.8      |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2319899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2330000, episode_reward=4782.08 +/- 23.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2329899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2340000, episode_reward=4769.85 +/- 13.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -457     |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | 1.1      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2339899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2350000, episode_reward=4835.95 +/- 20.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -459     |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.521   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2349899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2360000, episode_reward=1295.84 +/- 1769.70\n",
      "Episode length: 331.00 +/- 334.76\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 331      |\n",
      "|    mean_reward     | 1.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -457     |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | 1.33     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2370000, episode_reward=4831.61 +/- 9.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -458     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 0.458    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2369899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2380000, episode_reward=4831.45 +/- 9.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -455     |\n",
      "|    critic_loss     | 1.98     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 1.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2379899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2390000, episode_reward=4806.88 +/- 30.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -454     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | 0.35     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2389899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2400000, episode_reward=4778.63 +/- 29.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -453     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | -0.546   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2399899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2410000, episode_reward=4839.98 +/- 14.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -465     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2409899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2420000, episode_reward=2913.33 +/- 2257.12\n",
      "Episode length: 637.20 +/- 444.34\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 637      |\n",
      "|    mean_reward     | 2.91e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -453     |\n",
      "|    critic_loss     | 2.9      |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -0.0874  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2419899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2430000, episode_reward=4836.57 +/- 13.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -459     |\n",
      "|    critic_loss     | 3.28     |\n",
      "|    ent_coef        | 0.0364   |\n",
      "|    ent_coef_loss   | 0.132    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2429899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2440000, episode_reward=4873.52 +/- 22.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -460     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0354   |\n",
      "|    ent_coef_loss   | 0.509    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2439899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2450000, episode_reward=4877.11 +/- 5.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -458     |\n",
      "|    critic_loss     | 2.74     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2449899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2460000, episode_reward=4765.63 +/- 43.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.77e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -458     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.57     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2459899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2470000, episode_reward=3737.12 +/- 1797.46\n",
      "Episode length: 820.20 +/- 359.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 820      |\n",
      "|    mean_reward     | 3.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -465     |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.517    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2469899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2480000, episode_reward=4826.09 +/- 64.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -461     |\n",
      "|    critic_loss     | 1.73     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | 1.97     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2479899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2490000, episode_reward=3046.48 +/- 2188.83\n",
      "Episode length: 690.20 +/- 383.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 690      |\n",
      "|    mean_reward     | 3.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -465     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | -0.267   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2489899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500000, episode_reward=4791.58 +/- 47.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -466     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.884   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2499899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2510000, episode_reward=4805.32 +/- 31.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -466     |\n",
      "|    critic_loss     | 1.02     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.0881   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2509899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2520000, episode_reward=4851.38 +/- 6.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -464     |\n",
      "|    critic_loss     | 2.92     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 0.718    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2519899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2530000, episode_reward=4831.28 +/- 40.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -460     |\n",
      "|    critic_loss     | 1.99     |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | 0.845    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2529899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2540000, episode_reward=4799.90 +/- 63.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -466     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.0459  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2539899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2550000, episode_reward=4899.25 +/- 14.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -464     |\n",
      "|    critic_loss     | 3.34     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 0.0624   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2549899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2560000, episode_reward=4834.03 +/- 40.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -467     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 2.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2559899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2570000, episode_reward=4834.54 +/- 44.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -465     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.328   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2569899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2580000, episode_reward=3937.82 +/- 1878.22\n",
      "Episode length: 821.20 +/- 357.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 821      |\n",
      "|    mean_reward     | 3.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -464     |\n",
      "|    critic_loss     | 1.13     |\n",
      "|    ent_coef        | 0.0309   |\n",
      "|    ent_coef_loss   | 0.149    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2579899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2590000, episode_reward=4844.09 +/- 26.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.867   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2589899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2600000, episode_reward=4879.24 +/- 43.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -464     |\n",
      "|    critic_loss     | 2.02     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.648    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2599899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2610000, episode_reward=4869.25 +/- 31.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -470     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2609899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2620000, episode_reward=4924.14 +/- 10.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.00651 |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2619899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2630000, episode_reward=4838.72 +/- 18.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.84e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -472     |\n",
      "|    critic_loss     | 1.08     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -1.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2629899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2640000, episode_reward=4875.02 +/- 8.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.88e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 2.81     |\n",
      "|    ent_coef        | 0.0317   |\n",
      "|    ent_coef_loss   | -0.245   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2639899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2650000, episode_reward=4711.23 +/- 20.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.71e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -470     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.356   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2649899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2660000, episode_reward=3591.07 +/- 1576.53\n",
      "Episode length: 758.40 +/- 297.09\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 758      |\n",
      "|    mean_reward     | 3.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | -0.217   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2659899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2670000, episode_reward=4867.93 +/- 18.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.87e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 2.38     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -0.884   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2669899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2680000, episode_reward=4806.95 +/- 25.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.81e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 2.76     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2679899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2690000, episode_reward=4797.18 +/- 25.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.8e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.917    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2689899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2700000, episode_reward=4891.22 +/- 11.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -470     |\n",
      "|    critic_loss     | 1.71     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.145   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2699899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2710000, episode_reward=4834.72 +/- 65.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -471     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.843    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2709899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2720000, episode_reward=4852.03 +/- 25.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.85e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -473     |\n",
      "|    critic_loss     | 1.41     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.221    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2719899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2730000, episode_reward=4824.65 +/- 83.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -473     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2729899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2740000, episode_reward=4961.98 +/- 7.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -1.28    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2739899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2750000, episode_reward=4891.57 +/- 23.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 111      |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 2.8      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2749899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2760000, episode_reward=4894.64 +/- 11.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.89e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -468     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | 1.32     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2759899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2770000, episode_reward=4929.87 +/- 14.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -470     |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.0864  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2769899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2780000, episode_reward=4915.49 +/- 11.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 3.49     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.658    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2779899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2790000, episode_reward=2308.10 +/- 2096.88\n",
      "Episode length: 517.80 +/- 394.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 518      |\n",
      "|    mean_reward     | 2.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -469     |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    ent_coef        | 0.0306   |\n",
      "|    ent_coef_loss   | 0.608    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2789899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2800000, episode_reward=4916.34 +/- 17.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -475     |\n",
      "|    critic_loss     | 2.21     |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2799899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2810000, episode_reward=4983.02 +/- 37.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0301   |\n",
      "|    ent_coef_loss   | 0.833    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2809899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2820000, episode_reward=4983.50 +/- 13.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0299   |\n",
      "|    ent_coef_loss   | 1.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2819899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2830000, episode_reward=5005.29 +/- 54.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 1.51     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -2.82    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2829899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2840000, episode_reward=4978.53 +/- 10.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 1.56     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | -1.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2839899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2850000, episode_reward=4936.47 +/- 60.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.94e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2849899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2860000, episode_reward=4923.63 +/- 69.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -477     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | -0.0881  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2859899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2870000, episode_reward=4923.96 +/- 55.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.92e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.031    |\n",
      "|    ent_coef_loss   | -1.44    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2880000, episode_reward=4933.37 +/- 19.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -478     |\n",
      "|    critic_loss     | 1.27     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | -0.131   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2879899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2890000, episode_reward=4992.04 +/- 22.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -473     |\n",
      "|    critic_loss     | 3.13     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.803    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2889899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2900000, episode_reward=4979.60 +/- 27.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.179    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2910000, episode_reward=339.27 +/- 73.76\n",
      "Episode length: 127.80 +/- 16.94\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 128      |\n",
      "|    mean_reward     | 339      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -478     |\n",
      "|    critic_loss     | 1.54     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -1.97    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2909899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2920000, episode_reward=4895.44 +/- 48.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 8.78     |\n",
      "|    ent_coef        | 0.0315   |\n",
      "|    ent_coef_loss   | -0.808   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2919899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2930000, episode_reward=4982.59 +/- 32.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -476     |\n",
      "|    critic_loss     | 1.74     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | -0.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2929899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2940000, episode_reward=4991.87 +/- 42.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 3.89     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.717   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2939899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2950000, episode_reward=5041.59 +/- 24.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2949899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2960000, episode_reward=4740.19 +/- 189.53\n",
      "Episode length: 986.20 +/- 27.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 986      |\n",
      "|    mean_reward     | 4.74e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -480     |\n",
      "|    critic_loss     | 1.62     |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.837   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2959899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2970000, episode_reward=4818.58 +/- 23.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 2.53     |\n",
      "|    ent_coef        | 0.0316   |\n",
      "|    ent_coef_loss   | 2.48     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2969899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2980000, episode_reward=4788.32 +/- 47.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.79e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 3.6      |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.42     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2979899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2990000, episode_reward=5012.84 +/- 50.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 4.31     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -1.17    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2989899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3000000, episode_reward=4900.41 +/- 96.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.9e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -477     |\n",
      "|    critic_loss     | 1.77     |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | 0.588    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3010000, episode_reward=5041.97 +/- 31.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 3.64     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -0.432   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3009899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3020000, episode_reward=4993.32 +/- 74.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 3.57     |\n",
      "|    ent_coef        | 0.032    |\n",
      "|    ent_coef_loss   | -0.126   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3030000, episode_reward=5008.56 +/- 17.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 1.09     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 0.928    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3029899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3040000, episode_reward=3863.88 +/- 1752.79\n",
      "Episode length: 832.60 +/- 334.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 833      |\n",
      "|    mean_reward     | 3.86e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.03     |\n",
      "|    ent_coef_loss   | -0.659   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3039899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3050000, episode_reward=5014.17 +/- 41.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.707    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3049899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3060000, episode_reward=5033.62 +/- 22.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -488     |\n",
      "|    critic_loss     | 3.2      |\n",
      "|    ent_coef        | 0.0321   |\n",
      "|    ent_coef_loss   | -0.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3059899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3070000, episode_reward=4970.43 +/- 39.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.97e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.0326   |\n",
      "|    ent_coef_loss   | -0.414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3069899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3080000, episode_reward=4992.80 +/- 63.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | -0.233   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3079899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3090000, episode_reward=4947.44 +/- 32.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 5.07     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | 0.114    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3089899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3100000, episode_reward=4533.10 +/- 550.97\n",
      "Episode length: 932.80 +/- 83.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 933      |\n",
      "|    mean_reward     | 4.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -478     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 0.576    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3099899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3110000, episode_reward=5083.56 +/- 6.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | 0.33     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3109899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3120000, episode_reward=5002.32 +/- 69.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -486     |\n",
      "|    critic_loss     | 88.6     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.382   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3119899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3130000, episode_reward=4925.19 +/- 100.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.93e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | 0.331    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3129899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3140000, episode_reward=5064.31 +/- 17.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.06e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -487     |\n",
      "|    critic_loss     | 1.35     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3139899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3150000, episode_reward=2144.65 +/- 1937.83\n",
      "Episode length: 516.80 +/- 404.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 517      |\n",
      "|    mean_reward     | 2.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -489     |\n",
      "|    critic_loss     | 1.63     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -0.198   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3149899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3160000, episode_reward=5007.53 +/- 76.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.01e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -0.891   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3159899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3170000, episode_reward=5021.26 +/- 46.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3169899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3180000, episode_reward=5045.98 +/- 20.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.05e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -488     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.0355   |\n",
      "|    ent_coef_loss   | -1.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3179899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3190000, episode_reward=5022.27 +/- 42.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.02e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 2.33     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.972    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3189899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3200000, episode_reward=4948.02 +/- 79.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.95e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -480     |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    ent_coef        | 0.0334   |\n",
      "|    ent_coef_loss   | -1.41    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3199899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3210000, episode_reward=5044.43 +/- 40.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -490     |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.968    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3209899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3220000, episode_reward=1096.16 +/- 1826.81\n",
      "Episode length: 281.80 +/- 359.22\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 282      |\n",
      "|    mean_reward     | 1.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -488     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | -2.14    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3219899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3230000, episode_reward=4989.08 +/- 44.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 1.64     |\n",
      "|    ent_coef        | 0.0368   |\n",
      "|    ent_coef_loss   | -0.148   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3229899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3240000, episode_reward=4996.54 +/- 55.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 4.11     |\n",
      "|    ent_coef        | 0.0374   |\n",
      "|    ent_coef_loss   | -1.32    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3239899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3250000, episode_reward=5040.79 +/- 34.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 3.24     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -1.73    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3249899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3260000, episode_reward=4994.58 +/- 35.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.99e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -486     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | 0.661    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3259899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3270000, episode_reward=4958.61 +/- 29.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.96e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0385   |\n",
      "|    ent_coef_loss   | 0.44     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3269899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3280000, episode_reward=4999.87 +/- 66.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 106      |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 0.52     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3279899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3290000, episode_reward=5044.99 +/- 6.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -485     |\n",
      "|    critic_loss     | 4.74     |\n",
      "|    ent_coef        | 0.0362   |\n",
      "|    ent_coef_loss   | 0.0417   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3289899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3300000, episode_reward=5077.79 +/- 23.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 2.02     |\n",
      "|    ent_coef        | 0.0363   |\n",
      "|    ent_coef_loss   | -0.986   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3299899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3310000, episode_reward=5035.72 +/- 8.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -479     |\n",
      "|    critic_loss     | 4.07     |\n",
      "|    ent_coef        | 0.0367   |\n",
      "|    ent_coef_loss   | 1.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3309899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3320000, episode_reward=5114.15 +/- 15.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -489     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | -0.525   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3319899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3330000, episode_reward=2278.97 +/- 2274.49\n",
      "Episode length: 492.40 +/- 414.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 492      |\n",
      "|    mean_reward     | 2.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -487     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3329899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3340000, episode_reward=5040.13 +/- 17.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -487     |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | -0.705   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3339899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3350000, episode_reward=5073.78 +/- 8.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -482     |\n",
      "|    critic_loss     | 2.8      |\n",
      "|    ent_coef        | 0.036    |\n",
      "|    ent_coef_loss   | 0.796    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3349899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3360000, episode_reward=5115.17 +/- 18.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    ent_coef        | 0.0353   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3359899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3370000, episode_reward=5114.80 +/- 19.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -480     |\n",
      "|    critic_loss     | 14.4     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 1.01     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3369899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3380000, episode_reward=5068.05 +/- 21.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.07e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -488     |\n",
      "|    critic_loss     | 3.39     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.308    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3379899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3390000, episode_reward=5112.44 +/- 14.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -483     |\n",
      "|    critic_loss     | 3.84     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.667    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3389899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3400000, episode_reward=5110.25 +/- 6.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 2.89     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -1.76    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3399899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3410000, episode_reward=5163.69 +/- 28.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 2.82     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | -0.155   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3409899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3420000, episode_reward=5136.77 +/- 15.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0325   |\n",
      "|    ent_coef_loss   | 0.158    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3419899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3430000, episode_reward=5120.19 +/- 14.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.12e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 0.464    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3429899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3440000, episode_reward=5174.55 +/- 20.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 2.62     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3439899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3450000, episode_reward=5101.29 +/- 33.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.1e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 2.05     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | 0.0191   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3449899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3460000, episode_reward=5165.69 +/- 12.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 3.5      |\n",
      "|    ent_coef        | 0.0336   |\n",
      "|    ent_coef_loss   | -0.0767  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3459899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3470000, episode_reward=5156.79 +/- 19.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -486     |\n",
      "|    critic_loss     | 6        |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 1.2      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3469899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3480000, episode_reward=5133.58 +/- 24.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0339   |\n",
      "|    ent_coef_loss   | -1.56    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3479899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3490000, episode_reward=4324.73 +/- 1731.61\n",
      "Episode length: 849.40 +/- 301.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 849      |\n",
      "|    mean_reward     | 4.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -490     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.179    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3489899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500000, episode_reward=5105.99 +/- 23.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 1.69     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3499899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3510000, episode_reward=2271.57 +/- 2327.29\n",
      "Episode length: 478.80 +/- 425.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 479      |\n",
      "|    mean_reward     | 2.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -493     |\n",
      "|    critic_loss     | 5.21     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 0.428    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3509899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3520000, episode_reward=5210.94 +/- 17.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -492     |\n",
      "|    critic_loss     | 8.77     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -1.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3519899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3530000, episode_reward=5172.61 +/- 32.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -487     |\n",
      "|    critic_loss     | 3.85     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.345    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3529899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3540000, episode_reward=5111.13 +/- 58.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -495     |\n",
      "|    critic_loss     | 1.79     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 0.272    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3539899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3550000, episode_reward=5177.67 +/- 7.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -491     |\n",
      "|    critic_loss     | 2.23     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.727    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3549899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3560000, episode_reward=4313.72 +/- 1733.78\n",
      "Episode length: 852.40 +/- 295.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 852      |\n",
      "|    mean_reward     | 4.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | 1.84     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3559899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3570000, episode_reward=5084.67 +/- 118.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.08e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | -0.533   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3569899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3580000, episode_reward=4087.33 +/- 1892.69\n",
      "Episode length: 825.00 +/- 350.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 825      |\n",
      "|    mean_reward     | 4.09e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 1.61     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -1.98    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3579899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3590000, episode_reward=5232.20 +/- 12.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | 0.724    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3589899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3600000, episode_reward=4222.41 +/- 1670.28\n",
      "Episode length: 849.20 +/- 301.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 849      |\n",
      "|    mean_reward     | 4.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3599899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3610000, episode_reward=5211.32 +/- 31.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0341   |\n",
      "|    ent_coef_loss   | -0.651   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3609899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3620000, episode_reward=4204.36 +/- 1936.04\n",
      "Episode length: 824.40 +/- 351.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 4.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -497     |\n",
      "|    critic_loss     | 3.08     |\n",
      "|    ent_coef        | 0.0345   |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3619899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3630000, episode_reward=5207.18 +/- 7.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -495     |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | -1.3     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3629899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3640000, episode_reward=5135.32 +/- 34.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.033    |\n",
      "|    ent_coef_loss   | -0.431   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3639899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3650000, episode_reward=5194.63 +/- 23.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -497     |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.034    |\n",
      "|    ent_coef_loss   | -1.37    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3649899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3660000, episode_reward=3677.80 +/- 1872.28\n",
      "Episode length: 733.60 +/- 326.65\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 734      |\n",
      "|    mean_reward     | 3.68e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 2.4      |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -1.26    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3659899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3670000, episode_reward=5184.29 +/- 29.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -501     |\n",
      "|    critic_loss     | 1.84     |\n",
      "|    ent_coef        | 0.0347   |\n",
      "|    ent_coef_loss   | -0.073   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3669899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3680000, episode_reward=5213.02 +/- 38.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | 1.55     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3679899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3690000, episode_reward=5167.38 +/- 9.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -501     |\n",
      "|    critic_loss     | 2.06     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 1.22     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3689899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3700000, episode_reward=5209.38 +/- 22.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -498     |\n",
      "|    critic_loss     | 2.19     |\n",
      "|    ent_coef        | 0.0342   |\n",
      "|    ent_coef_loss   | 0.82     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3699899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3710000, episode_reward=1182.86 +/- 1802.63\n",
      "Episode length: 314.80 +/- 342.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 315      |\n",
      "|    mean_reward     | 1.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 1.76     |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | 0.51     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3709899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3720000, episode_reward=5134.59 +/- 21.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    ent_coef        | 0.0343   |\n",
      "|    ent_coef_loss   | -0.842   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3719899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3730000, episode_reward=5152.91 +/- 63.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -498     |\n",
      "|    critic_loss     | 2.06     |\n",
      "|    ent_coef        | 0.0324   |\n",
      "|    ent_coef_loss   | -0.628   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3729899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3740000, episode_reward=5201.09 +/- 28.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -503     |\n",
      "|    critic_loss     | 1.57     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 1.61     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3739899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3750000, episode_reward=5114.41 +/- 116.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0351   |\n",
      "|    ent_coef_loss   | -0.181   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3749899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3760000, episode_reward=5185.21 +/- 65.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 3.36     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | 0.225    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3759899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3770000, episode_reward=5231.93 +/- 19.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 3.79     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | 0.164    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3769899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3780000, episode_reward=5241.20 +/- 17.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | -0.783   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3779899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3790000, episode_reward=5263.24 +/- 31.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 2.08     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.474   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3789899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3800000, episode_reward=5243.95 +/- 23.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 1.81     |\n",
      "|    ent_coef        | 0.0346   |\n",
      "|    ent_coef_loss   | -2.2     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3799899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3810000, episode_reward=5173.49 +/- 31.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.17e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 1        |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3809899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3820000, episode_reward=4405.03 +/- 1692.79\n",
      "Episode length: 853.40 +/- 293.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 853      |\n",
      "|    mean_reward     | 4.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 1.3      |\n",
      "|    ent_coef        | 0.035    |\n",
      "|    ent_coef_loss   | -0.834   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3819899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3830000, episode_reward=5226.15 +/- 25.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0338   |\n",
      "|    ent_coef_loss   | -0.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3829899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3840000, episode_reward=5238.60 +/- 13.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -0.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3839899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3850000, episode_reward=5213.18 +/- 21.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | -0.19    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3849899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3860000, episode_reward=5238.35 +/- 25.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 5.29     |\n",
      "|    ent_coef        | 0.0332   |\n",
      "|    ent_coef_loss   | 0.0437   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3859899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3870000, episode_reward=5250.90 +/- 23.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -498     |\n",
      "|    critic_loss     | 2.25     |\n",
      "|    ent_coef        | 0.0327   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3869899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3880000, episode_reward=5245.29 +/- 16.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -498     |\n",
      "|    critic_loss     | 2.78     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | 1.06     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3879899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3890000, episode_reward=5252.91 +/- 19.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -504     |\n",
      "|    critic_loss     | 2.66     |\n",
      "|    ent_coef        | 0.0329   |\n",
      "|    ent_coef_loss   | 1.89     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3889899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3900000, episode_reward=5213.95 +/- 17.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 1.39     |\n",
      "|    ent_coef        | 0.0344   |\n",
      "|    ent_coef_loss   | 0.424    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3910000, episode_reward=5282.41 +/- 9.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -503     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.053   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3909899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3920000, episode_reward=5240.38 +/- 23.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 2.37     |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.188    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3919899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3930000, episode_reward=5246.35 +/- 12.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 0.962    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3929899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3940000, episode_reward=4287.43 +/- 1766.05\n",
      "Episode length: 845.20 +/- 309.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 845      |\n",
      "|    mean_reward     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 0.913    |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | -0.142   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3939899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3950000, episode_reward=5189.97 +/- 16.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -503     |\n",
      "|    critic_loss     | 2.3      |\n",
      "|    ent_coef        | 0.0311   |\n",
      "|    ent_coef_loss   | 0.0414   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3949899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3960000, episode_reward=5280.04 +/- 19.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.9      |\n",
      "|    ent_coef        | 0.0313   |\n",
      "|    ent_coef_loss   | 0.508    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3959899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3970000, episode_reward=5265.11 +/- 21.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 2.15     |\n",
      "|    ent_coef        | 0.0323   |\n",
      "|    ent_coef_loss   | -0.852   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3969899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3980000, episode_reward=4192.80 +/- 2043.00\n",
      "Episode length: 812.60 +/- 374.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 813      |\n",
      "|    mean_reward     | 4.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 3.22     |\n",
      "|    ent_coef        | 0.0296   |\n",
      "|    ent_coef_loss   | 0.98     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3979899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3990000, episode_reward=5237.45 +/- 18.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 0.858    |\n",
      "|    ent_coef        | 0.0333   |\n",
      "|    ent_coef_loss   | -0.751   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3989899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000000, episode_reward=5306.21 +/- 16.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 5.2      |\n",
      "|    ent_coef        | 0.0322   |\n",
      "|    ent_coef_loss   | 0.254    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3999899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4010000, episode_reward=5202.78 +/- 34.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 2.42     |\n",
      "|    ent_coef        | 0.0328   |\n",
      "|    ent_coef_loss   | -0.825   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4009899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4020000, episode_reward=5240.95 +/- 49.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 2.32     |\n",
      "|    ent_coef        | 0.0331   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4019899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4030000, episode_reward=4534.31 +/- 1419.42\n",
      "Episode length: 880.00 +/- 240.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 880      |\n",
      "|    mean_reward     | 4.53e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -518     |\n",
      "|    critic_loss     | 1.75     |\n",
      "|    ent_coef        | 0.0337   |\n",
      "|    ent_coef_loss   | -0.288   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4029899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4040000, episode_reward=4245.81 +/- 1902.79\n",
      "Episode length: 843.40 +/- 313.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 843      |\n",
      "|    mean_reward     | 4.25e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 3.87     |\n",
      "|    ent_coef        | 0.0349   |\n",
      "|    ent_coef_loss   | 0.499    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4039899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4050000, episode_reward=3035.74 +/- 2229.14\n",
      "Episode length: 712.00 +/- 352.73\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 712      |\n",
      "|    mean_reward     | 3.04e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 8.79     |\n",
      "|    ent_coef        | 0.0372   |\n",
      "|    ent_coef_loss   | -0.565   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4049899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4060000, episode_reward=-2.18 +/- 0.10\n",
      "Episode length: 7.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 7        |\n",
      "|    mean_reward     | -2.18    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 5.84     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.621    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4059899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4070000, episode_reward=5295.54 +/- 16.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -504     |\n",
      "|    critic_loss     | 3.47     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 1.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4069899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4080000, episode_reward=5270.01 +/- 42.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 3.88     |\n",
      "|    ent_coef        | 0.04     |\n",
      "|    ent_coef_loss   | 0.401    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4079899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4090000, episode_reward=5264.76 +/- 29.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 2.95     |\n",
      "|    ent_coef        | 0.0392   |\n",
      "|    ent_coef_loss   | -0.649   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4089899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4100000, episode_reward=5227.58 +/- 45.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 1.91     |\n",
      "|    ent_coef        | 0.0375   |\n",
      "|    ent_coef_loss   | 0.0218   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4099899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4110000, episode_reward=5229.02 +/- 173.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -501     |\n",
      "|    critic_loss     | 2.56     |\n",
      "|    ent_coef        | 0.0378   |\n",
      "|    ent_coef_loss   | 0.457    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4109899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4120000, episode_reward=4377.67 +/- 1670.50\n",
      "Episode length: 857.00 +/- 286.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 857      |\n",
      "|    mean_reward     | 4.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | 1.09     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4119899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4130000, episode_reward=3259.57 +/- 1882.76\n",
      "Episode length: 686.20 +/- 335.52\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 686      |\n",
      "|    mean_reward     | 3.26e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -500     |\n",
      "|    critic_loss     | 6.99     |\n",
      "|    ent_coef        | 0.0403   |\n",
      "|    ent_coef_loss   | -0.443   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4129899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4140000, episode_reward=5161.52 +/- 64.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.16e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -503     |\n",
      "|    critic_loss     | 3.4      |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.419    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4139899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4150000, episode_reward=4606.39 +/- 1266.85\n",
      "Episode length: 898.20 +/- 203.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 898      |\n",
      "|    mean_reward     | 4.61e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 4.41     |\n",
      "|    ent_coef        | 0.0424   |\n",
      "|    ent_coef_loss   | 0.035    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4149899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4160000, episode_reward=4027.28 +/- 1492.03\n",
      "Episode length: 808.40 +/- 234.85\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 808      |\n",
      "|    mean_reward     | 4.03e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 3.74     |\n",
      "|    ent_coef        | 0.0434   |\n",
      "|    ent_coef_loss   | 0.123    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4159899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4170000, episode_reward=5142.60 +/- 34.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.14e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 3.29     |\n",
      "|    ent_coef        | 0.0428   |\n",
      "|    ent_coef_loss   | -0.71    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4169899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4180000, episode_reward=5111.69 +/- 209.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.11e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -499     |\n",
      "|    critic_loss     | 2.99     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.704    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4179899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4190000, episode_reward=4998.08 +/- 32.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5e+03    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 2.22     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | -0.556   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4189899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4200000, episode_reward=5178.78 +/- 34.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.79     |\n",
      "|    ent_coef        | 0.041    |\n",
      "|    ent_coef_loss   | 1.56     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4199899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4210000, episode_reward=5224.89 +/- 19.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.22e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 3.35     |\n",
      "|    ent_coef        | 0.0387   |\n",
      "|    ent_coef_loss   | 0.994    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4209899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4220000, episode_reward=5132.24 +/- 49.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 2.83     |\n",
      "|    ent_coef        | 0.0381   |\n",
      "|    ent_coef_loss   | 0.509    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4219899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4230000, episode_reward=5236.67 +/- 8.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.8      |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -0.273   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4229899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4240000, episode_reward=5291.39 +/- 23.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.68     |\n",
      "|    ent_coef        | 0.0357   |\n",
      "|    ent_coef_loss   | -0.991   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4239899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4250000, episode_reward=4276.59 +/- 1955.46\n",
      "Episode length: 827.20 +/- 345.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 827      |\n",
      "|    mean_reward     | 4.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.57     |\n",
      "|    ent_coef        | 0.0361   |\n",
      "|    ent_coef_loss   | -0.424   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4249899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4260000, episode_reward=5282.99 +/- 59.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 2.11     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | -0.887   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4259899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4270000, episode_reward=5331.75 +/- 15.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 2.47     |\n",
      "|    ent_coef        | 0.0379   |\n",
      "|    ent_coef_loss   | 0.0488   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4269899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4280000, episode_reward=4519.05 +/- 1679.97\n",
      "Episode length: 858.20 +/- 283.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 858      |\n",
      "|    mean_reward     | 4.52e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 1.49     |\n",
      "|    ent_coef        | 0.0356   |\n",
      "|    ent_coef_loss   | 0.366    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4279899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4290000, episode_reward=4643.79 +/- 1324.49\n",
      "Episode length: 891.20 +/- 217.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 891      |\n",
      "|    mean_reward     | 4.64e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 5.56     |\n",
      "|    ent_coef        | 0.0388   |\n",
      "|    ent_coef_loss   | 1.02     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4289899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4300000, episode_reward=4360.94 +/- 1738.72\n",
      "Episode length: 850.40 +/- 299.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 850      |\n",
      "|    mean_reward     | 4.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 5.04     |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.557    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4299899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4310000, episode_reward=3193.96 +/- 2370.22\n",
      "Episode length: 649.00 +/- 429.92\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 649      |\n",
      "|    mean_reward     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -1.83    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4309899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4320000, episode_reward=4208.74 +/- 1995.35\n",
      "Episode length: 823.80 +/- 352.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 824      |\n",
      "|    mean_reward     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 2.87     |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | -0.0259  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4319899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4330000, episode_reward=5269.82 +/- 21.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 5.71     |\n",
      "|    ent_coef        | 0.0388   |\n",
      "|    ent_coef_loss   | -1.87    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4329899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4340000, episode_reward=5334.51 +/- 10.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 2.35     |\n",
      "|    ent_coef        | 0.0407   |\n",
      "|    ent_coef_loss   | 0.377    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4339899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4350000, episode_reward=3464.57 +/- 2173.59\n",
      "Episode length: 695.00 +/- 379.24\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 695      |\n",
      "|    mean_reward     | 3.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 5.2      |\n",
      "|    ent_coef        | 0.0458   |\n",
      "|    ent_coef_loss   | 0.684    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4349899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4360000, episode_reward=5209.13 +/- 184.50\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.21e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 6.86     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | -0.849   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4359899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4370000, episode_reward=5274.98 +/- 41.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.27e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 1.96     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | 0.263    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4369899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4380000, episode_reward=5284.53 +/- 15.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -504     |\n",
      "|    critic_loss     | 3.66     |\n",
      "|    ent_coef        | 0.0465   |\n",
      "|    ent_coef_loss   | 0.265    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4379899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4390000, episode_reward=5320.73 +/- 14.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.63     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | -2.04    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4389899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4400000, episode_reward=5278.83 +/- 32.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.28e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 2.21     |\n",
      "|    ent_coef        | 0.0433   |\n",
      "|    ent_coef_loss   | 0.0408   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4399899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4410000, episode_reward=5238.73 +/- 11.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.24e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -506     |\n",
      "|    critic_loss     | 4.16     |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -0.952   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4409899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4420000, episode_reward=5290.57 +/- 19.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 3.78     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4419899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4430000, episode_reward=5291.15 +/- 78.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.29e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 2.77     |\n",
      "|    ent_coef        | 0.0435   |\n",
      "|    ent_coef_loss   | -0.202   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4429899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4440000, episode_reward=5230.70 +/- 85.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.23e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -514     |\n",
      "|    critic_loss     | 6.13     |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | -1.48    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4439899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4450000, episode_reward=5306.89 +/- 2.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 4.15     |\n",
      "|    ent_coef        | 0.045    |\n",
      "|    ent_coef_loss   | -0.23    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4449899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4460000, episode_reward=5333.39 +/- 15.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 2.48     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.115    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4459899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4470000, episode_reward=5359.62 +/- 30.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -507     |\n",
      "|    critic_loss     | 7.28     |\n",
      "|    ent_coef        | 0.0397   |\n",
      "|    ent_coef_loss   | 0.889    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4469899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4480000, episode_reward=5415.04 +/- 33.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0401   |\n",
      "|    ent_coef_loss   | -0.0813  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4479899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4490000, episode_reward=5381.79 +/- 19.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 5.19     |\n",
      "|    ent_coef        | 0.0414   |\n",
      "|    ent_coef_loss   | -0.144   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4489899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4500000, episode_reward=5382.74 +/- 12.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 1.97     |\n",
      "|    ent_coef        | 0.0412   |\n",
      "|    ent_coef_loss   | -1.92    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4499899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4510000, episode_reward=1819.69 +/- 1722.03\n",
      "Episode length: 543.60 +/- 373.63\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 544      |\n",
      "|    mean_reward     | 1.82e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 6.35     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | 1.78     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4509899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4520000, episode_reward=4488.52 +/- 1770.71\n",
      "Episode length: 852.00 +/- 296.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 852      |\n",
      "|    mean_reward     | 4.49e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 2.51     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.0272   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4519899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4530000, episode_reward=4432.80 +/- 1865.73\n",
      "Episode length: 842.00 +/- 316.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 842      |\n",
      "|    mean_reward     | 4.43e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.44     |\n",
      "|    ent_coef        | 0.0436   |\n",
      "|    ent_coef_loss   | -1.33    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4529899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4540000, episode_reward=5308.15 +/- 10.46\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.31e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 3.58     |\n",
      "|    ent_coef        | 0.0455   |\n",
      "|    ent_coef_loss   | -0.0521  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4539899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4550000, episode_reward=4335.82 +/- 2084.68\n",
      "Episode length: 817.60 +/- 364.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 818      |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -510     |\n",
      "|    critic_loss     | 2.91     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | -0.533   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4549899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4560000, episode_reward=5337.04 +/- 8.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    ent_coef        | 0.042    |\n",
      "|    ent_coef_loss   | 0.983    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4559899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4570000, episode_reward=5356.02 +/- 9.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.0426   |\n",
      "|    ent_coef_loss   | -1.34    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4569899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4580000, episode_reward=5402.51 +/- 7.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 5.75     |\n",
      "|    ent_coef        | 0.045    |\n",
      "|    ent_coef_loss   | -0.774   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4579899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4590000, episode_reward=5363.13 +/- 7.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 2.72     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | 0.274    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4589899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4600000, episode_reward=2468.02 +/- 2361.82\n",
      "Episode length: 503.20 +/- 406.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 503      |\n",
      "|    mean_reward     | 2.47e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 3.1      |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.6      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4599899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4610000, episode_reward=5317.67 +/- 25.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 5.25     |\n",
      "|    ent_coef        | 0.0411   |\n",
      "|    ent_coef_loss   | 0.442    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4609899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4620000, episode_reward=5370.05 +/- 7.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -514     |\n",
      "|    critic_loss     | 3.89     |\n",
      "|    ent_coef        | 0.0418   |\n",
      "|    ent_coef_loss   | -1.77    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4619899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4630000, episode_reward=5346.21 +/- 15.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 2.68     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | 0.214    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4629899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4640000, episode_reward=5396.75 +/- 23.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 1.7      |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | -0.637   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4639899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4650000, episode_reward=5323.48 +/- 34.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -526     |\n",
      "|    critic_loss     | 4.02     |\n",
      "|    ent_coef        | 0.0386   |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4649899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4660000, episode_reward=5392.34 +/- 27.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 2.87     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 0.533    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4659899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4670000, episode_reward=5410.18 +/- 6.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.59     |\n",
      "|    ent_coef        | 0.0413   |\n",
      "|    ent_coef_loss   | 0.26     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4669899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4680000, episode_reward=5346.79 +/- 14.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.35e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -522     |\n",
      "|    critic_loss     | 3.49     |\n",
      "|    ent_coef        | 0.0383   |\n",
      "|    ent_coef_loss   | -0.842   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4679899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4690000, episode_reward=5149.55 +/- 103.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.15e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    ent_coef        | 0.0438   |\n",
      "|    ent_coef_loss   | -0.483   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4689899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4700000, episode_reward=5402.43 +/- 7.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 9.53     |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | -0.181   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4710000, episode_reward=3361.75 +/- 2354.28\n",
      "Episode length: 666.00 +/- 414.32\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 666      |\n",
      "|    mean_reward     | 3.36e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 3.55     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | -1.64    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4709899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4720000, episode_reward=4184.15 +/- 2067.65\n",
      "Episode length: 810.80 +/- 378.40\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 811      |\n",
      "|    mean_reward     | 4.18e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 2.84     |\n",
      "|    ent_coef        | 0.0459   |\n",
      "|    ent_coef_loss   | -0.394   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4719899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4730000, episode_reward=5329.87 +/- 26.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 6.79     |\n",
      "|    ent_coef        | 0.0439   |\n",
      "|    ent_coef_loss   | 0.0293   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4729899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4740000, episode_reward=5389.06 +/- 7.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.03     |\n",
      "|    ent_coef        | 0.0406   |\n",
      "|    ent_coef_loss   | 0.488    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4739899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4750000, episode_reward=5412.04 +/- 50.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -511     |\n",
      "|    critic_loss     | 4.7      |\n",
      "|    ent_coef        | 0.0393   |\n",
      "|    ent_coef_loss   | 0.19     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4749899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4760000, episode_reward=5377.22 +/- 12.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.38e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 2.12     |\n",
      "|    ent_coef        | 0.0399   |\n",
      "|    ent_coef_loss   | 1.29     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4759899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4770000, episode_reward=5414.75 +/- 15.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 3.42     |\n",
      "|    ent_coef        | 0.0417   |\n",
      "|    ent_coef_loss   | -0.494   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4769899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4780000, episode_reward=5387.53 +/- 14.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.39e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 6.29     |\n",
      "|    ent_coef        | 0.0402   |\n",
      "|    ent_coef_loss   | -0.289   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4779899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4790000, episode_reward=5414.16 +/- 39.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 9.8      |\n",
      "|    ent_coef        | 0.0405   |\n",
      "|    ent_coef_loss   | 0.401    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4789899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4800000, episode_reward=5374.07 +/- 11.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 2.64     |\n",
      "|    ent_coef        | 0.0394   |\n",
      "|    ent_coef_loss   | -0.809   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4799899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4810000, episode_reward=5297.64 +/- 60.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.3e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 3.09     |\n",
      "|    ent_coef        | 0.0404   |\n",
      "|    ent_coef_loss   | -2.15    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4809899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4820000, episode_reward=5421.82 +/- 17.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -525     |\n",
      "|    critic_loss     | 4.28     |\n",
      "|    ent_coef        | 0.044    |\n",
      "|    ent_coef_loss   | -0.202   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4819899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4830000, episode_reward=5194.64 +/- 42.13\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.19e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 1.72     |\n",
      "|    ent_coef        | 0.0445   |\n",
      "|    ent_coef_loss   | 0.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4829899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4840000, episode_reward=5401.89 +/- 16.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.4e+03  |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 5.81     |\n",
      "|    ent_coef        | 0.0432   |\n",
      "|    ent_coef_loss   | 0.909    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4839899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4850000, episode_reward=5415.30 +/- 8.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 4.77     |\n",
      "|    ent_coef        | 0.0436   |\n",
      "|    ent_coef_loss   | 0.484    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4849899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4860000, episode_reward=5407.19 +/- 22.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 2.57     |\n",
      "|    ent_coef        | 0.0448   |\n",
      "|    ent_coef_loss   | 1.5      |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4859899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4870000, episode_reward=5448.83 +/- 8.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -517     |\n",
      "|    critic_loss     | 3.26     |\n",
      "|    ent_coef        | 0.0423   |\n",
      "|    ent_coef_loss   | -0.525   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4869899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4880000, episode_reward=5368.54 +/- 48.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -515     |\n",
      "|    critic_loss     | 2.2      |\n",
      "|    ent_coef        | 0.0466   |\n",
      "|    ent_coef_loss   | 0.523    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4879899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4890000, episode_reward=5414.62 +/- 35.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 2.46     |\n",
      "|    ent_coef        | 0.0442   |\n",
      "|    ent_coef_loss   | -0.782   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4889899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4900000, episode_reward=1483.95 +/- 2061.16\n",
      "Episode length: 327.80 +/- 361.60\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 328      |\n",
      "|    mean_reward     | 1.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 3.75     |\n",
      "|    ent_coef        | 0.0408   |\n",
      "|    ent_coef_loss   | -0.191   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4899899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4910000, episode_reward=4593.31 +/- 1639.80\n",
      "Episode length: 868.60 +/- 262.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 869      |\n",
      "|    mean_reward     | 4.59e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -512     |\n",
      "|    critic_loss     | 4.02     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | 0.797    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4909899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4920000, episode_reward=5420.81 +/- 45.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.42e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 2.59     |\n",
      "|    ent_coef        | 0.0437   |\n",
      "|    ent_coef_loss   | 0.167    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4919899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4930000, episode_reward=5438.31 +/- 8.39\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.44e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -521     |\n",
      "|    critic_loss     | 8.3      |\n",
      "|    ent_coef        | 0.0453   |\n",
      "|    ent_coef_loss   | 1.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4929899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4940000, episode_reward=5461.29 +/- 10.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -521     |\n",
      "|    critic_loss     | 1.43     |\n",
      "|    ent_coef        | 0.043    |\n",
      "|    ent_coef_loss   | 0.269    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4939899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4950000, episode_reward=3127.91 +/- 2340.61\n",
      "Episode length: 661.00 +/- 415.39\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 661      |\n",
      "|    mean_reward     | 3.13e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 4.11     |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | 0.853    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4949899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4960000, episode_reward=5411.60 +/- 12.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 3.79     |\n",
      "|    ent_coef        | 0.046    |\n",
      "|    ent_coef_loss   | 0.712    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4959899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4970000, episode_reward=5373.03 +/- 12.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.37e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -528     |\n",
      "|    critic_loss     | 9.81     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | -1.29    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4969899  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4980000, episode_reward=5464.75 +/- 17.33\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.46e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -516     |\n",
      "|    critic_loss     | 7.64     |\n",
      "|    ent_coef        | 0.0483   |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4979899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4990000, episode_reward=5478.88 +/- 9.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.48e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -505     |\n",
      "|    critic_loss     | 6.78     |\n",
      "|    ent_coef        | 0.0513   |\n",
      "|    ent_coef_loss   | 0.0208   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4989899  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5000000, episode_reward=5317.08 +/- 17.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 5.32e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -519     |\n",
      "|    critic_loss     | 2.28     |\n",
      "|    ent_coef        | 0.0474   |\n",
      "|    ent_coef_loss   | -0.148   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4999899  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Walker2d-v4\n",
    "env_id = 'Walker2d-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "sac_model = SAC(\"MlpPolicy\", env, verbose=10)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1e5, save_path=f'../logs/{env_id}')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'./logs/{env_id}/best_model',\n",
    "                             log_path=f'./logs/{env_id}/results', eval_freq=10000)\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "sac_model.learn(total_timesteps=5e6, log_interval=10000, callback=callback)\n",
    "sac_model.save(f\"logs/expert/{env_id}-sac/model5e6\")\n",
    "sac_model.save_replay_buffer(f\"logs/expert/{env_id}-sac/buffer5e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -209     |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.3    |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.0574   |\n",
      "|    ent_coef_loss   | -17.5    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -186     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 124      |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -10.3    |\n",
      "|    critic_loss     | 0.644    |\n",
      "|    ent_coef        | 0.00701  |\n",
      "|    ent_coef_loss   | 1.64     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 19899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | -109     |\n",
      "| time/              |          |\n",
      "|    episodes        | 30       |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 194      |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.4     |\n",
      "|    critic_loss     | 0.788    |\n",
      "|    ent_coef        | 0.00603  |\n",
      "|    ent_coef_loss   | -0.446   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 29899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 158      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.5    |\n",
      "|    critic_loss     | 1.78     |\n",
      "|    ent_coef        | 0.0183   |\n",
      "|    ent_coef_loss   | 0.228    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 39899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 425      |\n",
      "| time/              |          |\n",
      "|    episodes        | 50       |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 349      |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -33.1    |\n",
      "|    critic_loss     | 2.5      |\n",
      "|    ent_coef        | 0.0308   |\n",
      "|    ent_coef_loss   | -0.0803  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 49899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 764      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 427      |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.8    |\n",
      "|    critic_loss     | 2.95     |\n",
      "|    ent_coef        | 0.0444   |\n",
      "|    ent_coef_loss   | -0.549   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 59899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 70       |\n",
      "|    fps             | 137      |\n",
      "|    time_elapsed    | 508      |\n",
      "|    total_timesteps | 70000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -91.1    |\n",
      "|    critic_loss     | 4.69     |\n",
      "|    ent_coef        | 0.0514   |\n",
      "|    ent_coef_loss   | 1.38     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 69899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 570      |\n",
      "|    total_timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -112     |\n",
      "|    critic_loss     | 4.61     |\n",
      "|    ent_coef        | 0.0595   |\n",
      "|    ent_coef_loss   | 0.846    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 79899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 90       |\n",
      "|    fps             | 141      |\n",
      "|    time_elapsed    | 637      |\n",
      "|    total_timesteps | 90000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -135     |\n",
      "|    critic_loss     | 4.62     |\n",
      "|    ent_coef        | 0.0698   |\n",
      "|    ent_coef_loss   | 0.411    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 89899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 703      |\n",
      "|    total_timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -157     |\n",
      "|    critic_loss     | 7.37     |\n",
      "|    ent_coef        | 0.0797   |\n",
      "|    ent_coef_loss   | -0.698   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 99899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 110      |\n",
      "|    fps             | 143      |\n",
      "|    time_elapsed    | 764      |\n",
      "|    total_timesteps | 110000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -174     |\n",
      "|    critic_loss     | 7.51     |\n",
      "|    ent_coef        | 0.087    |\n",
      "|    ent_coef_loss   | 0.843    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 109899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 145      |\n",
      "|    time_elapsed    | 823      |\n",
      "|    total_timesteps | 120000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -197     |\n",
      "|    critic_loss     | 6.58     |\n",
      "|    ent_coef        | 0.0959   |\n",
      "|    ent_coef_loss   | 0.383    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 119899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 130      |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 879      |\n",
      "|    total_timesteps | 130000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -196     |\n",
      "|    critic_loss     | 5.39     |\n",
      "|    ent_coef        | 0.105    |\n",
      "|    ent_coef_loss   | 0.00156  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 129899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 148      |\n",
      "|    time_elapsed    | 940      |\n",
      "|    total_timesteps | 140000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -215     |\n",
      "|    critic_loss     | 6.5      |\n",
      "|    ent_coef        | 0.112    |\n",
      "|    ent_coef_loss   | 0.719    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 139899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 150      |\n",
      "|    fps             | 149      |\n",
      "|    time_elapsed    | 1000     |\n",
      "|    total_timesteps | 150000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -230     |\n",
      "|    critic_loss     | 5.49     |\n",
      "|    ent_coef        | 0.116    |\n",
      "|    ent_coef_loss   | -0.519   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 149899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 1059     |\n",
      "|    total_timesteps | 160000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -237     |\n",
      "|    critic_loss     | 5.95     |\n",
      "|    ent_coef        | 0.123    |\n",
      "|    ent_coef_loss   | 1.47     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 159899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 3.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 170      |\n",
      "|    fps             | 151      |\n",
      "|    time_elapsed    | 1119     |\n",
      "|    total_timesteps | 170000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -228     |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    ent_coef        | 0.128    |\n",
      "|    ent_coef_loss   | -0.389   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 169899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 152      |\n",
      "|    time_elapsed    | 1178     |\n",
      "|    total_timesteps | 180000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -242     |\n",
      "|    critic_loss     | 6.59     |\n",
      "|    ent_coef        | 0.132    |\n",
      "|    ent_coef_loss   | -0.646   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 179899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 190      |\n",
      "|    fps             | 153      |\n",
      "|    time_elapsed    | 1236     |\n",
      "|    total_timesteps | 190000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -253     |\n",
      "|    critic_loss     | 5.5      |\n",
      "|    ent_coef        | 0.135    |\n",
      "|    ent_coef_loss   | -1.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 189899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.29e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 1297     |\n",
      "|    total_timesteps | 200000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -262     |\n",
      "|    critic_loss     | 5.64     |\n",
      "|    ent_coef        | 0.134    |\n",
      "|    ent_coef_loss   | -0.276   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 199899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.36e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 210      |\n",
      "|    fps             | 154      |\n",
      "|    time_elapsed    | 1358     |\n",
      "|    total_timesteps | 210000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -266     |\n",
      "|    critic_loss     | 3.61     |\n",
      "|    ent_coef        | 0.136    |\n",
      "|    ent_coef_loss   | -0.425   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 209899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.43e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 1418     |\n",
      "|    total_timesteps | 220000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -271     |\n",
      "|    critic_loss     | 5.44     |\n",
      "|    ent_coef        | 0.14     |\n",
      "|    ent_coef_loss   | -0.0592  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 219899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 230      |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 1479     |\n",
      "|    total_timesteps | 230000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -278     |\n",
      "|    critic_loss     | 8.14     |\n",
      "|    ent_coef        | 0.139    |\n",
      "|    ent_coef_loss   | 0.0157   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 229899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 155      |\n",
      "|    time_elapsed    | 1540     |\n",
      "|    total_timesteps | 240000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -287     |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    ent_coef        | 0.142    |\n",
      "|    ent_coef_loss   | -0.297   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 239899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 250      |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 1598     |\n",
      "|    total_timesteps | 250000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -302     |\n",
      "|    critic_loss     | 5.37     |\n",
      "|    ent_coef        | 0.142    |\n",
      "|    ent_coef_loss   | 0.323    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 249899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 1656     |\n",
      "|    total_timesteps | 260000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -299     |\n",
      "|    critic_loss     | 4.61     |\n",
      "|    ent_coef        | 0.144    |\n",
      "|    ent_coef_loss   | -0.395   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 259899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 270      |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 1717     |\n",
      "|    total_timesteps | 270000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -304     |\n",
      "|    critic_loss     | 5.91     |\n",
      "|    ent_coef        | 0.147    |\n",
      "|    ent_coef_loss   | 0.336    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 269899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 157      |\n",
      "|    time_elapsed    | 1775     |\n",
      "|    total_timesteps | 280000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -309     |\n",
      "|    critic_loss     | 6.81     |\n",
      "|    ent_coef        | 0.149    |\n",
      "|    ent_coef_loss   | -1.03    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 279899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 290      |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 1833     |\n",
      "|    total_timesteps | 290000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -315     |\n",
      "|    critic_loss     | 5.64     |\n",
      "|    ent_coef        | 0.151    |\n",
      "|    ent_coef_loss   | -0.775   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 289899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 1893     |\n",
      "|    total_timesteps | 300000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -320     |\n",
      "|    critic_loss     | 4.86     |\n",
      "|    ent_coef        | 0.155    |\n",
      "|    ent_coef_loss   | -1.24    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 299899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 310      |\n",
      "|    fps             | 158      |\n",
      "|    time_elapsed    | 1952     |\n",
      "|    total_timesteps | 310000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 3.76     |\n",
      "|    ent_coef        | 0.154    |\n",
      "|    ent_coef_loss   | -0.664   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 309899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.13e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 2012     |\n",
      "|    total_timesteps | 320000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -341     |\n",
      "|    critic_loss     | 6.63     |\n",
      "|    ent_coef        | 0.155    |\n",
      "|    ent_coef_loss   | 0.39     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 319899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 330      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 2071     |\n",
      "|    total_timesteps | 330000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -344     |\n",
      "|    critic_loss     | 5.27     |\n",
      "|    ent_coef        | 0.156    |\n",
      "|    ent_coef_loss   | 0.466    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 329899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 2131     |\n",
      "|    total_timesteps | 340000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -345     |\n",
      "|    critic_loss     | 3.65     |\n",
      "|    ent_coef        | 0.159    |\n",
      "|    ent_coef_loss   | -1.13    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 339899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 350      |\n",
      "|    fps             | 159      |\n",
      "|    time_elapsed    | 2189     |\n",
      "|    total_timesteps | 350000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -357     |\n",
      "|    critic_loss     | 4.42     |\n",
      "|    ent_coef        | 0.16     |\n",
      "|    ent_coef_loss   | 0.657    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 349899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 360      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 2249     |\n",
      "|    total_timesteps | 360000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 4.46     |\n",
      "|    ent_coef        | 0.161    |\n",
      "|    ent_coef_loss   | -0.758   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 359899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.47e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 370      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 2309     |\n",
      "|    total_timesteps | 370000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 5.56     |\n",
      "|    ent_coef        | 0.164    |\n",
      "|    ent_coef_loss   | -0.552   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 369899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 380      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 2372     |\n",
      "|    total_timesteps | 380000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -375     |\n",
      "|    critic_loss     | 5.77     |\n",
      "|    ent_coef        | 0.165    |\n",
      "|    ent_coef_loss   | 0.58     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 379899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 390      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 2433     |\n",
      "|    total_timesteps | 390000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -381     |\n",
      "|    critic_loss     | 5.48     |\n",
      "|    ent_coef        | 0.168    |\n",
      "|    ent_coef_loss   | 0.295    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 389899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 2494     |\n",
      "|    total_timesteps | 400000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -366     |\n",
      "|    critic_loss     | 5.02     |\n",
      "|    ent_coef        | 0.172    |\n",
      "|    ent_coef_loss   | -0.496   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 399899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.7e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 410      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 2556     |\n",
      "|    total_timesteps | 410000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -387     |\n",
      "|    critic_loss     | 4.63     |\n",
      "|    ent_coef        | 0.172    |\n",
      "|    ent_coef_loss   | -0.656   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 409899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 420      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 2617     |\n",
      "|    total_timesteps | 420000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -393     |\n",
      "|    critic_loss     | 4.85     |\n",
      "|    ent_coef        | 0.173    |\n",
      "|    ent_coef_loss   | 0.536    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 419899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.81e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 430      |\n",
      "|    fps             | 160      |\n",
      "|    time_elapsed    | 2675     |\n",
      "|    total_timesteps | 430000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -388     |\n",
      "|    critic_loss     | 5.77     |\n",
      "|    ent_coef        | 0.174    |\n",
      "|    ent_coef_loss   | 0.448    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 429899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 440      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 2731     |\n",
      "|    total_timesteps | 440000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -402     |\n",
      "|    critic_loss     | 4.16     |\n",
      "|    ent_coef        | 0.172    |\n",
      "|    ent_coef_loss   | 0.973    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 439899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 450      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 2788     |\n",
      "|    total_timesteps | 450000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -397     |\n",
      "|    critic_loss     | 4.7      |\n",
      "|    ent_coef        | 0.173    |\n",
      "|    ent_coef_loss   | -0.035   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 449899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 5.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 460      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 2848     |\n",
      "|    total_timesteps | 460000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -405     |\n",
      "|    critic_loss     | 5.99     |\n",
      "|    ent_coef        | 0.177    |\n",
      "|    ent_coef_loss   | 0.435    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 459899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6e+03    |\n",
      "| time/              |          |\n",
      "|    episodes        | 470      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 2906     |\n",
      "|    total_timesteps | 470000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -405     |\n",
      "|    critic_loss     | 5.43     |\n",
      "|    ent_coef        | 0.175    |\n",
      "|    ent_coef_loss   | 0.0398   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 469899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 480      |\n",
      "|    fps             | 161      |\n",
      "|    time_elapsed    | 2964     |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 6.81     |\n",
      "|    ent_coef        | 0.177    |\n",
      "|    ent_coef_loss   | 0.539    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 479899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 490      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 3021     |\n",
      "|    total_timesteps | 490000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -411     |\n",
      "|    critic_loss     | 8.63     |\n",
      "|    ent_coef        | 0.177    |\n",
      "|    ent_coef_loss   | -0.591   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 489899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 500      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 3077     |\n",
      "|    total_timesteps | 500000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -423     |\n",
      "|    critic_loss     | 6.19     |\n",
      "|    ent_coef        | 0.178    |\n",
      "|    ent_coef_loss   | 0.116    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 499899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.15e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 510      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 3135     |\n",
      "|    total_timesteps | 510000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 4.57     |\n",
      "|    ent_coef        | 0.176    |\n",
      "|    ent_coef_loss   | 0.288    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 509899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.18e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 520      |\n",
      "|    fps             | 162      |\n",
      "|    time_elapsed    | 3192     |\n",
      "|    total_timesteps | 520000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -424     |\n",
      "|    critic_loss     | 3.99     |\n",
      "|    ent_coef        | 0.179    |\n",
      "|    ent_coef_loss   | -1.11    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 519899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 530      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3250     |\n",
      "|    total_timesteps | 530000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -415     |\n",
      "|    critic_loss     | 5.24     |\n",
      "|    ent_coef        | 0.179    |\n",
      "|    ent_coef_loss   | -0.437   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 529899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 540      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3307     |\n",
      "|    total_timesteps | 540000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -431     |\n",
      "|    critic_loss     | 3.91     |\n",
      "|    ent_coef        | 0.179    |\n",
      "|    ent_coef_loss   | -0.1     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 539899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 550      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3364     |\n",
      "|    total_timesteps | 550000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -440     |\n",
      "|    critic_loss     | 4.28     |\n",
      "|    ent_coef        | 0.18     |\n",
      "|    ent_coef_loss   | 0.28     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 549899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.31e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 560      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3421     |\n",
      "|    total_timesteps | 560000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -437     |\n",
      "|    critic_loss     | 6.02     |\n",
      "|    ent_coef        | 0.183    |\n",
      "|    ent_coef_loss   | 0.0679   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 559899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 570      |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 3478     |\n",
      "|    total_timesteps | 570000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -446     |\n",
      "|    critic_loss     | 4.78     |\n",
      "|    ent_coef        | 0.181    |\n",
      "|    ent_coef_loss   | 0.287    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 569899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.39e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 580      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3535     |\n",
      "|    total_timesteps | 580000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -445     |\n",
      "|    critic_loss     | 5.05     |\n",
      "|    ent_coef        | 0.181    |\n",
      "|    ent_coef_loss   | -0.0243  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 579899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.42e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 590      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3592     |\n",
      "|    total_timesteps | 590000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 4.58     |\n",
      "|    ent_coef        | 0.183    |\n",
      "|    ent_coef_loss   | 0.148    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 589899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 600      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3649     |\n",
      "|    total_timesteps | 600000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -451     |\n",
      "|    critic_loss     | 5.37     |\n",
      "|    ent_coef        | 0.182    |\n",
      "|    ent_coef_loss   | 0.604    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 599899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 610      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3706     |\n",
      "|    total_timesteps | 610000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -445     |\n",
      "|    critic_loss     | 4.8      |\n",
      "|    ent_coef        | 0.181    |\n",
      "|    ent_coef_loss   | -0.846   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 609899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 620      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3763     |\n",
      "|    total_timesteps | 620000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -454     |\n",
      "|    critic_loss     | 4.92     |\n",
      "|    ent_coef        | 0.179    |\n",
      "|    ent_coef_loss   | 0.465    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 619899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 630      |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 3821     |\n",
      "|    total_timesteps | 630000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -464     |\n",
      "|    critic_loss     | 3.89     |\n",
      "|    ent_coef        | 0.18     |\n",
      "|    ent_coef_loss   | 0.336    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 629899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.56e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 640      |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 3878     |\n",
      "|    total_timesteps | 640000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -448     |\n",
      "|    critic_loss     | 4.71     |\n",
      "|    ent_coef        | 0.181    |\n",
      "|    ent_coef_loss   | -0.003   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 639899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.59e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 650      |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 3935     |\n",
      "|    total_timesteps | 650000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -473     |\n",
      "|    critic_loss     | 6.22     |\n",
      "|    ent_coef        | 0.18     |\n",
      "|    ent_coef_loss   | 0.24     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 649899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 660      |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 3992     |\n",
      "|    total_timesteps | 660000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -470     |\n",
      "|    critic_loss     | 4.38     |\n",
      "|    ent_coef        | 0.185    |\n",
      "|    ent_coef_loss   | 0.774    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 659899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.61e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 670      |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 4050     |\n",
      "|    total_timesteps | 670000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 5.52     |\n",
      "|    ent_coef        | 0.183    |\n",
      "|    ent_coef_loss   | 0.277    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 669899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 680      |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 4107     |\n",
      "|    total_timesteps | 680000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -467     |\n",
      "|    critic_loss     | 6.63     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | -0.39    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 679899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 690      |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 4165     |\n",
      "|    total_timesteps | 690000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -477     |\n",
      "|    critic_loss     | 5.47     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | 0.444    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 689899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 700      |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 4221     |\n",
      "|    total_timesteps | 700000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -458     |\n",
      "|    critic_loss     | 7.82     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | -0.584   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 699899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.71e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 710      |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 4278     |\n",
      "|    total_timesteps | 710000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -474     |\n",
      "|    critic_loss     | 5.39     |\n",
      "|    ent_coef        | 0.186    |\n",
      "|    ent_coef_loss   | 0.286    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 709899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.74e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 720      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4335     |\n",
      "|    total_timesteps | 720000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -497     |\n",
      "|    critic_loss     | 3.96     |\n",
      "|    ent_coef        | 0.188    |\n",
      "|    ent_coef_loss   | -0.0697  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 719899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 730      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4392     |\n",
      "|    total_timesteps | 730000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -481     |\n",
      "|    critic_loss     | 4.87     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | -0.378   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 729899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 740      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4450     |\n",
      "|    total_timesteps | 740000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -484     |\n",
      "|    critic_loss     | 5.18     |\n",
      "|    ent_coef        | 0.185    |\n",
      "|    ent_coef_loss   | 0.171    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 739899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 750      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4509     |\n",
      "|    total_timesteps | 750000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -501     |\n",
      "|    critic_loss     | 5.01     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | -0.501   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 749899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.89e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 760      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4567     |\n",
      "|    total_timesteps | 760000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -503     |\n",
      "|    critic_loss     | 5.13     |\n",
      "|    ent_coef        | 0.185    |\n",
      "|    ent_coef_loss   | -0.249   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 759899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 770      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4624     |\n",
      "|    total_timesteps | 770000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -496     |\n",
      "|    critic_loss     | 5.14     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | 0.138    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 769899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 780      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4680     |\n",
      "|    total_timesteps | 780000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -494     |\n",
      "|    critic_loss     | 6.38     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | 0.156    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 779899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.96e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 790      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4737     |\n",
      "|    total_timesteps | 790000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -502     |\n",
      "|    critic_loss     | 5.45     |\n",
      "|    ent_coef        | 0.183    |\n",
      "|    ent_coef_loss   | -0.0731  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 789899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 6.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 800      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4794     |\n",
      "|    total_timesteps | 800000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -508     |\n",
      "|    critic_loss     | 5.89     |\n",
      "|    ent_coef        | 0.186    |\n",
      "|    ent_coef_loss   | 0.0568   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 799899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 810      |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 4851     |\n",
      "|    total_timesteps | 810000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -518     |\n",
      "|    critic_loss     | 6.18     |\n",
      "|    ent_coef        | 0.186    |\n",
      "|    ent_coef_loss   | 0.353    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 809899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.04e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 820      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 4908     |\n",
      "|    total_timesteps | 820000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -509     |\n",
      "|    critic_loss     | 5.05     |\n",
      "|    ent_coef        | 0.187    |\n",
      "|    ent_coef_loss   | -0.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 819899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.07e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 830      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 4966     |\n",
      "|    total_timesteps | 830000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -514     |\n",
      "|    critic_loss     | 5.55     |\n",
      "|    ent_coef        | 0.185    |\n",
      "|    ent_coef_loss   | -0.0328  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 829899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.09e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 840      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5023     |\n",
      "|    total_timesteps | 840000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -513     |\n",
      "|    critic_loss     | 4.4      |\n",
      "|    ent_coef        | 0.186    |\n",
      "|    ent_coef_loss   | -0.169   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 839899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.11e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 850      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5080     |\n",
      "|    total_timesteps | 850000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -518     |\n",
      "|    critic_loss     | 4.86     |\n",
      "|    ent_coef        | 0.186    |\n",
      "|    ent_coef_loss   | -0.032   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 849899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 860      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5137     |\n",
      "|    total_timesteps | 860000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -524     |\n",
      "|    critic_loss     | 4.66     |\n",
      "|    ent_coef        | 0.183    |\n",
      "|    ent_coef_loss   | 0.353    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 859899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.16e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 870      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5195     |\n",
      "|    total_timesteps | 870000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -525     |\n",
      "|    critic_loss     | 6.38     |\n",
      "|    ent_coef        | 0.184    |\n",
      "|    ent_coef_loss   | 0.548    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 869899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.19e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 880      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5252     |\n",
      "|    total_timesteps | 880000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -520     |\n",
      "|    critic_loss     | 4.98     |\n",
      "|    ent_coef        | 0.191    |\n",
      "|    ent_coef_loss   | 0.198    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 879899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.22e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 890      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5308     |\n",
      "|    total_timesteps | 890000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -527     |\n",
      "|    critic_loss     | 5.44     |\n",
      "|    ent_coef        | 0.187    |\n",
      "|    ent_coef_loss   | -0.208   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 889899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.27e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 900      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5366     |\n",
      "|    total_timesteps | 900000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -522     |\n",
      "|    critic_loss     | 6.27     |\n",
      "|    ent_coef        | 0.189    |\n",
      "|    ent_coef_loss   | 0.158    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 899899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.32e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 910      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5424     |\n",
      "|    total_timesteps | 910000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -534     |\n",
      "|    critic_loss     | 5.01     |\n",
      "|    ent_coef        | 0.188    |\n",
      "|    ent_coef_loss   | -0.158   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 909899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 920      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5481     |\n",
      "|    total_timesteps | 920000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -533     |\n",
      "|    critic_loss     | 5.68     |\n",
      "|    ent_coef        | 0.187    |\n",
      "|    ent_coef_loss   | -0.255   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 919899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 930      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5538     |\n",
      "|    total_timesteps | 930000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -539     |\n",
      "|    critic_loss     | 7.28     |\n",
      "|    ent_coef        | 0.188    |\n",
      "|    ent_coef_loss   | -0.866   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 929899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.48e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 940      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5596     |\n",
      "|    total_timesteps | 940000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -544     |\n",
      "|    critic_loss     | 24.1     |\n",
      "|    ent_coef        | 0.194    |\n",
      "|    ent_coef_loss   | -0.0606  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 939899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.52e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 950      |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 5655     |\n",
      "|    total_timesteps | 950000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -550     |\n",
      "|    critic_loss     | 6.97     |\n",
      "|    ent_coef        | 0.194    |\n",
      "|    ent_coef_loss   | -0.811   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 949899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 960      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 5712     |\n",
      "|    total_timesteps | 960000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -548     |\n",
      "|    critic_loss     | 6.99     |\n",
      "|    ent_coef        | 0.195    |\n",
      "|    ent_coef_loss   | 0.255    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 959899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.63e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 970      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 5769     |\n",
      "|    total_timesteps | 970000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -556     |\n",
      "|    critic_loss     | 9.05     |\n",
      "|    ent_coef        | 0.194    |\n",
      "|    ent_coef_loss   | -0.423   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 969899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.69e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 980      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 5828     |\n",
      "|    total_timesteps | 980000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -554     |\n",
      "|    critic_loss     | 7.57     |\n",
      "|    ent_coef        | 0.198    |\n",
      "|    ent_coef_loss   | 0.154    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 979899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.76e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 990      |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 5886     |\n",
      "|    total_timesteps | 990000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -556     |\n",
      "|    critic_loss     | 7.37     |\n",
      "|    ent_coef        | 0.198    |\n",
      "|    ent_coef_loss   | -0.698   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 989899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.73e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1000     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 5946     |\n",
      "|    total_timesteps | 1000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -550     |\n",
      "|    critic_loss     | 7.23     |\n",
      "|    ent_coef        | 0.2      |\n",
      "|    ent_coef_loss   | -0.31    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 999899   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1010     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6005     |\n",
      "|    total_timesteps | 1010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -567     |\n",
      "|    critic_loss     | 8.46     |\n",
      "|    ent_coef        | 0.206    |\n",
      "|    ent_coef_loss   | 0.452    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.83e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1020     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6063     |\n",
      "|    total_timesteps | 1020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -573     |\n",
      "|    critic_loss     | 6.08     |\n",
      "|    ent_coef        | 0.213    |\n",
      "|    ent_coef_loss   | -0.195   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.87e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1030     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6120     |\n",
      "|    total_timesteps | 1030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -576     |\n",
      "|    critic_loss     | 8.56     |\n",
      "|    ent_coef        | 0.212    |\n",
      "|    ent_coef_loss   | -1.06    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1040     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6177     |\n",
      "|    total_timesteps | 1040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -583     |\n",
      "|    critic_loss     | 5.89     |\n",
      "|    ent_coef        | 0.217    |\n",
      "|    ent_coef_loss   | 0.761    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1050     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6234     |\n",
      "|    total_timesteps | 1050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -585     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.217    |\n",
      "|    ent_coef_loss   | -0.512   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 7.99e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1060     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6292     |\n",
      "|    total_timesteps | 1060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -589     |\n",
      "|    critic_loss     | 12.2     |\n",
      "|    ent_coef        | 0.217    |\n",
      "|    ent_coef_loss   | 0.0466   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.02e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1070     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6354     |\n",
      "|    total_timesteps | 1070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -596     |\n",
      "|    critic_loss     | 6.16     |\n",
      "|    ent_coef        | 0.22     |\n",
      "|    ent_coef_loss   | 0.332    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.06e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1080     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6413     |\n",
      "|    total_timesteps | 1080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -603     |\n",
      "|    critic_loss     | 7.68     |\n",
      "|    ent_coef        | 0.224    |\n",
      "|    ent_coef_loss   | -0.358   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.1e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1090     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6472     |\n",
      "|    total_timesteps | 1090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -602     |\n",
      "|    critic_loss     | 8.05     |\n",
      "|    ent_coef        | 0.226    |\n",
      "|    ent_coef_loss   | -0.161   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.2e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1100     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6530     |\n",
      "|    total_timesteps | 1100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -603     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.222    |\n",
      "|    ent_coef_loss   | 0.117    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.23e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1110     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6589     |\n",
      "|    total_timesteps | 1110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -603     |\n",
      "|    critic_loss     | 7.46     |\n",
      "|    ent_coef        | 0.225    |\n",
      "|    ent_coef_loss   | -0.621   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.26e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1120     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6654     |\n",
      "|    total_timesteps | 1120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -608     |\n",
      "|    critic_loss     | 10.5     |\n",
      "|    ent_coef        | 0.23     |\n",
      "|    ent_coef_loss   | 0.694    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1130     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 6721     |\n",
      "|    total_timesteps | 1130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -619     |\n",
      "|    critic_loss     | 8.11     |\n",
      "|    ent_coef        | 0.228    |\n",
      "|    ent_coef_loss   | -0.182   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.35e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1140     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 6787     |\n",
      "|    total_timesteps | 1140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -626     |\n",
      "|    critic_loss     | 9.93     |\n",
      "|    ent_coef        | 0.228    |\n",
      "|    ent_coef_loss   | -0.593   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.4e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1150     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 6856     |\n",
      "|    total_timesteps | 1150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -626     |\n",
      "|    critic_loss     | 5.66     |\n",
      "|    ent_coef        | 0.227    |\n",
      "|    ent_coef_loss   | 0.404    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1160     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 6920     |\n",
      "|    total_timesteps | 1160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -625     |\n",
      "|    critic_loss     | 6.54     |\n",
      "|    ent_coef        | 0.228    |\n",
      "|    ent_coef_loss   | -0.228   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.51e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1170     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 6992     |\n",
      "|    total_timesteps | 1170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -635     |\n",
      "|    critic_loss     | 5.45     |\n",
      "|    ent_coef        | 0.233    |\n",
      "|    ent_coef_loss   | 0.474    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.54e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1180     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 7053     |\n",
      "|    total_timesteps | 1180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -638     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.227    |\n",
      "|    ent_coef_loss   | -0.482   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.58e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1190     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 7129     |\n",
      "|    total_timesteps | 1190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -639     |\n",
      "|    critic_loss     | 9.43     |\n",
      "|    ent_coef        | 0.231    |\n",
      "|    ent_coef_loss   | -0.403   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.64e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1200     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 7188     |\n",
      "|    total_timesteps | 1200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -647     |\n",
      "|    critic_loss     | 8.03     |\n",
      "|    ent_coef        | 0.232    |\n",
      "|    ent_coef_loss   | -0.101   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.67e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1210     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 7252     |\n",
      "|    total_timesteps | 1210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -649     |\n",
      "|    critic_loss     | 9.37     |\n",
      "|    ent_coef        | 0.234    |\n",
      "|    ent_coef_loss   | 0.0157   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1220     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 7328     |\n",
      "|    total_timesteps | 1220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -653     |\n",
      "|    critic_loss     | 14.4     |\n",
      "|    ent_coef        | 0.233    |\n",
      "|    ent_coef_loss   | 0.293    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.77e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1230     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 7394     |\n",
      "|    total_timesteps | 1230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -657     |\n",
      "|    critic_loss     | 8.33     |\n",
      "|    ent_coef        | 0.239    |\n",
      "|    ent_coef_loss   | -0.668   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1240     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 7456     |\n",
      "|    total_timesteps | 1240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -655     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.239    |\n",
      "|    ent_coef_loss   | 0.363    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.86e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1250     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 7524     |\n",
      "|    total_timesteps | 1250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -665     |\n",
      "|    critic_loss     | 8.69     |\n",
      "|    ent_coef        | 0.241    |\n",
      "|    ent_coef_loss   | 0.249    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1260     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 7589     |\n",
      "|    total_timesteps | 1260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -670     |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.244    |\n",
      "|    ent_coef_loss   | 0.83     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 8.94e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1270     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7654     |\n",
      "|    total_timesteps | 1270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -668     |\n",
      "|    critic_loss     | 7.4      |\n",
      "|    ent_coef        | 0.244    |\n",
      "|    ent_coef_loss   | -0.111   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.01e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1280     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7718     |\n",
      "|    total_timesteps | 1280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -669     |\n",
      "|    critic_loss     | 11.5     |\n",
      "|    ent_coef        | 0.241    |\n",
      "|    ent_coef_loss   | 0.121    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.05e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1290     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7786     |\n",
      "|    total_timesteps | 1290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -681     |\n",
      "|    critic_loss     | 7.66     |\n",
      "|    ent_coef        | 0.246    |\n",
      "|    ent_coef_loss   | -0.688   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.08e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1300     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7853     |\n",
      "|    total_timesteps | 1300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -683     |\n",
      "|    critic_loss     | 9.26     |\n",
      "|    ent_coef        | 0.246    |\n",
      "|    ent_coef_loss   | 0.246    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.14e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1310     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7916     |\n",
      "|    total_timesteps | 1310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -689     |\n",
      "|    critic_loss     | 9.38     |\n",
      "|    ent_coef        | 0.247    |\n",
      "|    ent_coef_loss   | 0.423    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1309899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.17e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1320     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 7991     |\n",
      "|    total_timesteps | 1320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -691     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.246    |\n",
      "|    ent_coef_loss   | -0.606   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1319899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.21e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1330     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 8063     |\n",
      "|    total_timesteps | 1330000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -689     |\n",
      "|    critic_loss     | 7.28     |\n",
      "|    ent_coef        | 0.244    |\n",
      "|    ent_coef_loss   | 0.472    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1329899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.24e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1340     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 8137     |\n",
      "|    total_timesteps | 1340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -697     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.249    |\n",
      "|    ent_coef_loss   | 0.946    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1339899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.28e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1350     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 8205     |\n",
      "|    total_timesteps | 1350000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -697     |\n",
      "|    critic_loss     | 7.35     |\n",
      "|    ent_coef        | 0.248    |\n",
      "|    ent_coef_loss   | 0.334    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1349899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.3e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1360     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 8272     |\n",
      "|    total_timesteps | 1360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -699     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.25     |\n",
      "|    ent_coef_loss   | -0.129   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1359899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1370     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 8341     |\n",
      "|    total_timesteps | 1370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -702     |\n",
      "|    critic_loss     | 9.62     |\n",
      "|    ent_coef        | 0.251    |\n",
      "|    ent_coef_loss   | -0.365   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1369899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.37e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1380     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 8412     |\n",
      "|    total_timesteps | 1380000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -704     |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.249    |\n",
      "|    ent_coef_loss   | 0.0109   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1379899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.41e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1390     |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 8483     |\n",
      "|    total_timesteps | 1390000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -715     |\n",
      "|    critic_loss     | 9.93     |\n",
      "|    ent_coef        | 0.25     |\n",
      "|    ent_coef_loss   | 0.179    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1389899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.45e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1400     |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 8542     |\n",
      "|    total_timesteps | 1400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -707     |\n",
      "|    critic_loss     | 8.52     |\n",
      "|    ent_coef        | 0.251    |\n",
      "|    ent_coef_loss   | -0.645   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1399899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.5e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1410     |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 8609     |\n",
      "|    total_timesteps | 1410000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -717     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.251    |\n",
      "|    ent_coef_loss   | 0.186    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1409899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1420     |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 8666     |\n",
      "|    total_timesteps | 1420000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -718     |\n",
      "|    critic_loss     | 13.9     |\n",
      "|    ent_coef        | 0.249    |\n",
      "|    ent_coef_loss   | 0.0421   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1419899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.57e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1430     |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 8723     |\n",
      "|    total_timesteps | 1430000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -726     |\n",
      "|    critic_loss     | 9.55     |\n",
      "|    ent_coef        | 0.253    |\n",
      "|    ent_coef_loss   | -0.459   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1429899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.62e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1440     |\n",
      "|    fps             | 163      |\n",
      "|    time_elapsed    | 8780     |\n",
      "|    total_timesteps | 1440000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -735     |\n",
      "|    critic_loss     | 62.5     |\n",
      "|    ent_coef        | 0.255    |\n",
      "|    ent_coef_loss   | 0.168    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1439899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.66e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1450     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 8838     |\n",
      "|    total_timesteps | 1450000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -740     |\n",
      "|    critic_loss     | 9.95     |\n",
      "|    ent_coef        | 0.249    |\n",
      "|    ent_coef_loss   | -0.413   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1449899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.72e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1460     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 8895     |\n",
      "|    total_timesteps | 1460000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -734     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.254    |\n",
      "|    ent_coef_loss   | 0.0449   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1459899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1470     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 8952     |\n",
      "|    total_timesteps | 1470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -746     |\n",
      "|    critic_loss     | 6.42     |\n",
      "|    ent_coef        | 0.252    |\n",
      "|    ent_coef_loss   | 0.327    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1469899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.82e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1480     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9009     |\n",
      "|    total_timesteps | 1480000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -751     |\n",
      "|    critic_loss     | 8.42     |\n",
      "|    ent_coef        | 0.253    |\n",
      "|    ent_coef_loss   | 0.25     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1479899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1490     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9065     |\n",
      "|    total_timesteps | 1490000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -754     |\n",
      "|    critic_loss     | 10       |\n",
      "|    ent_coef        | 0.257    |\n",
      "|    ent_coef_loss   | -0.412   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1489899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.88e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1500     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9123     |\n",
      "|    total_timesteps | 1500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -747     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.263    |\n",
      "|    ent_coef_loss   | 0.424    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1499899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.9e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1510     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9181     |\n",
      "|    total_timesteps | 1510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -753     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.263    |\n",
      "|    ent_coef_loss   | 0.109    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1509899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.93e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1520     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9238     |\n",
      "|    total_timesteps | 1520000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -758     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.259    |\n",
      "|    ent_coef_loss   | -0.358   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1519899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.98e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1530     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9296     |\n",
      "|    total_timesteps | 1530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -765     |\n",
      "|    critic_loss     | 9.49     |\n",
      "|    ent_coef        | 0.26     |\n",
      "|    ent_coef_loss   | 0.202    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.95e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1540     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9353     |\n",
      "|    total_timesteps | 1540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -770     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.266    |\n",
      "|    ent_coef_loss   | -0.484   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.91e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1550     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9410     |\n",
      "|    total_timesteps | 1550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -774     |\n",
      "|    critic_loss     | 16.1     |\n",
      "|    ent_coef        | 0.268    |\n",
      "|    ent_coef_loss   | -0.0291  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1549899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 9.97e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1560     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9467     |\n",
      "|    total_timesteps | 1560000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -770     |\n",
      "|    critic_loss     | 12       |\n",
      "|    ent_coef        | 0.268    |\n",
      "|    ent_coef_loss   | 0.0584   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1559899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1e+04    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1570     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9524     |\n",
      "|    total_timesteps | 1570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -769     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.268    |\n",
      "|    ent_coef_loss   | -0.125   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1569899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1e+04    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1580     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9581     |\n",
      "|    total_timesteps | 1580000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -777     |\n",
      "|    critic_loss     | 10.9     |\n",
      "|    ent_coef        | 0.272    |\n",
      "|    ent_coef_loss   | 0.182    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1579899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1590     |\n",
      "|    fps             | 164      |\n",
      "|    time_elapsed    | 9638     |\n",
      "|    total_timesteps | 1590000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -772     |\n",
      "|    critic_loss     | 39.4     |\n",
      "|    ent_coef        | 0.271    |\n",
      "|    ent_coef_loss   | -0.211   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1589899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1600     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9695     |\n",
      "|    total_timesteps | 1600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -783     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    ent_coef        | 0.271    |\n",
      "|    ent_coef_loss   | -0.182   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1599899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.01e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1610     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9753     |\n",
      "|    total_timesteps | 1610000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -781     |\n",
      "|    critic_loss     | 16.7     |\n",
      "|    ent_coef        | 0.276    |\n",
      "|    ent_coef_loss   | -0.174   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1609899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1620     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9810     |\n",
      "|    total_timesteps | 1620000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -795     |\n",
      "|    critic_loss     | 12.7     |\n",
      "|    ent_coef        | 0.275    |\n",
      "|    ent_coef_loss   | 0.114    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1619899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.02e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1630     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9867     |\n",
      "|    total_timesteps | 1630000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -793     |\n",
      "|    critic_loss     | 11.1     |\n",
      "|    ent_coef        | 0.276    |\n",
      "|    ent_coef_loss   | -0.428   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1629899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1640     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9924     |\n",
      "|    total_timesteps | 1640000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -798     |\n",
      "|    critic_loss     | 11.5     |\n",
      "|    ent_coef        | 0.274    |\n",
      "|    ent_coef_loss   | 0.175    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1639899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1650     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 9981     |\n",
      "|    total_timesteps | 1650000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -799     |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.278    |\n",
      "|    ent_coef_loss   | -0.0132  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1649899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1660     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10038    |\n",
      "|    total_timesteps | 1660000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -811     |\n",
      "|    critic_loss     | 10.3     |\n",
      "|    ent_coef        | 0.277    |\n",
      "|    ent_coef_loss   | -0.18    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1659899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.03e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1670     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10095    |\n",
      "|    total_timesteps | 1670000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -813     |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.282    |\n",
      "|    ent_coef_loss   | 0.389    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1669899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1680     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10152    |\n",
      "|    total_timesteps | 1680000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -809     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.278    |\n",
      "|    ent_coef_loss   | 0.162    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1679899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1690     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10210    |\n",
      "|    total_timesteps | 1690000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -808     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.28     |\n",
      "|    ent_coef_loss   | -0.66    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1689899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.04e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1700     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10267    |\n",
      "|    total_timesteps | 1700000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -811     |\n",
      "|    critic_loss     | 15.1     |\n",
      "|    ent_coef        | 0.278    |\n",
      "|    ent_coef_loss   | 0.603    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1699899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1710     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10327    |\n",
      "|    total_timesteps | 1710000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -815     |\n",
      "|    critic_loss     | 17.1     |\n",
      "|    ent_coef        | 0.28     |\n",
      "|    ent_coef_loss   | -0.291   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1709899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1720     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10390    |\n",
      "|    total_timesteps | 1720000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -812     |\n",
      "|    critic_loss     | 34.7     |\n",
      "|    ent_coef        | 0.281    |\n",
      "|    ent_coef_loss   | -0.279   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1719899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.06e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1730     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10449    |\n",
      "|    total_timesteps | 1730000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -832     |\n",
      "|    critic_loss     | 22.6     |\n",
      "|    ent_coef        | 0.284    |\n",
      "|    ent_coef_loss   | 0.21     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1729899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.07e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1740     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10513    |\n",
      "|    total_timesteps | 1740000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -836     |\n",
      "|    critic_loss     | 17.9     |\n",
      "|    ent_coef        | 0.283    |\n",
      "|    ent_coef_loss   | -0.0356  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1739899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.09e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1750     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10569    |\n",
      "|    total_timesteps | 1750000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -827     |\n",
      "|    critic_loss     | 10.6     |\n",
      "|    ent_coef        | 0.28     |\n",
      "|    ent_coef_loss   | 0.388    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1749899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.09e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1760     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10626    |\n",
      "|    total_timesteps | 1760000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -829     |\n",
      "|    critic_loss     | 14       |\n",
      "|    ent_coef        | 0.282    |\n",
      "|    ent_coef_loss   | -0.0109  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1759899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.09e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1770     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10684    |\n",
      "|    total_timesteps | 1770000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -847     |\n",
      "|    critic_loss     | 18.3     |\n",
      "|    ent_coef        | 0.289    |\n",
      "|    ent_coef_loss   | 0.223    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1769899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.1e+04  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1780     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10741    |\n",
      "|    total_timesteps | 1780000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -845     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.286    |\n",
      "|    ent_coef_loss   | 0.624    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1779899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.1e+04  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1790     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10798    |\n",
      "|    total_timesteps | 1790000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -852     |\n",
      "|    critic_loss     | 11.9     |\n",
      "|    ent_coef        | 0.29     |\n",
      "|    ent_coef_loss   | 0.598    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1789899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1800     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10855    |\n",
      "|    total_timesteps | 1800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -854     |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.283    |\n",
      "|    ent_coef_loss   | -0.0712  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1799899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1810     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10911    |\n",
      "|    total_timesteps | 1810000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -855     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.288    |\n",
      "|    ent_coef_loss   | 0.116    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1809899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.11e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1820     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 10968    |\n",
      "|    total_timesteps | 1820000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -855     |\n",
      "|    critic_loss     | 12.8     |\n",
      "|    ent_coef        | 0.286    |\n",
      "|    ent_coef_loss   | 0.0801   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1819899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1830     |\n",
      "|    fps             | 165      |\n",
      "|    time_elapsed    | 11026    |\n",
      "|    total_timesteps | 1830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -860     |\n",
      "|    critic_loss     | 12.4     |\n",
      "|    ent_coef        | 0.294    |\n",
      "|    ent_coef_loss   | 0.565    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1829899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1840     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11083    |\n",
      "|    total_timesteps | 1840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -857     |\n",
      "|    critic_loss     | 11.4     |\n",
      "|    ent_coef        | 0.289    |\n",
      "|    ent_coef_loss   | 0.0735   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1839899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1850     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11141    |\n",
      "|    total_timesteps | 1850000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -872     |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.296    |\n",
      "|    ent_coef_loss   | -0.344   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1849899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1860     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11198    |\n",
      "|    total_timesteps | 1860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -879     |\n",
      "|    critic_loss     | 46.1     |\n",
      "|    ent_coef        | 0.293    |\n",
      "|    ent_coef_loss   | 0.308    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1870     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11255    |\n",
      "|    total_timesteps | 1870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -866     |\n",
      "|    critic_loss     | 12.5     |\n",
      "|    ent_coef        | 0.295    |\n",
      "|    ent_coef_loss   | 0.0338   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1869899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1880     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11312    |\n",
      "|    total_timesteps | 1880000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -881     |\n",
      "|    critic_loss     | 12.3     |\n",
      "|    ent_coef        | 0.295    |\n",
      "|    ent_coef_loss   | -0.424   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1879899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1890     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11370    |\n",
      "|    total_timesteps | 1890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -881     |\n",
      "|    critic_loss     | 17.2     |\n",
      "|    ent_coef        | 0.294    |\n",
      "|    ent_coef_loss   | 0.544    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1900     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11427    |\n",
      "|    total_timesteps | 1900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -885     |\n",
      "|    critic_loss     | 15.7     |\n",
      "|    ent_coef        | 0.294    |\n",
      "|    ent_coef_loss   | 0.257    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1899899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1910     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11484    |\n",
      "|    total_timesteps | 1910000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -896     |\n",
      "|    critic_loss     | 14.4     |\n",
      "|    ent_coef        | 0.297    |\n",
      "|    ent_coef_loss   | 0.172    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1909899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1920     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11541    |\n",
      "|    total_timesteps | 1920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -896     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.298    |\n",
      "|    ent_coef_loss   | -0.437   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1919899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1930     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11598    |\n",
      "|    total_timesteps | 1930000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -891     |\n",
      "|    critic_loss     | 16.9     |\n",
      "|    ent_coef        | 0.302    |\n",
      "|    ent_coef_loss   | 0.0523   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1929899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1940     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11653    |\n",
      "|    total_timesteps | 1940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -905     |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.295    |\n",
      "|    ent_coef_loss   | -0.502   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1950     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11707    |\n",
      "|    total_timesteps | 1950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -896     |\n",
      "|    critic_loss     | 20.8     |\n",
      "|    ent_coef        | 0.303    |\n",
      "|    ent_coef_loss   | -0.0956  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1949899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.14e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1960     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11762    |\n",
      "|    total_timesteps | 1960000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -905     |\n",
      "|    critic_loss     | 19.2     |\n",
      "|    ent_coef        | 0.302    |\n",
      "|    ent_coef_loss   | 0.325    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1959899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1970     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11816    |\n",
      "|    total_timesteps | 1970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -903     |\n",
      "|    critic_loss     | 54.9     |\n",
      "|    ent_coef        | 0.303    |\n",
      "|    ent_coef_loss   | 0.117    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1969899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1980     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11871    |\n",
      "|    total_timesteps | 1980000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -895     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.306    |\n",
      "|    ent_coef_loss   | -0.482   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1979899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1990     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11925    |\n",
      "|    total_timesteps | 1990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -918     |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    ent_coef        | 0.303    |\n",
      "|    ent_coef_loss   | 0.191    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1989899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2000     |\n",
      "|    fps             | 166      |\n",
      "|    time_elapsed    | 11980    |\n",
      "|    total_timesteps | 2000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -918     |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.302    |\n",
      "|    ent_coef_loss   | 0.11     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 1999899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.15e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2010     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12034    |\n",
      "|    total_timesteps | 2010000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -921     |\n",
      "|    critic_loss     | 17.8     |\n",
      "|    ent_coef        | 0.312    |\n",
      "|    ent_coef_loss   | 0.104    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2009899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2020     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12088    |\n",
      "|    total_timesteps | 2020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -924     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.309    |\n",
      "|    ent_coef_loss   | 0.00699  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2019899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.16e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2030     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12143    |\n",
      "|    total_timesteps | 2030000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -934     |\n",
      "|    critic_loss     | 17.5     |\n",
      "|    ent_coef        | 0.309    |\n",
      "|    ent_coef_loss   | -0.224   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2029899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2040     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12198    |\n",
      "|    total_timesteps | 2040000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -922     |\n",
      "|    critic_loss     | 19       |\n",
      "|    ent_coef        | 0.308    |\n",
      "|    ent_coef_loss   | 0.31     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2039899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2050     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12253    |\n",
      "|    total_timesteps | 2050000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -940     |\n",
      "|    critic_loss     | 18.4     |\n",
      "|    ent_coef        | 0.312    |\n",
      "|    ent_coef_loss   | -0.0235  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2049899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2060     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12307    |\n",
      "|    total_timesteps | 2060000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -930     |\n",
      "|    critic_loss     | 14.4     |\n",
      "|    ent_coef        | 0.309    |\n",
      "|    ent_coef_loss   | -0.363   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2059899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2070     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12362    |\n",
      "|    total_timesteps | 2070000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -942     |\n",
      "|    critic_loss     | 15.4     |\n",
      "|    ent_coef        | 0.304    |\n",
      "|    ent_coef_loss   | 0.497    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2069899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2080     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12416    |\n",
      "|    total_timesteps | 2080000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -955     |\n",
      "|    critic_loss     | 18.4     |\n",
      "|    ent_coef        | 0.304    |\n",
      "|    ent_coef_loss   | -0.12    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2079899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2090     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12471    |\n",
      "|    total_timesteps | 2090000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -951     |\n",
      "|    critic_loss     | 27.2     |\n",
      "|    ent_coef        | 0.306    |\n",
      "|    ent_coef_loss   | -0.0832  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2089899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2100     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12525    |\n",
      "|    total_timesteps | 2100000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -945     |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.309    |\n",
      "|    ent_coef_loss   | 0.492    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2099899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2110     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12580    |\n",
      "|    total_timesteps | 2110000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -956     |\n",
      "|    critic_loss     | 14.7     |\n",
      "|    ent_coef        | 0.306    |\n",
      "|    ent_coef_loss   | 0.887    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2109899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2120     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12634    |\n",
      "|    total_timesteps | 2120000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -942     |\n",
      "|    critic_loss     | 17.2     |\n",
      "|    ent_coef        | 0.305    |\n",
      "|    ent_coef_loss   | -0.25    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2119899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2130     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12689    |\n",
      "|    total_timesteps | 2130000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -958     |\n",
      "|    critic_loss     | 28.6     |\n",
      "|    ent_coef        | 0.309    |\n",
      "|    ent_coef_loss   | -0.0971  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2129899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2140     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12743    |\n",
      "|    total_timesteps | 2140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -953     |\n",
      "|    critic_loss     | 21.1     |\n",
      "|    ent_coef        | 0.313    |\n",
      "|    ent_coef_loss   | 0.445    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2139899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2150     |\n",
      "|    fps             | 167      |\n",
      "|    time_elapsed    | 12798    |\n",
      "|    total_timesteps | 2150000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -971     |\n",
      "|    critic_loss     | 30.9     |\n",
      "|    ent_coef        | 0.314    |\n",
      "|    ent_coef_loss   | 0.0218   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2149899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2160     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 12852    |\n",
      "|    total_timesteps | 2160000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -971     |\n",
      "|    critic_loss     | 21       |\n",
      "|    ent_coef        | 0.319    |\n",
      "|    ent_coef_loss   | -0.519   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2159899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.2e+04  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2170     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 12907    |\n",
      "|    total_timesteps | 2170000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -966     |\n",
      "|    critic_loss     | 20.6     |\n",
      "|    ent_coef        | 0.31     |\n",
      "|    ent_coef_loss   | 0.00674  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2169899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2180     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 12961    |\n",
      "|    total_timesteps | 2180000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -974     |\n",
      "|    critic_loss     | 15       |\n",
      "|    ent_coef        | 0.311    |\n",
      "|    ent_coef_loss   | -0.112   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2179899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2190     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13016    |\n",
      "|    total_timesteps | 2190000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -976     |\n",
      "|    critic_loss     | 16.8     |\n",
      "|    ent_coef        | 0.315    |\n",
      "|    ent_coef_loss   | -0.127   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2189899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2200     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13070    |\n",
      "|    total_timesteps | 2200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -981     |\n",
      "|    critic_loss     | 17.4     |\n",
      "|    ent_coef        | 0.312    |\n",
      "|    ent_coef_loss   | 0.53     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2199899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.2e+04  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2210     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13125    |\n",
      "|    total_timesteps | 2210000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -980     |\n",
      "|    critic_loss     | 18.4     |\n",
      "|    ent_coef        | 0.308    |\n",
      "|    ent_coef_loss   | 0.342    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2209899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.2e+04  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2220     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13180    |\n",
      "|    total_timesteps | 2220000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -987     |\n",
      "|    critic_loss     | 23.6     |\n",
      "|    ent_coef        | 0.312    |\n",
      "|    ent_coef_loss   | -0.05    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2219899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.2e+04  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2230     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13234    |\n",
      "|    total_timesteps | 2230000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -992     |\n",
      "|    critic_loss     | 20.4     |\n",
      "|    ent_coef        | 0.318    |\n",
      "|    ent_coef_loss   | -0.0555  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2229899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2240     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13288    |\n",
      "|    total_timesteps | 2240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -986     |\n",
      "|    critic_loss     | 16.8     |\n",
      "|    ent_coef        | 0.311    |\n",
      "|    ent_coef_loss   | 0.241    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2239899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.2e+04  |\n",
      "| time/              |          |\n",
      "|    episodes        | 2250     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13343    |\n",
      "|    total_timesteps | 2250000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -992     |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.309    |\n",
      "|    ent_coef_loss   | -0.0941  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2249899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2260     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13397    |\n",
      "|    total_timesteps | 2260000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1e+03   |\n",
      "|    critic_loss     | 18.6     |\n",
      "|    ent_coef        | 0.316    |\n",
      "|    ent_coef_loss   | -0.0286  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2259899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2270     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13452    |\n",
      "|    total_timesteps | 2270000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -988     |\n",
      "|    critic_loss     | 22.6     |\n",
      "|    ent_coef        | 0.311    |\n",
      "|    ent_coef_loss   | -0.27    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2269899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2280     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13506    |\n",
      "|    total_timesteps | 2280000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -995     |\n",
      "|    critic_loss     | 19.7     |\n",
      "|    ent_coef        | 0.314    |\n",
      "|    ent_coef_loss   | 0.71     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2279899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2290     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13561    |\n",
      "|    total_timesteps | 2290000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -992     |\n",
      "|    critic_loss     | 32.8     |\n",
      "|    ent_coef        | 0.318    |\n",
      "|    ent_coef_loss   | 0.483    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2289899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.18e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2300     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13615    |\n",
      "|    total_timesteps | 2300000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1e+03   |\n",
      "|    critic_loss     | 29.2     |\n",
      "|    ent_coef        | 0.314    |\n",
      "|    ent_coef_loss   | -0.0274  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.19e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2310     |\n",
      "|    fps             | 168      |\n",
      "|    time_elapsed    | 13670    |\n",
      "|    total_timesteps | 2310000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -990     |\n",
      "|    critic_loss     | 38.1     |\n",
      "|    ent_coef        | 0.312    |\n",
      "|    ent_coef_loss   | 0.663    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2309899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.2e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 2320      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 13724     |\n",
      "|    total_timesteps | 2320000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 30.1      |\n",
      "|    ent_coef        | 0.303     |\n",
      "|    ent_coef_loss   | 0.102     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2319899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.2e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 2330      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 13779     |\n",
      "|    total_timesteps | 2330000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 14.2      |\n",
      "|    ent_coef        | 0.309     |\n",
      "|    ent_coef_loss   | -0.361    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2329899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.2e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 2340      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 13834     |\n",
      "|    total_timesteps | 2340000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 15.1      |\n",
      "|    ent_coef        | 0.312     |\n",
      "|    ent_coef_loss   | 0.555     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2339899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.2e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 2350      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 13889     |\n",
      "|    total_timesteps | 2350000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 10.2      |\n",
      "|    ent_coef        | 0.304     |\n",
      "|    ent_coef_loss   | -0.256    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2349899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.22e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2360     |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 13943    |\n",
      "|    total_timesteps | 2360000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -999     |\n",
      "|    critic_loss     | 25.1     |\n",
      "|    ent_coef        | 0.309    |\n",
      "|    ent_coef_loss   | -0.241   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2359899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.22e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2370      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 13998     |\n",
      "|    total_timesteps | 2370000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 20.8      |\n",
      "|    ent_coef        | 0.32      |\n",
      "|    ent_coef_loss   | 0.175     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2369899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.21e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2380      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14052     |\n",
      "|    total_timesteps | 2380000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 25.7      |\n",
      "|    ent_coef        | 0.3       |\n",
      "|    ent_coef_loss   | -0.11     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2379899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.2e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 2390      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14107     |\n",
      "|    total_timesteps | 2390000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 26.9      |\n",
      "|    ent_coef        | 0.32      |\n",
      "|    ent_coef_loss   | -0.22     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2389899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.21e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2400     |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 14161    |\n",
      "|    total_timesteps | 2400000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -977     |\n",
      "|    critic_loss     | 1.1e+03  |\n",
      "|    ent_coef        | 0.314    |\n",
      "|    ent_coef_loss   | 0.328    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2399899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.2e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 2410      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14216     |\n",
      "|    total_timesteps | 2410000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 31.8      |\n",
      "|    ent_coef        | 0.311     |\n",
      "|    ent_coef_loss   | 0.0121    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2409899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.19e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2420      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14270     |\n",
      "|    total_timesteps | 2420000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 20.5      |\n",
      "|    ent_coef        | 0.317     |\n",
      "|    ent_coef_loss   | -0.137    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2419899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.18e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2430      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14325     |\n",
      "|    total_timesteps | 2430000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 30.2      |\n",
      "|    ent_coef        | 0.318     |\n",
      "|    ent_coef_loss   | 0.207     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2429899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.17e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2440      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14379     |\n",
      "|    total_timesteps | 2440000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 28.2      |\n",
      "|    ent_coef        | 0.327     |\n",
      "|    ent_coef_loss   | -0.297    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2439899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.16e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2450      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14434     |\n",
      "|    total_timesteps | 2450000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 37.7      |\n",
      "|    ent_coef        | 0.318     |\n",
      "|    ent_coef_loss   | -0.0212   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2449899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.14e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2460      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14489     |\n",
      "|    total_timesteps | 2460000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 74.6      |\n",
      "|    ent_coef        | 0.305     |\n",
      "|    ent_coef_loss   | 0.118     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2459899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.13e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2470     |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 14543    |\n",
      "|    total_timesteps | 2470000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1e+03   |\n",
      "|    critic_loss     | 28.6     |\n",
      "|    ent_coef        | 0.306    |\n",
      "|    ent_coef_loss   | -0.516   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2469899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.12e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2480      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14597     |\n",
      "|    total_timesteps | 2480000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 23        |\n",
      "|    ent_coef        | 0.313     |\n",
      "|    ent_coef_loss   | 0.00678   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2479899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.13e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2490      |\n",
      "|    fps             | 169       |\n",
      "|    time_elapsed    | 14652     |\n",
      "|    total_timesteps | 2490000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 48.7      |\n",
      "|    ent_coef        | 0.314     |\n",
      "|    ent_coef_loss   | 0.336     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2489899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.12e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2500     |\n",
      "|    fps             | 169      |\n",
      "|    time_elapsed    | 14707    |\n",
      "|    total_timesteps | 2500000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -991     |\n",
      "|    critic_loss     | 28.4     |\n",
      "|    ent_coef        | 0.312    |\n",
      "|    ent_coef_loss   | -0.219   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2499899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.13e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2510      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 14761     |\n",
      "|    total_timesteps | 2510000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 23.3      |\n",
      "|    ent_coef        | 0.315     |\n",
      "|    ent_coef_loss   | -0.108    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2509899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.14e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2520      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 14816     |\n",
      "|    total_timesteps | 2520000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 52.8      |\n",
      "|    ent_coef        | 0.314     |\n",
      "|    ent_coef_loss   | -0.722    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2519899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.14e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2530      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 14870     |\n",
      "|    total_timesteps | 2530000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 38        |\n",
      "|    ent_coef        | 0.315     |\n",
      "|    ent_coef_loss   | -0.458    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2529899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.16e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2540      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 14925     |\n",
      "|    total_timesteps | 2540000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 19.9      |\n",
      "|    ent_coef        | 0.304     |\n",
      "|    ent_coef_loss   | 0.351     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2539899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.17e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2550      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 14979     |\n",
      "|    total_timesteps | 2550000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 22.8      |\n",
      "|    ent_coef        | 0.309     |\n",
      "|    ent_coef_loss   | -0.0744   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2549899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.19e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2560      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15034     |\n",
      "|    total_timesteps | 2560000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 24.6      |\n",
      "|    ent_coef        | 0.311     |\n",
      "|    ent_coef_loss   | 0.0101    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2559899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.2e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 2570      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15088     |\n",
      "|    total_timesteps | 2570000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 40        |\n",
      "|    ent_coef        | 0.301     |\n",
      "|    ent_coef_loss   | -0.356    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2569899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.22e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2580      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15143     |\n",
      "|    total_timesteps | 2580000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 26.6      |\n",
      "|    ent_coef        | 0.306     |\n",
      "|    ent_coef_loss   | -0.227    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2579899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.22e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2590      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15197     |\n",
      "|    total_timesteps | 2590000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 34.5      |\n",
      "|    ent_coef        | 0.311     |\n",
      "|    ent_coef_loss   | -0.341    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2589899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.23e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2600      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15252     |\n",
      "|    total_timesteps | 2600000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+03 |\n",
      "|    critic_loss     | 48.8      |\n",
      "|    ent_coef        | 0.308     |\n",
      "|    ent_coef_loss   | 0.12      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2599899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.22e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2610      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15306     |\n",
      "|    total_timesteps | 2610000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 32.9      |\n",
      "|    ent_coef        | 0.305     |\n",
      "|    ent_coef_loss   | 0.0132    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2609899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.22e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2620      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15361     |\n",
      "|    total_timesteps | 2620000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05e+03 |\n",
      "|    critic_loss     | 18.3      |\n",
      "|    ent_coef        | 0.305     |\n",
      "|    ent_coef_loss   | -0.643    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2619899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.23e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2630      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15421     |\n",
      "|    total_timesteps | 2630000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+03 |\n",
      "|    critic_loss     | 21.6      |\n",
      "|    ent_coef        | 0.294     |\n",
      "|    ent_coef_loss   | -0.0872   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2629899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.23e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2640      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15478     |\n",
      "|    total_timesteps | 2640000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+03 |\n",
      "|    critic_loss     | 50.9      |\n",
      "|    ent_coef        | 0.296     |\n",
      "|    ent_coef_loss   | -0.429    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2639899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.23e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2650      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15535     |\n",
      "|    total_timesteps | 2650000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 26.7      |\n",
      "|    ent_coef        | 0.306     |\n",
      "|    ent_coef_loss   | 0.082     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2649899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.24e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2660      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15592     |\n",
      "|    total_timesteps | 2660000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+03 |\n",
      "|    critic_loss     | 17.7      |\n",
      "|    ent_coef        | 0.296     |\n",
      "|    ent_coef_loss   | 0.0978    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2659899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.24e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2670      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15649     |\n",
      "|    total_timesteps | 2670000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+03 |\n",
      "|    critic_loss     | 27.8      |\n",
      "|    ent_coef        | 0.305     |\n",
      "|    ent_coef_loss   | -0.04     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2669899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.25e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2680      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15706     |\n",
      "|    total_timesteps | 2680000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.07e+03 |\n",
      "|    critic_loss     | 17.2      |\n",
      "|    ent_coef        | 0.298     |\n",
      "|    ent_coef_loss   | -0.0087   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2679899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.25e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2690      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15763     |\n",
      "|    total_timesteps | 2690000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05e+03 |\n",
      "|    critic_loss     | 18        |\n",
      "|    ent_coef        | 0.286     |\n",
      "|    ent_coef_loss   | -0.0148   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2689899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.25e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2700      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15820     |\n",
      "|    total_timesteps | 2700000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.06e+03 |\n",
      "|    critic_loss     | 24.9      |\n",
      "|    ent_coef        | 0.293     |\n",
      "|    ent_coef_loss   | -0.125    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2699899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.26e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2710      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15877     |\n",
      "|    total_timesteps | 2710000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.06e+03 |\n",
      "|    critic_loss     | 16.2      |\n",
      "|    ent_coef        | 0.299     |\n",
      "|    ent_coef_loss   | 0.00785   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2709899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.26e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2720      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15934     |\n",
      "|    total_timesteps | 2720000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.06e+03 |\n",
      "|    critic_loss     | 29.8      |\n",
      "|    ent_coef        | 0.293     |\n",
      "|    ent_coef_loss   | -0.217    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2719899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.26e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2730      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 15991     |\n",
      "|    total_timesteps | 2730000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.06e+03 |\n",
      "|    critic_loss     | 12.4      |\n",
      "|    ent_coef        | 0.289     |\n",
      "|    ent_coef_loss   | 0.288     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2729899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.26e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2740      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16047     |\n",
      "|    total_timesteps | 2740000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.06e+03 |\n",
      "|    critic_loss     | 22.3      |\n",
      "|    ent_coef        | 0.29      |\n",
      "|    ent_coef_loss   | -0.0627   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2739899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.26e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2750      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16104     |\n",
      "|    total_timesteps | 2750000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.07e+03 |\n",
      "|    critic_loss     | 15.1      |\n",
      "|    ent_coef        | 0.294     |\n",
      "|    ent_coef_loss   | -0.624    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2749899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.26e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2760      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16162     |\n",
      "|    total_timesteps | 2760000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05e+03 |\n",
      "|    critic_loss     | 15.9      |\n",
      "|    ent_coef        | 0.283     |\n",
      "|    ent_coef_loss   | -0.387    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2759899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.26e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2770      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16219     |\n",
      "|    total_timesteps | 2770000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.07e+03 |\n",
      "|    critic_loss     | 23.2      |\n",
      "|    ent_coef        | 0.281     |\n",
      "|    ent_coef_loss   | -0.14     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2769899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.26e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2780      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16277     |\n",
      "|    total_timesteps | 2780000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.07e+03 |\n",
      "|    critic_loss     | 20.2      |\n",
      "|    ent_coef        | 0.284     |\n",
      "|    ent_coef_loss   | -0.16     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2779899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.26e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2790      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16334     |\n",
      "|    total_timesteps | 2790000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+03 |\n",
      "|    critic_loss     | 19.4      |\n",
      "|    ent_coef        | 0.284     |\n",
      "|    ent_coef_loss   | -0.000453 |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2789899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.27e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2800      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16391     |\n",
      "|    total_timesteps | 2800000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+03 |\n",
      "|    critic_loss     | 23.7      |\n",
      "|    ent_coef        | 0.278     |\n",
      "|    ent_coef_loss   | 0.0868    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2799899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.27e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2810      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16448     |\n",
      "|    total_timesteps | 2810000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.09e+03 |\n",
      "|    critic_loss     | 18.1      |\n",
      "|    ent_coef        | 0.282     |\n",
      "|    ent_coef_loss   | -0.152    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2809899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.27e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2820      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16506     |\n",
      "|    total_timesteps | 2820000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.09e+03 |\n",
      "|    critic_loss     | 18.7      |\n",
      "|    ent_coef        | 0.277     |\n",
      "|    ent_coef_loss   | 0.273     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2819899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.27e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2830      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16563     |\n",
      "|    total_timesteps | 2830000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+03 |\n",
      "|    critic_loss     | 19.8      |\n",
      "|    ent_coef        | 0.273     |\n",
      "|    ent_coef_loss   | -0.0507   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2829899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2840     |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 16620    |\n",
      "|    total_timesteps | 2840000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 28.7     |\n",
      "|    ent_coef        | 0.278    |\n",
      "|    ent_coef_loss   | -0.151   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2839899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.27e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2850      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16677     |\n",
      "|    total_timesteps | 2850000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.06e+03 |\n",
      "|    critic_loss     | 34.7      |\n",
      "|    ent_coef        | 0.275     |\n",
      "|    ent_coef_loss   | 0.277     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2849899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2860     |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 16734    |\n",
      "|    total_timesteps | 2860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 13.5     |\n",
      "|    ent_coef        | 0.269    |\n",
      "|    ent_coef_loss   | -0.179   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2859899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2870     |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 16791    |\n",
      "|    total_timesteps | 2870000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 16.6     |\n",
      "|    ent_coef        | 0.277    |\n",
      "|    ent_coef_loss   | 0.366    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2869899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.27e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2880      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 16848     |\n",
      "|    total_timesteps | 2880000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+03 |\n",
      "|    critic_loss     | 18.9      |\n",
      "|    ent_coef        | 0.269     |\n",
      "|    ent_coef_loss   | -0.431    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2879899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2890     |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 16905    |\n",
      "|    total_timesteps | 2890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 14.9     |\n",
      "|    ent_coef        | 0.275    |\n",
      "|    ent_coef_loss   | 1.17     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2889899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.27e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2900     |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 16962    |\n",
      "|    total_timesteps | 2900000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 19.8     |\n",
      "|    ent_coef        | 0.269    |\n",
      "|    ent_coef_loss   | -0.141   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2910      |\n",
      "|    fps             | 170       |\n",
      "|    time_elapsed    | 17019     |\n",
      "|    total_timesteps | 2910000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.09e+03 |\n",
      "|    critic_loss     | 20.8      |\n",
      "|    ent_coef        | 0.273     |\n",
      "|    ent_coef_loss   | -0.366    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2909899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2920     |\n",
      "|    fps             | 170      |\n",
      "|    time_elapsed    | 17076    |\n",
      "|    total_timesteps | 2920000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 19.1     |\n",
      "|    ent_coef        | 0.271    |\n",
      "|    ent_coef_loss   | -0.189   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2919899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2930      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17132     |\n",
      "|    total_timesteps | 2930000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+03 |\n",
      "|    critic_loss     | 17.5      |\n",
      "|    ent_coef        | 0.268     |\n",
      "|    ent_coef_loss   | -0.0266   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2929899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2940     |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 17190    |\n",
      "|    total_timesteps | 2940000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 66.8     |\n",
      "|    ent_coef        | 0.274    |\n",
      "|    ent_coef_loss   | 0.117    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2939899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2950     |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 17248    |\n",
      "|    total_timesteps | 2950000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 19.3     |\n",
      "|    ent_coef        | 0.266    |\n",
      "|    ent_coef_loss   | -0.087   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2949899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2960      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17305     |\n",
      "|    total_timesteps | 2960000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 26        |\n",
      "|    ent_coef        | 0.27      |\n",
      "|    ent_coef_loss   | 0.213     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2959899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 2970     |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 17362    |\n",
      "|    total_timesteps | 2970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.27     |\n",
      "|    ent_coef_loss   | 0.0747   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2969899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2980      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17420     |\n",
      "|    total_timesteps | 2980000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 22.5      |\n",
      "|    ent_coef        | 0.274     |\n",
      "|    ent_coef_loss   | -0.00607  |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2979899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2990      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17477     |\n",
      "|    total_timesteps | 2990000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 11.9      |\n",
      "|    ent_coef        | 0.269     |\n",
      "|    ent_coef_loss   | 0.153     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2989899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3000     |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 17534    |\n",
      "|    total_timesteps | 3000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 14       |\n",
      "|    ent_coef        | 0.274    |\n",
      "|    ent_coef_loss   | -0.342   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2999899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3010      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17591     |\n",
      "|    total_timesteps | 3010000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 16.8      |\n",
      "|    ent_coef        | 0.275     |\n",
      "|    ent_coef_loss   | 0.55      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3009899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3020     |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 17648    |\n",
      "|    total_timesteps | 3020000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 10.1     |\n",
      "|    ent_coef        | 0.262    |\n",
      "|    ent_coef_loss   | 0.0182   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3019899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3030      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17705     |\n",
      "|    total_timesteps | 3030000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 16.4      |\n",
      "|    ent_coef        | 0.264     |\n",
      "|    ent_coef_loss   | -0.16     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3029899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3040      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17763     |\n",
      "|    total_timesteps | 3040000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 11.3      |\n",
      "|    ent_coef        | 0.267     |\n",
      "|    ent_coef_loss   | 0.217     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3039899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3050      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17822     |\n",
      "|    total_timesteps | 3050000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 12.8      |\n",
      "|    ent_coef        | 0.266     |\n",
      "|    ent_coef_loss   | -0.065    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3049899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3060      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17881     |\n",
      "|    total_timesteps | 3060000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 14.9      |\n",
      "|    ent_coef        | 0.272     |\n",
      "|    ent_coef_loss   | -0.261    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3059899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3070      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17939     |\n",
      "|    total_timesteps | 3070000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 8.74      |\n",
      "|    ent_coef        | 0.26      |\n",
      "|    ent_coef_loss   | -0.0758   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3069899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3080      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 17996     |\n",
      "|    total_timesteps | 3080000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 21.9      |\n",
      "|    ent_coef        | 0.265     |\n",
      "|    ent_coef_loss   | 0.465     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3079899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.27e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3090      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18053     |\n",
      "|    total_timesteps | 3090000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 25.7      |\n",
      "|    ent_coef        | 0.269     |\n",
      "|    ent_coef_loss   | 0.138     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3089899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.27e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3100      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18110     |\n",
      "|    total_timesteps | 3100000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 83.6      |\n",
      "|    ent_coef        | 0.261     |\n",
      "|    ent_coef_loss   | -0.277    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.27e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3110      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18167     |\n",
      "|    total_timesteps | 3110000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 10.2      |\n",
      "|    ent_coef        | 0.266     |\n",
      "|    ent_coef_loss   | -0.364    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3109899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3120      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18224     |\n",
      "|    total_timesteps | 3120000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 11.7      |\n",
      "|    ent_coef        | 0.263     |\n",
      "|    ent_coef_loss   | -0.486    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3119899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3130      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18281     |\n",
      "|    total_timesteps | 3130000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 20.4      |\n",
      "|    ent_coef        | 0.259     |\n",
      "|    ent_coef_loss   | 0.0467    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3129899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.28e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3140     |\n",
      "|    fps             | 171      |\n",
      "|    time_elapsed    | 18339    |\n",
      "|    total_timesteps | 3140000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.1e+03 |\n",
      "|    critic_loss     | 11.2     |\n",
      "|    ent_coef        | 0.268    |\n",
      "|    ent_coef_loss   | -0.478   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3139899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3150      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18395     |\n",
      "|    total_timesteps | 3150000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 17.3      |\n",
      "|    ent_coef        | 0.266     |\n",
      "|    ent_coef_loss   | 0.145     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3149899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3160      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18452     |\n",
      "|    total_timesteps | 3160000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 13.8      |\n",
      "|    ent_coef        | 0.265     |\n",
      "|    ent_coef_loss   | -0.0362   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3159899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3170      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18510     |\n",
      "|    total_timesteps | 3170000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 10.2      |\n",
      "|    ent_coef        | 0.259     |\n",
      "|    ent_coef_loss   | 0.165     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3169899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.28e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3180      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18566     |\n",
      "|    total_timesteps | 3180000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 15.4      |\n",
      "|    ent_coef        | 0.261     |\n",
      "|    ent_coef_loss   | 0.434     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3179899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3190      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18623     |\n",
      "|    total_timesteps | 3190000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 13.2      |\n",
      "|    ent_coef        | 0.27      |\n",
      "|    ent_coef_loss   | -0.109    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3189899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3200      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18680     |\n",
      "|    total_timesteps | 3200000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 11.2      |\n",
      "|    ent_coef        | 0.261     |\n",
      "|    ent_coef_loss   | -0.115    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3199899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3210      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18737     |\n",
      "|    total_timesteps | 3210000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 30        |\n",
      "|    ent_coef        | 0.262     |\n",
      "|    ent_coef_loss   | 0.882     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3209899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3220      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18794     |\n",
      "|    total_timesteps | 3220000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 20.8      |\n",
      "|    ent_coef        | 0.274     |\n",
      "|    ent_coef_loss   | -0.109    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3219899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3230      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18852     |\n",
      "|    total_timesteps | 3230000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 22.7      |\n",
      "|    ent_coef        | 0.267     |\n",
      "|    ent_coef_loss   | 0.0851    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3229899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3240      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18909     |\n",
      "|    total_timesteps | 3240000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 10.9      |\n",
      "|    ent_coef        | 0.265     |\n",
      "|    ent_coef_loss   | 0.248     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3239899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3250      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 18966     |\n",
      "|    total_timesteps | 3250000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 14.7      |\n",
      "|    ent_coef        | 0.275     |\n",
      "|    ent_coef_loss   | -0.0954   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3249899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3260      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19023     |\n",
      "|    total_timesteps | 3260000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 17.1      |\n",
      "|    ent_coef        | 0.271     |\n",
      "|    ent_coef_loss   | -0.281    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3259899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3270      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19080     |\n",
      "|    total_timesteps | 3270000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 15.4      |\n",
      "|    ent_coef        | 0.271     |\n",
      "|    ent_coef_loss   | -0.0338   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3269899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3280      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19137     |\n",
      "|    total_timesteps | 3280000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 12.7      |\n",
      "|    ent_coef        | 0.262     |\n",
      "|    ent_coef_loss   | -0.2      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3279899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3290      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19193     |\n",
      "|    total_timesteps | 3290000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 34.8      |\n",
      "|    ent_coef        | 0.279     |\n",
      "|    ent_coef_loss   | 0.231     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3289899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3300      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19250     |\n",
      "|    total_timesteps | 3300000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 18.2      |\n",
      "|    ent_coef        | 0.275     |\n",
      "|    ent_coef_loss   | 0.226     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3299899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3310      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19307     |\n",
      "|    total_timesteps | 3310000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 18        |\n",
      "|    ent_coef        | 0.268     |\n",
      "|    ent_coef_loss   | -0.00664  |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3309899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3320      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19364     |\n",
      "|    total_timesteps | 3320000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+03 |\n",
      "|    critic_loss     | 12.8      |\n",
      "|    ent_coef        | 0.269     |\n",
      "|    ent_coef_loss   | -0.188    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3319899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3330      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19421     |\n",
      "|    total_timesteps | 3330000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 24.9      |\n",
      "|    ent_coef        | 0.275     |\n",
      "|    ent_coef_loss   | 0.123     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3329899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3340      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19475     |\n",
      "|    total_timesteps | 3340000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 10.1      |\n",
      "|    ent_coef        | 0.264     |\n",
      "|    ent_coef_loss   | -0.202    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3339899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3350      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19530     |\n",
      "|    total_timesteps | 3350000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 25.4      |\n",
      "|    ent_coef        | 0.278     |\n",
      "|    ent_coef_loss   | -0.155    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3349899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3360      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19584     |\n",
      "|    total_timesteps | 3360000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 13        |\n",
      "|    ent_coef        | 0.269     |\n",
      "|    ent_coef_loss   | -0.363    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3359899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3370      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19639     |\n",
      "|    total_timesteps | 3370000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 31.6      |\n",
      "|    ent_coef        | 0.274     |\n",
      "|    ent_coef_loss   | -0.0457   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3369899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3380      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19694     |\n",
      "|    total_timesteps | 3380000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.15e+03 |\n",
      "|    critic_loss     | 12.9      |\n",
      "|    ent_coef        | 0.279     |\n",
      "|    ent_coef_loss   | 0.364     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3379899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3390      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19748     |\n",
      "|    total_timesteps | 3390000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 15.2      |\n",
      "|    ent_coef        | 0.27      |\n",
      "|    ent_coef_loss   | 0.481     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3389899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3400      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19803     |\n",
      "|    total_timesteps | 3400000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 31.4      |\n",
      "|    ent_coef        | 0.28      |\n",
      "|    ent_coef_loss   | 0.0522    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3399899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3410      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19857     |\n",
      "|    total_timesteps | 3410000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 18        |\n",
      "|    ent_coef        | 0.273     |\n",
      "|    ent_coef_loss   | -0.239    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3409899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3420      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19912     |\n",
      "|    total_timesteps | 3420000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 7.78      |\n",
      "|    ent_coef        | 0.276     |\n",
      "|    ent_coef_loss   | -0.264    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3419899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3430      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 19966     |\n",
      "|    total_timesteps | 3430000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.15e+03 |\n",
      "|    critic_loss     | 10.3      |\n",
      "|    ent_coef        | 0.273     |\n",
      "|    ent_coef_loss   | -0.388    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3429899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3440      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20020     |\n",
      "|    total_timesteps | 3440000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 11.2      |\n",
      "|    ent_coef        | 0.274     |\n",
      "|    ent_coef_loss   | 0.361     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3439899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3450      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20075     |\n",
      "|    total_timesteps | 3450000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.15e+03 |\n",
      "|    critic_loss     | 57.8      |\n",
      "|    ent_coef        | 0.271     |\n",
      "|    ent_coef_loss   | -0.242    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3449899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3460      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20129     |\n",
      "|    total_timesteps | 3460000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.15e+03 |\n",
      "|    critic_loss     | 16.4      |\n",
      "|    ent_coef        | 0.28      |\n",
      "|    ent_coef_loss   | 0.422     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3459899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3470      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20185     |\n",
      "|    total_timesteps | 3470000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.15e+03 |\n",
      "|    critic_loss     | 43.4      |\n",
      "|    ent_coef        | 0.27      |\n",
      "|    ent_coef_loss   | -0.393    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3469899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.29e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3480      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20242     |\n",
      "|    total_timesteps | 3480000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.15e+03 |\n",
      "|    critic_loss     | 20.6      |\n",
      "|    ent_coef        | 0.275     |\n",
      "|    ent_coef_loss   | -0.0676   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3479899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3490      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20299     |\n",
      "|    total_timesteps | 3490000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+03 |\n",
      "|    critic_loss     | 14.2      |\n",
      "|    ent_coef        | 0.276     |\n",
      "|    ent_coef_loss   | -0.169    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3489899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3500      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20356     |\n",
      "|    total_timesteps | 3500000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 11.4      |\n",
      "|    ent_coef        | 0.279     |\n",
      "|    ent_coef_loss   | 0.215     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3499899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.3e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 3510      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20413     |\n",
      "|    total_timesteps | 3510000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.15e+03 |\n",
      "|    critic_loss     | 12.3      |\n",
      "|    ent_coef        | 0.279     |\n",
      "|    ent_coef_loss   | 0.124     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3509899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.31e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3520      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20470     |\n",
      "|    total_timesteps | 3520000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 10.1      |\n",
      "|    ent_coef        | 0.276     |\n",
      "|    ent_coef_loss   | 0.354     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3519899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.31e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3530      |\n",
      "|    fps             | 171       |\n",
      "|    time_elapsed    | 20524     |\n",
      "|    total_timesteps | 3530000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 11.4      |\n",
      "|    ent_coef        | 0.284     |\n",
      "|    ent_coef_loss   | 0.173     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3529899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.31e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3540      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 20579     |\n",
      "|    total_timesteps | 3540000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 8.53      |\n",
      "|    ent_coef        | 0.277     |\n",
      "|    ent_coef_loss   | -0.157    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3539899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3550      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 20633     |\n",
      "|    total_timesteps | 3550000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 14.1      |\n",
      "|    ent_coef        | 0.281     |\n",
      "|    ent_coef_loss   | -0.435    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3549899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3560      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 20688     |\n",
      "|    total_timesteps | 3560000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 13.6      |\n",
      "|    ent_coef        | 0.274     |\n",
      "|    ent_coef_loss   | -0.111    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3559899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3570      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 20743     |\n",
      "|    total_timesteps | 3570000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 11.7      |\n",
      "|    ent_coef        | 0.27      |\n",
      "|    ent_coef_loss   | -0.0279   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3569899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3580      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 20800     |\n",
      "|    total_timesteps | 3580000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 11.5      |\n",
      "|    ent_coef        | 0.275     |\n",
      "|    ent_coef_loss   | 0.247     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3579899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3590      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 20857     |\n",
      "|    total_timesteps | 3590000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 8.1       |\n",
      "|    ent_coef        | 0.281     |\n",
      "|    ent_coef_loss   | -0.372    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3589899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3600      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 20914     |\n",
      "|    total_timesteps | 3600000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 15.5      |\n",
      "|    ent_coef        | 0.275     |\n",
      "|    ent_coef_loss   | 0.316     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3599899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3610      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 20971     |\n",
      "|    total_timesteps | 3610000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 5.97      |\n",
      "|    ent_coef        | 0.282     |\n",
      "|    ent_coef_loss   | 0.465     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3609899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3620      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21028     |\n",
      "|    total_timesteps | 3620000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 48.3      |\n",
      "|    ent_coef        | 0.282     |\n",
      "|    ent_coef_loss   | 0.101     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3619899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3630      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21085     |\n",
      "|    total_timesteps | 3630000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 19.7      |\n",
      "|    ent_coef        | 0.285     |\n",
      "|    ent_coef_loss   | 0.119     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3629899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3640      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21143     |\n",
      "|    total_timesteps | 3640000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 17.9      |\n",
      "|    ent_coef        | 0.277     |\n",
      "|    ent_coef_loss   | -0.329    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3639899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3650      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21202     |\n",
      "|    total_timesteps | 3650000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 27.9      |\n",
      "|    ent_coef        | 0.274     |\n",
      "|    ent_coef_loss   | -0.23     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3649899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3660      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21259     |\n",
      "|    total_timesteps | 3660000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 16.1      |\n",
      "|    ent_coef        | 0.276     |\n",
      "|    ent_coef_loss   | -0.248    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3659899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3670      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21316     |\n",
      "|    total_timesteps | 3670000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.16e+03 |\n",
      "|    critic_loss     | 15.5      |\n",
      "|    ent_coef        | 0.279     |\n",
      "|    ent_coef_loss   | -0.109    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3669899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3680      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21373     |\n",
      "|    total_timesteps | 3680000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 10.1      |\n",
      "|    ent_coef        | 0.278     |\n",
      "|    ent_coef_loss   | 0.611     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3679899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3690      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21430     |\n",
      "|    total_timesteps | 3690000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 11.5      |\n",
      "|    ent_coef        | 0.283     |\n",
      "|    ent_coef_loss   | 0.124     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3689899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3700      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21487     |\n",
      "|    total_timesteps | 3700000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 9.23      |\n",
      "|    ent_coef        | 0.281     |\n",
      "|    ent_coef_loss   | -0.443    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3699899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3710      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21544     |\n",
      "|    total_timesteps | 3710000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 33.8      |\n",
      "|    ent_coef        | 0.291     |\n",
      "|    ent_coef_loss   | -0.252    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3709899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3720      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21601     |\n",
      "|    total_timesteps | 3720000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 19.7      |\n",
      "|    ent_coef        | 0.286     |\n",
      "|    ent_coef_loss   | 0.71      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3719899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3730      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21659     |\n",
      "|    total_timesteps | 3730000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 10.6      |\n",
      "|    ent_coef        | 0.278     |\n",
      "|    ent_coef_loss   | -0.442    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3729899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3740      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21716     |\n",
      "|    total_timesteps | 3740000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 18        |\n",
      "|    ent_coef        | 0.286     |\n",
      "|    ent_coef_loss   | 0.561     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3739899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3750      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21773     |\n",
      "|    total_timesteps | 3750000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 8.26      |\n",
      "|    ent_coef        | 0.279     |\n",
      "|    ent_coef_loss   | -0.00752  |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3749899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3760      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21830     |\n",
      "|    total_timesteps | 3760000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 7.6       |\n",
      "|    ent_coef        | 0.293     |\n",
      "|    ent_coef_loss   | -0.127    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3759899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3770      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21887     |\n",
      "|    total_timesteps | 3770000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 9.6       |\n",
      "|    ent_coef        | 0.288     |\n",
      "|    ent_coef_loss   | 0.195     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3769899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3780      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 21945     |\n",
      "|    total_timesteps | 3780000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 6.02      |\n",
      "|    ent_coef        | 0.286     |\n",
      "|    ent_coef_loss   | -0.184    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3779899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3790      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22006     |\n",
      "|    total_timesteps | 3790000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 17.1      |\n",
      "|    ent_coef        | 0.3       |\n",
      "|    ent_coef_loss   | 0.0884    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3789899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3800      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22065     |\n",
      "|    total_timesteps | 3800000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 47.3      |\n",
      "|    ent_coef        | 0.294     |\n",
      "|    ent_coef_loss   | -0.0651   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3799899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3810      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22123     |\n",
      "|    total_timesteps | 3810000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 12.7      |\n",
      "|    ent_coef        | 0.289     |\n",
      "|    ent_coef_loss   | -0.0251   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3809899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3820      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22181     |\n",
      "|    total_timesteps | 3820000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 17.3      |\n",
      "|    ent_coef        | 0.288     |\n",
      "|    ent_coef_loss   | 0.163     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3819899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.32e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3830      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22238     |\n",
      "|    total_timesteps | 3830000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 17.4      |\n",
      "|    ent_coef        | 0.296     |\n",
      "|    ent_coef_loss   | 0.244     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3829899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3840      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22296     |\n",
      "|    total_timesteps | 3840000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 24.3      |\n",
      "|    ent_coef        | 0.296     |\n",
      "|    ent_coef_loss   | -0.156    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3839899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.33e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3850      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22353     |\n",
      "|    total_timesteps | 3850000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 24.7      |\n",
      "|    ent_coef        | 0.299     |\n",
      "|    ent_coef_loss   | -0.452    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3849899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.33e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3860     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 22411    |\n",
      "|    total_timesteps | 3860000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 8.9      |\n",
      "|    ent_coef        | 0.302    |\n",
      "|    ent_coef_loss   | -0.299   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3859899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.34e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3870      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22469     |\n",
      "|    total_timesteps | 3870000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 16.1      |\n",
      "|    ent_coef        | 0.299     |\n",
      "|    ent_coef_loss   | 0.124     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3869899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.34e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3880      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22527     |\n",
      "|    total_timesteps | 3880000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 13.1      |\n",
      "|    ent_coef        | 0.296     |\n",
      "|    ent_coef_loss   | -0.164    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3879899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.34e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3890      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22583     |\n",
      "|    total_timesteps | 3890000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 7.29      |\n",
      "|    ent_coef        | 0.297     |\n",
      "|    ent_coef_loss   | 0.0539    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3889899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.34e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3900      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22640     |\n",
      "|    total_timesteps | 3900000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 23.1      |\n",
      "|    ent_coef        | 0.305     |\n",
      "|    ent_coef_loss   | 0.352     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3899899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.34e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3910      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22698     |\n",
      "|    total_timesteps | 3910000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 12        |\n",
      "|    ent_coef        | 0.305     |\n",
      "|    ent_coef_loss   | -0.122    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3909899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.34e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3920      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22757     |\n",
      "|    total_timesteps | 3920000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 10.6      |\n",
      "|    ent_coef        | 0.305     |\n",
      "|    ent_coef_loss   | 0.281     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3919899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.35e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3930      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22813     |\n",
      "|    total_timesteps | 3930000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 14.3      |\n",
      "|    ent_coef        | 0.305     |\n",
      "|    ent_coef_loss   | 0.886     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3929899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.35e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3940      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22871     |\n",
      "|    total_timesteps | 3940000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 10.5      |\n",
      "|    ent_coef        | 0.31      |\n",
      "|    ent_coef_loss   | -0.151    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3939899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.35e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3950      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22929     |\n",
      "|    total_timesteps | 3950000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 7.21      |\n",
      "|    ent_coef        | 0.306     |\n",
      "|    ent_coef_loss   | -0.214    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3949899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.36e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3960      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 22989     |\n",
      "|    total_timesteps | 3960000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 11.6      |\n",
      "|    ent_coef        | 0.309     |\n",
      "|    ent_coef_loss   | 0.0135    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3959899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.36e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3970      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23046     |\n",
      "|    total_timesteps | 3970000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 9.43      |\n",
      "|    ent_coef        | 0.32      |\n",
      "|    ent_coef_loss   | -0.307    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3969899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.36e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 3980      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23108     |\n",
      "|    total_timesteps | 3980000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 7.78      |\n",
      "|    ent_coef        | 0.31      |\n",
      "|    ent_coef_loss   | 0.275     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3979899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.36e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 3990     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 23169    |\n",
      "|    total_timesteps | 3990000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 25.7     |\n",
      "|    ent_coef        | 0.314    |\n",
      "|    ent_coef_loss   | 0.117    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3989899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4000      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23226     |\n",
      "|    total_timesteps | 4000000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 9         |\n",
      "|    ent_coef        | 0.315     |\n",
      "|    ent_coef_loss   | -0.00603  |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3999899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4010      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23286     |\n",
      "|    total_timesteps | 4010000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 18.6      |\n",
      "|    ent_coef        | 0.318     |\n",
      "|    ent_coef_loss   | -0.498    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4009899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4020      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23343     |\n",
      "|    total_timesteps | 4020000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 14.1      |\n",
      "|    ent_coef        | 0.327     |\n",
      "|    ent_coef_loss   | 0.344     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4019899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4030      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23400     |\n",
      "|    total_timesteps | 4030000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 13.5      |\n",
      "|    ent_coef        | 0.317     |\n",
      "|    ent_coef_loss   | -0.114    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4029899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.38e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4040      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23457     |\n",
      "|    total_timesteps | 4040000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 31.1      |\n",
      "|    ent_coef        | 0.323     |\n",
      "|    ent_coef_loss   | 0.277     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4039899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.38e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4050      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23514     |\n",
      "|    total_timesteps | 4050000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 15.6      |\n",
      "|    ent_coef        | 0.322     |\n",
      "|    ent_coef_loss   | -0.183    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4049899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.38e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4060      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23571     |\n",
      "|    total_timesteps | 4060000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 12.3      |\n",
      "|    ent_coef        | 0.328     |\n",
      "|    ent_coef_loss   | 0.0964    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4059899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4070      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23628     |\n",
      "|    total_timesteps | 4070000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 16.4      |\n",
      "|    ent_coef        | 0.321     |\n",
      "|    ent_coef_loss   | -0.384    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4069899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4080      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23685     |\n",
      "|    total_timesteps | 4080000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 13.2      |\n",
      "|    ent_coef        | 0.327     |\n",
      "|    ent_coef_loss   | -0.0417   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4079899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4090      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23742     |\n",
      "|    total_timesteps | 4090000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 21        |\n",
      "|    ent_coef        | 0.336     |\n",
      "|    ent_coef_loss   | 0.063     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4089899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4100      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23799     |\n",
      "|    total_timesteps | 4100000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 18.8      |\n",
      "|    ent_coef        | 0.348     |\n",
      "|    ent_coef_loss   | -0.156    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4099899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4110      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23856     |\n",
      "|    total_timesteps | 4110000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 18.4      |\n",
      "|    ent_coef        | 0.345     |\n",
      "|    ent_coef_loss   | -0.146    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4109899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4120      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23913     |\n",
      "|    total_timesteps | 4120000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 19.7      |\n",
      "|    ent_coef        | 0.345     |\n",
      "|    ent_coef_loss   | -0.0138   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4119899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4130      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 23970     |\n",
      "|    total_timesteps | 4130000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 18.4      |\n",
      "|    ent_coef        | 0.35      |\n",
      "|    ent_coef_loss   | -0.253    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4129899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4140      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24027     |\n",
      "|    total_timesteps | 4140000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 30.5      |\n",
      "|    ent_coef        | 0.342     |\n",
      "|    ent_coef_loss   | 0.521     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4139899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4150      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24083     |\n",
      "|    total_timesteps | 4150000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 24.6      |\n",
      "|    ent_coef        | 0.349     |\n",
      "|    ent_coef_loss   | -0.0681   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4149899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.37e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4160      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24140     |\n",
      "|    total_timesteps | 4160000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 18.3      |\n",
      "|    ent_coef        | 0.352     |\n",
      "|    ent_coef_loss   | -0.593    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4159899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.38e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4170      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24197     |\n",
      "|    total_timesteps | 4170000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 30.1      |\n",
      "|    ent_coef        | 0.353     |\n",
      "|    ent_coef_loss   | -0.179    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4169899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.38e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4180      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24254     |\n",
      "|    total_timesteps | 4180000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 14.9      |\n",
      "|    ent_coef        | 0.346     |\n",
      "|    ent_coef_loss   | -0.46     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4179899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.39e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4190      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24310     |\n",
      "|    total_timesteps | 4190000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 13.6      |\n",
      "|    ent_coef        | 0.35      |\n",
      "|    ent_coef_loss   | 0.37      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4189899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.39e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4200      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24367     |\n",
      "|    total_timesteps | 4200000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 18.4      |\n",
      "|    ent_coef        | 0.34      |\n",
      "|    ent_coef_loss   | -0.414    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4199899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.38e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4210      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24425     |\n",
      "|    total_timesteps | 4210000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 15.9      |\n",
      "|    ent_coef        | 0.353     |\n",
      "|    ent_coef_loss   | -0.469    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4209899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.38e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4220      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24483     |\n",
      "|    total_timesteps | 4220000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 23.5      |\n",
      "|    ent_coef        | 0.351     |\n",
      "|    ent_coef_loss   | 0.254     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4219899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.39e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4230      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24540     |\n",
      "|    total_timesteps | 4230000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 93.6      |\n",
      "|    ent_coef        | 0.349     |\n",
      "|    ent_coef_loss   | -0.0853   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4229899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.39e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4240     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 24597    |\n",
      "|    total_timesteps | 4240000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.359    |\n",
      "|    ent_coef_loss   | -0.686   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4239899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.39e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4250      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24655     |\n",
      "|    total_timesteps | 4250000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 14.5      |\n",
      "|    ent_coef        | 0.347     |\n",
      "|    ent_coef_loss   | -0.309    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4249899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.39e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4260      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24712     |\n",
      "|    total_timesteps | 4260000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 43        |\n",
      "|    ent_coef        | 0.358     |\n",
      "|    ent_coef_loss   | 0.256     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4259899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.39e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4270      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24768     |\n",
      "|    total_timesteps | 4270000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 53.2      |\n",
      "|    ent_coef        | 0.357     |\n",
      "|    ent_coef_loss   | 0.317     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4269899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.39e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4280      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24826     |\n",
      "|    total_timesteps | 4280000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 14.7      |\n",
      "|    ent_coef        | 0.358     |\n",
      "|    ent_coef_loss   | -0.116    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4279899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.39e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4290      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24883     |\n",
      "|    total_timesteps | 4290000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 19.1      |\n",
      "|    ent_coef        | 0.364     |\n",
      "|    ent_coef_loss   | 0.31      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4289899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.4e+04   |\n",
      "| time/              |           |\n",
      "|    episodes        | 4300      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24941     |\n",
      "|    total_timesteps | 4300000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 69.8      |\n",
      "|    ent_coef        | 0.361     |\n",
      "|    ent_coef_loss   | 0.217     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4299899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.41e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4310      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 24998     |\n",
      "|    total_timesteps | 4310000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 27.6      |\n",
      "|    ent_coef        | 0.361     |\n",
      "|    ent_coef_loss   | -0.491    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4309899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.41e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4320     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 25054    |\n",
      "|    total_timesteps | 4320000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 18.4     |\n",
      "|    ent_coef        | 0.371    |\n",
      "|    ent_coef_loss   | 0.16     |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4319899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.41e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4330      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25117     |\n",
      "|    total_timesteps | 4330000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 16.1      |\n",
      "|    ent_coef        | 0.362     |\n",
      "|    ent_coef_loss   | 0.771     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4329899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.41e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4340     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 25179    |\n",
      "|    total_timesteps | 4340000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 10.7     |\n",
      "|    ent_coef        | 0.37     |\n",
      "|    ent_coef_loss   | -0.272   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4339899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.41e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4350      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25243     |\n",
      "|    total_timesteps | 4350000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 22.1      |\n",
      "|    ent_coef        | 0.376     |\n",
      "|    ent_coef_loss   | 0.254     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4349899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.41e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4360      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25307     |\n",
      "|    total_timesteps | 4360000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 12.8      |\n",
      "|    ent_coef        | 0.372     |\n",
      "|    ent_coef_loss   | 0.15      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4359899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4370     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 25365    |\n",
      "|    total_timesteps | 4370000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 49.7     |\n",
      "|    ent_coef        | 0.363    |\n",
      "|    ent_coef_loss   | 0.235    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4369899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4380      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25422     |\n",
      "|    total_timesteps | 4380000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 15.9      |\n",
      "|    ent_coef        | 0.376     |\n",
      "|    ent_coef_loss   | 0.227     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4379899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4390      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25479     |\n",
      "|    total_timesteps | 4390000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 27.8      |\n",
      "|    ent_coef        | 0.381     |\n",
      "|    ent_coef_loss   | -0.0456   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4389899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4400      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25536     |\n",
      "|    total_timesteps | 4400000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 26.8      |\n",
      "|    ent_coef        | 0.372     |\n",
      "|    ent_coef_loss   | -0.12     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4399899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.43e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4410      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25593     |\n",
      "|    total_timesteps | 4410000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 18.5      |\n",
      "|    ent_coef        | 0.373     |\n",
      "|    ent_coef_loss   | 0.508     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4409899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4420      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25650     |\n",
      "|    total_timesteps | 4420000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 39.8      |\n",
      "|    ent_coef        | 0.375     |\n",
      "|    ent_coef_loss   | 0.438     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4419899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4430      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25707     |\n",
      "|    total_timesteps | 4430000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 25.3      |\n",
      "|    ent_coef        | 0.391     |\n",
      "|    ent_coef_loss   | -0.503    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4429899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.43e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4440      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25764     |\n",
      "|    total_timesteps | 4440000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 13.8      |\n",
      "|    ent_coef        | 0.38      |\n",
      "|    ent_coef_loss   | -0.123    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4439899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.43e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4450      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25822     |\n",
      "|    total_timesteps | 4450000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 20        |\n",
      "|    ent_coef        | 0.387     |\n",
      "|    ent_coef_loss   | 0.311     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4449899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4460      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25879     |\n",
      "|    total_timesteps | 4460000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 16.8      |\n",
      "|    ent_coef        | 0.391     |\n",
      "|    ent_coef_loss   | -0.343    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4459899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4470      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25935     |\n",
      "|    total_timesteps | 4470000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 27.1      |\n",
      "|    ent_coef        | 0.389     |\n",
      "|    ent_coef_loss   | -0.0327   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4469899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4480      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 25992     |\n",
      "|    total_timesteps | 4480000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 27.5      |\n",
      "|    ent_coef        | 0.397     |\n",
      "|    ent_coef_loss   | 0.0998    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4479899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4490      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26049     |\n",
      "|    total_timesteps | 4490000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 28.2      |\n",
      "|    ent_coef        | 0.399     |\n",
      "|    ent_coef_loss   | -0.000307 |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4489899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4500      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26106     |\n",
      "|    total_timesteps | 4500000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 39.3      |\n",
      "|    ent_coef        | 0.403     |\n",
      "|    ent_coef_loss   | 0.478     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4499899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4510     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 26163    |\n",
      "|    total_timesteps | 4510000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 24.9     |\n",
      "|    ent_coef        | 0.398    |\n",
      "|    ent_coef_loss   | -0.0862  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4509899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.43e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4520      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26221     |\n",
      "|    total_timesteps | 4520000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 22.3      |\n",
      "|    ent_coef        | 0.402     |\n",
      "|    ent_coef_loss   | 0.105     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4519899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.43e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4530     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 26278    |\n",
      "|    total_timesteps | 4530000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 20.5     |\n",
      "|    ent_coef        | 0.4      |\n",
      "|    ent_coef_loss   | 0.173    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4529899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.43e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4540     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 26335    |\n",
      "|    total_timesteps | 4540000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 31.9     |\n",
      "|    ent_coef        | 0.412    |\n",
      "|    ent_coef_loss   | -0.0838  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4539899  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.43e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4550     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 26391    |\n",
      "|    total_timesteps | 4550000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 14.8     |\n",
      "|    ent_coef        | 0.399    |\n",
      "|    ent_coef_loss   | -0.0287  |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4549899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4560      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26448     |\n",
      "|    total_timesteps | 4560000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 59.2      |\n",
      "|    ent_coef        | 0.4       |\n",
      "|    ent_coef_loss   | -0.026    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4559899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.45e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4570     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 26508    |\n",
      "|    total_timesteps | 4570000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 57.9     |\n",
      "|    ent_coef        | 0.41     |\n",
      "|    ent_coef_loss   | -0.21    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4569899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4580      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26564     |\n",
      "|    total_timesteps | 4580000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 38.6      |\n",
      "|    ent_coef        | 0.413     |\n",
      "|    ent_coef_loss   | 0.156     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4579899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4590      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26621     |\n",
      "|    total_timesteps | 4590000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 20.9      |\n",
      "|    ent_coef        | 0.407     |\n",
      "|    ent_coef_loss   | 0.0625    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4589899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.45e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4600     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 26680    |\n",
      "|    total_timesteps | 4600000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 19.6     |\n",
      "|    ent_coef        | 0.397    |\n",
      "|    ent_coef_loss   | -0.237   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4599899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4610      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26737     |\n",
      "|    total_timesteps | 4610000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 34.2      |\n",
      "|    ent_coef        | 0.422     |\n",
      "|    ent_coef_loss   | -0.0518   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4609899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4620      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26794     |\n",
      "|    total_timesteps | 4620000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 47.9      |\n",
      "|    ent_coef        | 0.416     |\n",
      "|    ent_coef_loss   | 0.139     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4619899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4630      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26853     |\n",
      "|    total_timesteps | 4630000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 27.2      |\n",
      "|    ent_coef        | 0.402     |\n",
      "|    ent_coef_loss   | 0.232     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4629899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4640      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26911     |\n",
      "|    total_timesteps | 4640000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 21.5      |\n",
      "|    ent_coef        | 0.411     |\n",
      "|    ent_coef_loss   | -0.0479   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4639899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4650      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 26968     |\n",
      "|    total_timesteps | 4650000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 42.7      |\n",
      "|    ent_coef        | 0.429     |\n",
      "|    ent_coef_loss   | 0.138     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4649899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4660      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27025     |\n",
      "|    total_timesteps | 4660000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 16        |\n",
      "|    ent_coef        | 0.417     |\n",
      "|    ent_coef_loss   | 0.111     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4659899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.46e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4670      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27084     |\n",
      "|    total_timesteps | 4670000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 16.3      |\n",
      "|    ent_coef        | 0.405     |\n",
      "|    ent_coef_loss   | 0.337     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4669899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4680      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27142     |\n",
      "|    total_timesteps | 4680000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 16.6      |\n",
      "|    ent_coef        | 0.422     |\n",
      "|    ent_coef_loss   | 0.0283    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4679899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4690      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27199     |\n",
      "|    total_timesteps | 4690000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 34.9      |\n",
      "|    ent_coef        | 0.437     |\n",
      "|    ent_coef_loss   | -0.0276   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4689899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4700      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27258     |\n",
      "|    total_timesteps | 4700000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 16        |\n",
      "|    ent_coef        | 0.426     |\n",
      "|    ent_coef_loss   | -0.34     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4699899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4710      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27315     |\n",
      "|    total_timesteps | 4710000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 25.6      |\n",
      "|    ent_coef        | 0.416     |\n",
      "|    ent_coef_loss   | 0.208     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4709899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4720      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27372     |\n",
      "|    total_timesteps | 4720000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 32.8      |\n",
      "|    ent_coef        | 0.422     |\n",
      "|    ent_coef_loss   | 0.33      |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4719899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4730      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27430     |\n",
      "|    total_timesteps | 4730000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 16.5      |\n",
      "|    ent_coef        | 0.43      |\n",
      "|    ent_coef_loss   | 0.00855   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4729899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4740      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27487     |\n",
      "|    total_timesteps | 4740000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 19.2      |\n",
      "|    ent_coef        | 0.436     |\n",
      "|    ent_coef_loss   | 0.369     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4739899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.44e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4750      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27544     |\n",
      "|    total_timesteps | 4750000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 23.1      |\n",
      "|    ent_coef        | 0.446     |\n",
      "|    ent_coef_loss   | -0.314    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4749899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.44e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4760      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27602     |\n",
      "|    total_timesteps | 4760000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 35.6      |\n",
      "|    ent_coef        | 0.442     |\n",
      "|    ent_coef_loss   | 0.115     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4759899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.44e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4770      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27661     |\n",
      "|    total_timesteps | 4770000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.23e+03 |\n",
      "|    critic_loss     | 22.3      |\n",
      "|    ent_coef        | 0.437     |\n",
      "|    ent_coef_loss   | -0.0811   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4769899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.44e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4780      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27718     |\n",
      "|    total_timesteps | 4780000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.23e+03 |\n",
      "|    critic_loss     | 34.2      |\n",
      "|    ent_coef        | 0.444     |\n",
      "|    ent_coef_loss   | 0.00762   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4779899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.44e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4790      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27776     |\n",
      "|    total_timesteps | 4790000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 27.5      |\n",
      "|    ent_coef        | 0.447     |\n",
      "|    ent_coef_loss   | -0.038    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4789899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.45e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4800     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 27835    |\n",
      "|    total_timesteps | 4800000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 51.8     |\n",
      "|    ent_coef        | 0.437    |\n",
      "|    ent_coef_loss   | 0.131    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4799899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4810      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27896     |\n",
      "|    total_timesteps | 4810000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 36.4      |\n",
      "|    ent_coef        | 0.436     |\n",
      "|    ent_coef_loss   | 0.184     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4809899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4820      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 27956     |\n",
      "|    total_timesteps | 4820000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 25.5      |\n",
      "|    ent_coef        | 0.45      |\n",
      "|    ent_coef_loss   | -0.249    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4819899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.45e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4830     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 28012    |\n",
      "|    total_timesteps | 4830000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 20.7     |\n",
      "|    ent_coef        | 0.436    |\n",
      "|    ent_coef_loss   | -0.253   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4829899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4840      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28070     |\n",
      "|    total_timesteps | 4840000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 873       |\n",
      "|    ent_coef        | 0.452     |\n",
      "|    ent_coef_loss   | -0.0358   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4839899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.46e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4850      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28129     |\n",
      "|    total_timesteps | 4850000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 27        |\n",
      "|    ent_coef        | 0.462     |\n",
      "|    ent_coef_loss   | -0.301    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4849899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.43e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4860      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28185     |\n",
      "|    total_timesteps | 4860000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 88.8      |\n",
      "|    ent_coef        | 0.452     |\n",
      "|    ent_coef_loss   | 0.00957   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4859899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.43e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4870      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28242     |\n",
      "|    total_timesteps | 4870000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 26.4      |\n",
      "|    ent_coef        | 0.46      |\n",
      "|    ent_coef_loss   | -0.0766   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4869899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.43e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4880      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28302     |\n",
      "|    total_timesteps | 4880000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 29.9      |\n",
      "|    ent_coef        | 0.471     |\n",
      "|    ent_coef_loss   | -0.0218   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4879899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.42e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4890     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 28360    |\n",
      "|    total_timesteps | 4890000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 72.2     |\n",
      "|    ent_coef        | 0.47     |\n",
      "|    ent_coef_loss   | -0.604   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4889899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4900      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28417     |\n",
      "|    total_timesteps | 4900000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+03 |\n",
      "|    critic_loss     | 32.1      |\n",
      "|    ent_coef        | 0.449     |\n",
      "|    ent_coef_loss   | 0.784     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4899899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4910      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28475     |\n",
      "|    total_timesteps | 4910000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 22.9      |\n",
      "|    ent_coef        | 0.48      |\n",
      "|    ent_coef_loss   | -0.0968   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4909899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4920      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28532     |\n",
      "|    total_timesteps | 4920000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 22.9      |\n",
      "|    ent_coef        | 0.472     |\n",
      "|    ent_coef_loss   | 0.0756    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4919899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4930      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28590     |\n",
      "|    total_timesteps | 4930000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 47        |\n",
      "|    ent_coef        | 0.466     |\n",
      "|    ent_coef_loss   | 0.0604    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4929899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4940      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28648     |\n",
      "|    total_timesteps | 4940000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 20.4      |\n",
      "|    ent_coef        | 0.474     |\n",
      "|    ent_coef_loss   | -0.282    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4939899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.42e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4950      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28705     |\n",
      "|    total_timesteps | 4950000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 36        |\n",
      "|    ent_coef        | 0.462     |\n",
      "|    ent_coef_loss   | -0.179    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4949899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.46e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4960      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28762     |\n",
      "|    total_timesteps | 4960000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 25.1      |\n",
      "|    ent_coef        | 0.468     |\n",
      "|    ent_coef_loss   | -0.196    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4959899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.46e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 4970     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 28820    |\n",
      "|    total_timesteps | 4970000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 28.1     |\n",
      "|    ent_coef        | 0.458    |\n",
      "|    ent_coef_loss   | -0.133   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4969899  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.45e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4980      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28877     |\n",
      "|    total_timesteps | 4980000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 32.7      |\n",
      "|    ent_coef        | 0.463     |\n",
      "|    ent_coef_loss   | 0.389     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4979899   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 1.46e+04  |\n",
      "| time/              |           |\n",
      "|    episodes        | 4990      |\n",
      "|    fps             | 172       |\n",
      "|    time_elapsed    | 28936     |\n",
      "|    total_timesteps | 4990000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 16        |\n",
      "|    ent_coef        | 0.471     |\n",
      "|    ent_coef_loss   | -0.0814   |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 4989899   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 1.46e+04 |\n",
      "| time/              |          |\n",
      "|    episodes        | 5000     |\n",
      "|    fps             | 172      |\n",
      "|    time_elapsed    | 28996    |\n",
      "|    total_timesteps | 5000000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.2e+03 |\n",
      "|    critic_loss     | 35.7     |\n",
      "|    ent_coef        | 0.463    |\n",
      "|    ent_coef_loss   | 0.121    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4999899  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Humanoid-v4\n",
    "env_id = 'HalfCheetah-v4'\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "sac_model = SAC(\"MlpPolicy\", env, verbose=10)\n",
    "checkpoint_callback = CheckpointCallback(save_freq=100000, save_path=f'../logs/{env_id}')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'./logs/{env_id}/best_model',\n",
    "                             log_path=f'./logs/{env_id}/results', eval_freq=10000)\n",
    "callback = CallbackList([checkpoint_callback, ])\n",
    "sac_model.learn(total_timesteps=5e6, log_interval=10, )\n",
    "sac_model.save(f\"logs/expert/{env_id}-sac/model5e6\")\n",
    "sac_model.save_replay_buffer(f\"logs/expert/{env_id}-sac/buffer5e6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('irl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d2e891c9d48cbc5657a17ab4ab08b2c1d2ec0060cc3e51694592beb2a6aa825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
