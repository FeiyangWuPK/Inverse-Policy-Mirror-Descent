{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f737c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from apmd_on.apmd import PMD\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a4e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0, eval_env=None, model=None):\n",
    "        super(EvaluateCallback, self).__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.eval_env = eval_env\n",
    "        self.model = model\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        self.iter = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=5, deterministic=False)\n",
    "        print(f\"Iter {self.iter:d} mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        self.means.append(mean_reward)\n",
    "        self.stds.append(std_reward)\n",
    "        self.iter += 1\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Learning rate schedule \n",
    "from typing import Callable\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "def plot_costs(rewards, names, smoothing_window=10, n=3, fig_name=\"acrobot.png\", stds=None):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    colors = ['tomato', 'royalblue', 'mediumpurple']\n",
    "    for i in range(n):\n",
    "        extend = np.concatenate([np.ones(smoothing_window)*rewards[i][0], rewards[i]])\n",
    "        rewards_smoothed = pd.Series(extend).rolling(smoothing_window, min_periods=smoothing_window).mean().to_numpy()\n",
    "        rewards_smoothed = rewards_smoothed[smoothing_window-1:]\n",
    "        rewards_smoothed = rewards_smoothed[:5000]\n",
    "        x = np.linspace(1, 5000, num=5000)\n",
    "        if stds is None:\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3)\n",
    "        else:\n",
    "            lower = rewards_smoothed - stds[i][ :5000]\n",
    "            upper = rewards_smoothed + stds[i][ :5000]\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3, c=colors[i])\n",
    "            plt.fill_between(x, y1=lower, y2=upper, interpolate=True, c=colors[i], alpha=0.5)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episodic Costs\")\n",
    "    plt.title(fig_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "    # plt.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "742a8b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-10-15 12:03:09'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17971d",
   "metadata": {},
   "source": [
    "## Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd7efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from apmd_off.iapmd import IPMD\n",
    "env_id = 'Ant-v4'\n",
    "expert_samples_replay_buffer_loc = \"utils/logs/expert/Ant-v4-sac/buffer5e6.pkl\"\n",
    "\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "ipmd_model = IPMD(\"MlpPolicy\", env, gamma=1.0, verbose=1, \n",
    "                  batch_size=256, train_freq=1, learning_rate=linear_schedule(5e-3),\n",
    "                  gradient_steps=1, expert_replay_buffer_loc=expert_samples_replay_buffer_loc)\n",
    "\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "logtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'logs/{env_id}-iapmd-{logtime}/',\n",
    "                             log_path=f'logs/{env_id}-iapmd-{logtime}/', eval_freq=5000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "ipmd_model.learn(total_timesteps=3e6, log_interval=4, callback=eval_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Walker2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6d83b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.8     |\n",
      "|    ep_rew_mean     | 6.73     |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 75       |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.18    |\n",
      "|    critic_loss     | 0.283    |\n",
      "|    ent_coef        | 0.695    |\n",
      "|    ent_coef_loss   | -3.65    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 73       |\n",
      "|    reward_est_loss | -0.342   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.4     |\n",
      "|    ep_rew_mean     | 4.42     |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 26       |\n",
      "|    total_timesteps | 171      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.9     |\n",
      "|    critic_loss     | 0.198    |\n",
      "|    ent_coef        | 0.429    |\n",
      "|    ent_coef_loss   | -8.42    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 169      |\n",
      "|    reward_est_loss | -0.545   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22       |\n",
      "|    ep_rew_mean     | 3.18     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 264      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.2     |\n",
      "|    critic_loss     | 0.116    |\n",
      "|    ent_coef        | 0.27     |\n",
      "|    ent_coef_loss   | -12.9    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 262      |\n",
      "|    reward_est_loss | -0.651   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.8     |\n",
      "|    ep_rew_mean     | 2.65     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 55       |\n",
      "|    total_timesteps | 365      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.48    |\n",
      "|    critic_loss     | 0.0784   |\n",
      "|    ent_coef        | 0.166    |\n",
      "|    ent_coef_loss   | -17      |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 363      |\n",
      "|    reward_est_loss | -0.711   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32       |\n",
      "|    ep_rew_mean     | 0.857    |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 99       |\n",
      "|    total_timesteps | 641      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.52    |\n",
      "|    critic_loss     | 0.0652   |\n",
      "|    ent_coef        | 0.0542   |\n",
      "|    ent_coef_loss   | -15.2    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 639      |\n",
      "|    reward_est_loss | -0.74    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 54.3     |\n",
      "|    ep_rew_mean     | 25.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 1303     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.19    |\n",
      "|    critic_loss     | 0.0779   |\n",
      "|    ent_coef        | 0.0174   |\n",
      "|    ent_coef_loss   | -4.57    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 1301     |\n",
      "|    reward_est_loss | -0.803   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 76.4     |\n",
      "|    ep_rew_mean     | 51       |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 330      |\n",
      "|    total_timesteps | 2139     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.02    |\n",
      "|    critic_loss     | 0.0951   |\n",
      "|    ent_coef        | 0.0128   |\n",
      "|    ent_coef_loss   | 3.62     |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 2137     |\n",
      "|    reward_est_loss | -0.857   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 87.2     |\n",
      "|    ep_rew_mean     | 78.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 431      |\n",
      "|    total_timesteps | 2789     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.91    |\n",
      "|    critic_loss     | 0.293    |\n",
      "|    ent_coef        | 0.012    |\n",
      "|    ent_coef_loss   | 3.65     |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 2787     |\n",
      "|    reward_est_loss | -0.905   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.5     |\n",
      "|    ep_rew_mean     | 79.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 498      |\n",
      "|    total_timesteps | 3222     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.76    |\n",
      "|    critic_loss     | 0.321    |\n",
      "|    ent_coef        | 0.0143   |\n",
      "|    ent_coef_loss   | 1.92     |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 3220     |\n",
      "|    reward_est_loss | -0.916   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.2     |\n",
      "|    ep_rew_mean     | 90       |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 604      |\n",
      "|    total_timesteps | 3927     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.63    |\n",
      "|    critic_loss     | 0.222    |\n",
      "|    ent_coef        | 0.0164   |\n",
      "|    ent_coef_loss   | -0.863   |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 3925     |\n",
      "|    reward_est_loss | -0.926   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 97.4     |\n",
      "|    ep_rew_mean     | 81.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 657      |\n",
      "|    total_timesteps | 4284     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.99    |\n",
      "|    critic_loss     | 0.42     |\n",
      "|    ent_coef        | 0.0157   |\n",
      "|    ent_coef_loss   | -4.21    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 4282     |\n",
      "|    reward_est_loss | -0.928   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.7     |\n",
      "|    ep_rew_mean     | 80.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 725      |\n",
      "|    total_timesteps | 4739     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.78    |\n",
      "|    critic_loss     | 0.266    |\n",
      "|    ent_coef        | 0.0168   |\n",
      "|    ent_coef_loss   | 3.19     |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 4737     |\n",
      "|    reward_est_loss | -0.946   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=441.32 +/- 84.43\n",
      "Episode length: 287.80 +/- 119.12\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 288      |\n",
      "|    mean_reward     | 441      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.8    |\n",
      "|    critic_loss     | 0.399    |\n",
      "|    ent_coef        | 0.0186   |\n",
      "|    ent_coef_loss   | 0.782    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 4998     |\n",
      "|    reward_est_loss | -0.941   |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 103      |\n",
      "|    ep_rew_mean     | 96.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 820      |\n",
      "|    total_timesteps | 5358     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11.3    |\n",
      "|    critic_loss     | 0.325    |\n",
      "|    ent_coef        | 0.0206   |\n",
      "|    ent_coef_loss   | -0.492   |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 5356     |\n",
      "|    reward_est_loss | -0.946   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 106      |\n",
      "|    ep_rew_mean     | 103      |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 904      |\n",
      "|    total_timesteps | 5921     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -12.3    |\n",
      "|    critic_loss     | 0.704    |\n",
      "|    ent_coef        | 0.0249   |\n",
      "|    ent_coef_loss   | -0.475   |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 5919     |\n",
      "|    reward_est_loss | -0.956   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 108      |\n",
      "|    ep_rew_mean     | 108      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 986      |\n",
      "|    total_timesteps | 6469     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.1    |\n",
      "|    critic_loss     | 0.766    |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | -3.04    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 6467     |\n",
      "|    reward_est_loss | -0.962   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 110      |\n",
      "|    ep_rew_mean     | 111      |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 1074     |\n",
      "|    total_timesteps | 7058     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.2    |\n",
      "|    critic_loss     | 0.65     |\n",
      "|    ent_coef        | 0.0289   |\n",
      "|    ent_coef_loss   | -0.831   |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 7056     |\n",
      "|    reward_est_loss | -0.964   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 112      |\n",
      "|    ep_rew_mean     | 114      |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 1158     |\n",
      "|    total_timesteps | 7624     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15      |\n",
      "|    critic_loss     | 0.551    |\n",
      "|    ent_coef        | 0.0284   |\n",
      "|    ent_coef_loss   | 0.139    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 7622     |\n",
      "|    reward_est_loss | -0.969   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 116      |\n",
      "|    ep_rew_mean     | 120      |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 1261     |\n",
      "|    total_timesteps | 8316     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.2    |\n",
      "|    critic_loss     | 0.803    |\n",
      "|    ent_coef        | 0.0268   |\n",
      "|    ent_coef_loss   | 0.228    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 8314     |\n",
      "|    reward_est_loss | -0.964   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 119      |\n",
      "|    ep_rew_mean     | 117      |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 1372     |\n",
      "|    total_timesteps | 9059     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16      |\n",
      "|    critic_loss     | 0.811    |\n",
      "|    ent_coef        | 0.0246   |\n",
      "|    ent_coef_loss   | 1.9      |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 9057     |\n",
      "|    reward_est_loss | -0.972   |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=208.62 +/- 44.99\n",
      "Episode length: 124.60 +/- 22.45\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 125      |\n",
      "|    mean_reward     | 209      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -17.7    |\n",
      "|    critic_loss     | 0.615    |\n",
      "|    ent_coef        | 0.0261   |\n",
      "|    ent_coef_loss   | -2.68    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 9998     |\n",
      "|    reward_est_loss | -0.982   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 129      |\n",
      "|    ep_rew_mean     | 120      |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 1605     |\n",
      "|    total_timesteps | 10312    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.9    |\n",
      "|    critic_loss     | 0.852    |\n",
      "|    ent_coef        | 0.0262   |\n",
      "|    ent_coef_loss   | 0.663    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 10310    |\n",
      "|    reward_est_loss | -0.0133  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | 126      |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 6        |\n",
      "|    time_elapsed    | 1782     |\n",
      "|    total_timesteps | 10907    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -14.6    |\n",
      "|    critic_loss     | 0.878    |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | 0.32     |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 10905    |\n",
      "|    reward_est_loss | -0.0329  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 129      |\n",
      "|    ep_rew_mean     | 128      |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 5        |\n",
      "|    time_elapsed    | 1905     |\n",
      "|    total_timesteps | 11326    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.2    |\n",
      "|    critic_loss     | 1.24     |\n",
      "|    ent_coef        | 0.0335   |\n",
      "|    ent_coef_loss   | -3.07    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 11324    |\n",
      "|    reward_est_loss | -0.0365  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | 132      |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 5        |\n",
      "|    time_elapsed    | 2048     |\n",
      "|    total_timesteps | 11812    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -16.2    |\n",
      "|    critic_loss     | 1.01     |\n",
      "|    ent_coef        | 0.0287   |\n",
      "|    ent_coef_loss   | 2.32     |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 11810    |\n",
      "|    reward_est_loss | -0.0385  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 125      |\n",
      "|    ep_rew_mean     | 128      |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 5        |\n",
      "|    time_elapsed    | 2108     |\n",
      "|    total_timesteps | 12016    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -15.2    |\n",
      "|    critic_loss     | 0.813    |\n",
      "|    ent_coef        | 0.0319   |\n",
      "|    ent_coef_loss   | -0.295   |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 12014    |\n",
      "|    reward_est_loss | -0.039   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 125      |\n",
      "|    ep_rew_mean     | 131      |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 5        |\n",
      "|    time_elapsed    | 2256     |\n",
      "|    total_timesteps | 12515    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.7    |\n",
      "|    critic_loss     | 1.48     |\n",
      "|    ent_coef        | 0.0285   |\n",
      "|    ent_coef_loss   | 0.563    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 12513    |\n",
      "|    reward_est_loss | -0.0402  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | 135      |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 5        |\n",
      "|    time_elapsed    | 2361     |\n",
      "|    total_timesteps | 12868    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.4    |\n",
      "|    critic_loss     | 1.29     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | -0.0335  |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 12866    |\n",
      "|    reward_est_loss | -0.0422  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | 141      |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 5        |\n",
      "|    time_elapsed    | 2473     |\n",
      "|    total_timesteps | 13244    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.1    |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.0314   |\n",
      "|    ent_coef_loss   | 1.14     |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 13242    |\n",
      "|    reward_est_loss | -0.0421  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 134      |\n",
      "|    ep_rew_mean     | 144      |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 5        |\n",
      "|    time_elapsed    | 2586     |\n",
      "|    total_timesteps | 13624    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -20.2    |\n",
      "|    critic_loss     | 1.34     |\n",
      "|    ent_coef        | 0.0291   |\n",
      "|    ent_coef_loss   | 1.48     |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 13622    |\n",
      "|    reward_est_loss | -0.0422  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 136      |\n",
      "|    ep_rew_mean     | 148      |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 5        |\n",
      "|    time_elapsed    | 2695     |\n",
      "|    total_timesteps | 13990    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19      |\n",
      "|    critic_loss     | 1.31     |\n",
      "|    ent_coef        | 0.0292   |\n",
      "|    ent_coef_loss   | 1.99     |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 13988    |\n",
      "|    reward_est_loss | -0.0437  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=186.91 +/- 37.97\n",
      "Episode length: 286.20 +/- 34.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 286      |\n",
      "|    mean_reward     | 187      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -19.7    |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    ent_coef        | 0.0274   |\n",
      "|    ent_coef_loss   | -0.679   |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 14998    |\n",
      "|    reward_est_loss | -0.045   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 165      |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 3023     |\n",
      "|    total_timesteps | 15090    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21      |\n",
      "|    critic_loss     | 1.52     |\n",
      "|    ent_coef        | 0.0282   |\n",
      "|    ent_coef_loss   | 1.35     |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 15088    |\n",
      "|    reward_est_loss | -0.0451  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 148      |\n",
      "|    ep_rew_mean     | 174      |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 3313     |\n",
      "|    total_timesteps | 16065    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -21.5    |\n",
      "|    critic_loss     | 1.37     |\n",
      "|    ent_coef        | 0.0318   |\n",
      "|    ent_coef_loss   | 1.19     |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 16063    |\n",
      "|    reward_est_loss | -0.0457  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 175      |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 3466     |\n",
      "|    total_timesteps | 16577    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -22.5    |\n",
      "|    critic_loss     | 1.85     |\n",
      "|    ent_coef        | 0.0366   |\n",
      "|    ent_coef_loss   | -1.54    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 16575    |\n",
      "|    reward_est_loss | -0.046   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 174      |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 3648     |\n",
      "|    total_timesteps | 17189    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24      |\n",
      "|    critic_loss     | 2.34     |\n",
      "|    ent_coef        | 0.0376   |\n",
      "|    ent_coef_loss   | -2.52    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 17187    |\n",
      "|    reward_est_loss | -0.0465  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 146      |\n",
      "|    ep_rew_mean     | 181      |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 3842     |\n",
      "|    total_timesteps | 17838    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.1    |\n",
      "|    critic_loss     | 2.57     |\n",
      "|    ent_coef        | 0.0391   |\n",
      "|    ent_coef_loss   | 0.0846   |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 17836    |\n",
      "|    reward_est_loss | -0.0466  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 181      |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 3985     |\n",
      "|    total_timesteps | 18316    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -24.4    |\n",
      "|    critic_loss     | 3.43     |\n",
      "|    ent_coef        | 0.0415   |\n",
      "|    ent_coef_loss   | 1.8      |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 18314    |\n",
      "|    reward_est_loss | -0.047   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 145      |\n",
      "|    ep_rew_mean     | 189      |\n",
      "| time/              |          |\n",
      "|    episodes        | 144      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 4116     |\n",
      "|    total_timesteps | 18757    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.4    |\n",
      "|    critic_loss     | 4.29     |\n",
      "|    ent_coef        | 0.0431   |\n",
      "|    ent_coef_loss   | -2.21    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 18755    |\n",
      "|    reward_est_loss | -0.0466  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 144      |\n",
      "|    ep_rew_mean     | 194      |\n",
      "| time/              |          |\n",
      "|    episodes        | 148      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 4235     |\n",
      "|    total_timesteps | 19157    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.1    |\n",
      "|    critic_loss     | 2.88     |\n",
      "|    ent_coef        | 0.0463   |\n",
      "|    ent_coef_loss   | -0.885   |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 19155    |\n",
      "|    reward_est_loss | -0.0469  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 142      |\n",
      "|    ep_rew_mean     | 190      |\n",
      "| time/              |          |\n",
      "|    episodes        | 152      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 4364     |\n",
      "|    total_timesteps | 19588    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.2    |\n",
      "|    critic_loss     | 7.99     |\n",
      "|    ent_coef        | 0.0447   |\n",
      "|    ent_coef_loss   | 1.15     |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 19586    |\n",
      "|    reward_est_loss | -0.0467  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 140      |\n",
      "|    ep_rew_mean     | 189      |\n",
      "| time/              |          |\n",
      "|    episodes        | 156      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 4476     |\n",
      "|    total_timesteps | 19961    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.5    |\n",
      "|    critic_loss     | 7.22     |\n",
      "|    ent_coef        | 0.0486   |\n",
      "|    ent_coef_loss   | 1.45     |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 19959    |\n",
      "|    reward_est_loss | -0.0465  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=90.63 +/- 5.96\n",
      "Episode length: 59.00 +/- 2.68\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 59       |\n",
      "|    mean_reward     | 90.6     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -27      |\n",
      "|    critic_loss     | 3.5      |\n",
      "|    ent_coef        | 0.0494   |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 19998    |\n",
      "|    reward_est_loss | -0.0468  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 138      |\n",
      "|    ep_rew_mean     | 188      |\n",
      "| time/              |          |\n",
      "|    episodes        | 160      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 4582     |\n",
      "|    total_timesteps | 20318    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.9    |\n",
      "|    critic_loss     | 6.55     |\n",
      "|    ent_coef        | 0.051    |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 20316    |\n",
      "|    reward_est_loss | -0.0471  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 136      |\n",
      "|    ep_rew_mean     | 187      |\n",
      "| time/              |          |\n",
      "|    episodes        | 164      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 4686     |\n",
      "|    total_timesteps | 20667    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.2    |\n",
      "|    critic_loss     | 3.86     |\n",
      "|    ent_coef        | 0.0505   |\n",
      "|    ent_coef_loss   | 1.04     |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 20665    |\n",
      "|    reward_est_loss | -0.0468  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 134      |\n",
      "|    ep_rew_mean     | 185      |\n",
      "| time/              |          |\n",
      "|    episodes        | 168      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 4786     |\n",
      "|    total_timesteps | 21002    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -31.5    |\n",
      "|    critic_loss     | 4.07     |\n",
      "|    ent_coef        | 0.0508   |\n",
      "|    ent_coef_loss   | 0.422    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 21000    |\n",
      "|    reward_est_loss | -0.0465  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 132      |\n",
      "|    ep_rew_mean     | 184      |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 4933     |\n",
      "|    total_timesteps | 21493    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -29.8    |\n",
      "|    critic_loss     | 6.07     |\n",
      "|    ent_coef        | 0.055    |\n",
      "|    ent_coef_loss   | 0.668    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 21491    |\n",
      "|    reward_est_loss | -0.046   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 131      |\n",
      "|    ep_rew_mean     | 194      |\n",
      "| time/              |          |\n",
      "|    episodes        | 176      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 5127     |\n",
      "|    total_timesteps | 22142    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -30.8    |\n",
      "|    critic_loss     | 5.03     |\n",
      "|    ent_coef        | 0.0551   |\n",
      "|    ent_coef_loss   | 1.74     |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 22140    |\n",
      "|    reward_est_loss | -0.0459  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | 201      |\n",
      "| time/              |          |\n",
      "|    episodes        | 180      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 5362     |\n",
      "|    total_timesteps | 22928    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -34.2    |\n",
      "|    critic_loss     | 4.8      |\n",
      "|    ent_coef        | 0.0603   |\n",
      "|    ent_coef_loss   | -1.67    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 22926    |\n",
      "|    reward_est_loss | -0.0465  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | 201      |\n",
      "| time/              |          |\n",
      "|    episodes        | 184      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 5573     |\n",
      "|    total_timesteps | 23636    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -37.2    |\n",
      "|    critic_loss     | 5.16     |\n",
      "|    ent_coef        | 0.0527   |\n",
      "|    ent_coef_loss   | 0.134    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 23634    |\n",
      "|    reward_est_loss | -0.0478  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | 200      |\n",
      "| time/              |          |\n",
      "|    episodes        | 188      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 5721     |\n",
      "|    total_timesteps | 24131    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -38.9    |\n",
      "|    critic_loss     | 6.28     |\n",
      "|    ent_coef        | 0.057    |\n",
      "|    ent_coef_loss   | 1.4      |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 24129    |\n",
      "|    reward_est_loss | -0.048   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | 194      |\n",
      "| time/              |          |\n",
      "|    episodes        | 192      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 5790     |\n",
      "|    total_timesteps | 24362    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39.3    |\n",
      "|    critic_loss     | 6.57     |\n",
      "|    ent_coef        | 0.0556   |\n",
      "|    ent_coef_loss   | -1.86    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 24360    |\n",
      "|    reward_est_loss | -0.0474  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | 196      |\n",
      "| time/              |          |\n",
      "|    episodes        | 196      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 5883     |\n",
      "|    total_timesteps | 24672    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39.2    |\n",
      "|    critic_loss     | 5.3      |\n",
      "|    ent_coef        | 0.0574   |\n",
      "|    ent_coef_loss   | 0.856    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 24670    |\n",
      "|    reward_est_loss | -0.0482  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 125      |\n",
      "|    ep_rew_mean     | 193      |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 5973     |\n",
      "|    total_timesteps | 24976    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -43.9    |\n",
      "|    critic_loss     | 6.71     |\n",
      "|    ent_coef        | 0.0625   |\n",
      "|    ent_coef_loss   | -0.769   |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 24974    |\n",
      "|    reward_est_loss | -0.0481  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=115.13 +/- 29.66\n",
      "Episode length: 87.20 +/- 24.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 87.2     |\n",
      "|    mean_reward     | 115      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -38.6    |\n",
      "|    critic_loss     | 4.96     |\n",
      "|    ent_coef        | 0.0618   |\n",
      "|    ent_coef_loss   | -1.01    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 24998    |\n",
      "|    reward_est_loss | -0.0483  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | 193      |\n",
      "| time/              |          |\n",
      "|    episodes        | 204      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 6062     |\n",
      "|    total_timesteps | 25254    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -40.2    |\n",
      "|    critic_loss     | 5.9      |\n",
      "|    ent_coef        | 0.0631   |\n",
      "|    ent_coef_loss   | 1.35     |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 25252    |\n",
      "|    reward_est_loss | -0.0482  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 125      |\n",
      "|    ep_rew_mean     | 193      |\n",
      "| time/              |          |\n",
      "|    episodes        | 208      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 6211     |\n",
      "|    total_timesteps | 25702    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -45.9    |\n",
      "|    critic_loss     | 7.11     |\n",
      "|    ent_coef        | 0.0544   |\n",
      "|    ent_coef_loss   | -0.121   |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 25700    |\n",
      "|    reward_est_loss | -0.0481  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 125      |\n",
      "|    ep_rew_mean     | 197      |\n",
      "| time/              |          |\n",
      "|    episodes        | 212      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 6373     |\n",
      "|    total_timesteps | 26166    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.4    |\n",
      "|    critic_loss     | 6.19     |\n",
      "|    ent_coef        | 0.0581   |\n",
      "|    ent_coef_loss   | -1.77    |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 26164    |\n",
      "|    reward_est_loss | -0.0472  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 127      |\n",
      "|    ep_rew_mean     | 199      |\n",
      "| time/              |          |\n",
      "|    episodes        | 216      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 6544     |\n",
      "|    total_timesteps | 26656    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47.1    |\n",
      "|    critic_loss     | 6.28     |\n",
      "|    ent_coef        | 0.0576   |\n",
      "|    ent_coef_loss   | -0.348   |\n",
      "|    learning_rate   | 0.00496  |\n",
      "|    n_updates       | 26654    |\n",
      "|    reward_est_loss | -0.0473  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 121      |\n",
      "|    ep_rew_mean     | 188      |\n",
      "| time/              |          |\n",
      "|    episodes        | 220      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 6717     |\n",
      "|    total_timesteps | 27159    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -50.6    |\n",
      "|    critic_loss     | 5.41     |\n",
      "|    ent_coef        | 0.0672   |\n",
      "|    ent_coef_loss   | 0.786    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 27157    |\n",
      "|    reward_est_loss | -0.0473  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 117      |\n",
      "|    ep_rew_mean     | 178      |\n",
      "| time/              |          |\n",
      "|    episodes        | 224      |\n",
      "|    fps             | 4        |\n",
      "|    time_elapsed    | 6933     |\n",
      "|    total_timesteps | 27796    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -55.6    |\n",
      "|    critic_loss     | 9.53     |\n",
      "|    ent_coef        | 0.0665   |\n",
      "|    ent_coef_loss   | 0.456    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 27794    |\n",
      "|    reward_est_loss | -0.048   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | 182      |\n",
      "| time/              |          |\n",
      "|    episodes        | 228      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 7341     |\n",
      "|    total_timesteps | 29004    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -55.8    |\n",
      "|    critic_loss     | 7.5      |\n",
      "|    ent_coef        | 0.0537   |\n",
      "|    ent_coef_loss   | 0.158    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 29002    |\n",
      "|    reward_est_loss | -0.0471  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 124      |\n",
      "|    ep_rew_mean     | 173      |\n",
      "| time/              |          |\n",
      "|    episodes        | 232      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 7540     |\n",
      "|    total_timesteps | 29612    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -58.5    |\n",
      "|    critic_loss     | 13       |\n",
      "|    ent_coef        | 0.0583   |\n",
      "|    ent_coef_loss   | -0.247   |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 29610    |\n",
      "|    reward_est_loss | -0.0469  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-41.66 +/- 30.67\n",
      "Episode length: 121.20 +/- 21.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 121      |\n",
      "|    mean_reward     | -41.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.4    |\n",
      "|    critic_loss     | 10.2     |\n",
      "|    ent_coef        | 0.0662   |\n",
      "|    ent_coef_loss   | -1.04    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 29998    |\n",
      "|    reward_est_loss | -0.0475  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 126      |\n",
      "|    ep_rew_mean     | 172      |\n",
      "| time/              |          |\n",
      "|    episodes        | 236      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 7802     |\n",
      "|    total_timesteps | 30389    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -65.1    |\n",
      "|    critic_loss     | 10.4     |\n",
      "|    ent_coef        | 0.065    |\n",
      "|    ent_coef_loss   | 4.06     |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 30387    |\n",
      "|    reward_est_loss | -0.0477  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 128      |\n",
      "|    ep_rew_mean     | 169      |\n",
      "| time/              |          |\n",
      "|    episodes        | 240      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 8043     |\n",
      "|    total_timesteps | 31112    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.6    |\n",
      "|    critic_loss     | 27.2     |\n",
      "|    ent_coef        | 0.0765   |\n",
      "|    ent_coef_loss   | 0.364    |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 31110    |\n",
      "|    reward_est_loss | -0.0476  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 130      |\n",
      "|    ep_rew_mean     | 163      |\n",
      "| time/              |          |\n",
      "|    episodes        | 244      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 8246     |\n",
      "|    total_timesteps | 31728    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -68.8    |\n",
      "|    critic_loss     | 13.2     |\n",
      "|    ent_coef        | 0.0858   |\n",
      "|    ent_coef_loss   | -0.00746 |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 31726    |\n",
      "|    reward_est_loss | -0.047   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 132      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    episodes        | 248      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 8458     |\n",
      "|    total_timesteps | 32359    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.9    |\n",
      "|    critic_loss     | 12.6     |\n",
      "|    ent_coef        | 0.0906   |\n",
      "|    ent_coef_loss   | -0.00398 |\n",
      "|    learning_rate   | 0.00495  |\n",
      "|    n_updates       | 32357    |\n",
      "|    reward_est_loss | -0.0474  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 146      |\n",
      "|    ep_rew_mean     | 155      |\n",
      "| time/              |          |\n",
      "|    episodes        | 252      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 9041     |\n",
      "|    total_timesteps | 34138    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -92.5    |\n",
      "|    critic_loss     | 24.1     |\n",
      "|    ent_coef        | 0.112    |\n",
      "|    ent_coef_loss   | 1.43     |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 34136    |\n",
      "|    reward_est_loss | -0.0474  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=180.28 +/- 76.05\n",
      "Episode length: 150.60 +/- 38.83\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 151      |\n",
      "|    mean_reward     | 180      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -103     |\n",
      "|    critic_loss     | 20       |\n",
      "|    ent_coef        | 0.137    |\n",
      "|    ent_coef_loss   | 0.346    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 34998    |\n",
      "|    reward_est_loss | -0.0478  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 157      |\n",
      "|    ep_rew_mean     | 156      |\n",
      "| time/              |          |\n",
      "|    episodes        | 256      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 9555     |\n",
      "|    total_timesteps | 35695    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -104     |\n",
      "|    critic_loss     | 28.2     |\n",
      "|    ent_coef        | 0.126    |\n",
      "|    ent_coef_loss   | -1       |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 35693    |\n",
      "|    reward_est_loss | -0.0483  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 174      |\n",
      "|    ep_rew_mean     | 164      |\n",
      "| time/              |          |\n",
      "|    episodes        | 260      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 10198    |\n",
      "|    total_timesteps | 37692    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -114     |\n",
      "|    critic_loss     | 25.3     |\n",
      "|    ent_coef        | 0.127    |\n",
      "|    ent_coef_loss   | 0.276    |\n",
      "|    learning_rate   | 0.00494  |\n",
      "|    n_updates       | 37690    |\n",
      "|    reward_est_loss | -0.0486  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 193      |\n",
      "|    ep_rew_mean     | 179      |\n",
      "| time/              |          |\n",
      "|    episodes        | 264      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 10955    |\n",
      "|    total_timesteps | 39919    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -134     |\n",
      "|    critic_loss     | 89.8     |\n",
      "|    ent_coef        | 0.154    |\n",
      "|    ent_coef_loss   | 0.733    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 39917    |\n",
      "|    reward_est_loss | -0.0482  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=82.19 +/- 7.45\n",
      "Episode length: 205.40 +/- 6.62\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 205      |\n",
      "|    mean_reward     | 82.2     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -148     |\n",
      "|    critic_loss     | 69.1     |\n",
      "|    ent_coef        | 0.176    |\n",
      "|    ent_coef_loss   | 0.881    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 39998    |\n",
      "|    reward_est_loss | -0.0478  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 195      |\n",
      "|    ep_rew_mean     | 175      |\n",
      "| time/              |          |\n",
      "|    episodes        | 268      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 11152    |\n",
      "|    total_timesteps | 40486    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -167     |\n",
      "|    critic_loss     | 140      |\n",
      "|    ent_coef        | 0.222    |\n",
      "|    ent_coef_loss   | 0.298    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 40484    |\n",
      "|    reward_est_loss | -0.0479  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | 168      |\n",
      "| time/              |          |\n",
      "|    episodes        | 272      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 11368    |\n",
      "|    total_timesteps | 41140    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -179     |\n",
      "|    critic_loss     | 246      |\n",
      "|    ent_coef        | 0.259    |\n",
      "|    ent_coef_loss   | 0.325    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 41138    |\n",
      "|    reward_est_loss | -0.0483  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 195      |\n",
      "|    ep_rew_mean     | 157      |\n",
      "| time/              |          |\n",
      "|    episodes        | 276      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 11549    |\n",
      "|    total_timesteps | 41684    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -190     |\n",
      "|    critic_loss     | 162      |\n",
      "|    ent_coef        | 0.296    |\n",
      "|    ent_coef_loss   | 0.43     |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 41682    |\n",
      "|    reward_est_loss | -0.0482  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 193      |\n",
      "|    ep_rew_mean     | 142      |\n",
      "| time/              |          |\n",
      "|    episodes        | 280      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 11733    |\n",
      "|    total_timesteps | 42208    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -240     |\n",
      "|    critic_loss     | 208      |\n",
      "|    ent_coef        | 0.325    |\n",
      "|    ent_coef_loss   | 0.476    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 42206    |\n",
      "|    reward_est_loss | -0.0478  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 191      |\n",
      "|    ep_rew_mean     | 133      |\n",
      "| time/              |          |\n",
      "|    episodes        | 284      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 11900    |\n",
      "|    total_timesteps | 42699    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -251     |\n",
      "|    critic_loss     | 283      |\n",
      "|    ent_coef        | 0.363    |\n",
      "|    ent_coef_loss   | -0.0982  |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 42697    |\n",
      "|    reward_est_loss | -0.0481  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 190      |\n",
      "|    ep_rew_mean     | 127      |\n",
      "| time/              |          |\n",
      "|    episodes        | 288      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 12059    |\n",
      "|    total_timesteps | 43165    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -293     |\n",
      "|    critic_loss     | 239      |\n",
      "|    ent_coef        | 0.399    |\n",
      "|    ent_coef_loss   | -0.217   |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 43163    |\n",
      "|    reward_est_loss | -0.0489  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 193      |\n",
      "|    ep_rew_mean     | 125      |\n",
      "| time/              |          |\n",
      "|    episodes        | 292      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 12237    |\n",
      "|    total_timesteps | 43679    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -327     |\n",
      "|    critic_loss     | 273      |\n",
      "|    ent_coef        | 0.467    |\n",
      "|    ent_coef_loss   | 0.0737   |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 43677    |\n",
      "|    reward_est_loss | -0.0489  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 196      |\n",
      "|    ep_rew_mean     | 121      |\n",
      "| time/              |          |\n",
      "|    episodes        | 296      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 12435    |\n",
      "|    total_timesteps | 44259    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -329     |\n",
      "|    critic_loss     | 265      |\n",
      "|    ent_coef        | 0.481    |\n",
      "|    ent_coef_loss   | 0.0929   |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 44257    |\n",
      "|    reward_est_loss | -0.0491  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 198      |\n",
      "|    ep_rew_mean     | 118      |\n",
      "| time/              |          |\n",
      "|    episodes        | 300      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 12626    |\n",
      "|    total_timesteps | 44826    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -393     |\n",
      "|    critic_loss     | 517      |\n",
      "|    ent_coef        | 0.439    |\n",
      "|    ent_coef_loss   | 0.109    |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 44824    |\n",
      "|    reward_est_loss | -0.0491  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=15.81 +/- 1.55\n",
      "Episode length: 121.20 +/- 2.48\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 121      |\n",
      "|    mean_reward     | 15.8     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -382     |\n",
      "|    critic_loss     | 314      |\n",
      "|    ent_coef        | 0.468    |\n",
      "|    ent_coef_loss   | 0.4      |\n",
      "|    learning_rate   | 0.00493  |\n",
      "|    n_updates       | 44998    |\n",
      "|    reward_est_loss | -0.0489  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 201      |\n",
      "|    ep_rew_mean     | 116      |\n",
      "| time/              |          |\n",
      "|    episodes        | 304      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 12792    |\n",
      "|    total_timesteps | 45333    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -358     |\n",
      "|    critic_loss     | 423      |\n",
      "|    ent_coef        | 0.448    |\n",
      "|    ent_coef_loss   | 0.0897   |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 45331    |\n",
      "|    reward_est_loss | -0.0489  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 203      |\n",
      "|    ep_rew_mean     | 113      |\n",
      "| time/              |          |\n",
      "|    episodes        | 308      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 13001    |\n",
      "|    total_timesteps | 45969    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -430     |\n",
      "|    critic_loss     | 493      |\n",
      "|    ent_coef        | 0.438    |\n",
      "|    ent_coef_loss   | -0.0662  |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 45967    |\n",
      "|    reward_est_loss | -0.0483  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 214      |\n",
      "|    ep_rew_mean     | 126      |\n",
      "| time/              |          |\n",
      "|    episodes        | 312      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 13500    |\n",
      "|    total_timesteps | 47586    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -450     |\n",
      "|    critic_loss     | 400      |\n",
      "|    ent_coef        | 0.494    |\n",
      "|    ent_coef_loss   | -0.0598  |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 47584    |\n",
      "|    reward_est_loss | -0.0486  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 229      |\n",
      "|    ep_rew_mean     | 150      |\n",
      "| time/              |          |\n",
      "|    episodes        | 316      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 14153    |\n",
      "|    total_timesteps | 49577    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -489     |\n",
      "|    critic_loss     | 466      |\n",
      "|    ent_coef        | 0.435    |\n",
      "|    ent_coef_loss   | -0.393   |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 49575    |\n",
      "|    reward_est_loss | -0.0482  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=436.12 +/- 347.74\n",
      "Episode length: 542.20 +/- 243.79\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 542      |\n",
      "|    mean_reward     | 436      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -561     |\n",
      "|    critic_loss     | 419      |\n",
      "|    ent_coef        | 0.458    |\n",
      "|    ent_coef_loss   | 0.434    |\n",
      "|    learning_rate   | 0.00492  |\n",
      "|    n_updates       | 49998    |\n",
      "|    reward_est_loss | -0.0478  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 244      |\n",
      "|    ep_rew_mean     | 170      |\n",
      "| time/              |          |\n",
      "|    episodes        | 320      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 14792    |\n",
      "|    total_timesteps | 51542    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -558     |\n",
      "|    critic_loss     | 641      |\n",
      "|    ent_coef        | 0.457    |\n",
      "|    ent_coef_loss   | -0.352   |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 51540    |\n",
      "|    reward_est_loss | -0.0479  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 249      |\n",
      "|    ep_rew_mean     | 184      |\n",
      "| time/              |          |\n",
      "|    episodes        | 324      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 15166    |\n",
      "|    total_timesteps | 52691    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -557     |\n",
      "|    critic_loss     | 385      |\n",
      "|    ent_coef        | 0.454    |\n",
      "|    ent_coef_loss   | -0.301   |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 52689    |\n",
      "|    reward_est_loss | -0.0476  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=998.56 +/- 5.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 999      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -649     |\n",
      "|    critic_loss     | 757      |\n",
      "|    ent_coef        | 0.434    |\n",
      "|    ent_coef_loss   | 0.284    |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 54998    |\n",
      "|    reward_est_loss | -0.0481  |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 273      |\n",
      "|    ep_rew_mean     | 209      |\n",
      "| time/              |          |\n",
      "|    episodes        | 328      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 16251    |\n",
      "|    total_timesteps | 56260    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -683     |\n",
      "|    critic_loss     | 543      |\n",
      "|    ent_coef        | 0.451    |\n",
      "|    ent_coef_loss   | -0.091   |\n",
      "|    learning_rate   | 0.00491  |\n",
      "|    n_updates       | 56258    |\n",
      "|    reward_est_loss | -0.0486  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 289      |\n",
      "|    ep_rew_mean     | 228      |\n",
      "| time/              |          |\n",
      "|    episodes        | 332      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 17064    |\n",
      "|    total_timesteps | 58464    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -739     |\n",
      "|    critic_loss     | 637      |\n",
      "|    ent_coef        | 0.408    |\n",
      "|    ent_coef_loss   | 1.03     |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 58462    |\n",
      "|    reward_est_loss | -0.0485  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=899.42 +/- 346.91\n",
      "Episode length: 835.60 +/- 328.80\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 836      |\n",
      "|    mean_reward     | 899      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -751     |\n",
      "|    critic_loss     | 680      |\n",
      "|    ent_coef        | 0.434    |\n",
      "|    ent_coef_loss   | 0.174    |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 59998    |\n",
      "|    reward_est_loss | -0.0483  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 305      |\n",
      "|    ep_rew_mean     | 240      |\n",
      "| time/              |          |\n",
      "|    episodes        | 336      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 17956    |\n",
      "|    total_timesteps | 60859    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -774     |\n",
      "|    critic_loss     | 610      |\n",
      "|    ent_coef        | 0.445    |\n",
      "|    ent_coef_loss   | -0.0696  |\n",
      "|    learning_rate   | 0.0049   |\n",
      "|    n_updates       | 60857    |\n",
      "|    reward_est_loss | -0.0484  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 330      |\n",
      "|    ep_rew_mean     | 263      |\n",
      "| time/              |          |\n",
      "|    episodes        | 340      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 18913    |\n",
      "|    total_timesteps | 64077    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -809     |\n",
      "|    critic_loss     | 610      |\n",
      "|    ent_coef        | 0.505    |\n",
      "|    ent_coef_loss   | 0.138    |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 64075    |\n",
      "|    reward_est_loss | -0.0485  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=683.18 +/- 402.95\n",
      "Episode length: 687.40 +/- 384.58\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 687      |\n",
      "|    mean_reward     | 683      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 65000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -837     |\n",
      "|    critic_loss     | 722      |\n",
      "|    ent_coef        | 0.514    |\n",
      "|    ent_coef_loss   | -0.306   |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 64998    |\n",
      "|    reward_est_loss | -0.0486  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 342      |\n",
      "|    ep_rew_mean     | 279      |\n",
      "| time/              |          |\n",
      "|    episodes        | 344      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 19463    |\n",
      "|    total_timesteps | 65920    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -878     |\n",
      "|    critic_loss     | 779      |\n",
      "|    ent_coef        | 0.624    |\n",
      "|    ent_coef_loss   | -0.237   |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 65918    |\n",
      "|    reward_est_loss | -0.0487  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 368      |\n",
      "|    ep_rew_mean     | 306      |\n",
      "| time/              |          |\n",
      "|    episodes        | 348      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 20417    |\n",
      "|    total_timesteps | 69130    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -998     |\n",
      "|    critic_loss     | 1.17e+03 |\n",
      "|    ent_coef        | 0.76     |\n",
      "|    ent_coef_loss   | -0.0211  |\n",
      "|    learning_rate   | 0.00488  |\n",
      "|    n_updates       | 69128    |\n",
      "|    reward_est_loss | -0.0487  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=713.26 +/- 379.28\n",
      "Episode length: 414.20 +/- 229.49\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 414       |\n",
      "|    mean_reward     | 713       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 70000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+03 |\n",
      "|    critic_loss     | 1.28e+03  |\n",
      "|    ent_coef        | 0.804     |\n",
      "|    ent_coef_loss   | -0.0266   |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 69998     |\n",
      "|    reward_est_loss | -0.0488   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 371       |\n",
      "|    ep_rew_mean     | 318       |\n",
      "| time/              |           |\n",
      "|    episodes        | 352       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 21035     |\n",
      "|    total_timesteps | 71201     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.09e+03 |\n",
      "|    critic_loss     | 1.46e+03  |\n",
      "|    ent_coef        | 0.702     |\n",
      "|    ent_coef_loss   | 0.24      |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 71199     |\n",
      "|    reward_est_loss | -0.0487   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 382       |\n",
      "|    ep_rew_mean     | 335       |\n",
      "| time/              |           |\n",
      "|    episodes        | 356       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 21822     |\n",
      "|    total_timesteps | 73851     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.15e+03 |\n",
      "|    critic_loss     | 1.21e+03  |\n",
      "|    ent_coef        | 0.572     |\n",
      "|    ent_coef_loss   | 0.0427    |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 73849     |\n",
      "|    reward_est_loss | -0.0487   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=89.80 +/- 96.59\n",
      "Episode length: 202.40 +/- 92.23\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 202       |\n",
      "|    mean_reward     | 89.8      |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 75000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 1.06e+03  |\n",
      "|    ent_coef        | 0.614     |\n",
      "|    ent_coef_loss   | 0.164     |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 74998     |\n",
      "|    reward_est_loss | -0.0484   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 374       |\n",
      "|    ep_rew_mean     | 331       |\n",
      "| time/              |           |\n",
      "|    episodes        | 360       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 22198     |\n",
      "|    total_timesteps | 75116     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+03 |\n",
      "|    critic_loss     | 1.43e+03  |\n",
      "|    ent_coef        | 0.628     |\n",
      "|    ent_coef_loss   | -0.155    |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 75114     |\n",
      "|    reward_est_loss | -0.0484   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 359       |\n",
      "|    ep_rew_mean     | 313       |\n",
      "| time/              |           |\n",
      "|    episodes        | 364       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 22403     |\n",
      "|    total_timesteps | 75810     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.17e+03 |\n",
      "|    critic_loss     | 1.18e+03  |\n",
      "|    ent_coef        | 0.59      |\n",
      "|    ent_coef_loss   | 0.0877    |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 75808     |\n",
      "|    reward_est_loss | -0.0486   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 359       |\n",
      "|    ep_rew_mean     | 312       |\n",
      "| time/              |           |\n",
      "|    episodes        | 368       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 22560     |\n",
      "|    total_timesteps | 76337     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 2.44e+03  |\n",
      "|    ent_coef        | 0.627     |\n",
      "|    ent_coef_loss   | -0.155    |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 76335     |\n",
      "|    reward_est_loss | -0.0488   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 357       |\n",
      "|    ep_rew_mean     | 311       |\n",
      "| time/              |           |\n",
      "|    episodes        | 372       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 22703     |\n",
      "|    total_timesteps | 76819     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 1.63e+03  |\n",
      "|    ent_coef        | 0.639     |\n",
      "|    ent_coef_loss   | -0.152    |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 76817     |\n",
      "|    reward_est_loss | -0.0487   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 360       |\n",
      "|    ep_rew_mean     | 315       |\n",
      "| time/              |           |\n",
      "|    episodes        | 376       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 22958     |\n",
      "|    total_timesteps | 77680     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 1.78e+03  |\n",
      "|    ent_coef        | 0.704     |\n",
      "|    ent_coef_loss   | -0.23     |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 77678     |\n",
      "|    reward_est_loss | -0.0486   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 365       |\n",
      "|    ep_rew_mean     | 325       |\n",
      "| time/              |           |\n",
      "|    episodes        | 380       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 23263     |\n",
      "|    total_timesteps | 78710     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 2.6e+03   |\n",
      "|    ent_coef        | 0.768     |\n",
      "|    ent_coef_loss   | -0.148    |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 78708     |\n",
      "|    reward_est_loss | -0.0486   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=842.84 +/- 40.72\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 843       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 80000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.24e+03 |\n",
      "|    critic_loss     | 2.02e+03  |\n",
      "|    ent_coef        | 0.854     |\n",
      "|    ent_coef_loss   | 0.077     |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 79998     |\n",
      "|    reward_est_loss | -0.0488   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 383      |\n",
      "|    ep_rew_mean     | 340      |\n",
      "| time/              |          |\n",
      "|    episodes        | 384      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 23932    |\n",
      "|    total_timesteps | 80965    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.4e+03 |\n",
      "|    critic_loss     | 2.59e+03 |\n",
      "|    ent_coef        | 0.889    |\n",
      "|    ent_coef_loss   | 0.00257  |\n",
      "|    learning_rate   | 0.00487  |\n",
      "|    n_updates       | 80963    |\n",
      "|    reward_est_loss | -0.0489  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 418       |\n",
      "|    ep_rew_mean     | 375       |\n",
      "| time/              |           |\n",
      "|    episodes        | 388       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 25120     |\n",
      "|    total_timesteps | 84965     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.56e+03 |\n",
      "|    critic_loss     | 4.64e+03  |\n",
      "|    ent_coef        | 1.27      |\n",
      "|    ent_coef_loss   | -0.0389   |\n",
      "|    learning_rate   | 0.00486   |\n",
      "|    n_updates       | 84963     |\n",
      "|    reward_est_loss | -0.0491   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=-34.41 +/- 30.55\n",
      "Episode length: 137.80 +/- 12.07\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 138      |\n",
      "|    mean_reward     | -34.4    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 85000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.6e+03 |\n",
      "|    critic_loss     | 3.25e+03 |\n",
      "|    ent_coef        | 1.27     |\n",
      "|    ent_coef_loss   | -0.0653  |\n",
      "|    learning_rate   | 0.00486  |\n",
      "|    n_updates       | 84998    |\n",
      "|    reward_est_loss | -0.0491  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 436       |\n",
      "|    ep_rew_mean     | 390       |\n",
      "| time/              |           |\n",
      "|    episodes        | 392       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 25812     |\n",
      "|    total_timesteps | 87294     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.87e+03 |\n",
      "|    critic_loss     | 5.83e+03  |\n",
      "|    ent_coef        | 1.52      |\n",
      "|    ent_coef_loss   | 0.13      |\n",
      "|    learning_rate   | 0.00485   |\n",
      "|    n_updates       | 87292     |\n",
      "|    reward_est_loss | -0.0488   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 451       |\n",
      "|    ep_rew_mean     | 407       |\n",
      "| time/              |           |\n",
      "|    episodes        | 396       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 26416     |\n",
      "|    total_timesteps | 89326     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.95e+03 |\n",
      "|    critic_loss     | 5.38e+03  |\n",
      "|    ent_coef        | 1.41      |\n",
      "|    ent_coef_loss   | 0.256     |\n",
      "|    learning_rate   | 0.00485   |\n",
      "|    n_updates       | 89324     |\n",
      "|    reward_est_loss | -0.0489   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=442.59 +/- 278.73\n",
      "Episode length: 557.00 +/- 311.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 557       |\n",
      "|    mean_reward     | 443       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 90000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.03e+03 |\n",
      "|    critic_loss     | 6.49e+03  |\n",
      "|    ent_coef        | 1.38      |\n",
      "|    ent_coef_loss   | 0.227     |\n",
      "|    learning_rate   | 0.00485   |\n",
      "|    n_updates       | 89998     |\n",
      "|    reward_est_loss | -0.0491   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 462      |\n",
      "|    ep_rew_mean     | 415      |\n",
      "| time/              |          |\n",
      "|    episodes        | 400      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 26914    |\n",
      "|    total_timesteps | 90999    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.1e+03 |\n",
      "|    critic_loss     | 2.47e+04 |\n",
      "|    ent_coef        | 1.39     |\n",
      "|    ent_coef_loss   | -0.0765  |\n",
      "|    learning_rate   | 0.00485  |\n",
      "|    n_updates       | 90997    |\n",
      "|    reward_est_loss | -0.0488  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 472       |\n",
      "|    ep_rew_mean     | 425       |\n",
      "| time/              |           |\n",
      "|    episodes        | 404       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 27383     |\n",
      "|    total_timesteps | 92581     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.95e+03 |\n",
      "|    critic_loss     | 5.19e+03  |\n",
      "|    ent_coef        | 1.25      |\n",
      "|    ent_coef_loss   | -0.0685   |\n",
      "|    learning_rate   | 0.00485   |\n",
      "|    n_updates       | 92579     |\n",
      "|    reward_est_loss | -0.049    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=987.15 +/- 23.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 987       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 95000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.06e+03 |\n",
      "|    critic_loss     | 7.7e+03   |\n",
      "|    ent_coef        | 1.19      |\n",
      "|    ent_coef_loss   | 0.119     |\n",
      "|    learning_rate   | 0.00484   |\n",
      "|    n_updates       | 94998     |\n",
      "|    reward_est_loss | -0.049    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 499       |\n",
      "|    ep_rew_mean     | 455       |\n",
      "| time/              |           |\n",
      "|    episodes        | 408       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 28370     |\n",
      "|    total_timesteps | 95902     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.12e+03 |\n",
      "|    critic_loss     | 6.73e+03  |\n",
      "|    ent_coef        | 1.24      |\n",
      "|    ent_coef_loss   | 0.0993    |\n",
      "|    learning_rate   | 0.00484   |\n",
      "|    n_updates       | 95900     |\n",
      "|    reward_est_loss | -0.0489   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 493       |\n",
      "|    ep_rew_mean     | 449       |\n",
      "| time/              |           |\n",
      "|    episodes        | 412       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 28671     |\n",
      "|    total_timesteps | 96918     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.09e+03 |\n",
      "|    critic_loss     | 5.56e+03  |\n",
      "|    ent_coef        | 1.12      |\n",
      "|    ent_coef_loss   | -0.0567   |\n",
      "|    learning_rate   | 0.00484   |\n",
      "|    n_updates       | 96916     |\n",
      "|    reward_est_loss | -0.0491   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 485       |\n",
      "|    ep_rew_mean     | 430       |\n",
      "| time/              |           |\n",
      "|    episodes        | 416       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 29023     |\n",
      "|    total_timesteps | 98108     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.13e+03 |\n",
      "|    critic_loss     | 8.65e+03  |\n",
      "|    ent_coef        | 1.26      |\n",
      "|    ent_coef_loss   | -0.0189   |\n",
      "|    learning_rate   | 0.00484   |\n",
      "|    n_updates       | 98106     |\n",
      "|    reward_est_loss | -0.0489   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=916.07 +/- 43.94\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 916       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 100000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.22e+03 |\n",
      "|    critic_loss     | 6.54e+03  |\n",
      "|    ent_coef        | 1.52      |\n",
      "|    ent_coef_loss   | 0.0799    |\n",
      "|    learning_rate   | 0.00483   |\n",
      "|    n_updates       | 99998     |\n",
      "|    reward_est_loss | -0.0489   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 489       |\n",
      "|    ep_rew_mean     | 421       |\n",
      "| time/              |           |\n",
      "|    episodes        | 420       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 29729     |\n",
      "|    total_timesteps | 100485    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.38e+03 |\n",
      "|    critic_loss     | 7.48e+03  |\n",
      "|    ent_coef        | 1.47      |\n",
      "|    ent_coef_loss   | -0.0171   |\n",
      "|    learning_rate   | 0.00483   |\n",
      "|    n_updates       | 100483    |\n",
      "|    reward_est_loss | -0.0491   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 492       |\n",
      "|    ep_rew_mean     | 413       |\n",
      "| time/              |           |\n",
      "|    episodes        | 424       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 30138     |\n",
      "|    total_timesteps | 101868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.56e+03 |\n",
      "|    critic_loss     | 1.07e+04  |\n",
      "|    ent_coef        | 2.11      |\n",
      "|    ent_coef_loss   | -0.211    |\n",
      "|    learning_rate   | 0.00483   |\n",
      "|    n_updates       | 101866    |\n",
      "|    reward_est_loss | -0.0489   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 485       |\n",
      "|    ep_rew_mean     | 395       |\n",
      "| time/              |           |\n",
      "|    episodes        | 428       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 31000     |\n",
      "|    total_timesteps | 104779    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.85e+03 |\n",
      "|    critic_loss     | 1.04e+04  |\n",
      "|    ent_coef        | 2.1       |\n",
      "|    ent_coef_loss   | 0.0068    |\n",
      "|    learning_rate   | 0.00483   |\n",
      "|    n_updates       | 104777    |\n",
      "|    reward_est_loss | -0.0491   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=229.97 +/- 175.50\n",
      "Episode length: 439.00 +/- 296.81\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 439       |\n",
      "|    mean_reward     | 230       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 105000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.88e+03 |\n",
      "|    critic_loss     | 1.33e+04  |\n",
      "|    ent_coef        | 1.94      |\n",
      "|    ent_coef_loss   | 0.262     |\n",
      "|    learning_rate   | 0.00483   |\n",
      "|    n_updates       | 104998    |\n",
      "|    reward_est_loss | -0.0491   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 503       |\n",
      "|    ep_rew_mean     | 413       |\n",
      "| time/              |           |\n",
      "|    episodes        | 432       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 32184     |\n",
      "|    total_timesteps | 108779    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.28e+03 |\n",
      "|    critic_loss     | 1.24e+04  |\n",
      "|    ent_coef        | 2.05      |\n",
      "|    ent_coef_loss   | -0.0574   |\n",
      "|    learning_rate   | 0.00482   |\n",
      "|    n_updates       | 108777    |\n",
      "|    reward_est_loss | -0.0491   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=1044.99 +/- 1.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 1.04e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 110000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.39e+03 |\n",
      "|    critic_loss     | 1.25e+04  |\n",
      "|    ent_coef        | 1.81      |\n",
      "|    ent_coef_loss   | -0.35     |\n",
      "|    learning_rate   | 0.00482   |\n",
      "|    n_updates       | 109998    |\n",
      "|    reward_est_loss | -0.0489   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 516       |\n",
      "|    ep_rew_mean     | 436       |\n",
      "| time/              |           |\n",
      "|    episodes        | 436       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 33277     |\n",
      "|    total_timesteps | 112449    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.63e+03 |\n",
      "|    critic_loss     | 1.8e+04   |\n",
      "|    ent_coef        | 1.89      |\n",
      "|    ent_coef_loss   | -0.455    |\n",
      "|    learning_rate   | 0.00481   |\n",
      "|    n_updates       | 112447    |\n",
      "|    reward_est_loss | -0.049    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=1087.22 +/- 102.60\n",
      "Episode length: 963.20 +/- 73.60\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 963       |\n",
      "|    mean_reward     | 1.09e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 115000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.01e+03 |\n",
      "|    critic_loss     | 1.06e+04  |\n",
      "|    ent_coef        | 1.76      |\n",
      "|    ent_coef_loss   | 0.304     |\n",
      "|    learning_rate   | 0.00481   |\n",
      "|    n_updates       | 114998    |\n",
      "|    reward_est_loss | -0.0493   |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 509       |\n",
      "|    ep_rew_mean     | 435       |\n",
      "| time/              |           |\n",
      "|    episodes        | 440       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 34036     |\n",
      "|    total_timesteps | 115001    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.91e+03 |\n",
      "|    critic_loss     | 1.64e+04  |\n",
      "|    ent_coef        | 1.76      |\n",
      "|    ent_coef_loss   | 0.133     |\n",
      "|    learning_rate   | 0.00481   |\n",
      "|    n_updates       | 114999    |\n",
      "|    reward_est_loss | -0.0493   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 519       |\n",
      "|    ep_rew_mean     | 441       |\n",
      "| time/              |           |\n",
      "|    episodes        | 444       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 34860     |\n",
      "|    total_timesteps | 117786    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.15e+03 |\n",
      "|    critic_loss     | 1.33e+04  |\n",
      "|    ent_coef        | 1.58      |\n",
      "|    ent_coef_loss   | -0.168    |\n",
      "|    learning_rate   | 0.0048    |\n",
      "|    n_updates       | 117784    |\n",
      "|    reward_est_loss | -0.0493   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=985.14 +/- 69.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 985       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 120000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.22e+03 |\n",
      "|    critic_loss     | 1.27e+04  |\n",
      "|    ent_coef        | 1.6       |\n",
      "|    ent_coef_loss   | 0.123     |\n",
      "|    learning_rate   | 0.0048    |\n",
      "|    n_updates       | 119998    |\n",
      "|    reward_est_loss | -0.0492   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 511       |\n",
      "|    ep_rew_mean     | 437       |\n",
      "| time/              |           |\n",
      "|    episodes        | 448       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 35593     |\n",
      "|    total_timesteps | 120261    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.21e+03 |\n",
      "|    critic_loss     | 2.04e+04  |\n",
      "|    ent_coef        | 1.73      |\n",
      "|    ent_coef_loss   | -0.152    |\n",
      "|    learning_rate   | 0.0048    |\n",
      "|    n_updates       | 120259    |\n",
      "|    reward_est_loss | -0.0493   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 516       |\n",
      "|    ep_rew_mean     | 445       |\n",
      "| time/              |           |\n",
      "|    episodes        | 452       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 36331     |\n",
      "|    total_timesteps | 122754    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.27e+03 |\n",
      "|    critic_loss     | 1.23e+04  |\n",
      "|    ent_coef        | 1.7       |\n",
      "|    ent_coef_loss   | 0.161     |\n",
      "|    learning_rate   | 0.0048    |\n",
      "|    n_updates       | 122752    |\n",
      "|    reward_est_loss | -0.0492   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=456.14 +/- 64.56\n",
      "Episode length: 288.80 +/- 52.77\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 289       |\n",
      "|    mean_reward     | 456       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 125000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.17e+03 |\n",
      "|    critic_loss     | 1.67e+04  |\n",
      "|    ent_coef        | 1.62      |\n",
      "|    ent_coef_loss   | 0.342     |\n",
      "|    learning_rate   | 0.00479   |\n",
      "|    n_updates       | 124998    |\n",
      "|    reward_est_loss | -0.0493   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 517       |\n",
      "|    ep_rew_mean     | 453       |\n",
      "| time/              |           |\n",
      "|    episodes        | 456       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 37169     |\n",
      "|    total_timesteps | 125582    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.33e+03 |\n",
      "|    critic_loss     | 1.49e+04  |\n",
      "|    ent_coef        | 1.68      |\n",
      "|    ent_coef_loss   | 0.212     |\n",
      "|    learning_rate   | 0.00479   |\n",
      "|    n_updates       | 125580    |\n",
      "|    reward_est_loss | -0.0494   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 522       |\n",
      "|    ep_rew_mean     | 466       |\n",
      "| time/              |           |\n",
      "|    episodes        | 460       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 37677     |\n",
      "|    total_timesteps | 127300    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.31e+03 |\n",
      "|    critic_loss     | 1.53e+04  |\n",
      "|    ent_coef        | 1.75      |\n",
      "|    ent_coef_loss   | 0.297     |\n",
      "|    learning_rate   | 0.00479   |\n",
      "|    n_updates       | 127298    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=1006.56 +/- 6.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 1.01e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 130000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.34e+03 |\n",
      "|    critic_loss     | 1.22e+04  |\n",
      "|    ent_coef        | 1.76      |\n",
      "|    ent_coef_loss   | -0.033    |\n",
      "|    learning_rate   | 0.00478   |\n",
      "|    n_updates       | 129998    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 555       |\n",
      "|    ep_rew_mean     | 508       |\n",
      "| time/              |           |\n",
      "|    episodes        | 464       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 38853     |\n",
      "|    total_timesteps | 131275    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.47e+03 |\n",
      "|    critic_loss     | 1.6e+04   |\n",
      "|    ent_coef        | 1.5       |\n",
      "|    ent_coef_loss   | -0.000996 |\n",
      "|    learning_rate   | 0.00478   |\n",
      "|    n_updates       | 131273    |\n",
      "|    reward_est_loss | -0.0496   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=116.47 +/- 9.85\n",
      "Episode length: 255.60 +/- 12.31\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 256       |\n",
      "|    mean_reward     | 116       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 135000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.54e+03 |\n",
      "|    critic_loss     | 1.51e+04  |\n",
      "|    ent_coef        | 1.56      |\n",
      "|    ent_coef_loss   | -0.145    |\n",
      "|    learning_rate   | 0.00478   |\n",
      "|    n_updates       | 134998    |\n",
      "|    reward_est_loss | -0.0496   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 589      |\n",
      "|    ep_rew_mean     | 549      |\n",
      "| time/              |          |\n",
      "|    episodes        | 468      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 40036    |\n",
      "|    total_timesteps | 135275   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.5e+03 |\n",
      "|    critic_loss     | 2e+04    |\n",
      "|    ent_coef        | 1.54     |\n",
      "|    ent_coef_loss   | -0.205   |\n",
      "|    learning_rate   | 0.00477  |\n",
      "|    n_updates       | 135273   |\n",
      "|    reward_est_loss | -0.0496  |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 594       |\n",
      "|    ep_rew_mean     | 553       |\n",
      "| time/              |           |\n",
      "|    episodes        | 472       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 40310     |\n",
      "|    total_timesteps | 136202    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.55e+03 |\n",
      "|    critic_loss     | 1.6e+04   |\n",
      "|    ent_coef        | 1.58      |\n",
      "|    ent_coef_loss   | -0.0253   |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 136200    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 594       |\n",
      "|    ep_rew_mean     | 552       |\n",
      "| time/              |           |\n",
      "|    episodes        | 476       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 40579     |\n",
      "|    total_timesteps | 137109    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.71e+03 |\n",
      "|    critic_loss     | 1.14e+04  |\n",
      "|    ent_coef        | 1.53      |\n",
      "|    ent_coef_loss   | -0.076    |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 137107    |\n",
      "|    reward_est_loss | -0.0496   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 592       |\n",
      "|    ep_rew_mean     | 545       |\n",
      "| time/              |           |\n",
      "|    episodes        | 480       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 40816     |\n",
      "|    total_timesteps | 137910    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.59e+03 |\n",
      "|    critic_loss     | 1.88e+04  |\n",
      "|    ent_coef        | 1.58      |\n",
      "|    ent_coef_loss   | -0.335    |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 137908    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 580       |\n",
      "|    ep_rew_mean     | 535       |\n",
      "| time/              |           |\n",
      "|    episodes        | 484       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 41140     |\n",
      "|    total_timesteps | 139006    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.68e+03 |\n",
      "|    critic_loss     | 1.34e+04  |\n",
      "|    ent_coef        | 1.58      |\n",
      "|    ent_coef_loss   | -0.172    |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 139004    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=27.85 +/- 1.34\n",
      "Episode length: 59.20 +/- 0.75\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 59.2      |\n",
      "|    mean_reward     | 27.9      |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 140000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.62e+03 |\n",
      "|    critic_loss     | 1.37e+04  |\n",
      "|    ent_coef        | 1.74      |\n",
      "|    ent_coef_loss   | 0.123     |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 139998    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 551       |\n",
      "|    ep_rew_mean     | 505       |\n",
      "| time/              |           |\n",
      "|    episodes        | 488       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 41441     |\n",
      "|    total_timesteps | 140020    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.73e+03 |\n",
      "|    critic_loss     | 1.99e+04  |\n",
      "|    ent_coef        | 1.6       |\n",
      "|    ent_coef_loss   | -0.307    |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 140018    |\n",
      "|    reward_est_loss | -0.0496   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 534       |\n",
      "|    ep_rew_mean     | 492       |\n",
      "| time/              |           |\n",
      "|    episodes        | 492       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 41627     |\n",
      "|    total_timesteps | 140649    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.79e+03 |\n",
      "|    critic_loss     | 3.03e+04  |\n",
      "|    ent_coef        | 1.97      |\n",
      "|    ent_coef_loss   | 0.348     |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 140647    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 529       |\n",
      "|    ep_rew_mean     | 484       |\n",
      "| time/              |           |\n",
      "|    episodes        | 496       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 42089     |\n",
      "|    total_timesteps | 142211    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.88e+03 |\n",
      "|    critic_loss     | 1.16e+04  |\n",
      "|    ent_coef        | 1.77      |\n",
      "|    ent_coef_loss   | 0.299     |\n",
      "|    learning_rate   | 0.00476   |\n",
      "|    n_updates       | 142209    |\n",
      "|    reward_est_loss | -0.0494   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=896.51 +/- 6.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 897       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 145000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.87e+03 |\n",
      "|    critic_loss     | 2.37e+04  |\n",
      "|    ent_coef        | 1.74      |\n",
      "|    ent_coef_loss   | -0.431    |\n",
      "|    learning_rate   | 0.00476   |\n",
      "|    n_updates       | 144998    |\n",
      "|    reward_est_loss | -0.0493   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 543       |\n",
      "|    ep_rew_mean     | 502       |\n",
      "| time/              |           |\n",
      "|    episodes        | 500       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 43006     |\n",
      "|    total_timesteps | 145307    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.05e+03 |\n",
      "|    critic_loss     | 1.19e+04  |\n",
      "|    ent_coef        | 1.56      |\n",
      "|    ent_coef_loss   | -0.0867   |\n",
      "|    learning_rate   | 0.00476   |\n",
      "|    n_updates       | 145305    |\n",
      "|    reward_est_loss | -0.0494   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 567      |\n",
      "|    ep_rew_mean     | 524      |\n",
      "| time/              |          |\n",
      "|    episodes        | 504      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 44194    |\n",
      "|    total_timesteps | 149307   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.3e+03 |\n",
      "|    critic_loss     | 1.32e+04 |\n",
      "|    ent_coef        | 1.72     |\n",
      "|    ent_coef_loss   | 0.238    |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 149305   |\n",
      "|    reward_est_loss | -0.0493  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=826.37 +/- 12.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 826       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 150000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.22e+03 |\n",
      "|    critic_loss     | 1.17e+04  |\n",
      "|    ent_coef        | 1.6       |\n",
      "|    ent_coef_loss   | 0.129     |\n",
      "|    learning_rate   | 0.00475   |\n",
      "|    n_updates       | 149998    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 567      |\n",
      "|    ep_rew_mean     | 516      |\n",
      "| time/              |          |\n",
      "|    episodes        | 508      |\n",
      "|    fps             | 3        |\n",
      "|    time_elapsed    | 45172    |\n",
      "|    total_timesteps | 152596   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.2e+03 |\n",
      "|    critic_loss     | 1.57e+04 |\n",
      "|    ent_coef        | 1.75     |\n",
      "|    ent_coef_loss   | -0.224   |\n",
      "|    learning_rate   | 0.00475  |\n",
      "|    n_updates       | 152594   |\n",
      "|    reward_est_loss | -0.0495  |\n",
      "---------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=761.92 +/- 8.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 762       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 155000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.56e+03 |\n",
      "|    critic_loss     | 2.15e+04  |\n",
      "|    ent_coef        | 2.1       |\n",
      "|    ent_coef_loss   | -0.456    |\n",
      "|    learning_rate   | 0.00474   |\n",
      "|    n_updates       | 154998    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 597       |\n",
      "|    ep_rew_mean     | 536       |\n",
      "| time/              |           |\n",
      "|    episodes        | 512       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 46361     |\n",
      "|    total_timesteps | 156596    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.21e+03 |\n",
      "|    critic_loss     | 2.43e+04  |\n",
      "|    ent_coef        | 1.8       |\n",
      "|    ent_coef_loss   | -0.435    |\n",
      "|    learning_rate   | 0.00474   |\n",
      "|    n_updates       | 156594    |\n",
      "|    reward_est_loss | -0.0496   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 600       |\n",
      "|    ep_rew_mean     | 533       |\n",
      "| time/              |           |\n",
      "|    episodes        | 516       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 46809     |\n",
      "|    total_timesteps | 158106    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.81e+03 |\n",
      "|    critic_loss     | 3.89e+04  |\n",
      "|    ent_coef        | 2.08      |\n",
      "|    ent_coef_loss   | -0.739    |\n",
      "|    learning_rate   | 0.00474   |\n",
      "|    n_updates       | 158104    |\n",
      "|    reward_est_loss | -0.0496   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=888.21 +/- 18.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 888       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 160000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.94e+03 |\n",
      "|    critic_loss     | 2.3e+04   |\n",
      "|    ent_coef        | 2.22      |\n",
      "|    ent_coef_loss   | -0.0223   |\n",
      "|    learning_rate   | 0.00473   |\n",
      "|    n_updates       | 159998    |\n",
      "|    reward_est_loss | -0.0496   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 616       |\n",
      "|    ep_rew_mean     | 547       |\n",
      "| time/              |           |\n",
      "|    episodes        | 520       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 47994     |\n",
      "|    total_timesteps | 162106    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.48e+03 |\n",
      "|    critic_loss     | 2.32e+04  |\n",
      "|    ent_coef        | 2.25      |\n",
      "|    ent_coef_loss   | -0.288    |\n",
      "|    learning_rate   | 0.00473   |\n",
      "|    n_updates       | 162104    |\n",
      "|    reward_est_loss | -0.0495   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=894.57 +/- 47.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 895       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 165000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.62e+03 |\n",
      "|    critic_loss     | 2.53e+04  |\n",
      "|    ent_coef        | 2.58      |\n",
      "|    ent_coef_loss   | 0.122     |\n",
      "|    learning_rate   | 0.00473   |\n",
      "|    n_updates       | 164998    |\n",
      "|    reward_est_loss | -0.0496   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 642       |\n",
      "|    ep_rew_mean     | 564       |\n",
      "| time/              |           |\n",
      "|    episodes        | 524       |\n",
      "|    fps             | 3         |\n",
      "|    time_elapsed    | 49176     |\n",
      "|    total_timesteps | 166106    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.68e+03 |\n",
      "|    critic_loss     | 2.45e+04  |\n",
      "|    ent_coef        | 2.61      |\n",
      "|    ent_coef_loss   | 0.254     |\n",
      "|    learning_rate   | 0.00472   |\n",
      "|    n_updates       | 166104    |\n",
      "|    reward_est_loss | -0.0493   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy Inverse Policy Mirror Descent.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m logtime \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(eval_env, best_model_save_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/\u001b[39m\u001b[39m{\u001b[39;00menv_id\u001b[39m}\u001b[39;00m\u001b[39m-iapmd-\u001b[39m\u001b[39m{\u001b[39;00mlogtime\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W5sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                              log_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/\u001b[39m\u001b[39m{\u001b[39;00menv_id\u001b[39m}\u001b[39;00m\u001b[39m-iapmd-\u001b[39m\u001b[39m{\u001b[39;00mlogtime\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, eval_freq\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                              deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ipmd_model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m3e6\u001b[39;49m, log_interval\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[0;32m~/Developer/Inverse-Policy-Mirror-Descent/apmd_off/iapmd.py:377\u001b[0m, in \u001b[0;36mIPMD.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    365\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    366\u001b[0m         total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    374\u001b[0m         reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    375\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[0;32m--> 377\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    378\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    379\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    380\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    381\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    382\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    383\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    384\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    385\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    386\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    387\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/irl/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:366\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    365\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 366\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[1;32m    368\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    370\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Developer/Inverse-Policy-Mirror-Descent/apmd_off/iapmd.py:335\u001b[0m, in \u001b[0;36mIPMD.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    333\u001b[0m     reward_est_losses\u001b[39m.\u001b[39mappend(reward_est_loss\u001b[39m.\u001b[39mitem())\n\u001b[1;32m    334\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_est\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 335\u001b[0m     reward_est_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    336\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_est\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m    339\u001b[0m \u001b[39m# Update target networks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/irl/lib/python3.10/site-packages/torch/_tensor.py:400\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    392\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    393\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    394\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    399\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 400\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/miniforge3/envs/irl/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from apmd_off.iapmd import IPMD\n",
    "env_id = 'Walker2d-v4'\n",
    "expert_samples_replay_buffer_loc = f\"utils/logs/expert/{env_id}-sac/buffer.pkl\"\n",
    "\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "ipmd_model = IPMD(\"MlpPolicy\", env, gamma=1.0, verbose=1,\n",
    "                  batch_size=256, train_freq=1, learning_rate=linear_schedule(5e-3),\n",
    "                  gradient_steps=1, expert_replay_buffer_loc=expert_samples_replay_buffer_loc)\n",
    "\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "logtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'logs/{env_id}-iapmd-{logtime}/',\n",
    "                             log_path=f'logs/{env_id}-iapmd-{logtime}/', eval_freq=5000,\n",
    "                             deterministic=True, render=False)\n",
    "                             \n",
    "ipmd_model.learn(total_timesteps=3e6, log_interval=4, callback=eval_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d2e891c9d48cbc5657a17ab4ab08b2c1d2ec0060cc3e51694592beb2a6aa825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
