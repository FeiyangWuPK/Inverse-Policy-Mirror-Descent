{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66f737c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from apmd_on.apmd import PMD\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "% load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a4e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0, eval_env=None, model=None):\n",
    "        super(EvaluateCallback, self).__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.eval_env = eval_env\n",
    "        self.model = model\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        self.iter = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=5, deterministic=False)\n",
    "        print(f\"Iter {self.iter:d} mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        self.means.append(mean_reward)\n",
    "        self.stds.append(std_reward)\n",
    "        self.iter += 1\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "\n",
    "# Learning rate schedule\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def plot_costs(rewards, names, smoothing_window=10, n=3, fig_name=\"acrobot.png\", stds=None):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    colors = ['tomato', 'royalblue', 'mediumpurple']\n",
    "    for i in range(n):\n",
    "        extend = np.concatenate([np.ones(smoothing_window) * rewards[i][0], rewards[i]])\n",
    "        rewards_smoothed = pd.Series(extend).rolling(smoothing_window, min_periods=smoothing_window).mean().to_numpy()\n",
    "        rewards_smoothed = rewards_smoothed[smoothing_window - 1:]\n",
    "        rewards_smoothed = rewards_smoothed[:5000]\n",
    "        x = np.linspace(1, 5000, num=5000)\n",
    "        if stds is None:\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3)\n",
    "        else:\n",
    "            lower = rewards_smoothed - stds[i][:5000]\n",
    "            upper = rewards_smoothed + stds[i][:5000]\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3, c=colors[i])\n",
    "            plt.fill_between(x, y1=lower, y2=upper, interpolate=True, c=colors[i], alpha=0.5)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episodic Costs\")\n",
    "    plt.title(fig_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "    # plt.savefig(fig_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17971d",
   "metadata": {},
   "source": [
    "## Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd7efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from apmd_off.iapmd import IPMD\n",
    "\n",
    "env_id = 'Ant-v4'\n",
    "expert_samples_replay_buffer_loc = \"utils/logs/expert/Ant-v4-sac/buffer5e6.pkl\"\n",
    "\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "ipmd_model = IPMD(\"MlpPolicy\", env, gamma=1.0, verbose=1,\n",
    "                  batch_size=256, train_freq=1, learning_rate=linear_schedule(5e-3),\n",
    "                  gradient_steps=1, expert_replay_buffer_loc=expert_samples_replay_buffer_loc)\n",
    "\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "logtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'logs/{env_id}-iapmd-{logtime}/',\n",
    "                             log_path=f'logs/{env_id}-iapmd-{logtime}/', eval_freq=5000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "ipmd_model.learn(total_timesteps=3e6, log_interval=4, callback=eval_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Walker2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6d83b8",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from apmd_off.iapmd import IPMD\n",
    "\n",
    "env_id = 'Walker2d-v4'\n",
    "expert_samples_replay_buffer_loc = f\"utils/logs/expert/{env_id}-sac/buffer.pkl\"\n",
    "\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "ipmd_model = IPMD(\"MlpPolicy\", env, gamma=1.0, verbose=1,\n",
    "                  batch_size=256, train_freq=1, learning_rate=linear_schedule(5e-3),\n",
    "                  gradient_steps=1, expert_replay_buffer_loc=expert_samples_replay_buffer_loc)\n",
    "\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "logtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'logs/{env_id}-iapmd-{logtime}/',\n",
    "                             log_path=f'logs/{env_id}-iapmd-{logtime}/', eval_freq=5000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "ipmd_model.learn(total_timesteps=3e6, log_interval=4, callback=eval_callback)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d2e891c9d48cbc5657a17ab4ab08b2c1d2ec0060cc3e51694592beb2a6aa825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
