{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66f737c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CheckpointCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from apmd_on.apmd import PMD\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31a4e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    :param verbose: (int) Verbosity level 0: not output 1: info 2: debug\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0, eval_env=None, model=None):\n",
    "        super(EvaluateCallback, self).__init__(verbose)\n",
    "        # Those variables will be accessible in the callback\n",
    "        # (they are defined in the base class)\n",
    "        # The RL model\n",
    "        # self.model = None  # type: BaseRLModel\n",
    "        # An alias for self.model.get_env(), the environment used for training\n",
    "        # self.training_env = None  # type: Union[gym.Env, VecEnv, None]\n",
    "        # Number of time the callback was called\n",
    "        # self.n_calls = 0  # type: int\n",
    "        # self.num_timesteps = 0  # type: int\n",
    "        # local and global variables\n",
    "        # self.locals = None  # type: Dict[str, Any]\n",
    "        # self.globals = None  # type: Dict[str, Any]\n",
    "        # The logger object, used to report things in the terminal\n",
    "        # self.logger = None  # type: logger.Logger\n",
    "        # # Sometimes, for event callback, it is useful\n",
    "        # # to have access to the parent object\n",
    "        # self.parent = None  # type: Optional[BaseCallback]\n",
    "        self.eval_env = eval_env\n",
    "        self.model = model\n",
    "        self.means = []\n",
    "        self.stds = []\n",
    "        self.iter = 0\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: (bool) If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        mean_reward, std_reward = evaluate_policy(self.model, self.eval_env, n_eval_episodes=5, deterministic=False)\n",
    "        print(f\"Iter {self.iter:d} mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "        self.means.append(mean_reward)\n",
    "        self.stds.append(std_reward)\n",
    "        self.iter += 1\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "# Learning rate schedule \n",
    "from typing import Callable\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "\n",
    "    :param initial_value: Initial learning rate.\n",
    "    :return: schedule that computes\n",
    "      current learning rate depending on remaining progress\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress will decrease from 1 (beginning) to 0.\n",
    "\n",
    "        :param progress_remaining:\n",
    "        :return: current learning rate\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "\n",
    "    return func\n",
    "\n",
    "def plot_costs(rewards, names, smoothing_window=10, n=3, fig_name=\"acrobot.png\", stds=None):\n",
    "    fig = plt.figure(figsize=(7, 5))\n",
    "    colors = ['tomato', 'royalblue', 'mediumpurple']\n",
    "    for i in range(n):\n",
    "        extend = np.concatenate([np.ones(smoothing_window)*rewards[i][0], rewards[i]])\n",
    "        rewards_smoothed = pd.Series(extend).rolling(smoothing_window, min_periods=smoothing_window).mean().to_numpy()\n",
    "        rewards_smoothed = rewards_smoothed[smoothing_window-1:]\n",
    "        rewards_smoothed = rewards_smoothed[:5000]\n",
    "        x = np.linspace(1, 5000, num=5000)\n",
    "        if stds is None:\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3)\n",
    "        else:\n",
    "            lower = rewards_smoothed - stds[i][ :5000]\n",
    "            upper = rewards_smoothed + stds[i][ :5000]\n",
    "            plt.plot(rewards_smoothed, label=names[i], linewidth=3, c=colors[i])\n",
    "            plt.fill_between(x, y1=lower, y2=upper, interpolate=True, c=colors[i], alpha=0.5)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episodic Costs\")\n",
    "    plt.title(fig_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    # plt.close()\n",
    "    # plt.savefig(fig_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "742a8b9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-10-15 12:03:09'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "now = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b17971d",
   "metadata": {},
   "source": [
    "## Ant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00dd7efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patrick/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/patrick/.local/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 45       |\n",
      "|    ep_rew_mean     | -2.49    |\n",
      "| time/              |          |\n",
      "|    episodes        | 1        |\n",
      "|    fps             | 97       |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 45       |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.16    |\n",
      "|    critic_loss     | 0.811    |\n",
      "|    ent_coef        | 0.809    |\n",
      "|    ent_coef_loss   | -2.8     |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 43       |\n",
      "|    reward_est_loss | nan      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 46       |\n",
      "|    ep_rew_mean     | -6.45    |\n",
      "| time/              |          |\n",
      "|    episodes        | 2        |\n",
      "|    fps             | 54       |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 92       |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.32    |\n",
      "|    critic_loss     | 0.436    |\n",
      "|    ent_coef        | 0.639    |\n",
      "|    ent_coef_loss   | -5.86    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 90       |\n",
      "|    reward_est_loss | 0.85     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 71.3     |\n",
      "|    ep_rew_mean     | -27      |\n",
      "| time/              |          |\n",
      "|    episodes        | 3        |\n",
      "|    fps             | 69       |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 214      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.59    |\n",
      "|    critic_loss     | 0.12     |\n",
      "|    ent_coef        | 0.348    |\n",
      "|    ent_coef_loss   | -13.6    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 212      |\n",
      "|    reward_est_loss | nan      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 93.2     |\n",
      "|    ep_rew_mean     | -39.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 77       |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 373      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.29    |\n",
      "|    critic_loss     | 0.0519   |\n",
      "|    ent_coef        | 0.159    |\n",
      "|    ent_coef_loss   | -23.3    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 371      |\n",
      "|    reward_est_loss | nan      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.6     |\n",
      "|    ep_rew_mean     | -35.7    |\n",
      "| time/              |          |\n",
      "|    episodes        | 5        |\n",
      "|    fps             | 80       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 478      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.02    |\n",
      "|    critic_loss     | 0.0447   |\n",
      "|    ent_coef        | 0.0959   |\n",
      "|    ent_coef_loss   | -28.5    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 476      |\n",
      "|    reward_est_loss | nan      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.5     |\n",
      "|    ep_rew_mean     | -36.3    |\n",
      "| time/              |          |\n",
      "|    episodes        | 6        |\n",
      "|    fps             | 81       |\n",
      "|    time_elapsed    | 6        |\n",
      "|    total_timesteps | 519      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5       |\n",
      "|    critic_loss     | 0.0434   |\n",
      "|    ent_coef        | 0.0791   |\n",
      "|    ent_coef_loss   | -30.1    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 517      |\n",
      "|    reward_est_loss | nan      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.1     |\n",
      "|    ep_rew_mean     | -44.6    |\n",
      "| time/              |          |\n",
      "|    episodes        | 7        |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 645      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.85    |\n",
      "|    critic_loss     | 0.0406   |\n",
      "|    ent_coef        | 0.0447   |\n",
      "|    ent_coef_loss   | -31.7    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 643      |\n",
      "|    reward_est_loss | nan      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 84.2     |\n",
      "|    ep_rew_mean     | -43.1    |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 82       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 674      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.84    |\n",
      "|    critic_loss     | 0.0487   |\n",
      "|    ent_coef        | 0.0395   |\n",
      "|    ent_coef_loss   | -32.1    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 672      |\n",
      "|    reward_est_loss | nan      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.4     |\n",
      "|    ep_rew_mean     | -45      |\n",
      "| time/              |          |\n",
      "|    episodes        | 9        |\n",
      "|    fps             | 83       |\n",
      "|    time_elapsed    | 8        |\n",
      "|    total_timesteps | 733      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.78    |\n",
      "|    critic_loss     | 0.04     |\n",
      "|    ent_coef        | 0.0312   |\n",
      "|    ent_coef_loss   | -35.4    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 731      |\n",
      "|    reward_est_loss | nan      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 82.4     |\n",
      "|    ep_rew_mean     | -42      |\n",
      "| time/              |          |\n",
      "|    episodes        | 10       |\n",
      "|    fps             | 84       |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 824      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.85    |\n",
      "|    critic_loss     | 0.0357   |\n",
      "|    ent_coef        | 0.0221   |\n",
      "|    ent_coef_loss   | -31.3    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 822      |\n",
      "|    reward_est_loss | nan      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy Inverse Policy Mirror Descent.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m logtime \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mdatetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m'\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%\u001b[39m\u001b[39mH:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mS\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W3sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m eval_callback \u001b[39m=\u001b[39m EvalCallback(eval_env, best_model_save_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/\u001b[39m\u001b[39m{\u001b[39;00menv_id\u001b[39m}\u001b[39;00m\u001b[39m-iapmd-\u001b[39m\u001b[39m{\u001b[39;00mlogtime\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W3sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                              log_path\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlogs/\u001b[39m\u001b[39m{\u001b[39;00menv_id\u001b[39m}\u001b[39;00m\u001b[39m-iapmd-\u001b[39m\u001b[39m{\u001b[39;00mlogtime\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, eval_freq\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                              deterministic\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, render\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/patrick/Developer/Inverse-Policy-Mirror-Descent/Off-policy%20Inverse%20Policy%20Mirror%20Descent.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m ipmd_model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m3e6\u001b[39;49m, log_interval\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, callback\u001b[39m=\u001b[39;49meval_callback)\n",
      "File \u001b[0;32m~/Developer/Inverse-Policy-Mirror-Descent/apmd_off/iapmd.py:376\u001b[0m, in \u001b[0;36mIPMD.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    364\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    365\u001b[0m         total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    373\u001b[0m         reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    374\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[0;32m--> 376\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    377\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    378\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    379\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    380\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    381\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    382\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    383\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    384\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    385\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    386\u001b[0m     )\n",
      "File \u001b[0;32m~/miniforge3/envs/irl/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:366\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    365\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 366\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[1;32m    368\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    370\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/Developer/Inverse-Policy-Mirror-Descent/apmd_off/iapmd.py:340\u001b[0m, in \u001b[0;36mIPMD.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39m# Update target networks\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m gradient_step \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_update_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 340\u001b[0m     polyak_update(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic\u001b[39m.\u001b[39;49mparameters(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic_target\u001b[39m.\u001b[39;49mparameters(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtau)\n\u001b[1;32m    341\u001b[0m     \u001b[39m# Copy running stats, see GH issue #996\u001b[39;00m\n\u001b[1;32m    342\u001b[0m     polyak_update(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm_stats, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_norm_stats_target, \u001b[39m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/irl/lib/python3.10/site-packages/stable_baselines3/common/utils.py:449\u001b[0m, in \u001b[0;36mpolyak_update\u001b[0;34m(params, target_params, tau)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39mPerform a Polyak average update on ``target_params`` using ``params``:\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39mtarget parameters are slowly updated towards the main parameters.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[39m:param tau: the soft update coefficient (\"Polyak update\", between 0 and 1)\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    447\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m    448\u001b[0m     \u001b[39m# zip does not raise an exception if length of parameters does not match.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mfor\u001b[39;00m param, target_param \u001b[39min\u001b[39;00m zip_strict(params, target_params):\n\u001b[1;32m    450\u001b[0m         target_param\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mmul_(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m tau)\n\u001b[1;32m    451\u001b[0m         th\u001b[39m.\u001b[39madd(target_param\u001b[39m.\u001b[39mdata, param\u001b[39m.\u001b[39mdata, alpha\u001b[39m=\u001b[39mtau, out\u001b[39m=\u001b[39mtarget_param\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[0;32m~/miniforge3/envs/irl/lib/python3.10/site-packages/stable_baselines3/common/utils.py:422\u001b[0m, in \u001b[0;36mzip_strict\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    420\u001b[0m sentinel \u001b[39m=\u001b[39m \u001b[39mobject\u001b[39m()\n\u001b[1;32m    421\u001b[0m \u001b[39mfor\u001b[39;00m combo \u001b[39min\u001b[39;00m zip_longest(\u001b[39m*\u001b[39miterables, fillvalue\u001b[39m=\u001b[39msentinel):\n\u001b[0;32m--> 422\u001b[0m     \u001b[39mif\u001b[39;00m sentinel \u001b[39min\u001b[39;00m combo:\n\u001b[1;32m    423\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIterables have different lengths\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    424\u001b[0m     \u001b[39myield\u001b[39;00m combo\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from apmd_off.iapmd import IPMD\n",
    "env_id = 'Ant-v4'\n",
    "expert_samples_replay_buffer_loc = \"utils/logs/expert/Ant-v4-sac/buffer5e6.pkl\"\n",
    "\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "ipmd_model = IPMD(\"MlpPolicy\", env, gamma=1.0, verbose=1, \n",
    "                  batch_size=256, train_freq=1, learning_rate=linear_schedule(5e-3),\n",
    "                  gradient_steps=1, expert_replay_buffer_loc=expert_samples_replay_buffer_loc)\n",
    "\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "logtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=f'logs/{env_id}-iapmd-{logtime}/',\n",
    "                             log_path=f'logs/{env_id}-iapmd-{logtime}/', eval_freq=5000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "ipmd_model.learn(total_timesteps=3e6, log_interval=1, callback=eval_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Walker2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e6d83b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 17.8     |\n",
      "|    ep_rew_mean     | 0.696    |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 44       |\n",
      "|    time_elapsed    | 1        |\n",
      "|    total_timesteps | 71       |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.26    |\n",
      "|    critic_loss     | 0.291    |\n",
      "|    ent_coef        | 0.709    |\n",
      "|    ent_coef_loss   | -3.47    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 69       |\n",
      "|    reward_est_loss | -0.873   |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 16.4     |\n",
      "|    ep_rew_mean     | -0.173   |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 42       |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 131      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.51    |\n",
      "|    critic_loss     | 0.318    |\n",
      "|    ent_coef        | 0.524    |\n",
      "|    ent_coef_loss   | -6.53    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 129      |\n",
      "|    reward_est_loss | -1.8     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 19       |\n",
      "|    ep_rew_mean     | 0.63     |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 41       |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 228      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.66    |\n",
      "|    critic_loss     | 0.213    |\n",
      "|    ent_coef        | 0.322    |\n",
      "|    ent_coef_loss   | -11.4    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 226      |\n",
      "|    reward_est_loss | -3.73    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.6     |\n",
      "|    ep_rew_mean     | 1.07     |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 42       |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 298      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.25    |\n",
      "|    critic_loss     | 0.628    |\n",
      "|    ent_coef        | 0.227    |\n",
      "|    ent_coef_loss   | -15      |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 296      |\n",
      "|    reward_est_loss | -5.84    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | 2.76     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 42       |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 437      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.985   |\n",
      "|    critic_loss     | 4.14     |\n",
      "|    ent_coef        | 0.114    |\n",
      "|    ent_coef_loss   | -20.9    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 435      |\n",
      "|    reward_est_loss | -10.9    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.5     |\n",
      "|    ep_rew_mean     | 1.21     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 43       |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 563      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.01    |\n",
      "|    critic_loss     | 2.88     |\n",
      "|    ent_coef        | 0.0644   |\n",
      "|    ent_coef_loss   | -21.9    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 561      |\n",
      "|    reward_est_loss | -17.3    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 33.6     |\n",
      "|    ep_rew_mean     | -5.13    |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 45       |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 941      |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.89    |\n",
      "|    critic_loss     | 11.3     |\n",
      "|    ent_coef        | 0.0303   |\n",
      "|    ent_coef_loss   | -2.94    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 939      |\n",
      "|    reward_est_loss | -38.9    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 48.6     |\n",
      "|    ep_rew_mean     | 6.65     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 1554     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.42    |\n",
      "|    critic_loss     | 44.6     |\n",
      "|    ent_coef        | 0.0572   |\n",
      "|    ent_coef_loss   | 0.564    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 1552     |\n",
      "|    reward_est_loss | -81.4    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 58.6     |\n",
      "|    ep_rew_mean     | 27.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 44       |\n",
      "|    total_timesteps | 2111     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -13.3    |\n",
      "|    critic_loss     | 220      |\n",
      "|    ent_coef        | 0.0874   |\n",
      "|    ent_coef_loss   | -0.352   |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 2109     |\n",
      "|    reward_est_loss | -135     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 68.3     |\n",
      "|    ep_rew_mean     | 50.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 2731     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -23      |\n",
      "|    critic_loss     | 677      |\n",
      "|    ent_coef        | 0.119    |\n",
      "|    ent_coef_loss   | 0.761    |\n",
      "|    learning_rate   | 0.005    |\n",
      "|    n_updates       | 2729     |\n",
      "|    reward_est_loss | -210     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 74.1     |\n",
      "|    ep_rew_mean     | 58.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 69       |\n",
      "|    total_timesteps | 3260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -30.3    |\n",
      "|    critic_loss     | 1.3e+03  |\n",
      "|    ent_coef        | 0.154    |\n",
      "|    ent_coef_loss   | 2.44     |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 3258     |\n",
      "|    reward_est_loss | -275     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 77.4     |\n",
      "|    ep_rew_mean     | 56.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 79       |\n",
      "|    total_timesteps | 3713     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -41.1    |\n",
      "|    critic_loss     | 2.13e+03 |\n",
      "|    ent_coef        | 0.192    |\n",
      "|    ent_coef_loss   | 0.393    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 3711     |\n",
      "|    reward_est_loss | -341     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.9     |\n",
      "|    ep_rew_mean     | 54.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total_timesteps | 4260     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -47      |\n",
      "|    critic_loss     | 3.66e+03 |\n",
      "|    ent_coef        | 0.232    |\n",
      "|    ent_coef_loss   | -0.558   |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 4258     |\n",
      "|    reward_est_loss | -409     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.2     |\n",
      "|    ep_rew_mean     | 51.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 47       |\n",
      "|    time_elapsed    | 98       |\n",
      "|    total_timesteps | 4661     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -55.5    |\n",
      "|    critic_loss     | 5.99e+03 |\n",
      "|    ent_coef        | 0.288    |\n",
      "|    ent_coef_loss   | 0.108    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 4659     |\n",
      "|    reward_est_loss | -441     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=160.31 +/- 342.57\n",
      "Episode length: 379.20 +/- 349.74\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 379      |\n",
      "|    mean_reward     | 160      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -77.9    |\n",
      "|    critic_loss     | 6.58e+03 |\n",
      "|    ent_coef        | 0.316    |\n",
      "|    ent_coef_loss   | 0.209    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 4998     |\n",
      "|    reward_est_loss | -502     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.9     |\n",
      "|    ep_rew_mean     | 46.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 113      |\n",
      "|    total_timesteps | 5212     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -81.6    |\n",
      "|    critic_loss     | 7.76e+03 |\n",
      "|    ent_coef        | 0.343    |\n",
      "|    ent_coef_loss   | 0.362    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 5210     |\n",
      "|    reward_est_loss | -509     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.8     |\n",
      "|    ep_rew_mean     | 43.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 119      |\n",
      "|    total_timesteps | 5553     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -97.6    |\n",
      "|    critic_loss     | 1.76e+04 |\n",
      "|    ent_coef        | 0.393    |\n",
      "|    ent_coef_loss   | -0.069   |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 5551     |\n",
      "|    reward_est_loss | -577     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 86.3     |\n",
      "|    ep_rew_mean     | 44.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 126      |\n",
      "|    total_timesteps | 5868     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -108     |\n",
      "|    critic_loss     | 1.27e+04 |\n",
      "|    ent_coef        | 0.426    |\n",
      "|    ent_coef_loss   | -0.141   |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 5866     |\n",
      "|    reward_est_loss | -421     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.4     |\n",
      "|    ep_rew_mean     | 48.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 138      |\n",
      "|    total_timesteps | 6440     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -134     |\n",
      "|    critic_loss     | 1.71e+04 |\n",
      "|    ent_coef        | 0.475    |\n",
      "|    ent_coef_loss   | -0.00331 |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 6438     |\n",
      "|    reward_est_loss | -697     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 89.6     |\n",
      "|    ep_rew_mean     | 47.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 145      |\n",
      "|    total_timesteps | 6813     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -165     |\n",
      "|    critic_loss     | 1.74e+04 |\n",
      "|    ent_coef        | 0.527    |\n",
      "|    ent_coef_loss   | 0.257    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 6811     |\n",
      "|    reward_est_loss | -721     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.9     |\n",
      "|    ep_rew_mean     | 52.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 157      |\n",
      "|    total_timesteps | 7349     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -193     |\n",
      "|    critic_loss     | 2.86e+04 |\n",
      "|    ent_coef        | 0.589    |\n",
      "|    ent_coef_loss   | -0.232   |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 7347     |\n",
      "|    reward_est_loss | -800     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.3     |\n",
      "|    ep_rew_mean     | 52.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 164      |\n",
      "|    total_timesteps | 7671     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -199     |\n",
      "|    critic_loss     | 7.49e+04 |\n",
      "|    ent_coef        | 0.599    |\n",
      "|    ent_coef_loss   | 0.261    |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 7669     |\n",
      "|    reward_est_loss | -787     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 91.6     |\n",
      "|    ep_rew_mean     | 53.3     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 172      |\n",
      "|    total_timesteps | 8058     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -219     |\n",
      "|    critic_loss     | 4.59e+04 |\n",
      "|    ent_coef        | 0.674    |\n",
      "|    ent_coef_loss   | -0.0504  |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 8056     |\n",
      "|    reward_est_loss | -846     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.3     |\n",
      "|    ep_rew_mean     | 55.6     |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 181      |\n",
      "|    total_timesteps | 8493     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -242     |\n",
      "|    critic_loss     | 5.88e+04 |\n",
      "|    ent_coef        | 0.722    |\n",
      "|    ent_coef_loss   | -0.124   |\n",
      "|    learning_rate   | 0.00499  |\n",
      "|    n_updates       | 8491     |\n",
      "|    reward_est_loss | -905     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 94       |\n",
      "|    ep_rew_mean     | 62.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 192      |\n",
      "|    total_timesteps | 9023     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -286     |\n",
      "|    critic_loss     | 7.7e+04  |\n",
      "|    ent_coef        | 0.758    |\n",
      "|    ent_coef_loss   | 0.0548   |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 9021     |\n",
      "|    reward_est_loss | -812     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 95.1     |\n",
      "|    ep_rew_mean     | 64.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 202      |\n",
      "|    total_timesteps | 9510     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -336     |\n",
      "|    critic_loss     | 1.04e+05 |\n",
      "|    ent_coef        | 0.833    |\n",
      "|    ent_coef_loss   | 0.124    |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 9508     |\n",
      "|    reward_est_loss | -761     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 98.7     |\n",
      "|    ep_rew_mean     | 67.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 46       |\n",
      "|    time_elapsed    | 211      |\n",
      "|    total_timesteps | 9937     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -359     |\n",
      "|    critic_loss     | 8.78e+04 |\n",
      "|    ent_coef        | 0.853    |\n",
      "|    ent_coef_loss   | 0.00887  |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 9935     |\n",
      "|    reward_est_loss | -833     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=192.87 +/- 46.93\n",
      "Episode length: 119.80 +/- 26.95\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 120      |\n",
      "|    mean_reward     | 193      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -333     |\n",
      "|    critic_loss     | 1.62e+05 |\n",
      "|    ent_coef        | 0.889    |\n",
      "|    ent_coef_loss   | -0.0133  |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 9998     |\n",
      "|    reward_est_loss | -820     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 103       |\n",
      "|    ep_rew_mean     | 72.5      |\n",
      "| time/              |           |\n",
      "|    episodes        | 108       |\n",
      "|    fps             | 44        |\n",
      "|    time_elapsed    | 233       |\n",
      "|    total_timesteps | 10431     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -389      |\n",
      "|    critic_loss     | 2.01e+05  |\n",
      "|    ent_coef        | 0.923     |\n",
      "|    ent_coef_loss   | 0.017     |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 10429     |\n",
      "|    reward_est_loss | -1.08e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 106       |\n",
      "|    ep_rew_mean     | 78.1      |\n",
      "| time/              |           |\n",
      "|    episodes        | 112       |\n",
      "|    fps             | 43        |\n",
      "|    time_elapsed    | 251       |\n",
      "|    total_timesteps | 10848     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -414      |\n",
      "|    critic_loss     | 1.68e+05  |\n",
      "|    ent_coef        | 1.05      |\n",
      "|    ent_coef_loss   | 0.00713   |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 10846     |\n",
      "|    reward_est_loss | -1.14e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 109       |\n",
      "|    ep_rew_mean     | 82.2      |\n",
      "| time/              |           |\n",
      "|    episodes        | 116       |\n",
      "|    fps             | 42        |\n",
      "|    time_elapsed    | 266       |\n",
      "|    total_timesteps | 11205     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -479      |\n",
      "|    critic_loss     | 1.6e+05   |\n",
      "|    ent_coef        | 1.18      |\n",
      "|    ent_coef_loss   | -0.042    |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 11203     |\n",
      "|    reward_est_loss | -1.21e+03 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 112      |\n",
      "|    ep_rew_mean     | 86.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 41       |\n",
      "|    time_elapsed    | 282      |\n",
      "|    total_timesteps | 11607    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -476     |\n",
      "|    critic_loss     | 1.8e+05  |\n",
      "|    ent_coef        | 1.25     |\n",
      "|    ent_coef_loss   | -0.0227  |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 11605    |\n",
      "|    reward_est_loss | -1.3e+03 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 115       |\n",
      "|    ep_rew_mean     | 91.2      |\n",
      "| time/              |           |\n",
      "|    episodes        | 124       |\n",
      "|    fps             | 39        |\n",
      "|    time_elapsed    | 303       |\n",
      "|    total_timesteps | 12104     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -542      |\n",
      "|    critic_loss     | 2.23e+05  |\n",
      "|    ent_coef        | 1.21      |\n",
      "|    ent_coef_loss   | -0.141    |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 12102     |\n",
      "|    reward_est_loss | -1.45e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 116       |\n",
      "|    ep_rew_mean     | 99        |\n",
      "| time/              |           |\n",
      "|    episodes        | 128       |\n",
      "|    fps             | 38        |\n",
      "|    time_elapsed    | 324       |\n",
      "|    total_timesteps | 12581     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -558      |\n",
      "|    critic_loss     | 2.08e+05  |\n",
      "|    ent_coef        | 1.32      |\n",
      "|    ent_coef_loss   | -0.13     |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 12579     |\n",
      "|    reward_est_loss | -1.59e+03 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 113      |\n",
      "|    ep_rew_mean     | 100      |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 38       |\n",
      "|    time_elapsed    | 336      |\n",
      "|    total_timesteps | 12876    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -606     |\n",
      "|    critic_loss     | 2.81e+05 |\n",
      "|    ent_coef        | 1.41     |\n",
      "|    ent_coef_loss   | 0.2      |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 12874    |\n",
      "|    reward_est_loss | -1.7e+03 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 110       |\n",
      "|    ep_rew_mean     | 97        |\n",
      "| time/              |           |\n",
      "|    episodes        | 136       |\n",
      "|    fps             | 37        |\n",
      "|    time_elapsed    | 347       |\n",
      "|    total_timesteps | 13137     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -660      |\n",
      "|    critic_loss     | 2.61e+05  |\n",
      "|    ent_coef        | 1.43      |\n",
      "|    ent_coef_loss   | -0.0236   |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 13135     |\n",
      "|    reward_est_loss | -1.82e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 106       |\n",
      "|    ep_rew_mean     | 90.8      |\n",
      "| time/              |           |\n",
      "|    episodes        | 140       |\n",
      "|    fps             | 37        |\n",
      "|    time_elapsed    | 357       |\n",
      "|    total_timesteps | 13380     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -628      |\n",
      "|    critic_loss     | 2.81e+05  |\n",
      "|    ent_coef        | 1.56      |\n",
      "|    ent_coef_loss   | -0.0606   |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 13378     |\n",
      "|    reward_est_loss | -1.92e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 104       |\n",
      "|    ep_rew_mean     | 89        |\n",
      "| time/              |           |\n",
      "|    episodes        | 144       |\n",
      "|    fps             | 37        |\n",
      "|    time_elapsed    | 367       |\n",
      "|    total_timesteps | 13616     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -751      |\n",
      "|    critic_loss     | 3.46e+05  |\n",
      "|    ent_coef        | 1.58      |\n",
      "|    ent_coef_loss   | 0.254     |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 13614     |\n",
      "|    reward_est_loss | -2.02e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 102       |\n",
      "|    ep_rew_mean     | 92.3      |\n",
      "| time/              |           |\n",
      "|    episodes        | 148       |\n",
      "|    fps             | 36        |\n",
      "|    time_elapsed    | 379       |\n",
      "|    total_timesteps | 13888     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -749      |\n",
      "|    critic_loss     | 3.78e+05  |\n",
      "|    ent_coef        | 1.59      |\n",
      "|    ent_coef_loss   | -0.177    |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 13886     |\n",
      "|    reward_est_loss | -2.04e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 99.2      |\n",
      "|    ep_rew_mean     | 95.8      |\n",
      "| time/              |           |\n",
      "|    episodes        | 152       |\n",
      "|    fps             | 36        |\n",
      "|    time_elapsed    | 392       |\n",
      "|    total_timesteps | 14175     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -741      |\n",
      "|    critic_loss     | 2.92e+05  |\n",
      "|    ent_coef        | 1.77      |\n",
      "|    ent_coef_loss   | -0.148    |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 14173     |\n",
      "|    reward_est_loss | -2.23e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 97.6      |\n",
      "|    ep_rew_mean     | 99.9      |\n",
      "| time/              |           |\n",
      "|    episodes        | 156       |\n",
      "|    fps             | 35        |\n",
      "|    time_elapsed    | 402       |\n",
      "|    total_timesteps | 14422     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -825      |\n",
      "|    critic_loss     | 4.94e+05  |\n",
      "|    ent_coef        | 1.85      |\n",
      "|    ent_coef_loss   | 0.133     |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 14420     |\n",
      "|    reward_est_loss | -2.33e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 95.7      |\n",
      "|    ep_rew_mean     | 107       |\n",
      "| time/              |           |\n",
      "|    episodes        | 160       |\n",
      "|    fps             | 35        |\n",
      "|    time_elapsed    | 418       |\n",
      "|    total_timesteps | 14786     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -901      |\n",
      "|    critic_loss     | 7.49e+05  |\n",
      "|    ent_coef        | 2         |\n",
      "|    ent_coef_loss   | -0.0481   |\n",
      "|    learning_rate   | 0.00498   |\n",
      "|    n_updates       | 14784     |\n",
      "|    reward_est_loss | -2.52e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=116.40 +/- 8.35\n",
      "Episode length: 66.20 +/- 10.50\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 66.2     |\n",
      "|    mean_reward     | 116      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -932     |\n",
      "|    critic_loss     | 5.01e+05 |\n",
      "|    ent_coef        | 2.01     |\n",
      "|    ent_coef_loss   | -0.371   |\n",
      "|    learning_rate   | 0.00498  |\n",
      "|    n_updates       | 14998    |\n",
      "|    reward_est_loss | -2.6e+03 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 96.1      |\n",
      "|    ep_rew_mean     | 113       |\n",
      "| time/              |           |\n",
      "|    episodes        | 164       |\n",
      "|    fps             | 34        |\n",
      "|    time_elapsed    | 434       |\n",
      "|    total_timesteps | 15160     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -943      |\n",
      "|    critic_loss     | 5.73e+05  |\n",
      "|    ent_coef        | 2.09      |\n",
      "|    ent_coef_loss   | -0.385    |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 15158     |\n",
      "|    reward_est_loss | -2.58e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 95.7      |\n",
      "|    ep_rew_mean     | 116       |\n",
      "| time/              |           |\n",
      "|    episodes        | 168       |\n",
      "|    fps             | 34        |\n",
      "|    time_elapsed    | 446       |\n",
      "|    total_timesteps | 15439     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+03 |\n",
      "|    critic_loss     | 7.41e+05  |\n",
      "|    ent_coef        | 2.22      |\n",
      "|    ent_coef_loss   | -0.229    |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 15437     |\n",
      "|    reward_est_loss | -2.77e+03 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 92.4     |\n",
      "|    ep_rew_mean     | 116      |\n",
      "| time/              |          |\n",
      "|    episodes        | 172      |\n",
      "|    fps             | 34       |\n",
      "|    time_elapsed    | 456      |\n",
      "|    total_timesteps | 15680    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -967     |\n",
      "|    critic_loss     | 6.93e+05 |\n",
      "|    ent_coef        | 2.34     |\n",
      "|    ent_coef_loss   | 0.448    |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 15678    |\n",
      "|    reward_est_loss | -2.9e+03 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 91.6      |\n",
      "|    ep_rew_mean     | 120       |\n",
      "| time/              |           |\n",
      "|    episodes        | 176       |\n",
      "|    fps             | 34        |\n",
      "|    time_elapsed    | 469       |\n",
      "|    total_timesteps | 15975     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -962      |\n",
      "|    critic_loss     | 4.39e+05  |\n",
      "|    ent_coef        | 2.35      |\n",
      "|    ent_coef_loss   | 0.233     |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 15973     |\n",
      "|    reward_est_loss | -3.02e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 89.2      |\n",
      "|    ep_rew_mean     | 120       |\n",
      "| time/              |           |\n",
      "|    episodes        | 180       |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 482       |\n",
      "|    total_timesteps | 16268     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+03 |\n",
      "|    critic_loss     | 8.63e+05  |\n",
      "|    ent_coef        | 2.41      |\n",
      "|    ent_coef_loss   | -0.368    |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 16266     |\n",
      "|    reward_est_loss | -3.09e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 89        |\n",
      "|    ep_rew_mean     | 122       |\n",
      "| time/              |           |\n",
      "|    episodes        | 184       |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 494       |\n",
      "|    total_timesteps | 16567     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+03 |\n",
      "|    critic_loss     | 6.93e+05  |\n",
      "|    ent_coef        | 2.51      |\n",
      "|    ent_coef_loss   | 0.547     |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 16565     |\n",
      "|    reward_est_loss | -3.25e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 88.4      |\n",
      "|    ep_rew_mean     | 125       |\n",
      "| time/              |           |\n",
      "|    episodes        | 188       |\n",
      "|    fps             | 33        |\n",
      "|    time_elapsed    | 508       |\n",
      "|    total_timesteps | 16897     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.25e+03 |\n",
      "|    critic_loss     | 9.23e+05  |\n",
      "|    ent_coef        | 2.43      |\n",
      "|    ent_coef_loss   | -0.197    |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 16895     |\n",
      "|    reward_est_loss | -3.38e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 87.5      |\n",
      "|    ep_rew_mean     | 127       |\n",
      "| time/              |           |\n",
      "|    episodes        | 192       |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 523       |\n",
      "|    total_timesteps | 17246     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+03 |\n",
      "|    critic_loss     | 1.14e+06  |\n",
      "|    ent_coef        | 2.72      |\n",
      "|    ent_coef_loss   | -0.594    |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 17244     |\n",
      "|    reward_est_loss | -3.54e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 85.3      |\n",
      "|    ep_rew_mean     | 123       |\n",
      "| time/              |           |\n",
      "|    episodes        | 196       |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 535       |\n",
      "|    total_timesteps | 17553     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.22e+03 |\n",
      "|    critic_loss     | 8e+05     |\n",
      "|    ent_coef        | 2.81      |\n",
      "|    ent_coef_loss   | 0.706     |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 17551     |\n",
      "|    reward_est_loss | -3.51e+03 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 83.4     |\n",
      "|    ep_rew_mean     | 123      |\n",
      "| time/              |          |\n",
      "|    episodes        | 200      |\n",
      "|    fps             | 32       |\n",
      "|    time_elapsed    | 548      |\n",
      "|    total_timesteps | 17847    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.3e+03 |\n",
      "|    critic_loss     | 1.06e+06 |\n",
      "|    ent_coef        | 2.76     |\n",
      "|    ent_coef_loss   | 0.00202  |\n",
      "|    learning_rate   | 0.00497  |\n",
      "|    n_updates       | 17845    |\n",
      "|    reward_est_loss | -3.7e+03 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 82.1      |\n",
      "|    ep_rew_mean     | 125       |\n",
      "| time/              |           |\n",
      "|    episodes        | 204       |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 560       |\n",
      "|    total_timesteps | 18146     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.39e+03 |\n",
      "|    critic_loss     | 1.09e+06  |\n",
      "|    ent_coef        | 2.99      |\n",
      "|    ent_coef_loss   | 0.233     |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 18144     |\n",
      "|    reward_est_loss | -3.89e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 80.3      |\n",
      "|    ep_rew_mean     | 126       |\n",
      "| time/              |           |\n",
      "|    episodes        | 208       |\n",
      "|    fps             | 32        |\n",
      "|    time_elapsed    | 573       |\n",
      "|    total_timesteps | 18461     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.57e+03 |\n",
      "|    critic_loss     | 1.25e+06  |\n",
      "|    ent_coef        | 3         |\n",
      "|    ent_coef_loss   | -0.283    |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 18459     |\n",
      "|    reward_est_loss | -4.02e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 80        |\n",
      "|    ep_rew_mean     | 127       |\n",
      "| time/              |           |\n",
      "|    episodes        | 212       |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 590       |\n",
      "|    total_timesteps | 18845     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.54e+03 |\n",
      "|    critic_loss     | 1.25e+06  |\n",
      "|    ent_coef        | 2.99      |\n",
      "|    ent_coef_loss   | -0.0699   |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 18843     |\n",
      "|    reward_est_loss | -4.22e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 79.5      |\n",
      "|    ep_rew_mean     | 128       |\n",
      "| time/              |           |\n",
      "|    episodes        | 216       |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 604       |\n",
      "|    total_timesteps | 19152     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.47e+03 |\n",
      "|    critic_loss     | 1.54e+06  |\n",
      "|    ent_coef        | 3.16      |\n",
      "|    ent_coef_loss   | -0.0809   |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 19150     |\n",
      "|    reward_est_loss | -4.34e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 78.6      |\n",
      "|    ep_rew_mean     | 129       |\n",
      "| time/              |           |\n",
      "|    episodes        | 220       |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 617       |\n",
      "|    total_timesteps | 19466     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.5e+03  |\n",
      "|    critic_loss     | 1.39e+06  |\n",
      "|    ent_coef        | 3.26      |\n",
      "|    ent_coef_loss   | -0.0338   |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 19464     |\n",
      "|    reward_est_loss | -4.45e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 76.8      |\n",
      "|    ep_rew_mean     | 130       |\n",
      "| time/              |           |\n",
      "|    episodes        | 224       |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 631       |\n",
      "|    total_timesteps | 19782     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.65e+03 |\n",
      "|    critic_loss     | 1.45e+06  |\n",
      "|    ent_coef        | 3.38      |\n",
      "|    ent_coef_loss   | -0.387    |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 19780     |\n",
      "|    reward_est_loss | -4.56e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=127.83 +/- 1.55\n",
      "Episode length: 72.80 +/- 0.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 72.8      |\n",
      "|    mean_reward     | 128       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 20000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.73e+03 |\n",
      "|    critic_loss     | 1.32e+06  |\n",
      "|    ent_coef        | 3.44      |\n",
      "|    ent_coef_loss   | -0.331    |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 19998     |\n",
      "|    reward_est_loss | -4.39e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 76.2      |\n",
      "|    ep_rew_mean     | 131       |\n",
      "| time/              |           |\n",
      "|    episodes        | 228       |\n",
      "|    fps             | 31        |\n",
      "|    time_elapsed    | 648       |\n",
      "|    total_timesteps | 20197     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.76e+03 |\n",
      "|    critic_loss     | 1.36e+06  |\n",
      "|    ent_coef        | 3.49      |\n",
      "|    ent_coef_loss   | 0.59      |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 20195     |\n",
      "|    reward_est_loss | -4.63e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 76.7      |\n",
      "|    ep_rew_mean     | 133       |\n",
      "| time/              |           |\n",
      "|    episodes        | 232       |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 663       |\n",
      "|    total_timesteps | 20547     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.78e+03 |\n",
      "|    critic_loss     | 2.07e+06  |\n",
      "|    ent_coef        | 3.64      |\n",
      "|    ent_coef_loss   | 0.111     |\n",
      "|    learning_rate   | 0.00497   |\n",
      "|    n_updates       | 20545     |\n",
      "|    reward_est_loss | -3.52e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 78.8      |\n",
      "|    ep_rew_mean     | 138       |\n",
      "| time/              |           |\n",
      "|    episodes        | 236       |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 683       |\n",
      "|    total_timesteps | 21016     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.79e+03 |\n",
      "|    critic_loss     | 2.57e+06  |\n",
      "|    ent_coef        | 3.84      |\n",
      "|    ent_coef_loss   | 0.836     |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 21014     |\n",
      "|    reward_est_loss | -4.85e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 81        |\n",
      "|    ep_rew_mean     | 143       |\n",
      "| time/              |           |\n",
      "|    episodes        | 240       |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 702       |\n",
      "|    total_timesteps | 21475     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.95e+03 |\n",
      "|    critic_loss     | 2.27e+06  |\n",
      "|    ent_coef        | 3.68      |\n",
      "|    ent_coef_loss   | 0.134     |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 21473     |\n",
      "|    reward_est_loss | -5.13e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 82.7      |\n",
      "|    ep_rew_mean     | 147       |\n",
      "| time/              |           |\n",
      "|    episodes        | 244       |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 719       |\n",
      "|    total_timesteps | 21888     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.08e+03 |\n",
      "|    critic_loss     | 2.35e+06  |\n",
      "|    ent_coef        | 3.86      |\n",
      "|    ent_coef_loss   | -0.592    |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 21886     |\n",
      "|    reward_est_loss | -5.29e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 84.3      |\n",
      "|    ep_rew_mean     | 149       |\n",
      "| time/              |           |\n",
      "|    episodes        | 248       |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 737       |\n",
      "|    total_timesteps | 22314     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.28e+03 |\n",
      "|    critic_loss     | 2.28e+06  |\n",
      "|    ent_coef        | 4.04      |\n",
      "|    ent_coef_loss   | -0.052    |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 22312     |\n",
      "|    reward_est_loss | -5.44e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 86.9      |\n",
      "|    ep_rew_mean     | 153       |\n",
      "| time/              |           |\n",
      "|    episodes        | 252       |\n",
      "|    fps             | 30        |\n",
      "|    time_elapsed    | 760       |\n",
      "|    total_timesteps | 22862     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.33e+03 |\n",
      "|    critic_loss     | 2.89e+06  |\n",
      "|    ent_coef        | 4.32      |\n",
      "|    ent_coef_loss   | 0.471     |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 22860     |\n",
      "|    reward_est_loss | -5.11e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 86.8      |\n",
      "|    ep_rew_mean     | 151       |\n",
      "| time/              |           |\n",
      "|    episodes        | 256       |\n",
      "|    fps             | 29        |\n",
      "|    time_elapsed    | 770       |\n",
      "|    total_timesteps | 23104     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.59e+03 |\n",
      "|    critic_loss     | 2.27e+06  |\n",
      "|    ent_coef        | 4.26      |\n",
      "|    ent_coef_loss   | -0.0883   |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 23102     |\n",
      "|    reward_est_loss | -5.67e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 88.2      |\n",
      "|    ep_rew_mean     | 152       |\n",
      "| time/              |           |\n",
      "|    episodes        | 260       |\n",
      "|    fps             | 29        |\n",
      "|    time_elapsed    | 791       |\n",
      "|    total_timesteps | 23610     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.6e+03  |\n",
      "|    critic_loss     | 3.7e+06   |\n",
      "|    ent_coef        | 4.32      |\n",
      "|    ent_coef_loss   | 0.0158    |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 23608     |\n",
      "|    reward_est_loss | -5.73e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 89.4      |\n",
      "|    ep_rew_mean     | 153       |\n",
      "| time/              |           |\n",
      "|    episodes        | 264       |\n",
      "|    fps             | 29        |\n",
      "|    time_elapsed    | 812       |\n",
      "|    total_timesteps | 24102     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.66e+03 |\n",
      "|    critic_loss     | 3.45e+06  |\n",
      "|    ent_coef        | 4.38      |\n",
      "|    ent_coef_loss   | -0.788    |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 24100     |\n",
      "|    reward_est_loss | -5.85e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 92.2      |\n",
      "|    ep_rew_mean     | 157       |\n",
      "| time/              |           |\n",
      "|    episodes        | 268       |\n",
      "|    fps             | 29        |\n",
      "|    time_elapsed    | 835       |\n",
      "|    total_timesteps | 24656     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.81e+03 |\n",
      "|    critic_loss     | 4.32e+06  |\n",
      "|    ent_coef        | 4.67      |\n",
      "|    ent_coef_loss   | 0.253     |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 24654     |\n",
      "|    reward_est_loss | -5.9e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 92.9      |\n",
      "|    ep_rew_mean     | 158       |\n",
      "| time/              |           |\n",
      "|    episodes        | 272       |\n",
      "|    fps             | 29        |\n",
      "|    time_elapsed    | 848       |\n",
      "|    total_timesteps | 24973     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.79e+03 |\n",
      "|    critic_loss     | 5.69e+06  |\n",
      "|    ent_coef        | 4.72      |\n",
      "|    ent_coef_loss   | 0.239     |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 24971     |\n",
      "|    reward_est_loss | -6.07e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=219.61 +/- 67.42\n",
      "Episode length: 176.80 +/- 18.27\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 177       |\n",
      "|    mean_reward     | 220       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 25000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.04e+03 |\n",
      "|    critic_loss     | 5.11e+06  |\n",
      "|    ent_coef        | 4.7       |\n",
      "|    ent_coef_loss   | 0.238     |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 24998     |\n",
      "|    reward_est_loss | -6.1e+03  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 94.9      |\n",
      "|    ep_rew_mean     | 160       |\n",
      "| time/              |           |\n",
      "|    episodes        | 276       |\n",
      "|    fps             | 29        |\n",
      "|    time_elapsed    | 871       |\n",
      "|    total_timesteps | 25461     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.36e+03 |\n",
      "|    critic_loss     | 6.1e+06   |\n",
      "|    ent_coef        | 4.73      |\n",
      "|    ent_coef_loss   | -0.167    |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 25459     |\n",
      "|    reward_est_loss | -5.04e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 96.5      |\n",
      "|    ep_rew_mean     | 160       |\n",
      "| time/              |           |\n",
      "|    episodes        | 280       |\n",
      "|    fps             | 29        |\n",
      "|    time_elapsed    | 890       |\n",
      "|    total_timesteps | 25923     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.22e+03 |\n",
      "|    critic_loss     | 6.1e+06   |\n",
      "|    ent_coef        | 4.64      |\n",
      "|    ent_coef_loss   | -0.328    |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 25921     |\n",
      "|    reward_est_loss | -6.34e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 99.5      |\n",
      "|    ep_rew_mean     | 166       |\n",
      "| time/              |           |\n",
      "|    episodes        | 284       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 914       |\n",
      "|    total_timesteps | 26512     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.63e+03 |\n",
      "|    critic_loss     | 5.64e+06  |\n",
      "|    ent_coef        | 4.76      |\n",
      "|    ent_coef_loss   | 0.644     |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 26510     |\n",
      "|    reward_est_loss | -6.22e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 101       |\n",
      "|    ep_rew_mean     | 167       |\n",
      "| time/              |           |\n",
      "|    episodes        | 288       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 934       |\n",
      "|    total_timesteps | 26972     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.48e+03 |\n",
      "|    critic_loss     | 6.96e+06  |\n",
      "|    ent_coef        | 4.87      |\n",
      "|    ent_coef_loss   | 0.586     |\n",
      "|    learning_rate   | 0.00496   |\n",
      "|    n_updates       | 26970     |\n",
      "|    reward_est_loss | -6.57e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 103       |\n",
      "|    ep_rew_mean     | 166       |\n",
      "| time/              |           |\n",
      "|    episodes        | 292       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 956       |\n",
      "|    total_timesteps | 27515     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.54e+03 |\n",
      "|    critic_loss     | 1.07e+07  |\n",
      "|    ent_coef        | 5.11      |\n",
      "|    ent_coef_loss   | -0.598    |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 27513     |\n",
      "|    reward_est_loss | -6.69e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 103       |\n",
      "|    ep_rew_mean     | 161       |\n",
      "| time/              |           |\n",
      "|    episodes        | 296       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 969       |\n",
      "|    total_timesteps | 27831     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4e+03    |\n",
      "|    critic_loss     | 7.71e+06  |\n",
      "|    ent_coef        | 5.1       |\n",
      "|    ent_coef_loss   | -1.99     |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 27829     |\n",
      "|    reward_est_loss | -6.64e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 102       |\n",
      "|    ep_rew_mean     | 156       |\n",
      "| time/              |           |\n",
      "|    episodes        | 300       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 978       |\n",
      "|    total_timesteps | 28032     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.16e+03 |\n",
      "|    critic_loss     | 8.45e+06  |\n",
      "|    ent_coef        | 5.05      |\n",
      "|    ent_coef_loss   | -1.48     |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 28030     |\n",
      "|    reward_est_loss | -6.66e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 102       |\n",
      "|    ep_rew_mean     | 151       |\n",
      "| time/              |           |\n",
      "|    episodes        | 304       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 989       |\n",
      "|    total_timesteps | 28309     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.12e+03 |\n",
      "|    critic_loss     | 7.92e+06  |\n",
      "|    ent_coef        | 5.43      |\n",
      "|    ent_coef_loss   | 0.379     |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 28307     |\n",
      "|    reward_est_loss | -6.91e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 105       |\n",
      "|    ep_rew_mean     | 147       |\n",
      "| time/              |           |\n",
      "|    episodes        | 308       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 1016      |\n",
      "|    total_timesteps | 28942     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.44e+03 |\n",
      "|    critic_loss     | 9.32e+06  |\n",
      "|    ent_coef        | 5.9       |\n",
      "|    ent_coef_loss   | -0.576    |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 28940     |\n",
      "|    reward_est_loss | -5.05e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 106       |\n",
      "|    ep_rew_mean     | 141       |\n",
      "| time/              |           |\n",
      "|    episodes        | 312       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 1037      |\n",
      "|    total_timesteps | 29448     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.46e+03 |\n",
      "|    critic_loss     | 9.37e+06  |\n",
      "|    ent_coef        | 5.98      |\n",
      "|    ent_coef_loss   | 0.153     |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 29446     |\n",
      "|    reward_est_loss | -6.36e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 108       |\n",
      "|    ep_rew_mean     | 134       |\n",
      "| time/              |           |\n",
      "|    episodes        | 316       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 1058      |\n",
      "|    total_timesteps | 29969     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.05e+03 |\n",
      "|    critic_loss     | 1.19e+07  |\n",
      "|    ent_coef        | 6.54      |\n",
      "|    ent_coef_loss   | 1.67      |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 29967     |\n",
      "|    reward_est_loss | -4.99e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=150.06 +/- 98.78\n",
      "Episode length: 193.40 +/- 45.50\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 193       |\n",
      "|    mean_reward     | 150       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 30000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.71e+03 |\n",
      "|    critic_loss     | 1.23e+07  |\n",
      "|    ent_coef        | 6.38      |\n",
      "|    ent_coef_loss   | -1.07     |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 29998     |\n",
      "|    reward_est_loss | -7.24e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 110       |\n",
      "|    ep_rew_mean     | 128       |\n",
      "| time/              |           |\n",
      "|    episodes        | 320       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 1080      |\n",
      "|    total_timesteps | 30466     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.06e+03 |\n",
      "|    critic_loss     | 1.93e+07  |\n",
      "|    ent_coef        | 7.2       |\n",
      "|    ent_coef_loss   | -0.183    |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 30464     |\n",
      "|    reward_est_loss | -7.51e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 116       |\n",
      "|    ep_rew_mean     | 128       |\n",
      "| time/              |           |\n",
      "|    episodes        | 324       |\n",
      "|    fps             | 28        |\n",
      "|    time_elapsed    | 1120      |\n",
      "|    total_timesteps | 31428     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.57e+03 |\n",
      "|    critic_loss     | 2.15e+07  |\n",
      "|    ent_coef        | 8.27      |\n",
      "|    ent_coef_loss   | -1.29     |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 31426     |\n",
      "|    reward_est_loss | -7.31e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 121       |\n",
      "|    ep_rew_mean     | 130       |\n",
      "| time/              |           |\n",
      "|    episodes        | 328       |\n",
      "|    fps             | 27        |\n",
      "|    time_elapsed    | 1158      |\n",
      "|    total_timesteps | 32334     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.08e+03 |\n",
      "|    critic_loss     | 2.11e+07  |\n",
      "|    ent_coef        | 8.98      |\n",
      "|    ent_coef_loss   | 0.879     |\n",
      "|    learning_rate   | 0.00495   |\n",
      "|    n_updates       | 32332     |\n",
      "|    reward_est_loss | -7.9e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 128       |\n",
      "|    ep_rew_mean     | 130       |\n",
      "| time/              |           |\n",
      "|    episodes        | 332       |\n",
      "|    fps             | 27        |\n",
      "|    time_elapsed    | 1203      |\n",
      "|    total_timesteps | 33355     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.12e+03 |\n",
      "|    critic_loss     | 2.75e+07  |\n",
      "|    ent_coef        | 10.2      |\n",
      "|    ent_coef_loss   | 1.34      |\n",
      "|    learning_rate   | 0.00494   |\n",
      "|    n_updates       | 33353     |\n",
      "|    reward_est_loss | -7.88e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 132       |\n",
      "|    ep_rew_mean     | 131       |\n",
      "| time/              |           |\n",
      "|    episodes        | 336       |\n",
      "|    fps             | 27        |\n",
      "|    time_elapsed    | 1243      |\n",
      "|    total_timesteps | 34253     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.16e+03 |\n",
      "|    critic_loss     | 2.98e+07  |\n",
      "|    ent_coef        | 9.35      |\n",
      "|    ent_coef_loss   | 0.737     |\n",
      "|    learning_rate   | 0.00494   |\n",
      "|    n_updates       | 34251     |\n",
      "|    reward_est_loss | -8.26e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=199.65 +/- 242.42\n",
      "Episode length: 271.80 +/- 209.96\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 272       |\n",
      "|    mean_reward     | 200       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 35000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.25e+03 |\n",
      "|    critic_loss     | 2.3e+07   |\n",
      "|    ent_coef        | 10.3      |\n",
      "|    ent_coef_loss   | -0.649    |\n",
      "|    learning_rate   | 0.00494   |\n",
      "|    n_updates       | 34998     |\n",
      "|    reward_est_loss | -7.77e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 139       |\n",
      "|    ep_rew_mean     | 134       |\n",
      "| time/              |           |\n",
      "|    episodes        | 340       |\n",
      "|    fps             | 27        |\n",
      "|    time_elapsed    | 1290      |\n",
      "|    total_timesteps | 35330     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.86e+03 |\n",
      "|    critic_loss     | 2.91e+07  |\n",
      "|    ent_coef        | 10.4      |\n",
      "|    ent_coef_loss   | 0.355     |\n",
      "|    learning_rate   | 0.00494   |\n",
      "|    n_updates       | 35328     |\n",
      "|    reward_est_loss | -8.18e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 143       |\n",
      "|    ep_rew_mean     | 131       |\n",
      "| time/              |           |\n",
      "|    episodes        | 344       |\n",
      "|    fps             | 27        |\n",
      "|    time_elapsed    | 1327      |\n",
      "|    total_timesteps | 36186     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.31e+03 |\n",
      "|    critic_loss     | 4.02e+07  |\n",
      "|    ent_coef        | 10.8      |\n",
      "|    ent_coef_loss   | -0.289    |\n",
      "|    learning_rate   | 0.00494   |\n",
      "|    n_updates       | 36184     |\n",
      "|    reward_est_loss | -8.3e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 144       |\n",
      "|    ep_rew_mean     | 127       |\n",
      "| time/              |           |\n",
      "|    episodes        | 348       |\n",
      "|    fps             | 27        |\n",
      "|    time_elapsed    | 1351      |\n",
      "|    total_timesteps | 36756     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.61e+03 |\n",
      "|    critic_loss     | 3.39e+07  |\n",
      "|    ent_coef        | 11.4      |\n",
      "|    ent_coef_loss   | -0.321    |\n",
      "|    learning_rate   | 0.00494   |\n",
      "|    n_updates       | 36754     |\n",
      "|    reward_est_loss | -8.85e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 146       |\n",
      "|    ep_rew_mean     | 121       |\n",
      "| time/              |           |\n",
      "|    episodes        | 352       |\n",
      "|    fps             | 27        |\n",
      "|    time_elapsed    | 1380      |\n",
      "|    total_timesteps | 37449     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.13e+03 |\n",
      "|    critic_loss     | 4.27e+07  |\n",
      "|    ent_coef        | 12.2      |\n",
      "|    ent_coef_loss   | 0.449     |\n",
      "|    learning_rate   | 0.00494   |\n",
      "|    n_updates       | 37447     |\n",
      "|    reward_est_loss | -9.15e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 153       |\n",
      "|    ep_rew_mean     | 126       |\n",
      "| time/              |           |\n",
      "|    episodes        | 356       |\n",
      "|    fps             | 27        |\n",
      "|    time_elapsed    | 1422      |\n",
      "|    total_timesteps | 38422     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.73e+03 |\n",
      "|    critic_loss     | 4.45e+07  |\n",
      "|    ent_coef        | 13.7      |\n",
      "|    ent_coef_loss   | -2.55     |\n",
      "|    learning_rate   | 0.00494   |\n",
      "|    n_updates       | 38420     |\n",
      "|    reward_est_loss | -9.4e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 155       |\n",
      "|    ep_rew_mean     | 131       |\n",
      "| time/              |           |\n",
      "|    episodes        | 360       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1450      |\n",
      "|    total_timesteps | 39068     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.07e+04 |\n",
      "|    critic_loss     | 3.89e+07  |\n",
      "|    ent_coef        | 13.7      |\n",
      "|    ent_coef_loss   | 0.0825    |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 39066     |\n",
      "|    reward_est_loss | -9.49e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 156       |\n",
      "|    ep_rew_mean     | 135       |\n",
      "| time/              |           |\n",
      "|    episodes        | 364       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1475      |\n",
      "|    total_timesteps | 39669     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05e+04 |\n",
      "|    critic_loss     | 5.61e+07  |\n",
      "|    ent_coef        | 13.6      |\n",
      "|    ent_coef_loss   | -0.552    |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 39667     |\n",
      "|    reward_est_loss | -9.58e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=496.69 +/- 70.40\n",
      "Episode length: 275.20 +/- 44.58\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 275       |\n",
      "|    mean_reward     | 497       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 40000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1e+04    |\n",
      "|    critic_loss     | 5.34e+07  |\n",
      "|    ent_coef        | 13.5      |\n",
      "|    ent_coef_loss   | 0.72      |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 39998     |\n",
      "|    reward_est_loss | -9.37e+03 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 157       |\n",
      "|    ep_rew_mean     | 140       |\n",
      "| time/              |           |\n",
      "|    episodes        | 368       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1511      |\n",
      "|    total_timesteps | 40405     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+04 |\n",
      "|    critic_loss     | 4.49e+07  |\n",
      "|    ent_coef        | 14.5      |\n",
      "|    ent_coef_loss   | 1.82      |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 40403     |\n",
      "|    reward_est_loss | -9.34e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 162       |\n",
      "|    ep_rew_mean     | 149       |\n",
      "| time/              |           |\n",
      "|    episodes        | 372       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1541      |\n",
      "|    total_timesteps | 41126     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.19e+04 |\n",
      "|    critic_loss     | 6.19e+07  |\n",
      "|    ent_coef        | 14.6      |\n",
      "|    ent_coef_loss   | 0.0388    |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 41124     |\n",
      "|    reward_est_loss | -9.54e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 164       |\n",
      "|    ep_rew_mean     | 157       |\n",
      "| time/              |           |\n",
      "|    episodes        | 376       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1573      |\n",
      "|    total_timesteps | 41877     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.18e+04 |\n",
      "|    critic_loss     | 5.85e+07  |\n",
      "|    ent_coef        | 14.6      |\n",
      "|    ent_coef_loss   | -1.62     |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 41875     |\n",
      "|    reward_est_loss | -9.26e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 165       |\n",
      "|    ep_rew_mean     | 164       |\n",
      "| time/              |           |\n",
      "|    episodes        | 380       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1597      |\n",
      "|    total_timesteps | 42468     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.35e+04 |\n",
      "|    critic_loss     | 6.37e+07  |\n",
      "|    ent_coef        | 15.8      |\n",
      "|    ent_coef_loss   | 1.42      |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 42466     |\n",
      "|    reward_est_loss | -9.84e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 166       |\n",
      "|    ep_rew_mean     | 165       |\n",
      "| time/              |           |\n",
      "|    episodes        | 384       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1622      |\n",
      "|    total_timesteps | 43063     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.35e+04 |\n",
      "|    critic_loss     | 6.59e+07  |\n",
      "|    ent_coef        | 16.3      |\n",
      "|    ent_coef_loss   | -0.631    |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 43061     |\n",
      "|    reward_est_loss | -9.88e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 167       |\n",
      "|    ep_rew_mean     | 169       |\n",
      "| time/              |           |\n",
      "|    episodes        | 388       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1646      |\n",
      "|    total_timesteps | 43632     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.46e+04 |\n",
      "|    critic_loss     | 7.12e+07  |\n",
      "|    ent_coef        | 17.3      |\n",
      "|    ent_coef_loss   | 0.00861   |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 43630     |\n",
      "|    reward_est_loss | -1.03e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 166       |\n",
      "|    ep_rew_mean     | 171       |\n",
      "| time/              |           |\n",
      "|    episodes        | 392       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1668      |\n",
      "|    total_timesteps | 44156     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.48e+04 |\n",
      "|    critic_loss     | 9.45e+07  |\n",
      "|    ent_coef        | 17.3      |\n",
      "|    ent_coef_loss   | 0.937     |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 44154     |\n",
      "|    reward_est_loss | -1.03e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 170       |\n",
      "|    ep_rew_mean     | 186       |\n",
      "| time/              |           |\n",
      "|    episodes        | 396       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1698      |\n",
      "|    total_timesteps | 44875     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.33e+04 |\n",
      "|    critic_loss     | 6.42e+07  |\n",
      "|    ent_coef        | 18.1      |\n",
      "|    ent_coef_loss   | 0.0469    |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 44873     |\n",
      "|    reward_est_loss | -1.02e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=340.59 +/- 13.71\n",
      "Episode length: 180.80 +/- 7.88\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 181       |\n",
      "|    mean_reward     | 341       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 45000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.53e+04 |\n",
      "|    critic_loss     | 1.4e+08   |\n",
      "|    ent_coef        | 17.9      |\n",
      "|    ent_coef_loss   | 0.112     |\n",
      "|    learning_rate   | 0.00493   |\n",
      "|    n_updates       | 44998     |\n",
      "|    reward_est_loss | -1.05e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 174       |\n",
      "|    ep_rew_mean     | 198       |\n",
      "| time/              |           |\n",
      "|    episodes        | 400       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1722      |\n",
      "|    total_timesteps | 45453     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.66e+04 |\n",
      "|    critic_loss     | 7.09e+07  |\n",
      "|    ent_coef        | 17.4      |\n",
      "|    ent_coef_loss   | -1.67     |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 45451     |\n",
      "|    reward_est_loss | -1.06e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 178       |\n",
      "|    ep_rew_mean     | 207       |\n",
      "| time/              |           |\n",
      "|    episodes        | 404       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1750      |\n",
      "|    total_timesteps | 46119     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.54e+04 |\n",
      "|    critic_loss     | 8.67e+07  |\n",
      "|    ent_coef        | 17.8      |\n",
      "|    ent_coef_loss   | -1.28     |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 46117     |\n",
      "|    reward_est_loss | -9.01e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 176       |\n",
      "|    ep_rew_mean     | 212       |\n",
      "| time/              |           |\n",
      "|    episodes        | 408       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1770      |\n",
      "|    total_timesteps | 46591     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.65e+04 |\n",
      "|    critic_loss     | 9.38e+07  |\n",
      "|    ent_coef        | 18.5      |\n",
      "|    ent_coef_loss   | -0.591    |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 46589     |\n",
      "|    reward_est_loss | -9.22e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 180       |\n",
      "|    ep_rew_mean     | 224       |\n",
      "| time/              |           |\n",
      "|    episodes        | 412       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1805      |\n",
      "|    total_timesteps | 47418     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.69e+04 |\n",
      "|    critic_loss     | 7.2e+07   |\n",
      "|    ent_coef        | 19.2      |\n",
      "|    ent_coef_loss   | -0.0468   |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 47416     |\n",
      "|    reward_est_loss | -9.74e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 181       |\n",
      "|    ep_rew_mean     | 236       |\n",
      "| time/              |           |\n",
      "|    episodes        | 416       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1833      |\n",
      "|    total_timesteps | 48085     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.68e+04 |\n",
      "|    critic_loss     | 8.39e+07  |\n",
      "|    ent_coef        | 18.9      |\n",
      "|    ent_coef_loss   | 1         |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 48083     |\n",
      "|    reward_est_loss | -9.58e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 183       |\n",
      "|    ep_rew_mean     | 243       |\n",
      "| time/              |           |\n",
      "|    episodes        | 420       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1860      |\n",
      "|    total_timesteps | 48733     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.66e+04 |\n",
      "|    critic_loss     | 1.15e+08  |\n",
      "|    ent_coef        | 19.1      |\n",
      "|    ent_coef_loss   | -0.976    |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 48731     |\n",
      "|    reward_est_loss | -1.06e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 182       |\n",
      "|    ep_rew_mean     | 250       |\n",
      "| time/              |           |\n",
      "|    episodes        | 424       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1896      |\n",
      "|    total_timesteps | 49584     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.69e+04 |\n",
      "|    critic_loss     | 9.56e+07  |\n",
      "|    ent_coef        | 19        |\n",
      "|    ent_coef_loss   | 0.452     |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 49582     |\n",
      "|    reward_est_loss | -9.84e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=277.52 +/- 143.31\n",
      "Episode length: 222.40 +/- 93.47\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 222       |\n",
      "|    mean_reward     | 278       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 50000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.79e+04 |\n",
      "|    critic_loss     | 7.62e+07  |\n",
      "|    ent_coef        | 19.1      |\n",
      "|    ent_coef_loss   | 0.419     |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 49998     |\n",
      "|    reward_est_loss | -1.14e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 180       |\n",
      "|    ep_rew_mean     | 251       |\n",
      "| time/              |           |\n",
      "|    episodes        | 428       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1926      |\n",
      "|    total_timesteps | 50298     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.72e+04 |\n",
      "|    critic_loss     | 9.22e+07  |\n",
      "|    ent_coef        | 19.2      |\n",
      "|    ent_coef_loss   | -1.06     |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 50296     |\n",
      "|    reward_est_loss | -1.04e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 176       |\n",
      "|    ep_rew_mean     | 254       |\n",
      "| time/              |           |\n",
      "|    episodes        | 432       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1955      |\n",
      "|    total_timesteps | 50963     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.75e+04 |\n",
      "|    critic_loss     | 9.57e+07  |\n",
      "|    ent_coef        | 20.6      |\n",
      "|    ent_coef_loss   | 0.245     |\n",
      "|    learning_rate   | 0.00492   |\n",
      "|    n_updates       | 50961     |\n",
      "|    reward_est_loss | -1.16e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 173       |\n",
      "|    ep_rew_mean     | 249       |\n",
      "| time/              |           |\n",
      "|    episodes        | 436       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 1978      |\n",
      "|    total_timesteps | 51509     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.73e+04 |\n",
      "|    critic_loss     | 1.14e+08  |\n",
      "|    ent_coef        | 19.6      |\n",
      "|    ent_coef_loss   | 0.263     |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 51507     |\n",
      "|    reward_est_loss | -1.16e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 167       |\n",
      "|    ep_rew_mean     | 241       |\n",
      "| time/              |           |\n",
      "|    episodes        | 440       |\n",
      "|    fps             | 26        |\n",
      "|    time_elapsed    | 2002      |\n",
      "|    total_timesteps | 52068     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.81e+04 |\n",
      "|    critic_loss     | 9e+07     |\n",
      "|    ent_coef        | 19.2      |\n",
      "|    ent_coef_loss   | -1.19     |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 52066     |\n",
      "|    reward_est_loss | -8.75e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 165       |\n",
      "|    ep_rew_mean     | 241       |\n",
      "| time/              |           |\n",
      "|    episodes        | 444       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2026      |\n",
      "|    total_timesteps | 52649     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.71e+04 |\n",
      "|    critic_loss     | 1.17e+08  |\n",
      "|    ent_coef        | 19.9      |\n",
      "|    ent_coef_loss   | -1.41     |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 52647     |\n",
      "|    reward_est_loss | -1.11e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 166       |\n",
      "|    ep_rew_mean     | 247       |\n",
      "| time/              |           |\n",
      "|    episodes        | 448       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2056      |\n",
      "|    total_timesteps | 53364     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.84e+04 |\n",
      "|    critic_loss     | 1.19e+08  |\n",
      "|    ent_coef        | 20.1      |\n",
      "|    ent_coef_loss   | -1.74     |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 53362     |\n",
      "|    reward_est_loss | -1.11e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 167       |\n",
      "|    ep_rew_mean     | 251       |\n",
      "| time/              |           |\n",
      "|    episodes        | 452       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2088      |\n",
      "|    total_timesteps | 54120     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.77e+04 |\n",
      "|    critic_loss     | 1.25e+08  |\n",
      "|    ent_coef        | 19.9      |\n",
      "|    ent_coef_loss   | 0.85      |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 54118     |\n",
      "|    reward_est_loss | -1.1e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 164       |\n",
      "|    ep_rew_mean     | 254       |\n",
      "| time/              |           |\n",
      "|    episodes        | 456       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2117      |\n",
      "|    total_timesteps | 54826     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.81e+04 |\n",
      "|    critic_loss     | 1.37e+08  |\n",
      "|    ent_coef        | 21.4      |\n",
      "|    ent_coef_loss   | 1.05      |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 54824     |\n",
      "|    reward_est_loss | -9.91e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=165.28 +/- 5.77\n",
      "Episode length: 139.00 +/- 3.29\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 139       |\n",
      "|    mean_reward     | 165       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 55000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.91e+04 |\n",
      "|    critic_loss     | 1.47e+08  |\n",
      "|    ent_coef        | 20        |\n",
      "|    ent_coef_loss   | 0.77      |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 54998     |\n",
      "|    reward_est_loss | -1.22e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 165       |\n",
      "|    ep_rew_mean     | 251       |\n",
      "| time/              |           |\n",
      "|    episodes        | 460       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2149      |\n",
      "|    total_timesteps | 55583     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2e+04    |\n",
      "|    critic_loss     | 1.16e+08  |\n",
      "|    ent_coef        | 19.7      |\n",
      "|    ent_coef_loss   | -0.281    |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 55581     |\n",
      "|    reward_est_loss | -1.23e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 167       |\n",
      "|    ep_rew_mean     | 252       |\n",
      "| time/              |           |\n",
      "|    episodes        | 464       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2180      |\n",
      "|    total_timesteps | 56334     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.92e+04 |\n",
      "|    critic_loss     | 1.78e+08  |\n",
      "|    ent_coef        | 21.4      |\n",
      "|    ent_coef_loss   | 1         |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 56332     |\n",
      "|    reward_est_loss | -1.24e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 165       |\n",
      "|    ep_rew_mean     | 244       |\n",
      "| time/              |           |\n",
      "|    episodes        | 468       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2203      |\n",
      "|    total_timesteps | 56875     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.84e+04 |\n",
      "|    critic_loss     | 3.67e+08  |\n",
      "|    ent_coef        | 22.6      |\n",
      "|    ent_coef_loss   | 0.0767    |\n",
      "|    learning_rate   | 0.00491   |\n",
      "|    n_updates       | 56873     |\n",
      "|    reward_est_loss | -1.03e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 163       |\n",
      "|    ep_rew_mean     | 237       |\n",
      "| time/              |           |\n",
      "|    episodes        | 472       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2228      |\n",
      "|    total_timesteps | 57468     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.82e+04 |\n",
      "|    critic_loss     | 1.64e+08  |\n",
      "|    ent_coef        | 22.1      |\n",
      "|    ent_coef_loss   | -2.29     |\n",
      "|    learning_rate   | 0.0049    |\n",
      "|    n_updates       | 57466     |\n",
      "|    reward_est_loss | -1.28e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 165       |\n",
      "|    ep_rew_mean     | 238       |\n",
      "| time/              |           |\n",
      "|    episodes        | 476       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2268      |\n",
      "|    total_timesteps | 58419     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.92e+04 |\n",
      "|    critic_loss     | 1.42e+08  |\n",
      "|    ent_coef        | 22.8      |\n",
      "|    ent_coef_loss   | 1.43      |\n",
      "|    learning_rate   | 0.0049    |\n",
      "|    n_updates       | 58417     |\n",
      "|    reward_est_loss | -1.13e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 166       |\n",
      "|    ep_rew_mean     | 238       |\n",
      "| time/              |           |\n",
      "|    episodes        | 480       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2297      |\n",
      "|    total_timesteps | 59109     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.02e+04 |\n",
      "|    critic_loss     | 1.63e+08  |\n",
      "|    ent_coef        | 24.2      |\n",
      "|    ent_coef_loss   | -1.84     |\n",
      "|    learning_rate   | 0.0049    |\n",
      "|    n_updates       | 59107     |\n",
      "|    reward_est_loss | -1.26e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=496.37 +/- 260.29\n",
      "Episode length: 334.80 +/- 217.57\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 335       |\n",
      "|    mean_reward     | 496       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 60000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.12e+04 |\n",
      "|    critic_loss     | 1.74e+08  |\n",
      "|    ent_coef        | 23.2      |\n",
      "|    ent_coef_loss   | 0.465     |\n",
      "|    learning_rate   | 0.0049    |\n",
      "|    n_updates       | 59998     |\n",
      "|    reward_est_loss | -1.29e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 170       |\n",
      "|    ep_rew_mean     | 242       |\n",
      "| time/              |           |\n",
      "|    episodes        | 484       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2335      |\n",
      "|    total_timesteps | 60023     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.98e+04 |\n",
      "|    critic_loss     | 1.35e+08  |\n",
      "|    ent_coef        | 23.8      |\n",
      "|    ent_coef_loss   | 0.184     |\n",
      "|    learning_rate   | 0.0049    |\n",
      "|    n_updates       | 60021     |\n",
      "|    reward_est_loss | -1.14e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 174       |\n",
      "|    ep_rew_mean     | 250       |\n",
      "| time/              |           |\n",
      "|    episodes        | 488       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2377      |\n",
      "|    total_timesteps | 61014     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.17e+04 |\n",
      "|    critic_loss     | 1.71e+08  |\n",
      "|    ent_coef        | 24.4      |\n",
      "|    ent_coef_loss   | 1.73      |\n",
      "|    learning_rate   | 0.0049    |\n",
      "|    n_updates       | 61012     |\n",
      "|    reward_est_loss | -1.31e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 177       |\n",
      "|    ep_rew_mean     | 250       |\n",
      "| time/              |           |\n",
      "|    episodes        | 492       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2411      |\n",
      "|    total_timesteps | 61840     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.14e+04 |\n",
      "|    critic_loss     | 1.81e+08  |\n",
      "|    ent_coef        | 25.9      |\n",
      "|    ent_coef_loss   | -1.09     |\n",
      "|    learning_rate   | 0.0049    |\n",
      "|    n_updates       | 61838     |\n",
      "|    reward_est_loss | -1.11e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 182       |\n",
      "|    ep_rew_mean     | 247       |\n",
      "| time/              |           |\n",
      "|    episodes        | 496       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2461      |\n",
      "|    total_timesteps | 63029     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.19e+04 |\n",
      "|    critic_loss     | 2.2e+08   |\n",
      "|    ent_coef        | 27.5      |\n",
      "|    ent_coef_loss   | 1.01      |\n",
      "|    learning_rate   | 0.00489   |\n",
      "|    n_updates       | 63027     |\n",
      "|    reward_est_loss | -1.37e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 183       |\n",
      "|    ep_rew_mean     | 245       |\n",
      "| time/              |           |\n",
      "|    episodes        | 500       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2493      |\n",
      "|    total_timesteps | 63787     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.27e+04 |\n",
      "|    critic_loss     | 2.21e+08  |\n",
      "|    ent_coef        | 25.4      |\n",
      "|    ent_coef_loss   | 1.11      |\n",
      "|    learning_rate   | 0.00489   |\n",
      "|    n_updates       | 63785     |\n",
      "|    reward_est_loss | -1.3e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=536.22 +/- 224.96\n",
      "Episode length: 600.40 +/- 122.24\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 600       |\n",
      "|    mean_reward     | 536       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 65000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.22e+04 |\n",
      "|    critic_loss     | 2.4e+08   |\n",
      "|    ent_coef        | 26.9      |\n",
      "|    ent_coef_loss   | 0.173     |\n",
      "|    learning_rate   | 0.00489   |\n",
      "|    n_updates       | 64998     |\n",
      "|    reward_est_loss | -1.41e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 190       |\n",
      "|    ep_rew_mean     | 249       |\n",
      "| time/              |           |\n",
      "|    episodes        | 504       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2552      |\n",
      "|    total_timesteps | 65089     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.32e+04 |\n",
      "|    critic_loss     | 1.78e+08  |\n",
      "|    ent_coef        | 25.9      |\n",
      "|    ent_coef_loss   | -0.878    |\n",
      "|    learning_rate   | 0.00489   |\n",
      "|    n_updates       | 65087     |\n",
      "|    reward_est_loss | -1.37e+04 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 198      |\n",
      "|    ep_rew_mean     | 256      |\n",
      "| time/              |          |\n",
      "|    episodes        | 508      |\n",
      "|    fps             | 25       |\n",
      "|    time_elapsed    | 2606     |\n",
      "|    total_timesteps | 66383    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.1e+04 |\n",
      "|    critic_loss     | 2.14e+08 |\n",
      "|    ent_coef        | 25.5     |\n",
      "|    ent_coef_loss   | 1.77     |\n",
      "|    learning_rate   | 0.00489  |\n",
      "|    n_updates       | 66381    |\n",
      "|    reward_est_loss | -5.1e+03 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 198       |\n",
      "|    ep_rew_mean     | 258       |\n",
      "| time/              |           |\n",
      "|    episodes        | 512       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2640      |\n",
      "|    total_timesteps | 67198     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.2e+04  |\n",
      "|    critic_loss     | 2.16e+08  |\n",
      "|    ent_coef        | 24.8      |\n",
      "|    ent_coef_loss   | -1.38     |\n",
      "|    learning_rate   | 0.00489   |\n",
      "|    n_updates       | 67196     |\n",
      "|    reward_est_loss | -1.37e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 201       |\n",
      "|    ep_rew_mean     | 266       |\n",
      "| time/              |           |\n",
      "|    episodes        | 516       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2684      |\n",
      "|    total_timesteps | 68233     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.29e+04 |\n",
      "|    critic_loss     | 2.33e+08  |\n",
      "|    ent_coef        | 25        |\n",
      "|    ent_coef_loss   | -0.314    |\n",
      "|    learning_rate   | 0.00489   |\n",
      "|    n_updates       | 68231     |\n",
      "|    reward_est_loss | -1.35e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 208       |\n",
      "|    ep_rew_mean     | 284       |\n",
      "| time/              |           |\n",
      "|    episodes        | 520       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2742      |\n",
      "|    total_timesteps | 69539     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.35e+04 |\n",
      "|    critic_loss     | 2.78e+08  |\n",
      "|    ent_coef        | 25.9      |\n",
      "|    ent_coef_loss   | 0.858     |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 69537     |\n",
      "|    reward_est_loss | -1.21e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=342.30 +/- 16.82\n",
      "Episode length: 212.20 +/- 60.39\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 212       |\n",
      "|    mean_reward     | 342       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 70000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.41e+04 |\n",
      "|    critic_loss     | 1.92e+08  |\n",
      "|    ent_coef        | 24.7      |\n",
      "|    ent_coef_loss   | -0.692    |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 69998     |\n",
      "|    reward_est_loss | -1.46e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 211       |\n",
      "|    ep_rew_mean     | 295       |\n",
      "| time/              |           |\n",
      "|    episodes        | 524       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2792      |\n",
      "|    total_timesteps | 70722     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.43e+04 |\n",
      "|    critic_loss     | 2.23e+08  |\n",
      "|    ent_coef        | 25.4      |\n",
      "|    ent_coef_loss   | -0.162    |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 70720     |\n",
      "|    reward_est_loss | -1.48e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 213       |\n",
      "|    ep_rew_mean     | 299       |\n",
      "| time/              |           |\n",
      "|    episodes        | 528       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2831      |\n",
      "|    total_timesteps | 71643     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.39e+04 |\n",
      "|    critic_loss     | 2.21e+08  |\n",
      "|    ent_coef        | 26.3      |\n",
      "|    ent_coef_loss   | 0.991     |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 71641     |\n",
      "|    reward_est_loss | -1.44e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 217       |\n",
      "|    ep_rew_mean     | 305       |\n",
      "| time/              |           |\n",
      "|    episodes        | 532       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2875      |\n",
      "|    total_timesteps | 72703     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.41e+04 |\n",
      "|    critic_loss     | 2.11e+08  |\n",
      "|    ent_coef        | 24.9      |\n",
      "|    ent_coef_loss   | -0.506    |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 72701     |\n",
      "|    reward_est_loss | -1.5e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 224       |\n",
      "|    ep_rew_mean     | 319       |\n",
      "| time/              |           |\n",
      "|    episodes        | 536       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2924      |\n",
      "|    total_timesteps | 73861     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.36e+04 |\n",
      "|    critic_loss     | 1.62e+08  |\n",
      "|    ent_coef        | 26        |\n",
      "|    ent_coef_loss   | -2.69     |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 73859     |\n",
      "|    reward_est_loss | -1.43e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 228       |\n",
      "|    ep_rew_mean     | 333       |\n",
      "| time/              |           |\n",
      "|    episodes        | 540       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 2966      |\n",
      "|    total_timesteps | 74864     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.47e+04 |\n",
      "|    critic_loss     | 2.19e+08  |\n",
      "|    ent_coef        | 26.3      |\n",
      "|    ent_coef_loss   | -0.0436   |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 74862     |\n",
      "|    reward_est_loss | -1.41e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=912.28 +/- 677.98\n",
      "Episode length: 369.60 +/- 211.91\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 370       |\n",
      "|    mean_reward     | 912       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 75000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.67e+04 |\n",
      "|    critic_loss     | 2.21e+08  |\n",
      "|    ent_coef        | 25.7      |\n",
      "|    ent_coef_loss   | -0.209    |\n",
      "|    learning_rate   | 0.00488   |\n",
      "|    n_updates       | 74998     |\n",
      "|    reward_est_loss | -1.53e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 232       |\n",
      "|    ep_rew_mean     | 348       |\n",
      "| time/              |           |\n",
      "|    episodes        | 544       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3008      |\n",
      "|    total_timesteps | 75802     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.53e+04 |\n",
      "|    critic_loss     | 2.22e+08  |\n",
      "|    ent_coef        | 27.1      |\n",
      "|    ent_coef_loss   | 0.963     |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 75800     |\n",
      "|    reward_est_loss | -1.35e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 236       |\n",
      "|    ep_rew_mean     | 359       |\n",
      "| time/              |           |\n",
      "|    episodes        | 548       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3055      |\n",
      "|    total_timesteps | 76945     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.65e+04 |\n",
      "|    critic_loss     | 1.98e+08  |\n",
      "|    ent_coef        | 27.2      |\n",
      "|    ent_coef_loss   | -1.3      |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 76943     |\n",
      "|    reward_est_loss | -1.58e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 236       |\n",
      "|    ep_rew_mean     | 365       |\n",
      "| time/              |           |\n",
      "|    episodes        | 552       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3086      |\n",
      "|    total_timesteps | 77679     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.7e+04  |\n",
      "|    critic_loss     | 2.27e+08  |\n",
      "|    ent_coef        | 26.7      |\n",
      "|    ent_coef_loss   | 0.535     |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 77677     |\n",
      "|    reward_est_loss | -1.44e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 239       |\n",
      "|    ep_rew_mean     | 377       |\n",
      "| time/              |           |\n",
      "|    episodes        | 556       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3132      |\n",
      "|    total_timesteps | 78748     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.79e+04 |\n",
      "|    critic_loss     | 3.41e+08  |\n",
      "|    ent_coef        | 27.5      |\n",
      "|    ent_coef_loss   | 1.92      |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 78746     |\n",
      "|    reward_est_loss | -1.59e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 241       |\n",
      "|    ep_rew_mean     | 389       |\n",
      "| time/              |           |\n",
      "|    episodes        | 560       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3171      |\n",
      "|    total_timesteps | 79662     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.8e+04  |\n",
      "|    critic_loss     | 2.87e+08  |\n",
      "|    ent_coef        | 28.1      |\n",
      "|    ent_coef_loss   | 1.14      |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 79660     |\n",
      "|    reward_est_loss | -1.42e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=357.39 +/- 58.97\n",
      "Episode length: 201.00 +/- 19.83\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 201       |\n",
      "|    mean_reward     | 357       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 80000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.71e+04 |\n",
      "|    critic_loss     | 1.96e+08  |\n",
      "|    ent_coef        | 28        |\n",
      "|    ent_coef_loss   | -0.857    |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 79998     |\n",
      "|    reward_est_loss | -1.62e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 245       |\n",
      "|    ep_rew_mean     | 400       |\n",
      "| time/              |           |\n",
      "|    episodes        | 564       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3221      |\n",
      "|    total_timesteps | 80859     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.72e+04 |\n",
      "|    critic_loss     | 2.31e+08  |\n",
      "|    ent_coef        | 27.8      |\n",
      "|    ent_coef_loss   | 2.73      |\n",
      "|    learning_rate   | 0.00487   |\n",
      "|    n_updates       | 80857     |\n",
      "|    reward_est_loss | -1.62e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 250       |\n",
      "|    ep_rew_mean     | 417       |\n",
      "| time/              |           |\n",
      "|    episodes        | 568       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3267      |\n",
      "|    total_timesteps | 81907     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.74e+04 |\n",
      "|    critic_loss     | 1.94e+08  |\n",
      "|    ent_coef        | 28.3      |\n",
      "|    ent_coef_loss   | 1.3       |\n",
      "|    learning_rate   | 0.00486   |\n",
      "|    n_updates       | 81905     |\n",
      "|    reward_est_loss | -1.64e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 255       |\n",
      "|    ep_rew_mean     | 432       |\n",
      "| time/              |           |\n",
      "|    episodes        | 572       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3310      |\n",
      "|    total_timesteps | 82950     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.81e+04 |\n",
      "|    critic_loss     | 2.56e+08  |\n",
      "|    ent_coef        | 27.7      |\n",
      "|    ent_coef_loss   | 1.25      |\n",
      "|    learning_rate   | 0.00486   |\n",
      "|    n_updates       | 82948     |\n",
      "|    reward_est_loss | -1.51e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 259       |\n",
      "|    ep_rew_mean     | 452       |\n",
      "| time/              |           |\n",
      "|    episodes        | 576       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3369      |\n",
      "|    total_timesteps | 84349     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.73e+04 |\n",
      "|    critic_loss     | 2.72e+08  |\n",
      "|    ent_coef        | 28.4      |\n",
      "|    ent_coef_loss   | 0.391     |\n",
      "|    learning_rate   | 0.00486   |\n",
      "|    n_updates       | 84347     |\n",
      "|    reward_est_loss | -1.52e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=691.35 +/- 64.61\n",
      "Episode length: 309.20 +/- 19.31\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 309       |\n",
      "|    mean_reward     | 691       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 85000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.87e+04 |\n",
      "|    critic_loss     | 2.56e+08  |\n",
      "|    ent_coef        | 27        |\n",
      "|    ent_coef_loss   | -2.2      |\n",
      "|    learning_rate   | 0.00486   |\n",
      "|    n_updates       | 84998     |\n",
      "|    reward_est_loss | -1.56e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 268       |\n",
      "|    ep_rew_mean     | 474       |\n",
      "| time/              |           |\n",
      "|    episodes        | 580       |\n",
      "|    fps             | 25        |\n",
      "|    time_elapsed    | 3437      |\n",
      "|    total_timesteps | 85946     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.88e+04 |\n",
      "|    critic_loss     | 3.21e+08  |\n",
      "|    ent_coef        | 30        |\n",
      "|    ent_coef_loss   | -1.89     |\n",
      "|    learning_rate   | 0.00486   |\n",
      "|    n_updates       | 85944     |\n",
      "|    reward_est_loss | -1.58e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 272       |\n",
      "|    ep_rew_mean     | 489       |\n",
      "| time/              |           |\n",
      "|    episodes        | 584       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 3493      |\n",
      "|    total_timesteps | 87248     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.9e+04  |\n",
      "|    critic_loss     | 2.24e+08  |\n",
      "|    ent_coef        | 28.3      |\n",
      "|    ent_coef_loss   | -2.44     |\n",
      "|    learning_rate   | 0.00485   |\n",
      "|    n_updates       | 87246     |\n",
      "|    reward_est_loss | -1.61e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 283       |\n",
      "|    ep_rew_mean     | 519       |\n",
      "| time/              |           |\n",
      "|    episodes        | 588       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 3583      |\n",
      "|    total_timesteps | 89348     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.9e+04  |\n",
      "|    critic_loss     | 1.8e+08   |\n",
      "|    ent_coef        | 28.2      |\n",
      "|    ent_coef_loss   | 1.18      |\n",
      "|    learning_rate   | 0.00485   |\n",
      "|    n_updates       | 89346     |\n",
      "|    reward_est_loss | -6.21e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=732.41 +/- 89.81\n",
      "Episode length: 338.80 +/- 55.08\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 339       |\n",
      "|    mean_reward     | 732       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 90000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3e+04    |\n",
      "|    critic_loss     | 2.78e+08  |\n",
      "|    ent_coef        | 28.7      |\n",
      "|    ent_coef_loss   | -1.36     |\n",
      "|    learning_rate   | 0.00485   |\n",
      "|    n_updates       | 89998     |\n",
      "|    reward_est_loss | -1.62e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 293       |\n",
      "|    ep_rew_mean     | 551       |\n",
      "| time/              |           |\n",
      "|    episodes        | 592       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 3665      |\n",
      "|    total_timesteps | 91094     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.95e+04 |\n",
      "|    critic_loss     | 2.96e+08  |\n",
      "|    ent_coef        | 29.2      |\n",
      "|    ent_coef_loss   | 1.11      |\n",
      "|    learning_rate   | 0.00485   |\n",
      "|    n_updates       | 91092     |\n",
      "|    reward_est_loss | -1.7e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 299       |\n",
      "|    ep_rew_mean     | 584       |\n",
      "| time/              |           |\n",
      "|    episodes        | 596       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 3746      |\n",
      "|    total_timesteps | 92897     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.19e+04 |\n",
      "|    critic_loss     | 2.12e+08  |\n",
      "|    ent_coef        | 29.1      |\n",
      "|    ent_coef_loss   | -0.258    |\n",
      "|    learning_rate   | 0.00485   |\n",
      "|    n_updates       | 92895     |\n",
      "|    reward_est_loss | -1.65e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 309       |\n",
      "|    ep_rew_mean     | 615       |\n",
      "| time/              |           |\n",
      "|    episodes        | 600       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 3830      |\n",
      "|    total_timesteps | 94644     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.38e+04 |\n",
      "|    critic_loss     | 3.22e+08  |\n",
      "|    ent_coef        | 28.8      |\n",
      "|    ent_coef_loss   | 0.471     |\n",
      "|    learning_rate   | 0.00484   |\n",
      "|    n_updates       | 94642     |\n",
      "|    reward_est_loss | -1.75e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=325.83 +/- 4.25\n",
      "Episode length: 179.80 +/- 1.72\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 180       |\n",
      "|    mean_reward     | 326       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 95000     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.43e+04 |\n",
      "|    critic_loss     | 3.42e+08  |\n",
      "|    ent_coef        | 29.6      |\n",
      "|    ent_coef_loss   | -2.19     |\n",
      "|    learning_rate   | 0.00484   |\n",
      "|    n_updates       | 94998     |\n",
      "|    reward_est_loss | -1.57e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 312       |\n",
      "|    ep_rew_mean     | 639       |\n",
      "| time/              |           |\n",
      "|    episodes        | 604       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 3900      |\n",
      "|    total_timesteps | 96301     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.42e+04 |\n",
      "|    critic_loss     | 3.71e+08  |\n",
      "|    ent_coef        | 32.1      |\n",
      "|    ent_coef_loss   | 0.537     |\n",
      "|    learning_rate   | 0.00484   |\n",
      "|    n_updates       | 96299     |\n",
      "|    reward_est_loss | -9.69e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 312       |\n",
      "|    ep_rew_mean     | 654       |\n",
      "| time/              |           |\n",
      "|    episodes        | 608       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 3952      |\n",
      "|    total_timesteps | 97535     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.33e+04 |\n",
      "|    critic_loss     | 3.28e+08  |\n",
      "|    ent_coef        | 30.4      |\n",
      "|    ent_coef_loss   | 1.21      |\n",
      "|    learning_rate   | 0.00484   |\n",
      "|    n_updates       | 97533     |\n",
      "|    reward_est_loss | -1.77e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 318       |\n",
      "|    ep_rew_mean     | 679       |\n",
      "| time/              |           |\n",
      "|    episodes        | 612       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4012      |\n",
      "|    total_timesteps | 98987     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.42e+04 |\n",
      "|    critic_loss     | 4.02e+08  |\n",
      "|    ent_coef        | 33.4      |\n",
      "|    ent_coef_loss   | -1.01     |\n",
      "|    learning_rate   | 0.00484   |\n",
      "|    n_updates       | 98985     |\n",
      "|    reward_est_loss | -1.78e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=688.13 +/- 127.08\n",
      "Episode length: 276.00 +/- 34.29\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 276       |\n",
      "|    mean_reward     | 688       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 100000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.54e+04 |\n",
      "|    critic_loss     | 6.33e+08  |\n",
      "|    ent_coef        | 32.7      |\n",
      "|    ent_coef_loss   | -0.349    |\n",
      "|    learning_rate   | 0.00483   |\n",
      "|    n_updates       | 99998     |\n",
      "|    reward_est_loss | -1.79e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 320       |\n",
      "|    ep_rew_mean     | 693       |\n",
      "| time/              |           |\n",
      "|    episodes        | 616       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4067      |\n",
      "|    total_timesteps | 100282    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.77e+04 |\n",
      "|    critic_loss     | 3.17e+08  |\n",
      "|    ent_coef        | 32.8      |\n",
      "|    ent_coef_loss   | 0.859     |\n",
      "|    learning_rate   | 0.00483   |\n",
      "|    n_updates       | 100280    |\n",
      "|    reward_est_loss | -1.79e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 327       |\n",
      "|    ep_rew_mean     | 725       |\n",
      "| time/              |           |\n",
      "|    episodes        | 620       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4147      |\n",
      "|    total_timesteps | 102207    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.62e+04 |\n",
      "|    critic_loss     | 3.48e+08  |\n",
      "|    ent_coef        | 32.9      |\n",
      "|    ent_coef_loss   | -0.412    |\n",
      "|    learning_rate   | 0.00483   |\n",
      "|    n_updates       | 102205    |\n",
      "|    reward_est_loss | -1.74e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=1359.12 +/- 369.45\n",
      "Episode length: 484.20 +/- 89.88\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 484       |\n",
      "|    mean_reward     | 1.36e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 105000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.96e+04 |\n",
      "|    critic_loss     | 3.85e+08  |\n",
      "|    ent_coef        | 34.4      |\n",
      "|    ent_coef_loss   | 3.34      |\n",
      "|    learning_rate   | 0.00483   |\n",
      "|    n_updates       | 104998    |\n",
      "|    reward_est_loss | -1.79e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 344       |\n",
      "|    ep_rew_mean     | 784       |\n",
      "| time/              |           |\n",
      "|    episodes        | 624       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4280      |\n",
      "|    total_timesteps | 105124    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.05e+04 |\n",
      "|    critic_loss     | 5.69e+08  |\n",
      "|    ent_coef        | 35.9      |\n",
      "|    ent_coef_loss   | 0.363     |\n",
      "|    learning_rate   | 0.00482   |\n",
      "|    n_updates       | 105122    |\n",
      "|    reward_est_loss | -1.53e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 352       |\n",
      "|    ep_rew_mean     | 817       |\n",
      "| time/              |           |\n",
      "|    episodes        | 628       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4353      |\n",
      "|    total_timesteps | 106843    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.23e+04 |\n",
      "|    critic_loss     | 6.25e+08  |\n",
      "|    ent_coef        | 36        |\n",
      "|    ent_coef_loss   | 0.472     |\n",
      "|    learning_rate   | 0.00482   |\n",
      "|    n_updates       | 106841    |\n",
      "|    reward_est_loss | -1.29e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 367       |\n",
      "|    ep_rew_mean     | 876       |\n",
      "| time/              |           |\n",
      "|    episodes        | 632       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4461      |\n",
      "|    total_timesteps | 109403    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.49e+04 |\n",
      "|    critic_loss     | 4.86e+08  |\n",
      "|    ent_coef        | 37.7      |\n",
      "|    ent_coef_loss   | 0.61      |\n",
      "|    learning_rate   | 0.00482   |\n",
      "|    n_updates       | 109401    |\n",
      "|    reward_est_loss | -8.84e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=1174.25 +/- 933.70\n",
      "Episode length: 436.80 +/- 291.46\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 437       |\n",
      "|    mean_reward     | 1.17e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 110000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.65e+04 |\n",
      "|    critic_loss     | 4.58e+08  |\n",
      "|    ent_coef        | 38.7      |\n",
      "|    ent_coef_loss   | -0.0744   |\n",
      "|    learning_rate   | 0.00482   |\n",
      "|    n_updates       | 109998    |\n",
      "|    reward_est_loss | -1.82e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 395       |\n",
      "|    ep_rew_mean     | 979       |\n",
      "| time/              |           |\n",
      "|    episodes        | 636       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4640      |\n",
      "|    total_timesteps | 113403    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.92e+04 |\n",
      "|    critic_loss     | 7.18e+08  |\n",
      "|    ent_coef        | 41        |\n",
      "|    ent_coef_loss   | 0.82      |\n",
      "|    learning_rate   | 0.00481   |\n",
      "|    n_updates       | 113401    |\n",
      "|    reward_est_loss | -1.81e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=1077.05 +/- 1013.27\n",
      "Episode length: 354.40 +/- 273.48\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 354       |\n",
      "|    mean_reward     | 1.08e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 115000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.31e+04 |\n",
      "|    critic_loss     | 8.55e+08  |\n",
      "|    ent_coef        | 42.4      |\n",
      "|    ent_coef_loss   | 2.44      |\n",
      "|    learning_rate   | 0.00481   |\n",
      "|    n_updates       | 114998    |\n",
      "|    reward_est_loss | -1.8e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 416       |\n",
      "|    ep_rew_mean     | 1.05e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 640       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4778      |\n",
      "|    total_timesteps | 116431    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.42e+04 |\n",
      "|    critic_loss     | 4.39e+08  |\n",
      "|    ent_coef        | 42.2      |\n",
      "|    ent_coef_loss   | 0.56      |\n",
      "|    learning_rate   | 0.00481   |\n",
      "|    n_updates       | 116429    |\n",
      "|    reward_est_loss | -1.82e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 423       |\n",
      "|    ep_rew_mean     | 1.08e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 644       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4852      |\n",
      "|    total_timesteps | 118059    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.52e+04 |\n",
      "|    critic_loss     | 5e+08     |\n",
      "|    ent_coef        | 44.9      |\n",
      "|    ent_coef_loss   | 0.655     |\n",
      "|    learning_rate   | 0.0048    |\n",
      "|    n_updates       | 118057    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 430       |\n",
      "|    ep_rew_mean     | 1.12e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 648       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 4936      |\n",
      "|    total_timesteps | 119922    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.84e+04 |\n",
      "|    critic_loss     | 5.86e+08  |\n",
      "|    ent_coef        | 46.5      |\n",
      "|    ent_coef_loss   | 0.191     |\n",
      "|    learning_rate   | 0.0048    |\n",
      "|    n_updates       | 119920    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=3329.12 +/- 38.32\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.33e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 120000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.92e+04 |\n",
      "|    critic_loss     | 5.02e+08  |\n",
      "|    ent_coef        | 47.6      |\n",
      "|    ent_coef_loss   | 0.85      |\n",
      "|    learning_rate   | 0.0048    |\n",
      "|    n_updates       | 119998    |\n",
      "|    reward_est_loss | -1.8e+04  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 462       |\n",
      "|    ep_rew_mean     | 1.23e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 652       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 5120      |\n",
      "|    total_timesteps | 123922    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.22e+04 |\n",
      "|    critic_loss     | 7.45e+08  |\n",
      "|    ent_coef        | 49        |\n",
      "|    ent_coef_loss   | 1.96      |\n",
      "|    learning_rate   | 0.00479   |\n",
      "|    n_updates       | 123920    |\n",
      "|    reward_est_loss | -1.21e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=3092.02 +/- 163.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.09e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 125000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.46e+04 |\n",
      "|    critic_loss     | 9.4e+08   |\n",
      "|    ent_coef        | 48        |\n",
      "|    ent_coef_loss   | 0.634     |\n",
      "|    learning_rate   | 0.00479   |\n",
      "|    n_updates       | 124998    |\n",
      "|    reward_est_loss | -1.16e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 491       |\n",
      "|    ep_rew_mean     | 1.35e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 656       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 5285      |\n",
      "|    total_timesteps | 127878    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.29e+04 |\n",
      "|    critic_loss     | 1.9e+09   |\n",
      "|    ent_coef        | 52.7      |\n",
      "|    ent_coef_loss   | -0.0252   |\n",
      "|    learning_rate   | 0.00479   |\n",
      "|    n_updates       | 127876    |\n",
      "|    reward_est_loss | -1.21e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=3685.12 +/- 12.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.69e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 130000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.53e+04 |\n",
      "|    critic_loss     | 7e+08     |\n",
      "|    ent_coef        | 52.7      |\n",
      "|    ent_coef_loss   | -0.958    |\n",
      "|    learning_rate   | 0.00478   |\n",
      "|    n_updates       | 129998    |\n",
      "|    reward_est_loss | -1.77e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 522       |\n",
      "|    ep_rew_mean     | 1.47e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 660       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 5451      |\n",
      "|    total_timesteps | 131878    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.62e+04 |\n",
      "|    critic_loss     | 7.57e+08  |\n",
      "|    ent_coef        | 54.3      |\n",
      "|    ent_coef_loss   | 1.1       |\n",
      "|    learning_rate   | 0.00478   |\n",
      "|    n_updates       | 131876    |\n",
      "|    reward_est_loss | -1.63e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=3369.01 +/- 29.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.37e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 135000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.28e+04 |\n",
      "|    critic_loss     | 1.18e+09  |\n",
      "|    ent_coef        | 51.2      |\n",
      "|    ent_coef_loss   | -1.19     |\n",
      "|    learning_rate   | 0.00478   |\n",
      "|    n_updates       | 134998    |\n",
      "|    reward_est_loss | -1.53e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 543       |\n",
      "|    ep_rew_mean     | 1.55e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 664       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 5585      |\n",
      "|    total_timesteps | 135124    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.23e+04 |\n",
      "|    critic_loss     | 9.9e+08   |\n",
      "|    ent_coef        | 55.5      |\n",
      "|    ent_coef_loss   | 1.43      |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 135122    |\n",
      "|    reward_est_loss | -1.58e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 572       |\n",
      "|    ep_rew_mean     | 1.67e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 668       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 5748      |\n",
      "|    total_timesteps | 139124    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.93e+04 |\n",
      "|    critic_loss     | 7.22e+08  |\n",
      "|    ent_coef        | 56.8      |\n",
      "|    ent_coef_loss   | 1.71      |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 139122    |\n",
      "|    reward_est_loss | -1.78e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=3256.98 +/- 229.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.26e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 140000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.25e+04 |\n",
      "|    critic_loss     | 8.38e+08  |\n",
      "|    ent_coef        | 56.6      |\n",
      "|    ent_coef_loss   | 2.8       |\n",
      "|    learning_rate   | 0.00477   |\n",
      "|    n_updates       | 139998    |\n",
      "|    reward_est_loss | -1.52e+04 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 593      |\n",
      "|    ep_rew_mean     | 1.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 672      |\n",
      "|    fps             | 24       |\n",
      "|    time_elapsed    | 5877     |\n",
      "|    total_timesteps | 142271   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -9.3e+04 |\n",
      "|    critic_loss     | 1.37e+09 |\n",
      "|    ent_coef        | 56.7     |\n",
      "|    ent_coef_loss   | -1.25    |\n",
      "|    learning_rate   | 0.00476  |\n",
      "|    n_updates       | 142269   |\n",
      "|    reward_est_loss | -1.5e+04 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=3104.72 +/- 31.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.1e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 145000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+05 |\n",
      "|    critic_loss     | 1.11e+09  |\n",
      "|    ent_coef        | 58.1      |\n",
      "|    ent_coef_loss   | 1.65      |\n",
      "|    learning_rate   | 0.00476   |\n",
      "|    n_updates       | 144998    |\n",
      "|    reward_est_loss | -1.38e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 619       |\n",
      "|    ep_rew_mean     | 1.86e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 676       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 6041      |\n",
      "|    total_timesteps | 146271    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.92e+04 |\n",
      "|    critic_loss     | 1.03e+09  |\n",
      "|    ent_coef        | 60.7      |\n",
      "|    ent_coef_loss   | -0.523    |\n",
      "|    learning_rate   | 0.00476   |\n",
      "|    n_updates       | 146269    |\n",
      "|    reward_est_loss | -1.5e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=2579.20 +/- 1186.16\n",
      "Episode length: 825.60 +/- 348.80\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 826       |\n",
      "|    mean_reward     | 2.58e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 150000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.06e+05 |\n",
      "|    critic_loss     | 2.11e+09  |\n",
      "|    ent_coef        | 63.3      |\n",
      "|    ent_coef_loss   | -0.751    |\n",
      "|    learning_rate   | 0.00475   |\n",
      "|    n_updates       | 149998    |\n",
      "|    reward_est_loss | -1.76e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 643       |\n",
      "|    ep_rew_mean     | 1.97e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 680       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 6206      |\n",
      "|    total_timesteps | 150271    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+05 |\n",
      "|    critic_loss     | 1.52e+09  |\n",
      "|    ent_coef        | 63.2      |\n",
      "|    ent_coef_loss   | -0.36     |\n",
      "|    learning_rate   | 0.00475   |\n",
      "|    n_updates       | 150269    |\n",
      "|    reward_est_loss | -1.37e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 670       |\n",
      "|    ep_rew_mean     | 2.08e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 684       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 6378      |\n",
      "|    total_timesteps | 154271    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+05 |\n",
      "|    critic_loss     | 1.14e+09  |\n",
      "|    ent_coef        | 67.6      |\n",
      "|    ent_coef_loss   | -0.399    |\n",
      "|    learning_rate   | 0.00474   |\n",
      "|    n_updates       | 154269    |\n",
      "|    reward_est_loss | -1.78e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=3198.26 +/- 38.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.2e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 155000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+05 |\n",
      "|    critic_loss     | 2.16e+09  |\n",
      "|    ent_coef        | 64.6      |\n",
      "|    ent_coef_loss   | -0.0335   |\n",
      "|    learning_rate   | 0.00474   |\n",
      "|    n_updates       | 154998    |\n",
      "|    reward_est_loss | -1.25e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 689       |\n",
      "|    ep_rew_mean     | 2.17e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 688       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 6549      |\n",
      "|    total_timesteps | 158271    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+05 |\n",
      "|    critic_loss     | 1.33e+09  |\n",
      "|    ent_coef        | 64.3      |\n",
      "|    ent_coef_loss   | -0.573    |\n",
      "|    learning_rate   | 0.00474   |\n",
      "|    n_updates       | 158269    |\n",
      "|    reward_est_loss | -1.75e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=2607.50 +/- 1282.30\n",
      "Episode length: 816.60 +/- 366.80\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 817       |\n",
      "|    mean_reward     | 2.61e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 160000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.21e+05 |\n",
      "|    critic_loss     | 1.43e+09  |\n",
      "|    ent_coef        | 65.7      |\n",
      "|    ent_coef_loss   | 0.296     |\n",
      "|    learning_rate   | 0.00473   |\n",
      "|    n_updates       | 159998    |\n",
      "|    reward_est_loss | -1.55e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 703       |\n",
      "|    ep_rew_mean     | 2.23e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 692       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 6679      |\n",
      "|    total_timesteps | 161352    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.23e+05 |\n",
      "|    critic_loss     | 1.44e+09  |\n",
      "|    ent_coef        | 64.7      |\n",
      "|    ent_coef_loss   | -2.1      |\n",
      "|    learning_rate   | 0.00473   |\n",
      "|    n_updates       | 161350    |\n",
      "|    reward_est_loss | -1.76e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 710       |\n",
      "|    ep_rew_mean     | 2.27e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 696       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 6789      |\n",
      "|    total_timesteps | 163906    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.24e+05 |\n",
      "|    critic_loss     | 1.44e+09  |\n",
      "|    ent_coef        | 68.8      |\n",
      "|    ent_coef_loss   | -1.2      |\n",
      "|    learning_rate   | 0.00473   |\n",
      "|    n_updates       | 163904    |\n",
      "|    reward_est_loss | -1.8e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=3028.80 +/- 74.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.03e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 165000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.25e+05 |\n",
      "|    critic_loss     | 4.81e+09  |\n",
      "|    ent_coef        | 68.2      |\n",
      "|    ent_coef_loss   | -0.31     |\n",
      "|    learning_rate   | 0.00473   |\n",
      "|    n_updates       | 164998    |\n",
      "|    reward_est_loss | -1.78e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 727       |\n",
      "|    ep_rew_mean     | 2.33e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 700       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 6933      |\n",
      "|    total_timesteps | 167315    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.28e+05 |\n",
      "|    critic_loss     | 1.14e+09  |\n",
      "|    ent_coef        | 63.9      |\n",
      "|    ent_coef_loss   | -0.984    |\n",
      "|    learning_rate   | 0.00472   |\n",
      "|    n_updates       | 167313    |\n",
      "|    reward_est_loss | -1.78e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=2972.11 +/- 1405.64\n",
      "Episode length: 823.00 +/- 354.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 823       |\n",
      "|    mean_reward     | 2.97e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 170000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.35e+05 |\n",
      "|    critic_loss     | 2.2e+09   |\n",
      "|    ent_coef        | 65.4      |\n",
      "|    ent_coef_loss   | -3.22     |\n",
      "|    learning_rate   | 0.00472   |\n",
      "|    n_updates       | 169998    |\n",
      "|    reward_est_loss | -1.8e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 750       |\n",
      "|    ep_rew_mean     | 2.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 704       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 7106      |\n",
      "|    total_timesteps | 171315    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.4e+05  |\n",
      "|    critic_loss     | 1.24e+09  |\n",
      "|    ent_coef        | 67.9      |\n",
      "|    ent_coef_loss   | -0.642    |\n",
      "|    learning_rate   | 0.00471   |\n",
      "|    n_updates       | 171313    |\n",
      "|    reward_est_loss | -1.56e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 747       |\n",
      "|    ep_rew_mean     | 2.42e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 708       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 7146      |\n",
      "|    total_timesteps | 172214    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.34e+05 |\n",
      "|    critic_loss     | 1.32e+09  |\n",
      "|    ent_coef        | 71.8      |\n",
      "|    ent_coef_loss   | 0.904     |\n",
      "|    learning_rate   | 0.00471   |\n",
      "|    n_updates       | 172212    |\n",
      "|    reward_est_loss | -1.38e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 759       |\n",
      "|    ep_rew_mean     | 2.49e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 712       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 7268      |\n",
      "|    total_timesteps | 174898    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.34e+05 |\n",
      "|    critic_loss     | 2.08e+09  |\n",
      "|    ent_coef        | 71.4      |\n",
      "|    ent_coef_loss   | -0.548    |\n",
      "|    learning_rate   | 0.00471   |\n",
      "|    n_updates       | 174896    |\n",
      "|    reward_est_loss | -1.72e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=2695.23 +/- 1037.95\n",
      "Episode length: 805.40 +/- 273.31\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 805       |\n",
      "|    mean_reward     | 2.7e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 175000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.37e+05 |\n",
      "|    critic_loss     | 2.07e+09  |\n",
      "|    ent_coef        | 69.9      |\n",
      "|    ent_coef_loss   | 0.454     |\n",
      "|    learning_rate   | 0.00471   |\n",
      "|    n_updates       | 174998    |\n",
      "|    reward_est_loss | -1.72e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 771       |\n",
      "|    ep_rew_mean     | 2.54e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 716       |\n",
      "|    fps             | 24        |\n",
      "|    time_elapsed    | 7382      |\n",
      "|    total_timesteps | 177386    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.46e+05 |\n",
      "|    critic_loss     | 1.24e+09  |\n",
      "|    ent_coef        | 73.2      |\n",
      "|    ent_coef_loss   | 1.76      |\n",
      "|    learning_rate   | 0.0047    |\n",
      "|    n_updates       | 177384    |\n",
      "|    reward_est_loss | -1.33e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=2982.99 +/- 36.18\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 2.98e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 180000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.47e+05 |\n",
      "|    critic_loss     | 1.35e+09  |\n",
      "|    ent_coef        | 79.2      |\n",
      "|    ent_coef_loss   | -1.43     |\n",
      "|    learning_rate   | 0.0047    |\n",
      "|    n_updates       | 179998    |\n",
      "|    reward_est_loss | -1.03e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 783       |\n",
      "|    ep_rew_mean     | 2.6e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 720       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 7525      |\n",
      "|    total_timesteps | 180538    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.48e+05 |\n",
      "|    critic_loss     | 1.57e+09  |\n",
      "|    ent_coef        | 77.1      |\n",
      "|    ent_coef_loss   | 2.15      |\n",
      "|    learning_rate   | 0.0047    |\n",
      "|    n_updates       | 180536    |\n",
      "|    reward_est_loss | -1.57e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 794       |\n",
      "|    ep_rew_mean     | 2.65e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 724       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 7706      |\n",
      "|    total_timesteps | 184538    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.55e+05 |\n",
      "|    critic_loss     | 1.94e+09  |\n",
      "|    ent_coef        | 79.5      |\n",
      "|    ent_coef_loss   | -0.698    |\n",
      "|    learning_rate   | 0.00469   |\n",
      "|    n_updates       | 184536    |\n",
      "|    reward_est_loss | -1.19e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=3706.86 +/- 172.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.71e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 185000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.58e+05 |\n",
      "|    critic_loss     | 1.54e+09  |\n",
      "|    ent_coef        | 78        |\n",
      "|    ent_coef_loss   | -0.258    |\n",
      "|    learning_rate   | 0.00469   |\n",
      "|    n_updates       | 184998    |\n",
      "|    reward_est_loss | -1.79e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 808       |\n",
      "|    ep_rew_mean     | 2.7e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 728       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 7851      |\n",
      "|    total_timesteps | 187690    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.58e+05 |\n",
      "|    critic_loss     | 1.72e+09  |\n",
      "|    ent_coef        | 82.8      |\n",
      "|    ent_coef_loss   | 0.206     |\n",
      "|    learning_rate   | 0.00469   |\n",
      "|    n_updates       | 187688    |\n",
      "|    reward_est_loss | -1.69e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=2623.78 +/- 1117.05\n",
      "Episode length: 850.80 +/- 298.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 851       |\n",
      "|    mean_reward     | 2.62e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 190000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.69e+05 |\n",
      "|    critic_loss     | 1.47e+09  |\n",
      "|    ent_coef        | 86        |\n",
      "|    ent_coef_loss   | 2.03      |\n",
      "|    learning_rate   | 0.00468   |\n",
      "|    n_updates       | 189998    |\n",
      "|    reward_est_loss | -1.61e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 816       |\n",
      "|    ep_rew_mean     | 2.72e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 732       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 8000      |\n",
      "|    total_timesteps | 190973    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.71e+05 |\n",
      "|    critic_loss     | 2.11e+09  |\n",
      "|    ent_coef        | 77.8      |\n",
      "|    ent_coef_loss   | -0.304    |\n",
      "|    learning_rate   | 0.00468   |\n",
      "|    n_updates       | 190971    |\n",
      "|    reward_est_loss | -1.32e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 807       |\n",
      "|    ep_rew_mean     | 2.69e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 736       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 8142      |\n",
      "|    total_timesteps | 194121    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.69e+05 |\n",
      "|    critic_loss     | 1.65e+09  |\n",
      "|    ent_coef        | 78.5      |\n",
      "|    ent_coef_loss   | 2.98      |\n",
      "|    learning_rate   | 0.00468   |\n",
      "|    n_updates       | 194119    |\n",
      "|    reward_est_loss | -1.56e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=3000.98 +/- 223.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3e+03     |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 195000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.72e+05 |\n",
      "|    critic_loss     | 1.85e+09  |\n",
      "|    ent_coef        | 77.5      |\n",
      "|    ent_coef_loss   | 1.48      |\n",
      "|    learning_rate   | 0.00468   |\n",
      "|    n_updates       | 194998    |\n",
      "|    reward_est_loss | -1.58e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 808       |\n",
      "|    ep_rew_mean     | 2.69e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 740       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 8284      |\n",
      "|    total_timesteps | 197220    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.74e+05 |\n",
      "|    critic_loss     | 1.7e+09   |\n",
      "|    ent_coef        | 80.9      |\n",
      "|    ent_coef_loss   | -2.64     |\n",
      "|    learning_rate   | 0.00467   |\n",
      "|    n_updates       | 197218    |\n",
      "|    reward_est_loss | -1.69e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=3147.33 +/- 324.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.15e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 200000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.79e+05 |\n",
      "|    critic_loss     | 2.26e+09  |\n",
      "|    ent_coef        | 73.2      |\n",
      "|    ent_coef_loss   | -2.75     |\n",
      "|    learning_rate   | 0.00467   |\n",
      "|    n_updates       | 199998    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 823       |\n",
      "|    ep_rew_mean     | 2.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 744       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 8419      |\n",
      "|    total_timesteps | 200382    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.81e+05 |\n",
      "|    critic_loss     | 1.97e+09  |\n",
      "|    ent_coef        | 74.5      |\n",
      "|    ent_coef_loss   | 1.48      |\n",
      "|    learning_rate   | 0.00467   |\n",
      "|    n_updates       | 200380    |\n",
      "|    reward_est_loss | -1.55e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 845       |\n",
      "|    ep_rew_mean     | 2.83e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 748       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 8595      |\n",
      "|    total_timesteps | 204382    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.9e+05  |\n",
      "|    critic_loss     | 1.96e+09  |\n",
      "|    ent_coef        | 82.5      |\n",
      "|    ent_coef_loss   | 1.13      |\n",
      "|    learning_rate   | 0.00466   |\n",
      "|    n_updates       | 204380    |\n",
      "|    reward_est_loss | -1.78e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=205000, episode_reward=3201.62 +/- 382.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.2e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 205000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.85e+05 |\n",
      "|    critic_loss     | 2.64e+09  |\n",
      "|    ent_coef        | 75.9      |\n",
      "|    ent_coef_loss   | -2.26     |\n",
      "|    learning_rate   | 0.00466   |\n",
      "|    n_updates       | 204998    |\n",
      "|    reward_est_loss | -1.47e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 837       |\n",
      "|    ep_rew_mean     | 2.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 752       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 8734      |\n",
      "|    total_timesteps | 207589    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.79e+05 |\n",
      "|    critic_loss     | 2.22e+09  |\n",
      "|    ent_coef        | 80.2      |\n",
      "|    ent_coef_loss   | -1.51     |\n",
      "|    learning_rate   | 0.00465   |\n",
      "|    n_updates       | 207587    |\n",
      "|    reward_est_loss | -1.81e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=3320.92 +/- 422.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.32e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 210000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.96e+05 |\n",
      "|    critic_loss     | 2.41e+09  |\n",
      "|    ent_coef        | 81.3      |\n",
      "|    ent_coef_loss   | 1.15      |\n",
      "|    learning_rate   | 0.00465   |\n",
      "|    n_updates       | 209998    |\n",
      "|    reward_est_loss | -1.77e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 837       |\n",
      "|    ep_rew_mean     | 2.81e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 756       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 8908      |\n",
      "|    total_timesteps | 211589    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.91e+05 |\n",
      "|    critic_loss     | 2.78e+09  |\n",
      "|    ent_coef        | 81        |\n",
      "|    ent_coef_loss   | 0.425     |\n",
      "|    learning_rate   | 0.00465   |\n",
      "|    n_updates       | 211587    |\n",
      "|    reward_est_loss | -1.84e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 828       |\n",
      "|    ep_rew_mean     | 2.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 760       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 9038      |\n",
      "|    total_timesteps | 214706    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.98e+05 |\n",
      "|    critic_loss     | 1.83e+09  |\n",
      "|    ent_coef        | 83.2      |\n",
      "|    ent_coef_loss   | 1.6       |\n",
      "|    learning_rate   | 0.00464   |\n",
      "|    n_updates       | 214704    |\n",
      "|    reward_est_loss | -1.82e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=215000, episode_reward=4166.75 +/- 292.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.17e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 215000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.89e+05 |\n",
      "|    critic_loss     | 2.98e+09  |\n",
      "|    ent_coef        | 81.5      |\n",
      "|    ent_coef_loss   | -2.66     |\n",
      "|    learning_rate   | 0.00464   |\n",
      "|    n_updates       | 214998    |\n",
      "|    reward_est_loss | -1.69e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 827       |\n",
      "|    ep_rew_mean     | 2.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 764       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 9174      |\n",
      "|    total_timesteps | 217819    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.03e+05 |\n",
      "|    critic_loss     | 2.04e+09  |\n",
      "|    ent_coef        | 81.1      |\n",
      "|    ent_coef_loss   | 1.34      |\n",
      "|    learning_rate   | 0.00464   |\n",
      "|    n_updates       | 217817    |\n",
      "|    reward_est_loss | 1.24e+03  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=3863.27 +/- 63.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.86e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 220000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.99e+05 |\n",
      "|    critic_loss     | 6.98e+09  |\n",
      "|    ent_coef        | 78.6      |\n",
      "|    ent_coef_loss   | -0.473    |\n",
      "|    learning_rate   | 0.00463   |\n",
      "|    n_updates       | 219998    |\n",
      "|    reward_est_loss | -1.82e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 827       |\n",
      "|    ep_rew_mean     | 2.81e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 768       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 9356      |\n",
      "|    total_timesteps | 221819    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.05e+05 |\n",
      "|    critic_loss     | 1.67e+09  |\n",
      "|    ent_coef        | 80.1      |\n",
      "|    ent_coef_loss   | 1.69      |\n",
      "|    learning_rate   | 0.00463   |\n",
      "|    n_updates       | 221817    |\n",
      "|    reward_est_loss | -1.8e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=225000, episode_reward=3647.45 +/- 172.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.65e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 225000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.14e+05 |\n",
      "|    critic_loss     | 3.18e+09  |\n",
      "|    ent_coef        | 77.8      |\n",
      "|    ent_coef_loss   | -3.12     |\n",
      "|    learning_rate   | 0.00463   |\n",
      "|    n_updates       | 224998    |\n",
      "|    reward_est_loss | -1.84e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 835       |\n",
      "|    ep_rew_mean     | 2.86e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 772       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 9537      |\n",
      "|    total_timesteps | 225819    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.07e+05 |\n",
      "|    critic_loss     | 2.47e+09  |\n",
      "|    ent_coef        | 83.6      |\n",
      "|    ent_coef_loss   | -1.36     |\n",
      "|    learning_rate   | 0.00462   |\n",
      "|    n_updates       | 225817    |\n",
      "|    reward_est_loss | -1.36e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 826       |\n",
      "|    ep_rew_mean     | 2.84e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 776       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 9675      |\n",
      "|    total_timesteps | 228885    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.12e+05 |\n",
      "|    critic_loss     | 2.55e+09  |\n",
      "|    ent_coef        | 83.4      |\n",
      "|    ent_coef_loss   | 2.25      |\n",
      "|    learning_rate   | 0.00462   |\n",
      "|    n_updates       | 228883    |\n",
      "|    reward_est_loss | -1.72e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=4124.63 +/- 90.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.12e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 230000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.09e+05 |\n",
      "|    critic_loss     | 2.64e+09  |\n",
      "|    ent_coef        | 84.4      |\n",
      "|    ent_coef_loss   | -0.324    |\n",
      "|    learning_rate   | 0.00462   |\n",
      "|    n_updates       | 229998    |\n",
      "|    reward_est_loss | -1.79e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 826       |\n",
      "|    ep_rew_mean     | 2.85e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 780       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 9852      |\n",
      "|    total_timesteps | 232885    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.18e+05 |\n",
      "|    critic_loss     | 2.98e+09  |\n",
      "|    ent_coef        | 81.2      |\n",
      "|    ent_coef_loss   | 4.45      |\n",
      "|    learning_rate   | 0.00461   |\n",
      "|    n_updates       | 232883    |\n",
      "|    reward_est_loss | -1.84e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=235000, episode_reward=1671.42 +/- 1545.64\n",
      "Episode length: 510.20 +/- 409.34\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 510       |\n",
      "|    mean_reward     | 1.67e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 235000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.19e+05 |\n",
      "|    critic_loss     | 2.31e+09  |\n",
      "|    ent_coef        | 85.8      |\n",
      "|    ent_coef_loss   | -1.15     |\n",
      "|    learning_rate   | 0.00461   |\n",
      "|    n_updates       | 234998    |\n",
      "|    reward_est_loss | -7.23e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 826       |\n",
      "|    ep_rew_mean     | 2.87e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 784       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 10029     |\n",
      "|    total_timesteps | 236885    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.24e+05 |\n",
      "|    critic_loss     | 2.24e+09  |\n",
      "|    ent_coef        | 85.5      |\n",
      "|    ent_coef_loss   | 1.93      |\n",
      "|    learning_rate   | 0.00461   |\n",
      "|    n_updates       | 236883    |\n",
      "|    reward_est_loss | -1.74e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=3547.25 +/- 27.59\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.55e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 240000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.13e+05 |\n",
      "|    critic_loss     | 2.39e+09  |\n",
      "|    ent_coef        | 84.9      |\n",
      "|    ent_coef_loss   | -1.51     |\n",
      "|    learning_rate   | 0.0046    |\n",
      "|    n_updates       | 239998    |\n",
      "|    reward_est_loss | -5.39e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 826       |\n",
      "|    ep_rew_mean     | 2.9e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 788       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 10200     |\n",
      "|    total_timesteps | 240885    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.18e+05 |\n",
      "|    critic_loss     | 2.06e+09  |\n",
      "|    ent_coef        | 81.4      |\n",
      "|    ent_coef_loss   | 0.636     |\n",
      "|    learning_rate   | 0.0046    |\n",
      "|    n_updates       | 240883    |\n",
      "|    reward_est_loss | -8.65e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 835       |\n",
      "|    ep_rew_mean     | 2.96e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 792       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 10367     |\n",
      "|    total_timesteps | 244885    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.3e+05  |\n",
      "|    critic_loss     | 3.87e+09  |\n",
      "|    ent_coef        | 81.5      |\n",
      "|    ent_coef_loss   | 1.14      |\n",
      "|    learning_rate   | 0.00459   |\n",
      "|    n_updates       | 244883    |\n",
      "|    reward_est_loss | -1.78e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=245000, episode_reward=3379.89 +/- 1628.30\n",
      "Episode length: 815.80 +/- 368.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 816       |\n",
      "|    mean_reward     | 3.38e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 245000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.27e+05 |\n",
      "|    critic_loss     | 3.06e+09  |\n",
      "|    ent_coef        | 81.5      |\n",
      "|    ent_coef_loss   | 0.6       |\n",
      "|    learning_rate   | 0.00459   |\n",
      "|    n_updates       | 244998    |\n",
      "|    reward_est_loss | -1.65e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 850       |\n",
      "|    ep_rew_mean     | 3.05e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 796       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 10541     |\n",
      "|    total_timesteps | 248885    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.36e+05 |\n",
      "|    critic_loss     | 3.12e+09  |\n",
      "|    ent_coef        | 89.3      |\n",
      "|    ent_coef_loss   | 0.385     |\n",
      "|    learning_rate   | 0.00459   |\n",
      "|    n_updates       | 248883    |\n",
      "|    reward_est_loss | -1.66e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=2506.11 +/- 1689.11\n",
      "Episode length: 687.40 +/- 383.89\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 687       |\n",
      "|    mean_reward     | 2.51e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 250000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.28e+05 |\n",
      "|    critic_loss     | 9.1e+09   |\n",
      "|    ent_coef        | 87.3      |\n",
      "|    ent_coef_loss   | 1.9       |\n",
      "|    learning_rate   | 0.00458   |\n",
      "|    n_updates       | 249998    |\n",
      "|    reward_est_loss | -1.72e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 856       |\n",
      "|    ep_rew_mean     | 3.11e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 800       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 10714     |\n",
      "|    total_timesteps | 252885    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.42e+05 |\n",
      "|    critic_loss     | 2.3e+09   |\n",
      "|    ent_coef        | 87.7      |\n",
      "|    ent_coef_loss   | -0.216    |\n",
      "|    learning_rate   | 0.00458   |\n",
      "|    n_updates       | 252883    |\n",
      "|    reward_est_loss | -1.47e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=255000, episode_reward=3603.18 +/- 1752.35\n",
      "Episode length: 812.20 +/- 375.60\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 812       |\n",
      "|    mean_reward     | 3.6e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 255000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.4e+05  |\n",
      "|    critic_loss     | 2.42e+09  |\n",
      "|    ent_coef        | 88.3      |\n",
      "|    ent_coef_loss   | 0.754     |\n",
      "|    learning_rate   | 0.00458   |\n",
      "|    n_updates       | 254998    |\n",
      "|    reward_est_loss | -1.68e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 847       |\n",
      "|    ep_rew_mean     | 3.1e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 804       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 10851     |\n",
      "|    total_timesteps | 256000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.38e+05 |\n",
      "|    critic_loss     | 3.46e+09  |\n",
      "|    ent_coef        | 89        |\n",
      "|    ent_coef_loss   | -2.52     |\n",
      "|    learning_rate   | 0.00457   |\n",
      "|    n_updates       | 255998    |\n",
      "|    reward_est_loss | -1.5e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=4072.45 +/- 67.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.07e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 260000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.48e+05 |\n",
      "|    critic_loss     | 2.15e+09  |\n",
      "|    ent_coef        | 90.3      |\n",
      "|    ent_coef_loss   | 4.77      |\n",
      "|    learning_rate   | 0.00457   |\n",
      "|    n_updates       | 259998    |\n",
      "|    reward_est_loss | -1.59e+04 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 878      |\n",
      "|    ep_rew_mean     | 3.25e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 808      |\n",
      "|    fps             | 23       |\n",
      "|    time_elapsed    | 11028    |\n",
      "|    total_timesteps | 260000   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 891       |\n",
      "|    ep_rew_mean     | 3.31e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 812       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 11200     |\n",
      "|    total_timesteps | 264000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.38e+05 |\n",
      "|    critic_loss     | 3.64e+09  |\n",
      "|    ent_coef        | 89.7      |\n",
      "|    ent_coef_loss   | 0.295     |\n",
      "|    learning_rate   | 0.00456   |\n",
      "|    n_updates       | 263998    |\n",
      "|    reward_est_loss | -1.59e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=265000, episode_reward=4413.35 +/- 78.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.41e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 265000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.45e+05 |\n",
      "|    critic_loss     | 2.82e+09  |\n",
      "|    ent_coef        | 90.9      |\n",
      "|    ent_coef_loss   | -1.96     |\n",
      "|    learning_rate   | 0.00456   |\n",
      "|    n_updates       | 264998    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 906       |\n",
      "|    ep_rew_mean     | 3.39e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 816       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 11375     |\n",
      "|    total_timesteps | 268000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.53e+05 |\n",
      "|    critic_loss     | 3.29e+09  |\n",
      "|    ent_coef        | 93.9      |\n",
      "|    ent_coef_loss   | 0.000862  |\n",
      "|    learning_rate   | 0.00455   |\n",
      "|    n_updates       | 267998    |\n",
      "|    reward_est_loss | -1.38e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=4285.56 +/- 74.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.29e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 270000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.56e+05 |\n",
      "|    critic_loss     | 2.16e+09  |\n",
      "|    ent_coef        | 97.4      |\n",
      "|    ent_coef_loss   | 1.46      |\n",
      "|    learning_rate   | 0.00455   |\n",
      "|    n_updates       | 269998    |\n",
      "|    reward_est_loss | -1.26e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 915       |\n",
      "|    ep_rew_mean     | 3.43e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 820       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 11550     |\n",
      "|    total_timesteps | 272000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.61e+05 |\n",
      "|    critic_loss     | 3.83e+09  |\n",
      "|    ent_coef        | 92.6      |\n",
      "|    ent_coef_loss   | -1.05     |\n",
      "|    learning_rate   | 0.00455   |\n",
      "|    n_updates       | 271998    |\n",
      "|    reward_est_loss | -9.94e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=275000, episode_reward=3722.08 +/- 143.24\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.72e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 275000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.56e+05 |\n",
      "|    critic_loss     | 3.11e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | -0.0118   |\n",
      "|    learning_rate   | 0.00454   |\n",
      "|    n_updates       | 274998    |\n",
      "|    reward_est_loss | -1.69e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 915       |\n",
      "|    ep_rew_mean     | 3.46e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 824       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 11724     |\n",
      "|    total_timesteps | 276000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.66e+05 |\n",
      "|    critic_loss     | 4.23e+09  |\n",
      "|    ent_coef        | 94.5      |\n",
      "|    ent_coef_loss   | 3.6       |\n",
      "|    learning_rate   | 0.00454   |\n",
      "|    n_updates       | 275998    |\n",
      "|    reward_est_loss | -1.26e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=3219.58 +/- 1418.43\n",
      "Episode length: 836.00 +/- 328.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 836       |\n",
      "|    mean_reward     | 3.22e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 280000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.73e+05 |\n",
      "|    critic_loss     | 3.35e+09  |\n",
      "|    ent_coef        | 92.4      |\n",
      "|    ent_coef_loss   | -0.654    |\n",
      "|    learning_rate   | 0.00453   |\n",
      "|    n_updates       | 279998    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 923      |\n",
      "|    ep_rew_mean     | 3.53e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 828      |\n",
      "|    fps             | 23       |\n",
      "|    time_elapsed    | 11899    |\n",
      "|    total_timesteps | 280000   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 930       |\n",
      "|    ep_rew_mean     | 3.6e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 832       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 12070     |\n",
      "|    total_timesteps | 284000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.66e+05 |\n",
      "|    critic_loss     | 4.15e+09  |\n",
      "|    ent_coef        | 94.7      |\n",
      "|    ent_coef_loss   | 0.0369    |\n",
      "|    learning_rate   | 0.00453   |\n",
      "|    n_updates       | 283998    |\n",
      "|    reward_est_loss | -1.82e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=285000, episode_reward=4340.21 +/- 75.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.34e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 285000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.84e+05 |\n",
      "|    critic_loss     | 2.49e+09  |\n",
      "|    ent_coef        | 94.6      |\n",
      "|    ent_coef_loss   | 1.6       |\n",
      "|    learning_rate   | 0.00453   |\n",
      "|    n_updates       | 284998    |\n",
      "|    reward_est_loss | -1.84e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 939       |\n",
      "|    ep_rew_mean     | 3.68e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 836       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 12242     |\n",
      "|    total_timesteps | 288000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.7e+05  |\n",
      "|    critic_loss     | 2.82e+09  |\n",
      "|    ent_coef        | 93.2      |\n",
      "|    ent_coef_loss   | 1.22      |\n",
      "|    learning_rate   | 0.00452   |\n",
      "|    n_updates       | 287998    |\n",
      "|    reward_est_loss | -1.76e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=4457.88 +/- 88.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.46e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 290000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.78e+05 |\n",
      "|    critic_loss     | 2.69e+09  |\n",
      "|    ent_coef        | 90.9      |\n",
      "|    ent_coef_loss   | -0.337    |\n",
      "|    learning_rate   | 0.00452   |\n",
      "|    n_updates       | 289998    |\n",
      "|    reward_est_loss | 1.72e+03  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 3.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 840       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 12413     |\n",
      "|    total_timesteps | 292000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.78e+05 |\n",
      "|    critic_loss     | 3.4e+09   |\n",
      "|    ent_coef        | 98        |\n",
      "|    ent_coef_loss   | 0.967     |\n",
      "|    learning_rate   | 0.00451   |\n",
      "|    n_updates       | 291998    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=295000, episode_reward=3206.05 +/- 1537.63\n",
      "Episode length: 820.40 +/- 359.20\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 820       |\n",
      "|    mean_reward     | 3.21e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 295000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.77e+05 |\n",
      "|    critic_loss     | 3.24e+09  |\n",
      "|    ent_coef        | 94.7      |\n",
      "|    ent_coef_loss   | 1.23      |\n",
      "|    learning_rate   | 0.00451   |\n",
      "|    n_updates       | 294998    |\n",
      "|    reward_est_loss | -6.51e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 956       |\n",
      "|    ep_rew_mean     | 3.83e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 844       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 12593     |\n",
      "|    total_timesteps | 296000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.79e+05 |\n",
      "|    critic_loss     | 6.09e+09  |\n",
      "|    ent_coef        | 94.3      |\n",
      "|    ent_coef_loss   | 1.2       |\n",
      "|    learning_rate   | 0.00451   |\n",
      "|    n_updates       | 295998    |\n",
      "|    reward_est_loss | -1.61e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=3239.70 +/- 1553.60\n",
      "Episode length: 818.60 +/- 362.80\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 819       |\n",
      "|    mean_reward     | 3.24e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 300000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.79e+05 |\n",
      "|    critic_loss     | 3.35e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | 1.61      |\n",
      "|    learning_rate   | 0.0045    |\n",
      "|    n_updates       | 299998    |\n",
      "|    reward_est_loss | -1.67e+04 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 956      |\n",
      "|    ep_rew_mean     | 3.85e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 848      |\n",
      "|    fps             | 23       |\n",
      "|    time_elapsed    | 12768    |\n",
      "|    total_timesteps | 300000   |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 964       |\n",
      "|    ep_rew_mean     | 3.92e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 852       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 12935     |\n",
      "|    total_timesteps | 304000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.91e+05 |\n",
      "|    critic_loss     | 2.24e+09  |\n",
      "|    ent_coef        | 97.7      |\n",
      "|    ent_coef_loss   | -2.02     |\n",
      "|    learning_rate   | 0.00449   |\n",
      "|    n_updates       | 303998    |\n",
      "|    reward_est_loss | -1.67e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=305000, episode_reward=3441.80 +/- 1629.40\n",
      "Episode length: 822.60 +/- 354.80\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 823       |\n",
      "|    mean_reward     | 3.44e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 305000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.99e+05 |\n",
      "|    critic_loss     | 3.02e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | 0.55      |\n",
      "|    learning_rate   | 0.00449   |\n",
      "|    n_updates       | 304998    |\n",
      "|    reward_est_loss | -1.79e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 964       |\n",
      "|    ep_rew_mean     | 3.93e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 856       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 13105     |\n",
      "|    total_timesteps | 308000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.98e+05 |\n",
      "|    critic_loss     | 3.55e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 0.583     |\n",
      "|    learning_rate   | 0.00449   |\n",
      "|    n_updates       | 307998    |\n",
      "|    reward_est_loss | -1.5e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=3792.40 +/- 32.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.79e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 310000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -2.94e+05 |\n",
      "|    critic_loss     | 4.7e+09   |\n",
      "|    ent_coef        | 99.9      |\n",
      "|    ent_coef_loss   | -4.34     |\n",
      "|    learning_rate   | 0.00448   |\n",
      "|    n_updates       | 309998    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 973       |\n",
      "|    ep_rew_mean     | 3.98e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 860       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 13271     |\n",
      "|    total_timesteps | 312000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.09e+05 |\n",
      "|    critic_loss     | 2.86e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | -2.01     |\n",
      "|    learning_rate   | 0.00448   |\n",
      "|    n_updates       | 311998    |\n",
      "|    reward_est_loss | -1.79e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=315000, episode_reward=3452.42 +/- 1560.38\n",
      "Episode length: 840.80 +/- 318.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 841       |\n",
      "|    mean_reward     | 3.45e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 315000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.16e+05 |\n",
      "|    critic_loss     | 3.28e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | 2.06      |\n",
      "|    learning_rate   | 0.00448   |\n",
      "|    n_updates       | 314998    |\n",
      "|    reward_est_loss | -1.37e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 975       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 864       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 13409     |\n",
      "|    total_timesteps | 315279    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.01e+05 |\n",
      "|    critic_loss     | 2.43e+09  |\n",
      "|    ent_coef        | 100       |\n",
      "|    ent_coef_loss   | -3.19     |\n",
      "|    learning_rate   | 0.00447   |\n",
      "|    n_updates       | 315277    |\n",
      "|    reward_est_loss | -1.85e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 975       |\n",
      "|    ep_rew_mean     | 4.01e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 868       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 13575     |\n",
      "|    total_timesteps | 319279    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.17e+05 |\n",
      "|    critic_loss     | 1.13e+10  |\n",
      "|    ent_coef        | 100       |\n",
      "|    ent_coef_loss   | 0.453     |\n",
      "|    learning_rate   | 0.00447   |\n",
      "|    n_updates       | 319277    |\n",
      "|    reward_est_loss | -1.39e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=3526.60 +/- 71.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.53e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 320000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3e+05    |\n",
      "|    critic_loss     | 2.77e+09  |\n",
      "|    ent_coef        | 95.7      |\n",
      "|    ent_coef_loss   | 0.609     |\n",
      "|    learning_rate   | 0.00447   |\n",
      "|    n_updates       | 319998    |\n",
      "|    reward_est_loss | -6.74e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 975       |\n",
      "|    ep_rew_mean     | 4.01e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 872       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 13741     |\n",
      "|    total_timesteps | 323279    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.27e+05 |\n",
      "|    critic_loss     | 3.57e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | 1.29      |\n",
      "|    learning_rate   | 0.00446   |\n",
      "|    n_updates       | 323277    |\n",
      "|    reward_est_loss | -1.78e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=325000, episode_reward=2541.10 +/- 1165.07\n",
      "Episode length: 855.60 +/- 288.80\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 856       |\n",
      "|    mean_reward     | 2.54e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 325000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.12e+05 |\n",
      "|    critic_loss     | 3.51e+09  |\n",
      "|    ent_coef        | 100       |\n",
      "|    ent_coef_loss   | 0.263     |\n",
      "|    learning_rate   | 0.00446   |\n",
      "|    n_updates       | 324998    |\n",
      "|    reward_est_loss | -1.3e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 977       |\n",
      "|    ep_rew_mean     | 4.01e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 876       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 13885     |\n",
      "|    total_timesteps | 326609    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.16e+05 |\n",
      "|    critic_loss     | 3.88e+09  |\n",
      "|    ent_coef        | 94.1      |\n",
      "|    ent_coef_loss   | 1.12      |\n",
      "|    learning_rate   | 0.00446   |\n",
      "|    n_updates       | 326607    |\n",
      "|    reward_est_loss | -1.66e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=3776.46 +/- 82.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.78e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 330000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.25e+05 |\n",
      "|    critic_loss     | 3.64e+09  |\n",
      "|    ent_coef        | 104       |\n",
      "|    ent_coef_loss   | -0.122    |\n",
      "|    learning_rate   | 0.00445   |\n",
      "|    n_updates       | 329998    |\n",
      "|    reward_est_loss | -1.63e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 977       |\n",
      "|    ep_rew_mean     | 4.02e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 880       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 14053     |\n",
      "|    total_timesteps | 330609    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.09e+05 |\n",
      "|    critic_loss     | 3.05e+09  |\n",
      "|    ent_coef        | 95.8      |\n",
      "|    ent_coef_loss   | -0.261    |\n",
      "|    learning_rate   | 0.00445   |\n",
      "|    n_updates       | 330607    |\n",
      "|    reward_est_loss | -1.42e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 977       |\n",
      "|    ep_rew_mean     | 4.03e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 884       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 14222     |\n",
      "|    total_timesteps | 334609    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.24e+05 |\n",
      "|    critic_loss     | 3.03e+09  |\n",
      "|    ent_coef        | 98.3      |\n",
      "|    ent_coef_loss   | -0.371    |\n",
      "|    learning_rate   | 0.00444   |\n",
      "|    n_updates       | 334607    |\n",
      "|    reward_est_loss | -1.84e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=335000, episode_reward=3897.69 +/- 40.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.9e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 335000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.25e+05 |\n",
      "|    critic_loss     | 2.4e+09   |\n",
      "|    ent_coef        | 98.7      |\n",
      "|    ent_coef_loss   | -1.91     |\n",
      "|    learning_rate   | 0.00444   |\n",
      "|    n_updates       | 334998    |\n",
      "|    reward_est_loss | -1.72e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 977       |\n",
      "|    ep_rew_mean     | 4.02e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 888       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 14393     |\n",
      "|    total_timesteps | 338609    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.26e+05 |\n",
      "|    critic_loss     | 3.91e+09  |\n",
      "|    ent_coef        | 95.5      |\n",
      "|    ent_coef_loss   | -0.727    |\n",
      "|    learning_rate   | 0.00444   |\n",
      "|    n_updates       | 338607    |\n",
      "|    reward_est_loss | -1.49e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=3324.82 +/- 1171.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.32e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 340000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.31e+05 |\n",
      "|    critic_loss     | 4.19e+09  |\n",
      "|    ent_coef        | 105       |\n",
      "|    ent_coef_loss   | -1.83     |\n",
      "|    learning_rate   | 0.00443   |\n",
      "|    n_updates       | 339998    |\n",
      "|    reward_est_loss | -1.51e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 969       |\n",
      "|    ep_rew_mean     | 3.98e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 892       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 14532     |\n",
      "|    total_timesteps | 341789    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.33e+05 |\n",
      "|    critic_loss     | 3.59e+09  |\n",
      "|    ent_coef        | 97        |\n",
      "|    ent_coef_loss   | 1.79      |\n",
      "|    learning_rate   | 0.00443   |\n",
      "|    n_updates       | 341787    |\n",
      "|    reward_est_loss | -1.09e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=345000, episode_reward=3829.03 +/- 114.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.83e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 345000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.28e+05 |\n",
      "|    critic_loss     | 2.82e+09  |\n",
      "|    ent_coef        | 97.5      |\n",
      "|    ent_coef_loss   | 0.51      |\n",
      "|    learning_rate   | 0.00443   |\n",
      "|    n_updates       | 344998    |\n",
      "|    reward_est_loss | -1.82e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 969       |\n",
      "|    ep_rew_mean     | 3.97e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 896       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 14703     |\n",
      "|    total_timesteps | 345789    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.37e+05 |\n",
      "|    critic_loss     | 2.8e+09   |\n",
      "|    ent_coef        | 96.9      |\n",
      "|    ent_coef_loss   | -1.96     |\n",
      "|    learning_rate   | 0.00442   |\n",
      "|    n_updates       | 345787    |\n",
      "|    reward_est_loss | -1.86e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 962       |\n",
      "|    ep_rew_mean     | 3.92e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 900       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 14841     |\n",
      "|    total_timesteps | 349093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.3e+05  |\n",
      "|    critic_loss     | 2.82e+09  |\n",
      "|    ent_coef        | 104       |\n",
      "|    ent_coef_loss   | 1.26      |\n",
      "|    learning_rate   | 0.00442   |\n",
      "|    n_updates       | 349091    |\n",
      "|    reward_est_loss | -1.81e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=4430.69 +/- 12.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 350000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.42e+05 |\n",
      "|    critic_loss     | 4.13e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | -1.09     |\n",
      "|    learning_rate   | 0.00442   |\n",
      "|    n_updates       | 349998    |\n",
      "|    reward_est_loss | -1.47e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.97e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 904       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 15010     |\n",
      "|    total_timesteps | 353093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.4e+05  |\n",
      "|    critic_loss     | 2.44e+09  |\n",
      "|    ent_coef        | 95.4      |\n",
      "|    ent_coef_loss   | -1.6      |\n",
      "|    learning_rate   | 0.00441   |\n",
      "|    n_updates       | 353091    |\n",
      "|    reward_est_loss | -1.48e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=355000, episode_reward=4424.49 +/- 37.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.42e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 355000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.34e+05 |\n",
      "|    critic_loss     | 3.9e+09   |\n",
      "|    ent_coef        | 98.8      |\n",
      "|    ent_coef_loss   | -0.948    |\n",
      "|    learning_rate   | 0.00441   |\n",
      "|    n_updates       | 354998    |\n",
      "|    reward_est_loss | -1.87e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.97e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 908       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 15179     |\n",
      "|    total_timesteps | 357093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.38e+05 |\n",
      "|    critic_loss     | 2.27e+09  |\n",
      "|    ent_coef        | 99.7      |\n",
      "|    ent_coef_loss   | 1.32      |\n",
      "|    learning_rate   | 0.0044    |\n",
      "|    n_updates       | 357091    |\n",
      "|    reward_est_loss | -1.75e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=3600.87 +/- 70.16\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.6e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 360000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.41e+05 |\n",
      "|    critic_loss     | 3.61e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | 0.558     |\n",
      "|    learning_rate   | 0.0044    |\n",
      "|    n_updates       | 359998    |\n",
      "|    reward_est_loss | -1.67e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.98e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 912       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 15346     |\n",
      "|    total_timesteps | 361093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.47e+05 |\n",
      "|    critic_loss     | 3.41e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -0.905    |\n",
      "|    learning_rate   | 0.0044    |\n",
      "|    n_updates       | 361091    |\n",
      "|    reward_est_loss | -1.79e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=365000, episode_reward=3911.29 +/- 44.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.91e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 365000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.42e+05 |\n",
      "|    critic_loss     | 3.79e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | 0.82      |\n",
      "|    learning_rate   | 0.00439   |\n",
      "|    n_updates       | 364998    |\n",
      "|    reward_est_loss | 4.56e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.98e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 916       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 15516     |\n",
      "|    total_timesteps | 365093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.58e+05 |\n",
      "|    critic_loss     | 4.72e+09  |\n",
      "|    ent_coef        | 95.6      |\n",
      "|    ent_coef_loss   | 2.42      |\n",
      "|    learning_rate   | 0.00439   |\n",
      "|    n_updates       | 365091    |\n",
      "|    reward_est_loss | -1.64e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 920       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 15686     |\n",
      "|    total_timesteps | 369093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.54e+05 |\n",
      "|    critic_loss     | 2.86e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | -1.38     |\n",
      "|    learning_rate   | 0.00438   |\n",
      "|    n_updates       | 369091    |\n",
      "|    reward_est_loss | -4.85e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=4324.98 +/- 26.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.32e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 370000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.55e+05 |\n",
      "|    critic_loss     | 3.09e+09  |\n",
      "|    ent_coef        | 99.7      |\n",
      "|    ent_coef_loss   | 0.117     |\n",
      "|    learning_rate   | 0.00438   |\n",
      "|    n_updates       | 369998    |\n",
      "|    reward_est_loss | -1.81e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 4e+03     |\n",
      "| time/              |           |\n",
      "|    episodes        | 924       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 15863     |\n",
      "|    total_timesteps | 373093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.58e+05 |\n",
      "|    critic_loss     | 3.49e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | 2         |\n",
      "|    learning_rate   | 0.00438   |\n",
      "|    n_updates       | 373091    |\n",
      "|    reward_est_loss | -1.7e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=375000, episode_reward=3789.02 +/- 15.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.79e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 375000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.56e+05 |\n",
      "|    critic_loss     | 3.49e+09  |\n",
      "|    ent_coef        | 97.3      |\n",
      "|    ent_coef_loss   | 0.492     |\n",
      "|    learning_rate   | 0.00438   |\n",
      "|    n_updates       | 374998    |\n",
      "|    reward_est_loss | -1.2e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 4.01e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 928       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 16042     |\n",
      "|    total_timesteps | 377093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.54e+05 |\n",
      "|    critic_loss     | 3.41e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | -1.97     |\n",
      "|    learning_rate   | 0.00437   |\n",
      "|    n_updates       | 377091    |\n",
      "|    reward_est_loss | -1.55e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=3649.11 +/- 11.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.65e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 380000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.56e+05 |\n",
      "|    critic_loss     | 2.65e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | 2.02      |\n",
      "|    learning_rate   | 0.00437   |\n",
      "|    n_updates       | 379998    |\n",
      "|    reward_est_loss | -3.19e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 4e+03     |\n",
      "| time/              |           |\n",
      "|    episodes        | 932       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 16209     |\n",
      "|    total_timesteps | 381093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.55e+05 |\n",
      "|    critic_loss     | 5.97e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -1.09     |\n",
      "|    learning_rate   | 0.00436   |\n",
      "|    n_updates       | 381091    |\n",
      "|    reward_est_loss | -1.59e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=385000, episode_reward=4192.50 +/- 94.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.19e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 385000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.63e+05 |\n",
      "|    critic_loss     | 3.49e+09  |\n",
      "|    ent_coef        | 91.5      |\n",
      "|    ent_coef_loss   | 0.0643    |\n",
      "|    learning_rate   | 0.00436   |\n",
      "|    n_updates       | 384998    |\n",
      "|    reward_est_loss | -1.1e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 936       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 16379     |\n",
      "|    total_timesteps | 385093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.59e+05 |\n",
      "|    critic_loss     | 3.08e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | -0.00154  |\n",
      "|    learning_rate   | 0.00436   |\n",
      "|    n_updates       | 385091    |\n",
      "|    reward_est_loss | -1.5e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 940       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 16549     |\n",
      "|    total_timesteps | 389093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.63e+05 |\n",
      "|    critic_loss     | 3e+09     |\n",
      "|    ent_coef        | 97        |\n",
      "|    ent_coef_loss   | 1.82      |\n",
      "|    learning_rate   | 0.00435   |\n",
      "|    n_updates       | 389091    |\n",
      "|    reward_est_loss | -1.88e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=3816.84 +/- 20.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.82e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 390000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.74e+05 |\n",
      "|    critic_loss     | 3.9e+09   |\n",
      "|    ent_coef        | 97.5      |\n",
      "|    ent_coef_loss   | 0.47      |\n",
      "|    learning_rate   | 0.00435   |\n",
      "|    n_updates       | 389998    |\n",
      "|    reward_est_loss | -1.89e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.98e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 944       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 16727     |\n",
      "|    total_timesteps | 393093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.69e+05 |\n",
      "|    critic_loss     | 3.42e+09  |\n",
      "|    ent_coef        | 98.5      |\n",
      "|    ent_coef_loss   | -1.35     |\n",
      "|    learning_rate   | 0.00434   |\n",
      "|    n_updates       | 393091    |\n",
      "|    reward_est_loss | 8.15e+03  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=395000, episode_reward=3946.97 +/- 21.47\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.95e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 395000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.74e+05 |\n",
      "|    critic_loss     | 4.24e+09  |\n",
      "|    ent_coef        | 98.2      |\n",
      "|    ent_coef_loss   | 0.807     |\n",
      "|    learning_rate   | 0.00434   |\n",
      "|    n_updates       | 394998    |\n",
      "|    reward_est_loss | -1.89e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.97e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 948       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 16905     |\n",
      "|    total_timesteps | 397093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.65e+05 |\n",
      "|    critic_loss     | 8.23e+09  |\n",
      "|    ent_coef        | 89.8      |\n",
      "|    ent_coef_loss   | -2.19     |\n",
      "|    learning_rate   | 0.00434   |\n",
      "|    n_updates       | 397091    |\n",
      "|    reward_est_loss | -1.87e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=4203.10 +/- 58.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.2e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 400000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.84e+05 |\n",
      "|    critic_loss     | 3.13e+09  |\n",
      "|    ent_coef        | 93        |\n",
      "|    ent_coef_loss   | 1.16      |\n",
      "|    learning_rate   | 0.00433   |\n",
      "|    n_updates       | 399998    |\n",
      "|    reward_est_loss | -9.7e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.97e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 952       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 17077     |\n",
      "|    total_timesteps | 401093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.83e+05 |\n",
      "|    critic_loss     | 2.63e+09  |\n",
      "|    ent_coef        | 93.2      |\n",
      "|    ent_coef_loss   | -0.203    |\n",
      "|    learning_rate   | 0.00433   |\n",
      "|    n_updates       | 401091    |\n",
      "|    reward_est_loss | -1.86e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=405000, episode_reward=4429.42 +/- 91.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 405000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.82e+05 |\n",
      "|    critic_loss     | 3.71e+09  |\n",
      "|    ent_coef        | 98.7      |\n",
      "|    ent_coef_loss   | -3.93     |\n",
      "|    learning_rate   | 0.00433   |\n",
      "|    n_updates       | 404998    |\n",
      "|    reward_est_loss | -1.42e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.98e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 956       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 17256     |\n",
      "|    total_timesteps | 405093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.87e+05 |\n",
      "|    critic_loss     | 1.99e+09  |\n",
      "|    ent_coef        | 97.6      |\n",
      "|    ent_coef_loss   | -2.13     |\n",
      "|    learning_rate   | 0.00432   |\n",
      "|    n_updates       | 405091    |\n",
      "|    reward_est_loss | -1.84e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 960       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 17434     |\n",
      "|    total_timesteps | 409093    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.77e+05 |\n",
      "|    critic_loss     | 4.13e+09  |\n",
      "|    ent_coef        | 94.3      |\n",
      "|    ent_coef_loss   | -2.08     |\n",
      "|    learning_rate   | 0.00432   |\n",
      "|    n_updates       | 409091    |\n",
      "|    reward_est_loss | -1.9e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=3627.17 +/- 1294.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.63e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 410000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.91e+05 |\n",
      "|    critic_loss     | 2.92e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -1.22     |\n",
      "|    learning_rate   | 0.00432   |\n",
      "|    n_updates       | 409998    |\n",
      "|    reward_est_loss | -4.34e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.01e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 964       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 17574     |\n",
      "|    total_timesteps | 412288    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.76e+05 |\n",
      "|    critic_loss     | 3.51e+09  |\n",
      "|    ent_coef        | 96        |\n",
      "|    ent_coef_loss   | -2.11     |\n",
      "|    learning_rate   | 0.00431   |\n",
      "|    n_updates       | 412286    |\n",
      "|    reward_est_loss | -1.61e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=415000, episode_reward=4519.45 +/- 66.84\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.52e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 415000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.94e+05 |\n",
      "|    critic_loss     | 2.6e+09   |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -0.121    |\n",
      "|    learning_rate   | 0.00431   |\n",
      "|    n_updates       | 414998    |\n",
      "|    reward_est_loss | -1.75e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.02e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 968       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 17749     |\n",
      "|    total_timesteps | 416288    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.94e+05 |\n",
      "|    critic_loss     | 1.85e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | 3.33      |\n",
      "|    learning_rate   | 0.00431   |\n",
      "|    n_updates       | 416286    |\n",
      "|    reward_est_loss | -1.78e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 961       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 972       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 17886     |\n",
      "|    total_timesteps | 419426    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.05e+05 |\n",
      "|    critic_loss     | 1.95e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | -0.977    |\n",
      "|    learning_rate   | 0.0043    |\n",
      "|    n_updates       | 419424    |\n",
      "|    reward_est_loss | 3.68e+03  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=4421.19 +/- 67.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.42e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 420000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.97e+05 |\n",
      "|    critic_loss     | 2.59e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -0.155    |\n",
      "|    learning_rate   | 0.0043    |\n",
      "|    n_updates       | 419998    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 959       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 976       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 18018     |\n",
      "|    total_timesteps | 422511    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.86e+05 |\n",
      "|    critic_loss     | 2.69e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -0.755    |\n",
      "|    learning_rate   | 0.0043    |\n",
      "|    n_updates       | 422509    |\n",
      "|    reward_est_loss | -1.73e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=425000, episode_reward=4420.67 +/- 44.22\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.42e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 425000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.93e+05 |\n",
      "|    critic_loss     | 3.98e+09  |\n",
      "|    ent_coef        | 94.5      |\n",
      "|    ent_coef_loss   | 1.61      |\n",
      "|    learning_rate   | 0.00429   |\n",
      "|    n_updates       | 424998    |\n",
      "|    reward_est_loss | -1.92e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 959       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 980       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 18195     |\n",
      "|    total_timesteps | 426511    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.01e+05 |\n",
      "|    critic_loss     | 3.98e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | 1.15      |\n",
      "|    learning_rate   | 0.00429   |\n",
      "|    n_updates       | 426509    |\n",
      "|    reward_est_loss | -1.92e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=3699.07 +/- 213.58\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.7e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 430000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.07e+05 |\n",
      "|    critic_loss     | 4.53e+09  |\n",
      "|    ent_coef        | 98.3      |\n",
      "|    ent_coef_loss   | 0.303     |\n",
      "|    learning_rate   | 0.00428   |\n",
      "|    n_updates       | 429998    |\n",
      "|    reward_est_loss | -1.74e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 959       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 984       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 18373     |\n",
      "|    total_timesteps | 430511    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.96e+05 |\n",
      "|    critic_loss     | 3.83e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | 1.44      |\n",
      "|    learning_rate   | 0.00428   |\n",
      "|    n_updates       | 430509    |\n",
      "|    reward_est_loss | -1.88e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 959       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 988       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 18553     |\n",
      "|    total_timesteps | 434511    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.07e+05 |\n",
      "|    critic_loss     | 3.84e+09  |\n",
      "|    ent_coef        | 92.7      |\n",
      "|    ent_coef_loss   | 0.139     |\n",
      "|    learning_rate   | 0.00428   |\n",
      "|    n_updates       | 434509    |\n",
      "|    reward_est_loss | -1.74e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=435000, episode_reward=3093.12 +/- 1463.10\n",
      "Episode length: 824.20 +/- 351.60\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 824       |\n",
      "|    mean_reward     | 3.09e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 435000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.08e+05 |\n",
      "|    critic_loss     | 2.5e+09   |\n",
      "|    ent_coef        | 95.5      |\n",
      "|    ent_coef_loss   | 0.524     |\n",
      "|    learning_rate   | 0.00428   |\n",
      "|    n_updates       | 434998    |\n",
      "|    reward_est_loss | -1.45e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 958       |\n",
      "|    ep_rew_mean     | 4e+03     |\n",
      "| time/              |           |\n",
      "|    episodes        | 992       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 18685     |\n",
      "|    total_timesteps | 437593    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.04e+05 |\n",
      "|    critic_loss     | 3.63e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | 4.13      |\n",
      "|    learning_rate   | 0.00427   |\n",
      "|    n_updates       | 437591    |\n",
      "|    reward_est_loss | -1.42e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=4473.23 +/- 160.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.47e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 440000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.17e+05 |\n",
      "|    critic_loss     | 3.11e+09  |\n",
      "|    ent_coef        | 89.9      |\n",
      "|    ent_coef_loss   | 0.805     |\n",
      "|    learning_rate   | 0.00427   |\n",
      "|    n_updates       | 439998    |\n",
      "|    reward_est_loss | -9.88e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 950       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 996       |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 18848     |\n",
      "|    total_timesteps | 440750    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.09e+05 |\n",
      "|    critic_loss     | 2.88e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | 0.988     |\n",
      "|    learning_rate   | 0.00427   |\n",
      "|    n_updates       | 440748    |\n",
      "|    reward_est_loss | -6.84e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 957       |\n",
      "|    ep_rew_mean     | 4.03e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1000      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 19013     |\n",
      "|    total_timesteps | 444750    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -3.95e+05 |\n",
      "|    critic_loss     | 4.67e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -0.694    |\n",
      "|    learning_rate   | 0.00426   |\n",
      "|    n_updates       | 444748    |\n",
      "|    reward_est_loss | -1.94e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=445000, episode_reward=4580.65 +/- 17.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.58e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 445000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.12e+05 |\n",
      "|    critic_loss     | 4.27e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | 1.08      |\n",
      "|    learning_rate   | 0.00426   |\n",
      "|    n_updates       | 444998    |\n",
      "|    reward_est_loss | -1.94e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 957       |\n",
      "|    ep_rew_mean     | 4.03e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1004      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 19178     |\n",
      "|    total_timesteps | 448750    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.08e+05 |\n",
      "|    critic_loss     | 5.04e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | 0.449     |\n",
      "|    learning_rate   | 0.00425   |\n",
      "|    n_updates       | 448748    |\n",
      "|    reward_est_loss | -1.74e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=3819.93 +/- 307.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.82e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 450000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.18e+05 |\n",
      "|    critic_loss     | 3.7e+09   |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | 2.14      |\n",
      "|    learning_rate   | 0.00425   |\n",
      "|    n_updates       | 449998    |\n",
      "|    reward_est_loss | -1.93e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 957       |\n",
      "|    ep_rew_mean     | 4.03e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1008      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 19340     |\n",
      "|    total_timesteps | 452750    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.22e+05 |\n",
      "|    critic_loss     | 3.7e+09   |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | 0.102     |\n",
      "|    learning_rate   | 0.00425   |\n",
      "|    n_updates       | 452748    |\n",
      "|    reward_est_loss | -1.93e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=455000, episode_reward=4489.30 +/- 176.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.49e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 455000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.2e+05  |\n",
      "|    critic_loss     | 3.94e+09  |\n",
      "|    ent_coef        | 108       |\n",
      "|    ent_coef_loss   | 3.67      |\n",
      "|    learning_rate   | 0.00424   |\n",
      "|    n_updates       | 454998    |\n",
      "|    reward_est_loss | -1.67e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1012      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 19467     |\n",
      "|    total_timesteps | 455868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.06e+05 |\n",
      "|    critic_loss     | 3.83e+09  |\n",
      "|    ent_coef        | 105       |\n",
      "|    ent_coef_loss   | -1.17     |\n",
      "|    learning_rate   | 0.00424   |\n",
      "|    n_updates       | 455866    |\n",
      "|    reward_est_loss | -1.9e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1016      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 19630     |\n",
      "|    total_timesteps | 459868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.23e+05 |\n",
      "|    critic_loss     | 1.06e+10  |\n",
      "|    ent_coef        | 104       |\n",
      "|    ent_coef_loss   | 0.606     |\n",
      "|    learning_rate   | 0.00423   |\n",
      "|    n_updates       | 459866    |\n",
      "|    reward_est_loss | -1.62e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=4531.28 +/- 44.15\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.53e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 460000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.24e+05 |\n",
      "|    critic_loss     | 4.66e+09  |\n",
      "|    ent_coef        | 105       |\n",
      "|    ent_coef_loss   | -0.232    |\n",
      "|    learning_rate   | 0.00423   |\n",
      "|    n_updates       | 459998    |\n",
      "|    reward_est_loss | -1.96e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1020      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 19798     |\n",
      "|    total_timesteps | 463868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.17e+05 |\n",
      "|    critic_loss     | 6.61e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 2.15      |\n",
      "|    learning_rate   | 0.00423   |\n",
      "|    n_updates       | 463866    |\n",
      "|    reward_est_loss | -1.94e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=465000, episode_reward=4515.84 +/- 13.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.52e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 465000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.19e+05 |\n",
      "|    critic_loss     | 4.23e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | -0.0991   |\n",
      "|    learning_rate   | 0.00423   |\n",
      "|    n_updates       | 464998    |\n",
      "|    reward_est_loss | -1.96e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 4e+03     |\n",
      "| time/              |           |\n",
      "|    episodes        | 1024      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 19958     |\n",
      "|    total_timesteps | 467868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.17e+05 |\n",
      "|    critic_loss     | 5.36e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -0.355    |\n",
      "|    learning_rate   | 0.00422   |\n",
      "|    n_updates       | 467866    |\n",
      "|    reward_est_loss | -1.91e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=3942.10 +/- 40.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.94e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 470000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.32e+05 |\n",
      "|    critic_loss     | 4.89e+09  |\n",
      "|    ent_coef        | 100       |\n",
      "|    ent_coef_loss   | -1.09     |\n",
      "|    learning_rate   | 0.00422   |\n",
      "|    n_updates       | 469998    |\n",
      "|    reward_est_loss | -1.89e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 4e+03     |\n",
      "| time/              |           |\n",
      "|    episodes        | 1028      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 20118     |\n",
      "|    total_timesteps | 471868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.27e+05 |\n",
      "|    critic_loss     | 3.68e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | 1.15      |\n",
      "|    learning_rate   | 0.00421   |\n",
      "|    n_updates       | 471866    |\n",
      "|    reward_est_loss | -1.59e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=475000, episode_reward=4585.20 +/- 47.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.59e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 475000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.23e+05 |\n",
      "|    critic_loss     | 5.56e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -0.713    |\n",
      "|    learning_rate   | 0.00421   |\n",
      "|    n_updates       | 474998    |\n",
      "|    reward_est_loss | -1.9e+04  |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 4.02e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1032      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 20286     |\n",
      "|    total_timesteps | 475868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.3e+05  |\n",
      "|    critic_loss     | 3.03e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -2.74     |\n",
      "|    learning_rate   | 0.00421   |\n",
      "|    n_updates       | 475866    |\n",
      "|    reward_est_loss | -8.85e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 4.03e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1036      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 20450     |\n",
      "|    total_timesteps | 479868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.33e+05 |\n",
      "|    critic_loss     | 3.57e+09  |\n",
      "|    ent_coef        | 105       |\n",
      "|    ent_coef_loss   | -2.09     |\n",
      "|    learning_rate   | 0.0042    |\n",
      "|    n_updates       | 479866    |\n",
      "|    reward_est_loss | -1.65e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=4451.94 +/- 41.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.45e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 480000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.4e+05 |\n",
      "|    critic_loss     | 4.21e+09 |\n",
      "|    ent_coef        | 108      |\n",
      "|    ent_coef_loss   | -0.135   |\n",
      "|    learning_rate   | 0.0042   |\n",
      "|    n_updates       | 479998   |\n",
      "|    reward_est_loss | 560      |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 4.04e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1040      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 20615     |\n",
      "|    total_timesteps | 483868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.37e+05 |\n",
      "|    critic_loss     | 2.42e+09  |\n",
      "|    ent_coef        | 108       |\n",
      "|    ent_coef_loss   | 1.34      |\n",
      "|    learning_rate   | 0.00419   |\n",
      "|    n_updates       | 483866    |\n",
      "|    reward_est_loss | -1.95e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=485000, episode_reward=3934.38 +/- 35.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.93e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 485000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.5e+05  |\n",
      "|    critic_loss     | 4.29e+09  |\n",
      "|    ent_coef        | 115       |\n",
      "|    ent_coef_loss   | 1.51      |\n",
      "|    learning_rate   | 0.00419   |\n",
      "|    n_updates       | 484998    |\n",
      "|    reward_est_loss | -5.69e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 4.05e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1044      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 20779     |\n",
      "|    total_timesteps | 487868    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.51e+05 |\n",
      "|    critic_loss     | 4.32e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | -0.224    |\n",
      "|    learning_rate   | 0.00419   |\n",
      "|    n_updates       | 487866    |\n",
      "|    reward_est_loss | -1.41e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=4222.87 +/- 41.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.22e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 490000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.48e+05 |\n",
      "|    critic_loss     | 5.21e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 1.52      |\n",
      "|    learning_rate   | 0.00418   |\n",
      "|    n_updates       | 489998    |\n",
      "|    reward_est_loss | 3.93e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 940       |\n",
      "|    ep_rew_mean     | 4.02e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1048      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 20913     |\n",
      "|    total_timesteps | 491134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.37e+05 |\n",
      "|    critic_loss     | 3.62e+09  |\n",
      "|    ent_coef        | 110       |\n",
      "|    ent_coef_loss   | 1.61      |\n",
      "|    learning_rate   | 0.00418   |\n",
      "|    n_updates       | 491132    |\n",
      "|    reward_est_loss | -1.88e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=495000, episode_reward=4689.16 +/- 29.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.69e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 495000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.52e+05 |\n",
      "|    critic_loss     | 2.89e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -1.24     |\n",
      "|    learning_rate   | 0.00418   |\n",
      "|    n_updates       | 494998    |\n",
      "|    reward_est_loss | -1.99e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 940       |\n",
      "|    ep_rew_mean     | 4.03e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1052      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 21079     |\n",
      "|    total_timesteps | 495134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.49e+05 |\n",
      "|    critic_loss     | 3.33e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | 2.16      |\n",
      "|    learning_rate   | 0.00417   |\n",
      "|    n_updates       | 495132    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 940       |\n",
      "|    ep_rew_mean     | 4.05e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1056      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 21245     |\n",
      "|    total_timesteps | 499134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.38e+05 |\n",
      "|    critic_loss     | 2.68e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | -2.38     |\n",
      "|    learning_rate   | 0.00417   |\n",
      "|    n_updates       | 499132    |\n",
      "|    reward_est_loss | -2e+04    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=4451.72 +/- 35.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.45e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 500000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.61e+05 |\n",
      "|    critic_loss     | 2.75e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 1.06      |\n",
      "|    learning_rate   | 0.00417   |\n",
      "|    n_updates       | 499998    |\n",
      "|    reward_est_loss | -9.83e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 940       |\n",
      "|    ep_rew_mean     | 4.06e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1060      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 21407     |\n",
      "|    total_timesteps | 503134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.59e+05 |\n",
      "|    critic_loss     | 3.38e+09  |\n",
      "|    ent_coef        | 108       |\n",
      "|    ent_coef_loss   | -0.744    |\n",
      "|    learning_rate   | 0.00416   |\n",
      "|    n_updates       | 503132    |\n",
      "|    reward_est_loss | -1.94e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=505000, episode_reward=4681.53 +/- 71.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.68e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 505000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.48e+05 |\n",
      "|    critic_loss     | 4.4e+09   |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -2.13     |\n",
      "|    learning_rate   | 0.00416   |\n",
      "|    n_updates       | 504998    |\n",
      "|    reward_est_loss | -1.89e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 4.1e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1064      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 21569     |\n",
      "|    total_timesteps | 507134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.46e+05 |\n",
      "|    critic_loss     | 3.35e+09  |\n",
      "|    ent_coef        | 105       |\n",
      "|    ent_coef_loss   | 0.461     |\n",
      "|    learning_rate   | 0.00415   |\n",
      "|    n_updates       | 507132    |\n",
      "|    reward_est_loss | -2.01e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=3841.08 +/- 101.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.84e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 510000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.55e+05 |\n",
      "|    critic_loss     | 4.04e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 1.04      |\n",
      "|    learning_rate   | 0.00415   |\n",
      "|    n_updates       | 509998    |\n",
      "|    reward_est_loss | -1.91e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 4.1e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1068      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 21732     |\n",
      "|    total_timesteps | 511134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.62e+05 |\n",
      "|    critic_loss     | 2.47e+09  |\n",
      "|    ent_coef        | 99.6      |\n",
      "|    ent_coef_loss   | -2.16     |\n",
      "|    learning_rate   | 0.00415   |\n",
      "|    n_updates       | 511132    |\n",
      "|    reward_est_loss | -1.05e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=515000, episode_reward=4124.05 +/- 32.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.12e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 515000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.6e+05  |\n",
      "|    critic_loss     | 3.55e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -1.6      |\n",
      "|    learning_rate   | 0.00414   |\n",
      "|    n_updates       | 514998    |\n",
      "|    reward_est_loss | -9.84e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 957       |\n",
      "|    ep_rew_mean     | 4.16e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1072      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 21895     |\n",
      "|    total_timesteps | 515134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.5e+05  |\n",
      "|    critic_loss     | 3.34e+09  |\n",
      "|    ent_coef        | 105       |\n",
      "|    ent_coef_loss   | 0.648     |\n",
      "|    learning_rate   | 0.00414   |\n",
      "|    n_updates       | 515132    |\n",
      "|    reward_est_loss | -1.82e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 966       |\n",
      "|    ep_rew_mean     | 4.22e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1076      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 22065     |\n",
      "|    total_timesteps | 519134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.79e+05 |\n",
      "|    critic_loss     | 2.29e+09  |\n",
      "|    ent_coef        | 111       |\n",
      "|    ent_coef_loss   | 1.61      |\n",
      "|    learning_rate   | 0.00413   |\n",
      "|    n_updates       | 519132    |\n",
      "|    reward_est_loss | -1.75e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=4722.16 +/- 11.05\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.72e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 520000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.7e+05  |\n",
      "|    critic_loss     | 3.43e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | -0.487    |\n",
      "|    learning_rate   | 0.00413   |\n",
      "|    n_updates       | 519998    |\n",
      "|    reward_est_loss | -1.57e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 966       |\n",
      "|    ep_rew_mean     | 4.23e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1080      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 22286     |\n",
      "|    total_timesteps | 523134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.68e+05 |\n",
      "|    critic_loss     | 2.47e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 0.117     |\n",
      "|    learning_rate   | 0.00413   |\n",
      "|    n_updates       | 523132    |\n",
      "|    reward_est_loss | -6.94e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=525000, episode_reward=4538.51 +/- 52.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.54e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 525000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.62e+05 |\n",
      "|    critic_loss     | 6.82e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -2.37     |\n",
      "|    learning_rate   | 0.00413   |\n",
      "|    n_updates       | 524998    |\n",
      "|    reward_est_loss | -1.8e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 966       |\n",
      "|    ep_rew_mean     | 4.24e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1084      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 22538     |\n",
      "|    total_timesteps | 527134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.69e+05 |\n",
      "|    critic_loss     | 7.14e+09  |\n",
      "|    ent_coef        | 108       |\n",
      "|    ent_coef_loss   | 1.46      |\n",
      "|    learning_rate   | 0.00412   |\n",
      "|    n_updates       | 527132    |\n",
      "|    reward_est_loss | -1.9e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=4342.04 +/- 12.71\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.34e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 530000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.69e+05 |\n",
      "|    critic_loss     | 3.35e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 0.423     |\n",
      "|    learning_rate   | 0.00412   |\n",
      "|    n_updates       | 529998    |\n",
      "|    reward_est_loss | -2.03e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 966       |\n",
      "|    ep_rew_mean     | 4.25e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1088      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 22799     |\n",
      "|    total_timesteps | 531134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.63e+05 |\n",
      "|    critic_loss     | 3.1e+09   |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | -1.08     |\n",
      "|    learning_rate   | 0.00411   |\n",
      "|    n_updates       | 531132    |\n",
      "|    reward_est_loss | -1.46e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=535000, episode_reward=4041.61 +/- 8.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.04e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 535000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.72e+05 |\n",
      "|    critic_loss     | 6.4e+09   |\n",
      "|    ent_coef        | 108       |\n",
      "|    ent_coef_loss   | 0.834     |\n",
      "|    learning_rate   | 0.00411   |\n",
      "|    n_updates       | 534998    |\n",
      "|    reward_est_loss | -1.85e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 975       |\n",
      "|    ep_rew_mean     | 4.3e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1092      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 23056     |\n",
      "|    total_timesteps | 535134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.85e+05 |\n",
      "|    critic_loss     | 3.72e+09  |\n",
      "|    ent_coef        | 110       |\n",
      "|    ent_coef_loss   | 1.95      |\n",
      "|    learning_rate   | 0.00411   |\n",
      "|    n_updates       | 535132    |\n",
      "|    reward_est_loss | -1.04e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 984       |\n",
      "|    ep_rew_mean     | 4.34e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1096      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 23314     |\n",
      "|    total_timesteps | 539134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.79e+05 |\n",
      "|    critic_loss     | 4.73e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | 0.627     |\n",
      "|    learning_rate   | 0.0041    |\n",
      "|    n_updates       | 539132    |\n",
      "|    reward_est_loss | -2.04e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=4225.38 +/- 10.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.23e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 540000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.65e+05 |\n",
      "|    critic_loss     | 4.02e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -2.38     |\n",
      "|    learning_rate   | 0.0041    |\n",
      "|    n_updates       | 539998    |\n",
      "|    reward_est_loss | -1.58e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 984       |\n",
      "|    ep_rew_mean     | 4.35e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1100      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 23572     |\n",
      "|    total_timesteps | 543134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.82e+05 |\n",
      "|    critic_loss     | 5.24e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -3.32     |\n",
      "|    learning_rate   | 0.00409   |\n",
      "|    n_updates       | 543132    |\n",
      "|    reward_est_loss | -8e+03    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=545000, episode_reward=4241.09 +/- 18.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.24e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 545000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.79e+05 |\n",
      "|    critic_loss     | 6.51e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | -0.485    |\n",
      "|    learning_rate   | 0.00409   |\n",
      "|    n_updates       | 544998    |\n",
      "|    reward_est_loss | -1.93e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 984       |\n",
      "|    ep_rew_mean     | 4.34e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1104      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 23808     |\n",
      "|    total_timesteps | 547134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.74e+05 |\n",
      "|    critic_loss     | 3.14e+09  |\n",
      "|    ent_coef        | 110       |\n",
      "|    ent_coef_loss   | -0.944    |\n",
      "|    learning_rate   | 0.00409   |\n",
      "|    n_updates       | 547132    |\n",
      "|    reward_est_loss | -1.95e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=4415.77 +/- 13.55\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.42e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 550000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.88e+05 |\n",
      "|    critic_loss     | 3.14e+09  |\n",
      "|    ent_coef        | 108       |\n",
      "|    ent_coef_loss   | 0.000538  |\n",
      "|    learning_rate   | 0.00408   |\n",
      "|    n_updates       | 549998    |\n",
      "|    reward_est_loss | -1.53e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 984       |\n",
      "|    ep_rew_mean     | 4.36e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1108      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 24062     |\n",
      "|    total_timesteps | 551134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.04e+05 |\n",
      "|    critic_loss     | 2.49e+09  |\n",
      "|    ent_coef        | 110       |\n",
      "|    ent_coef_loss   | 3.39      |\n",
      "|    learning_rate   | 0.00408   |\n",
      "|    n_updates       | 551132    |\n",
      "|    reward_est_loss | -3.22e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=555000, episode_reward=4124.06 +/- 33.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.12e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 555000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.81e+05 |\n",
      "|    critic_loss     | 6.79e+09  |\n",
      "|    ent_coef        | 108       |\n",
      "|    ent_coef_loss   | -2.14     |\n",
      "|    learning_rate   | 0.00408   |\n",
      "|    n_updates       | 554998    |\n",
      "|    reward_est_loss | -1.87e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 993       |\n",
      "|    ep_rew_mean     | 4.4e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1112      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 24316     |\n",
      "|    total_timesteps | 555134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.89e+05 |\n",
      "|    critic_loss     | 4.88e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | -0.514    |\n",
      "|    learning_rate   | 0.00407   |\n",
      "|    n_updates       | 555132    |\n",
      "|    reward_est_loss | -1.81e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 993       |\n",
      "|    ep_rew_mean     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1116      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 24569     |\n",
      "|    total_timesteps | 559134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.85e+05 |\n",
      "|    critic_loss     | 3.41e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | 1.15      |\n",
      "|    learning_rate   | 0.00407   |\n",
      "|    n_updates       | 559132    |\n",
      "|    reward_est_loss | -2.05e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=4260.93 +/- 57.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.26e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 560000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.97e+05 |\n",
      "|    critic_loss     | 2.62e+09  |\n",
      "|    ent_coef        | 112       |\n",
      "|    ent_coef_loss   | -1.84     |\n",
      "|    learning_rate   | 0.00407   |\n",
      "|    n_updates       | 559998    |\n",
      "|    reward_est_loss | -1.87e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 993       |\n",
      "|    ep_rew_mean     | 4.42e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1120      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 24803     |\n",
      "|    total_timesteps | 563134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.83e+05 |\n",
      "|    critic_loss     | 4.2e+09   |\n",
      "|    ent_coef        | 111       |\n",
      "|    ent_coef_loss   | 1.14      |\n",
      "|    learning_rate   | 0.00406   |\n",
      "|    n_updates       | 563132    |\n",
      "|    reward_est_loss | -1.97e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=565000, episode_reward=4494.52 +/- 24.68\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.49e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 565000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.73e+05 |\n",
      "|    critic_loss     | 5.31e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -0.197    |\n",
      "|    learning_rate   | 0.00406   |\n",
      "|    n_updates       | 564998    |\n",
      "|    reward_est_loss | 2.18e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 993       |\n",
      "|    ep_rew_mean     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1124      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 25014     |\n",
      "|    total_timesteps | 567134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.97e+05 |\n",
      "|    critic_loss     | 3.92e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 0.677     |\n",
      "|    learning_rate   | 0.00405   |\n",
      "|    n_updates       | 567132    |\n",
      "|    reward_est_loss | -7.39e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=4494.49 +/- 17.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.49e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 570000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.85e+05 |\n",
      "|    critic_loss     | 4.43e+09  |\n",
      "|    ent_coef        | 111       |\n",
      "|    ent_coef_loss   | 1.1       |\n",
      "|    learning_rate   | 0.00405   |\n",
      "|    n_updates       | 569998    |\n",
      "|    reward_est_loss | -1.94e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 993       |\n",
      "|    ep_rew_mean     | 4.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1128      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 25217     |\n",
      "|    total_timesteps | 571134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.92e+05 |\n",
      "|    critic_loss     | 4.47e+09  |\n",
      "|    ent_coef        | 113       |\n",
      "|    ent_coef_loss   | 0.793     |\n",
      "|    learning_rate   | 0.00405   |\n",
      "|    n_updates       | 571132    |\n",
      "|    reward_est_loss | -1.1e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=575000, episode_reward=4613.93 +/- 11.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.61e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 575000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.01e+05 |\n",
      "|    critic_loss     | 2.15e+09  |\n",
      "|    ent_coef        | 114       |\n",
      "|    ent_coef_loss   | 1.51      |\n",
      "|    learning_rate   | 0.00404   |\n",
      "|    n_updates       | 574998    |\n",
      "|    reward_est_loss | -1.01e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 993       |\n",
      "|    ep_rew_mean     | 4.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1132      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 25388     |\n",
      "|    total_timesteps | 575134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.91e+05 |\n",
      "|    critic_loss     | 4.42e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 0.122     |\n",
      "|    learning_rate   | 0.00404   |\n",
      "|    n_updates       | 575132    |\n",
      "|    reward_est_loss | -1.4e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 993       |\n",
      "|    ep_rew_mean     | 4.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1136      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 25556     |\n",
      "|    total_timesteps | 579134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.98e+05 |\n",
      "|    critic_loss     | 4.23e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 2.67      |\n",
      "|    learning_rate   | 0.00403   |\n",
      "|    n_updates       | 579132    |\n",
      "|    reward_est_loss | -1.34e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=4341.77 +/- 14.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.34e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 580000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.88e+05 |\n",
      "|    critic_loss     | 4.32e+09  |\n",
      "|    ent_coef        | 112       |\n",
      "|    ent_coef_loss   | -3.48     |\n",
      "|    learning_rate   | 0.00403   |\n",
      "|    n_updates       | 579998    |\n",
      "|    reward_est_loss | -2.01e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 993       |\n",
      "|    ep_rew_mean     | 4.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1140      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 25731     |\n",
      "|    total_timesteps | 583134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.04e+05 |\n",
      "|    critic_loss     | 2.97e+09  |\n",
      "|    ent_coef        | 111       |\n",
      "|    ent_coef_loss   | 2.64      |\n",
      "|    learning_rate   | 0.00403   |\n",
      "|    n_updates       | 583132    |\n",
      "|    reward_est_loss | -2e+04    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=585000, episode_reward=4488.31 +/- 11.21\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.49e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 585000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.91e+05 |\n",
      "|    critic_loss     | 5.35e+09  |\n",
      "|    ent_coef        | 111       |\n",
      "|    ent_coef_loss   | 1.95      |\n",
      "|    learning_rate   | 0.00403   |\n",
      "|    n_updates       | 584998    |\n",
      "|    reward_est_loss | -2.09e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 993       |\n",
      "|    ep_rew_mean     | 4.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1144      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 25903     |\n",
      "|    total_timesteps | 587134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.04e+05 |\n",
      "|    critic_loss     | 3.21e+09  |\n",
      "|    ent_coef        | 107       |\n",
      "|    ent_coef_loss   | -0.114    |\n",
      "|    learning_rate   | 0.00402   |\n",
      "|    n_updates       | 587132    |\n",
      "|    reward_est_loss | -2.04e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=4277.35 +/- 48.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.28e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 590000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.01e+05 |\n",
      "|    critic_loss     | 3.46e+09  |\n",
      "|    ent_coef        | 111       |\n",
      "|    ent_coef_loss   | -1.03     |\n",
      "|    learning_rate   | 0.00402   |\n",
      "|    n_updates       | 589998    |\n",
      "|    reward_est_loss | -1.42e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.48e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1148      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 26074     |\n",
      "|    total_timesteps | 591134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -4.99e+05 |\n",
      "|    critic_loss     | 3.83e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 0.645     |\n",
      "|    learning_rate   | 0.00401   |\n",
      "|    n_updates       | 591132    |\n",
      "|    reward_est_loss | -1.86e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=595000, episode_reward=4156.08 +/- 16.97\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.16e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 595000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.06e+05 |\n",
      "|    critic_loss     | 3.96e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | -1.03     |\n",
      "|    learning_rate   | 0.00401   |\n",
      "|    n_updates       | 594998    |\n",
      "|    reward_est_loss | -1.62e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.48e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1152      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 26240     |\n",
      "|    total_timesteps | 595134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.17e+05 |\n",
      "|    critic_loss     | 2.94e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | -1.38     |\n",
      "|    learning_rate   | 0.00401   |\n",
      "|    n_updates       | 595132    |\n",
      "|    reward_est_loss | -1.77e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.46e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1156      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 26401     |\n",
      "|    total_timesteps | 599134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.08e+05 |\n",
      "|    critic_loss     | 4e+09     |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -1.74     |\n",
      "|    learning_rate   | 0.004     |\n",
      "|    n_updates       | 599132    |\n",
      "|    reward_est_loss | -1.97e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=4318.25 +/- 76.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.32e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 600000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.13e+05 |\n",
      "|    critic_loss     | 3.3e+09   |\n",
      "|    ent_coef        | 100       |\n",
      "|    ent_coef_loss   | -0.166    |\n",
      "|    learning_rate   | 0.004     |\n",
      "|    n_updates       | 599998    |\n",
      "|    reward_est_loss | -2.11e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1160      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 26564     |\n",
      "|    total_timesteps | 603134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.1e+05  |\n",
      "|    critic_loss     | 2.69e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | 1.65      |\n",
      "|    learning_rate   | 0.00399   |\n",
      "|    n_updates       | 603132    |\n",
      "|    reward_est_loss | -2.11e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=605000, episode_reward=4370.56 +/- 12.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.37e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 605000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.17e+05 |\n",
      "|    critic_loss     | 5.81e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -1.33     |\n",
      "|    learning_rate   | 0.00399   |\n",
      "|    n_updates       | 604998    |\n",
      "|    reward_est_loss | -2.04e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1164      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 26728     |\n",
      "|    total_timesteps | 607134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.06e+05 |\n",
      "|    critic_loss     | 4.17e+09  |\n",
      "|    ent_coef        | 104       |\n",
      "|    ent_coef_loss   | 0.391     |\n",
      "|    learning_rate   | 0.00399   |\n",
      "|    n_updates       | 607132    |\n",
      "|    reward_est_loss | -1.93e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=4371.46 +/- 24.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.37e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 610000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.15e+05 |\n",
      "|    critic_loss     | 5.09e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -0.809    |\n",
      "|    learning_rate   | 0.00398   |\n",
      "|    n_updates       | 609998    |\n",
      "|    reward_est_loss | -2.11e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1168      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 26892     |\n",
      "|    total_timesteps | 611134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.05e+05 |\n",
      "|    critic_loss     | 4.05e+09  |\n",
      "|    ent_coef        | 99.9      |\n",
      "|    ent_coef_loss   | -4.24     |\n",
      "|    learning_rate   | 0.00398   |\n",
      "|    n_updates       | 611132    |\n",
      "|    reward_est_loss | -1.96e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=615000, episode_reward=3816.81 +/- 28.86\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.82e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 615000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.16e+05 |\n",
      "|    critic_loss     | 3.54e+09  |\n",
      "|    ent_coef        | 108       |\n",
      "|    ent_coef_loss   | 1.76      |\n",
      "|    learning_rate   | 0.00398   |\n",
      "|    n_updates       | 614998    |\n",
      "|    reward_est_loss | -2.14e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1172      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 27056     |\n",
      "|    total_timesteps | 615134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5e+05    |\n",
      "|    critic_loss     | 5.14e+09  |\n",
      "|    ent_coef        | 113       |\n",
      "|    ent_coef_loss   | -0.224    |\n",
      "|    learning_rate   | 0.00397   |\n",
      "|    n_updates       | 615132    |\n",
      "|    reward_est_loss | -1.53e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1176      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 27218     |\n",
      "|    total_timesteps | 619134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.13e+05 |\n",
      "|    critic_loss     | 3.36e+09  |\n",
      "|    ent_coef        | 108       |\n",
      "|    ent_coef_loss   | -1.59     |\n",
      "|    learning_rate   | 0.00397   |\n",
      "|    n_updates       | 619132    |\n",
      "|    reward_est_loss | -1.65e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=4245.59 +/- 17.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.25e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 620000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.08e+05 |\n",
      "|    critic_loss     | 5.86e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | -0.78     |\n",
      "|    learning_rate   | 0.00397   |\n",
      "|    n_updates       | 619998    |\n",
      "|    reward_est_loss | -2.06e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.42e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1180      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 27383     |\n",
      "|    total_timesteps | 623134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.19e+05 |\n",
      "|    critic_loss     | 4.64e+09  |\n",
      "|    ent_coef        | 105       |\n",
      "|    ent_coef_loss   | -0.078    |\n",
      "|    learning_rate   | 0.00396   |\n",
      "|    n_updates       | 623132    |\n",
      "|    reward_est_loss | -2.14e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=625000, episode_reward=3826.44 +/- 26.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.83e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 625000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.32e+05 |\n",
      "|    critic_loss     | 2.64e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 2.44      |\n",
      "|    learning_rate   | 0.00396   |\n",
      "|    n_updates       | 624998    |\n",
      "|    reward_est_loss | -2.15e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.42e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1184      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 27546     |\n",
      "|    total_timesteps | 627134    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.09e+05 |\n",
      "|    critic_loss     | 4.89e+09  |\n",
      "|    ent_coef        | 105       |\n",
      "|    ent_coef_loss   | -3.34     |\n",
      "|    learning_rate   | 0.00395   |\n",
      "|    n_updates       | 627132    |\n",
      "|    reward_est_loss | -2.13e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=1928.11 +/- 1278.72\n",
      "Episode length: 876.60 +/- 246.80\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 877       |\n",
      "|    mean_reward     | 1.93e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 630000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.17e+05 |\n",
      "|    critic_loss     | 4.12e+09  |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | 0.913     |\n",
      "|    learning_rate   | 0.00395   |\n",
      "|    n_updates       | 629998    |\n",
      "|    reward_est_loss | -1.25e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 999       |\n",
      "|    ep_rew_mean     | 4.33e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1188      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 27703     |\n",
      "|    total_timesteps | 631023    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.18e+05 |\n",
      "|    critic_loss     | 3.56e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | -1.16     |\n",
      "|    learning_rate   | 0.00395   |\n",
      "|    n_updates       | 631021    |\n",
      "|    reward_est_loss | -2.1e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 988       |\n",
      "|    ep_rew_mean     | 4.2e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1192      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 27822     |\n",
      "|    total_timesteps | 633968    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.14e+05 |\n",
      "|    critic_loss     | 5.39e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | 0.335     |\n",
      "|    learning_rate   | 0.00394   |\n",
      "|    n_updates       | 633966    |\n",
      "|    reward_est_loss | -8.67e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=635000, episode_reward=921.88 +/- 1528.36\n",
      "Episode length: 576.40 +/- 265.42\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 576       |\n",
      "|    mean_reward     | 922       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 635000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.27e+05 |\n",
      "|    critic_loss     | 3.22e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -0.654    |\n",
      "|    learning_rate   | 0.00394   |\n",
      "|    n_updates       | 634998    |\n",
      "|    reward_est_loss | -1.63e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 975       |\n",
      "|    ep_rew_mean     | 4.04e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1196      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 27932     |\n",
      "|    total_timesteps | 636676    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.11e+05 |\n",
      "|    critic_loss     | 4.15e+09  |\n",
      "|    ent_coef        | 99.7      |\n",
      "|    ent_coef_loss   | -2.66     |\n",
      "|    learning_rate   | 0.00394   |\n",
      "|    n_updates       | 636674    |\n",
      "|    reward_est_loss | -8e+03    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=4111.94 +/- 21.14\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.11e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 640000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.17e+05 |\n",
      "|    critic_loss     | 6e+09     |\n",
      "|    ent_coef        | 109       |\n",
      "|    ent_coef_loss   | -1.21     |\n",
      "|    learning_rate   | 0.00393   |\n",
      "|    n_updates       | 639998    |\n",
      "|    reward_est_loss | -2.16e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 3.97e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1200      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 28072     |\n",
      "|    total_timesteps | 640138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.03e+05 |\n",
      "|    critic_loss     | 9.64e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -3.56     |\n",
      "|    learning_rate   | 0.00393   |\n",
      "|    n_updates       | 640136    |\n",
      "|    reward_est_loss | -1.88e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 3.97e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1204      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 28233     |\n",
      "|    total_timesteps | 644138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.22e+05 |\n",
      "|    critic_loss     | 4.63e+09  |\n",
      "|    ent_coef        | 104       |\n",
      "|    ent_coef_loss   | 1.31      |\n",
      "|    learning_rate   | 0.00393   |\n",
      "|    n_updates       | 644136    |\n",
      "|    reward_est_loss | -1.18e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=645000, episode_reward=4526.42 +/- 23.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.53e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 645000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.24e+05 |\n",
      "|    critic_loss     | 3.59e+09  |\n",
      "|    ent_coef        | 96.4      |\n",
      "|    ent_coef_loss   | -0.535    |\n",
      "|    learning_rate   | 0.00393   |\n",
      "|    n_updates       | 644998    |\n",
      "|    reward_est_loss | -1.21e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 3.98e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1208      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 28394     |\n",
      "|    total_timesteps | 648138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.21e+05 |\n",
      "|    critic_loss     | 5.13e+09  |\n",
      "|    ent_coef        | 97.4      |\n",
      "|    ent_coef_loss   | 3.05      |\n",
      "|    learning_rate   | 0.00392   |\n",
      "|    n_updates       | 648136    |\n",
      "|    reward_est_loss | -6.37e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=4500.88 +/- 38.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.5e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 650000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.13e+05 |\n",
      "|    critic_loss     | 4.39e+09  |\n",
      "|    ent_coef        | 90.5      |\n",
      "|    ent_coef_loss   | 0.609     |\n",
      "|    learning_rate   | 0.00392   |\n",
      "|    n_updates       | 649998    |\n",
      "|    reward_est_loss | -2.2e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 3.98e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1212      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 28556     |\n",
      "|    total_timesteps | 652138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.14e+05 |\n",
      "|    critic_loss     | 9.12e+09  |\n",
      "|    ent_coef        | 97.1      |\n",
      "|    ent_coef_loss   | -4.3      |\n",
      "|    learning_rate   | 0.00391   |\n",
      "|    n_updates       | 652136    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=655000, episode_reward=4719.65 +/- 19.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.72e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 655000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.01e+05 |\n",
      "|    critic_loss     | 5.68e+09  |\n",
      "|    ent_coef        | 99        |\n",
      "|    ent_coef_loss   | -0.715    |\n",
      "|    learning_rate   | 0.00391   |\n",
      "|    n_updates       | 654998    |\n",
      "|    reward_est_loss | -2.1e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 3.98e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1216      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 28718     |\n",
      "|    total_timesteps | 656138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.21e+05 |\n",
      "|    critic_loss     | 5.19e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | 0.379     |\n",
      "|    learning_rate   | 0.00391   |\n",
      "|    n_updates       | 656136    |\n",
      "|    reward_est_loss | -1.64e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=4451.64 +/- 59.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.45e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 660000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.13e+05 |\n",
      "|    critic_loss     | 4.49e+09  |\n",
      "|    ent_coef        | 98.9      |\n",
      "|    ent_coef_loss   | 0.0445    |\n",
      "|    learning_rate   | 0.0039    |\n",
      "|    n_updates       | 659998    |\n",
      "|    reward_est_loss | -1.27e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1220      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 28881     |\n",
      "|    total_timesteps | 660138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.12e+05 |\n",
      "|    critic_loss     | 3.41e+09  |\n",
      "|    ent_coef        | 105       |\n",
      "|    ent_coef_loss   | 1.9       |\n",
      "|    learning_rate   | 0.0039    |\n",
      "|    n_updates       | 660136    |\n",
      "|    reward_est_loss | -1.68e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1224      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 29041     |\n",
      "|    total_timesteps | 664138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.16e+05 |\n",
      "|    critic_loss     | 6.29e+09  |\n",
      "|    ent_coef        | 100       |\n",
      "|    ent_coef_loss   | -1.53     |\n",
      "|    learning_rate   | 0.00389   |\n",
      "|    n_updates       | 664136    |\n",
      "|    reward_est_loss | -9.87e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=665000, episode_reward=4493.32 +/- 27.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.49e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 665000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.14e+05 |\n",
      "|    critic_loss     | 5.63e+09  |\n",
      "|    ent_coef        | 98.9      |\n",
      "|    ent_coef_loss   | -0.578    |\n",
      "|    learning_rate   | 0.00389   |\n",
      "|    n_updates       | 664998    |\n",
      "|    reward_est_loss | -2.15e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1228      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 29202     |\n",
      "|    total_timesteps | 668138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.19e+05 |\n",
      "|    critic_loss     | 3.56e+09  |\n",
      "|    ent_coef        | 95.1      |\n",
      "|    ent_coef_loss   | -2.07     |\n",
      "|    learning_rate   | 0.00389   |\n",
      "|    n_updates       | 668136    |\n",
      "|    reward_est_loss | -2.16e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=4822.49 +/- 45.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.82e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 670000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.14e+05 |\n",
      "|    critic_loss     | 5.66e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | -2.67     |\n",
      "|    learning_rate   | 0.00388   |\n",
      "|    n_updates       | 669998    |\n",
      "|    reward_est_loss | -1.96e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 3.99e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1232      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 29367     |\n",
      "|    total_timesteps | 672138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.23e+05 |\n",
      "|    critic_loss     | 3.25e+09  |\n",
      "|    ent_coef        | 99.6      |\n",
      "|    ent_coef_loss   | 1.37      |\n",
      "|    learning_rate   | 0.00388   |\n",
      "|    n_updates       | 672136    |\n",
      "|    reward_est_loss | -6.51e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=675000, episode_reward=997.24 +/- 0.61\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 997       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 675000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.14e+05 |\n",
      "|    critic_loss     | 3.9e+09   |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | -0.543    |\n",
      "|    learning_rate   | 0.00388   |\n",
      "|    n_updates       | 674998    |\n",
      "|    reward_est_loss | -2.19e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.01e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1236      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 29528     |\n",
      "|    total_timesteps | 676138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.21e+05 |\n",
      "|    critic_loss     | 2.82e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -1.26     |\n",
      "|    learning_rate   | 0.00387   |\n",
      "|    n_updates       | 676136    |\n",
      "|    reward_est_loss | -1.86e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=4187.96 +/- 37.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.19e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 680000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.28e+05 |\n",
      "|    critic_loss     | 2.53e+09  |\n",
      "|    ent_coef        | 96.5      |\n",
      "|    ent_coef_loss   | 0.776     |\n",
      "|    learning_rate   | 0.00387   |\n",
      "|    n_updates       | 679998    |\n",
      "|    reward_est_loss | -2.23e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.02e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1240      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 29690     |\n",
      "|    total_timesteps | 680138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.31e+05 |\n",
      "|    critic_loss     | 3.36e+09  |\n",
      "|    ent_coef        | 93.7      |\n",
      "|    ent_coef_loss   | 1.83      |\n",
      "|    learning_rate   | 0.00387   |\n",
      "|    n_updates       | 680136    |\n",
      "|    reward_est_loss | -1.9e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.03e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1244      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 29850     |\n",
      "|    total_timesteps | 684138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.24e+05 |\n",
      "|    critic_loss     | 9.67e+09  |\n",
      "|    ent_coef        | 92        |\n",
      "|    ent_coef_loss   | -0.163    |\n",
      "|    learning_rate   | 0.00386   |\n",
      "|    n_updates       | 684136    |\n",
      "|    reward_est_loss | -1.34e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=685000, episode_reward=4428.62 +/- 22.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 685000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.25e+05 |\n",
      "|    critic_loss     | 3.23e+09  |\n",
      "|    ent_coef        | 96.5      |\n",
      "|    ent_coef_loss   | -6.92     |\n",
      "|    learning_rate   | 0.00386   |\n",
      "|    n_updates       | 684998    |\n",
      "|    reward_est_loss | -2.1e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.04e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1248      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 30011     |\n",
      "|    total_timesteps | 688138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.24e+05 |\n",
      "|    critic_loss     | 6.01e+09  |\n",
      "|    ent_coef        | 104       |\n",
      "|    ent_coef_loss   | -4.26     |\n",
      "|    learning_rate   | 0.00385   |\n",
      "|    n_updates       | 688136    |\n",
      "|    reward_est_loss | -2.02e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=4315.47 +/- 24.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.32e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 690000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.26e+05 |\n",
      "|    critic_loss     | 4.91e+09  |\n",
      "|    ent_coef        | 99.2      |\n",
      "|    ent_coef_loss   | 1.63      |\n",
      "|    learning_rate   | 0.00385   |\n",
      "|    n_updates       | 689998    |\n",
      "|    reward_est_loss | -1.88e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.03e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1252      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 30174     |\n",
      "|    total_timesteps | 692138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.31e+05 |\n",
      "|    critic_loss     | 3.12e+09  |\n",
      "|    ent_coef        | 96.3      |\n",
      "|    ent_coef_loss   | -0.11     |\n",
      "|    learning_rate   | 0.00385   |\n",
      "|    n_updates       | 692136    |\n",
      "|    reward_est_loss | -2.24e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=695000, episode_reward=4509.10 +/- 51.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.51e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 695000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.09e+05 |\n",
      "|    critic_loss     | 4.19e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | -4.39     |\n",
      "|    learning_rate   | 0.00384   |\n",
      "|    n_updates       | 694998    |\n",
      "|    reward_est_loss | -2.25e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.04e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1256      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 30340     |\n",
      "|    total_timesteps | 696138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.31e+05 |\n",
      "|    critic_loss     | 9.91e+09  |\n",
      "|    ent_coef        | 101       |\n",
      "|    ent_coef_loss   | 2.7       |\n",
      "|    learning_rate   | 0.00384   |\n",
      "|    n_updates       | 696136    |\n",
      "|    reward_est_loss | -2.18e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=4323.46 +/- 51.64\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.32e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 700000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.22e+05 |\n",
      "|    critic_loss     | 4.81e+09  |\n",
      "|    ent_coef        | 96        |\n",
      "|    ent_coef_loss   | -4.63     |\n",
      "|    learning_rate   | 0.00383   |\n",
      "|    n_updates       | 699998    |\n",
      "|    reward_est_loss | -2.11e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.06e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1260      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 30505     |\n",
      "|    total_timesteps | 700138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.24e+05 |\n",
      "|    critic_loss     | 3.67e+09  |\n",
      "|    ent_coef        | 106       |\n",
      "|    ent_coef_loss   | -1.35     |\n",
      "|    learning_rate   | 0.00383   |\n",
      "|    n_updates       | 700136    |\n",
      "|    reward_est_loss | -9.19e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.06e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1264      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 30669     |\n",
      "|    total_timesteps | 704138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.28e+05 |\n",
      "|    critic_loss     | 3.9e+09   |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | 1.39      |\n",
      "|    learning_rate   | 0.00383   |\n",
      "|    n_updates       | 704136    |\n",
      "|    reward_est_loss | -2.02e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=705000, episode_reward=4766.05 +/- 10.57\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.77e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 705000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.25e+05 |\n",
      "|    critic_loss     | 2.15e+09  |\n",
      "|    ent_coef        | 98.3      |\n",
      "|    ent_coef_loss   | 2.33      |\n",
      "|    learning_rate   | 0.00383   |\n",
      "|    n_updates       | 704998    |\n",
      "|    reward_est_loss | -2.19e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.07e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1268      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 30834     |\n",
      "|    total_timesteps | 708138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.2e+05  |\n",
      "|    critic_loss     | 3.67e+09  |\n",
      "|    ent_coef        | 98.4      |\n",
      "|    ent_coef_loss   | -0.74     |\n",
      "|    learning_rate   | 0.00382   |\n",
      "|    n_updates       | 708136    |\n",
      "|    reward_est_loss | -1.25e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=2431.24 +/- 1185.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 2.43e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 710000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.33e+05 |\n",
      "|    critic_loss     | 3.19e+09  |\n",
      "|    ent_coef        | 103       |\n",
      "|    ent_coef_loss   | 2.97      |\n",
      "|    learning_rate   | 0.00382   |\n",
      "|    n_updates       | 709998    |\n",
      "|    reward_est_loss | -1.88e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.07e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1272      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 30998     |\n",
      "|    total_timesteps | 712138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.32e+05 |\n",
      "|    critic_loss     | 1.02e+11  |\n",
      "|    ent_coef        | 90.4      |\n",
      "|    ent_coef_loss   | -1.61     |\n",
      "|    learning_rate   | 0.00381   |\n",
      "|    n_updates       | 712136    |\n",
      "|    reward_est_loss | -7.71e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=715000, episode_reward=4593.03 +/- 35.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.59e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 715000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.22e+05 |\n",
      "|    critic_loss     | 2.72e+09  |\n",
      "|    ent_coef        | 95.2      |\n",
      "|    ent_coef_loss   | 1.84      |\n",
      "|    learning_rate   | 0.00381   |\n",
      "|    n_updates       | 714998    |\n",
      "|    reward_est_loss | -2e+04    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.08e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1276      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 31164     |\n",
      "|    total_timesteps | 716138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.21e+05 |\n",
      "|    critic_loss     | 3.09e+09  |\n",
      "|    ent_coef        | 96.4      |\n",
      "|    ent_coef_loss   | 0.921     |\n",
      "|    learning_rate   | 0.00381   |\n",
      "|    n_updates       | 716136    |\n",
      "|    reward_est_loss | -2.14e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=4456.39 +/- 9.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.46e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 720000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.19e+05 |\n",
      "|    critic_loss     | 4.34e+09  |\n",
      "|    ent_coef        | 98.3      |\n",
      "|    ent_coef_loss   | 0.991     |\n",
      "|    learning_rate   | 0.0038    |\n",
      "|    n_updates       | 719998    |\n",
      "|    reward_est_loss | -2.26e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.09e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1280      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 31329     |\n",
      "|    total_timesteps | 720138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.09e+05 |\n",
      "|    critic_loss     | 4.64e+09  |\n",
      "|    ent_coef        | 89.3      |\n",
      "|    ent_coef_loss   | 0.233     |\n",
      "|    learning_rate   | 0.0038    |\n",
      "|    n_updates       | 720136    |\n",
      "|    reward_est_loss | -2.18e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 970       |\n",
      "|    ep_rew_mean     | 4.11e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1284      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 31493     |\n",
      "|    total_timesteps | 724138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.08e+05 |\n",
      "|    critic_loss     | 3.15e+09  |\n",
      "|    ent_coef        | 95.9      |\n",
      "|    ent_coef_loss   | -0.599    |\n",
      "|    learning_rate   | 0.00379   |\n",
      "|    n_updates       | 724136    |\n",
      "|    reward_est_loss | -1.97e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=725000, episode_reward=4725.69 +/- 39.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 725000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.3e+05  |\n",
      "|    critic_loss     | 4.03e+09  |\n",
      "|    ent_coef        | 98.1      |\n",
      "|    ent_coef_loss   | -0.0634   |\n",
      "|    learning_rate   | 0.00379   |\n",
      "|    n_updates       | 724998    |\n",
      "|    reward_est_loss | -2.31e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 971       |\n",
      "|    ep_rew_mean     | 4.2e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1288      |\n",
      "|    fps             | 22        |\n",
      "|    time_elapsed    | 31658     |\n",
      "|    total_timesteps | 728138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.35e+05 |\n",
      "|    critic_loss     | 3.62e+09  |\n",
      "|    ent_coef        | 94        |\n",
      "|    ent_coef_loss   | 1.31      |\n",
      "|    learning_rate   | 0.00379   |\n",
      "|    n_updates       | 728136    |\n",
      "|    reward_est_loss | -1.23e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=4559.81 +/- 42.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.56e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 730000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.23e+05 |\n",
      "|    critic_loss     | 4.63e+09  |\n",
      "|    ent_coef        | 94.5      |\n",
      "|    ent_coef_loss   | 0.952     |\n",
      "|    learning_rate   | 0.00378   |\n",
      "|    n_updates       | 729998    |\n",
      "|    reward_est_loss | -2.28e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 982       |\n",
      "|    ep_rew_mean     | 4.34e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1292      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 31823     |\n",
      "|    total_timesteps | 732138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.31e+05 |\n",
      "|    critic_loss     | 4.1e+09   |\n",
      "|    ent_coef        | 95.4      |\n",
      "|    ent_coef_loss   | -0.755    |\n",
      "|    learning_rate   | 0.00378   |\n",
      "|    n_updates       | 732136    |\n",
      "|    reward_est_loss | -1.73e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=735000, episode_reward=4589.48 +/- 20.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.59e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 735000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.4e+05  |\n",
      "|    critic_loss     | 4.85e+09  |\n",
      "|    ent_coef        | 95.3      |\n",
      "|    ent_coef_loss   | 3.85      |\n",
      "|    learning_rate   | 0.00378   |\n",
      "|    n_updates       | 734998    |\n",
      "|    reward_est_loss | -1.34e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 995       |\n",
      "|    ep_rew_mean     | 4.51e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1296      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 31988     |\n",
      "|    total_timesteps | 736138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.28e+05 |\n",
      "|    critic_loss     | 2.56e+09  |\n",
      "|    ent_coef        | 97.7      |\n",
      "|    ent_coef_loss   | 0.526     |\n",
      "|    learning_rate   | 0.00377   |\n",
      "|    n_updates       | 736136    |\n",
      "|    reward_est_loss | -1.9e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=4615.11 +/- 22.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.62e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 740000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.28e+05 |\n",
      "|    critic_loss     | 3.94e+09  |\n",
      "|    ent_coef        | 93.4      |\n",
      "|    ent_coef_loss   | 1.17      |\n",
      "|    learning_rate   | 0.00377   |\n",
      "|    n_updates       | 739998    |\n",
      "|    reward_est_loss | -1.83e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.59e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1300      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 32153     |\n",
      "|    total_timesteps | 740138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.24e+05 |\n",
      "|    critic_loss     | 3.19e+09  |\n",
      "|    ent_coef        | 102       |\n",
      "|    ent_coef_loss   | -0.651    |\n",
      "|    learning_rate   | 0.00377   |\n",
      "|    n_updates       | 740136    |\n",
      "|    reward_est_loss | -1.21e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.6e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1304      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 32318     |\n",
      "|    total_timesteps | 744138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.28e+05 |\n",
      "|    critic_loss     | 5.35e+09  |\n",
      "|    ent_coef        | 91.9      |\n",
      "|    ent_coef_loss   | 0.669     |\n",
      "|    learning_rate   | 0.00376   |\n",
      "|    n_updates       | 744136    |\n",
      "|    reward_est_loss | -1.82e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=745000, episode_reward=4739.63 +/- 61.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 745000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.36e+05 |\n",
      "|    critic_loss     | 1e+10     |\n",
      "|    ent_coef        | 94.2      |\n",
      "|    ent_coef_loss   | -1.72     |\n",
      "|    learning_rate   | 0.00376   |\n",
      "|    n_updates       | 744998    |\n",
      "|    reward_est_loss | -1.96e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.6e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1308      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 32483     |\n",
      "|    total_timesteps | 748138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.4e+05  |\n",
      "|    critic_loss     | 2.87e+09  |\n",
      "|    ent_coef        | 96.3      |\n",
      "|    ent_coef_loss   | 1.82      |\n",
      "|    learning_rate   | 0.00375   |\n",
      "|    n_updates       | 748136    |\n",
      "|    reward_est_loss | -5.53e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=4397.42 +/- 25.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.4e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 750000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.32e+05 |\n",
      "|    critic_loss     | 5.89e+09  |\n",
      "|    ent_coef        | 92.3      |\n",
      "|    ent_coef_loss   | 0.422     |\n",
      "|    learning_rate   | 0.00375   |\n",
      "|    n_updates       | 749998    |\n",
      "|    reward_est_loss | -1.84e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.61e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1312      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 32648     |\n",
      "|    total_timesteps | 752138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.24e+05 |\n",
      "|    critic_loss     | 3.96e+09  |\n",
      "|    ent_coef        | 99.4      |\n",
      "|    ent_coef_loss   | -1.03     |\n",
      "|    learning_rate   | 0.00375   |\n",
      "|    n_updates       | 752136    |\n",
      "|    reward_est_loss | -2.04e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=755000, episode_reward=4927.12 +/- 30.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.93e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 755000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.24e+05 |\n",
      "|    critic_loss     | 2.63e+09  |\n",
      "|    ent_coef        | 99        |\n",
      "|    ent_coef_loss   | -0.507    |\n",
      "|    learning_rate   | 0.00374   |\n",
      "|    n_updates       | 754998    |\n",
      "|    reward_est_loss | -2.01e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.62e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1316      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 32816     |\n",
      "|    total_timesteps | 756138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.37e+05 |\n",
      "|    critic_loss     | 4.67e+09  |\n",
      "|    ent_coef        | 93.7      |\n",
      "|    ent_coef_loss   | -2.62     |\n",
      "|    learning_rate   | 0.00374   |\n",
      "|    n_updates       | 756136    |\n",
      "|    reward_est_loss | -1.26e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=4533.92 +/- 71.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.53e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 760000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.32e+05 |\n",
      "|    critic_loss     | 3.23e+09  |\n",
      "|    ent_coef        | 96.5      |\n",
      "|    ent_coef_loss   | -0.141    |\n",
      "|    learning_rate   | 0.00373   |\n",
      "|    n_updates       | 759998    |\n",
      "|    reward_est_loss | -1.29e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.63e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1320      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 32981     |\n",
      "|    total_timesteps | 760138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.39e+05 |\n",
      "|    critic_loss     | 2.52e+09  |\n",
      "|    ent_coef        | 95        |\n",
      "|    ent_coef_loss   | -2.47     |\n",
      "|    learning_rate   | 0.00373   |\n",
      "|    n_updates       | 760136    |\n",
      "|    reward_est_loss | -2.21e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.63e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1324      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 33145     |\n",
      "|    total_timesteps | 764138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.23e+05 |\n",
      "|    critic_loss     | 3.55e+09  |\n",
      "|    ent_coef        | 91.3      |\n",
      "|    ent_coef_loss   | -0.536    |\n",
      "|    learning_rate   | 0.00373   |\n",
      "|    n_updates       | 764136    |\n",
      "|    reward_est_loss | -2.21e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=765000, episode_reward=4811.76 +/- 38.45\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.81e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 765000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.32e+05 |\n",
      "|    critic_loss     | 3.64e+09  |\n",
      "|    ent_coef        | 85.9      |\n",
      "|    ent_coef_loss   | -2.93     |\n",
      "|    learning_rate   | 0.00373   |\n",
      "|    n_updates       | 764998    |\n",
      "|    reward_est_loss | -2.35e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.64e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1328      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 33310     |\n",
      "|    total_timesteps | 768138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.37e+05 |\n",
      "|    critic_loss     | 2.57e+09  |\n",
      "|    ent_coef        | 93.8      |\n",
      "|    ent_coef_loss   | -0.459    |\n",
      "|    learning_rate   | 0.00372   |\n",
      "|    n_updates       | 768136    |\n",
      "|    reward_est_loss | -2.3e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=4655.29 +/- 33.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.66e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 770000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.4e+05  |\n",
      "|    critic_loss     | 2.56e+09  |\n",
      "|    ent_coef        | 85.4      |\n",
      "|    ent_coef_loss   | 0.0318    |\n",
      "|    learning_rate   | 0.00372   |\n",
      "|    n_updates       | 769998    |\n",
      "|    reward_est_loss | -1.93e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.65e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1332      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 33475     |\n",
      "|    total_timesteps | 772138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.39e+05 |\n",
      "|    critic_loss     | 3.23e+09  |\n",
      "|    ent_coef        | 96.1      |\n",
      "|    ent_coef_loss   | 0.146     |\n",
      "|    learning_rate   | 0.00371   |\n",
      "|    n_updates       | 772136    |\n",
      "|    reward_est_loss | -2.27e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=775000, episode_reward=4632.58 +/- 22.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.63e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 775000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.26e+05 |\n",
      "|    critic_loss     | 5.23e+09  |\n",
      "|    ent_coef        | 98.7      |\n",
      "|    ent_coef_loss   | -2.84     |\n",
      "|    learning_rate   | 0.00371   |\n",
      "|    n_updates       | 774998    |\n",
      "|    reward_est_loss | 2.78e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.65e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1336      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 33640     |\n",
      "|    total_timesteps | 776138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.3e+05  |\n",
      "|    critic_loss     | 6.47e+09  |\n",
      "|    ent_coef        | 94        |\n",
      "|    ent_coef_loss   | -3.89     |\n",
      "|    learning_rate   | 0.00371   |\n",
      "|    n_updates       | 776136    |\n",
      "|    reward_est_loss | -1.64e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=4718.90 +/- 24.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.72e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 780000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.37e+05 |\n",
      "|    critic_loss     | 3.3e+09   |\n",
      "|    ent_coef        | 97.9      |\n",
      "|    ent_coef_loss   | -0.196    |\n",
      "|    learning_rate   | 0.0037    |\n",
      "|    n_updates       | 779998    |\n",
      "|    reward_est_loss | -2.26e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.66e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1340      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 33805     |\n",
      "|    total_timesteps | 780138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.49e+05 |\n",
      "|    critic_loss     | 3.06e+09  |\n",
      "|    ent_coef        | 99.5      |\n",
      "|    ent_coef_loss   | 1.5       |\n",
      "|    learning_rate   | 0.0037    |\n",
      "|    n_updates       | 780136    |\n",
      "|    reward_est_loss | -2.29e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.66e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1344      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 33969     |\n",
      "|    total_timesteps | 784138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.4e+05  |\n",
      "|    critic_loss     | 2.45e+09  |\n",
      "|    ent_coef        | 97.9      |\n",
      "|    ent_coef_loss   | 3.16      |\n",
      "|    learning_rate   | 0.00369   |\n",
      "|    n_updates       | 784136    |\n",
      "|    reward_est_loss | -2.39e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=785000, episode_reward=4825.38 +/- 26.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.83e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 785000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.39e+05 |\n",
      "|    critic_loss     | 2e+09     |\n",
      "|    ent_coef        | 97.2      |\n",
      "|    ent_coef_loss   | 1.64      |\n",
      "|    learning_rate   | 0.00369   |\n",
      "|    n_updates       | 784998    |\n",
      "|    reward_est_loss | -1.47e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.67e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1348      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 34134     |\n",
      "|    total_timesteps | 788138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.47e+05 |\n",
      "|    critic_loss     | 1.99e+09  |\n",
      "|    ent_coef        | 93.9      |\n",
      "|    ent_coef_loss   | 1.76      |\n",
      "|    learning_rate   | 0.00369   |\n",
      "|    n_updates       | 788136    |\n",
      "|    reward_est_loss | -1.55e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=4749.75 +/- 35.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.75e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 790000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.45e+05 |\n",
      "|    critic_loss     | 2.58e+09  |\n",
      "|    ent_coef        | 93.7      |\n",
      "|    ent_coef_loss   | -0.865    |\n",
      "|    learning_rate   | 0.00368   |\n",
      "|    n_updates       | 789998    |\n",
      "|    reward_est_loss | -1.32e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.69e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1352      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 34299     |\n",
      "|    total_timesteps | 792138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.42e+05 |\n",
      "|    critic_loss     | 3.38e+09  |\n",
      "|    ent_coef        | 94.6      |\n",
      "|    ent_coef_loss   | 2.57      |\n",
      "|    learning_rate   | 0.00368   |\n",
      "|    n_updates       | 792136    |\n",
      "|    reward_est_loss | -2.33e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=795000, episode_reward=4706.44 +/- 33.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.71e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 795000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.38e+05 |\n",
      "|    critic_loss     | 4.78e+09  |\n",
      "|    ent_coef        | 97.5      |\n",
      "|    ent_coef_loss   | -0.284    |\n",
      "|    learning_rate   | 0.00368   |\n",
      "|    n_updates       | 794998    |\n",
      "|    reward_est_loss | -2.39e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.7e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1356      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 34464     |\n",
      "|    total_timesteps | 796138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.4e+05  |\n",
      "|    critic_loss     | 1.55e+10  |\n",
      "|    ent_coef        | 92.7      |\n",
      "|    ent_coef_loss   | 1.01      |\n",
      "|    learning_rate   | 0.00367   |\n",
      "|    n_updates       | 796136    |\n",
      "|    reward_est_loss | -2.38e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=4661.07 +/- 22.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.66e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 800000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.5e+05  |\n",
      "|    critic_loss     | 2.33e+09  |\n",
      "|    ent_coef        | 96.1      |\n",
      "|    ent_coef_loss   | 3.11      |\n",
      "|    learning_rate   | 0.00367   |\n",
      "|    n_updates       | 799998    |\n",
      "|    reward_est_loss | -1.72e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.71e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1360      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 34629     |\n",
      "|    total_timesteps | 800138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.32e+05 |\n",
      "|    critic_loss     | 4.63e+09  |\n",
      "|    ent_coef        | 91.7      |\n",
      "|    ent_coef_loss   | -2.86     |\n",
      "|    learning_rate   | 0.00367   |\n",
      "|    n_updates       | 800136    |\n",
      "|    reward_est_loss | -1.25e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.72e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1364      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 34793     |\n",
      "|    total_timesteps | 804138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.44e+05 |\n",
      "|    critic_loss     | 2.75e+09  |\n",
      "|    ent_coef        | 94.7      |\n",
      "|    ent_coef_loss   | -0.0682   |\n",
      "|    learning_rate   | 0.00366   |\n",
      "|    n_updates       | 804136    |\n",
      "|    reward_est_loss | -2.27e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=805000, episode_reward=4993.55 +/- 27.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.99e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 805000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.38e+05 |\n",
      "|    critic_loss     | 3.01e+09  |\n",
      "|    ent_coef        | 93        |\n",
      "|    ent_coef_loss   | -0.509    |\n",
      "|    learning_rate   | 0.00366   |\n",
      "|    n_updates       | 804998    |\n",
      "|    reward_est_loss | -2.13e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1368      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 34960     |\n",
      "|    total_timesteps | 808138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.54e+05 |\n",
      "|    critic_loss     | 3.5e+09   |\n",
      "|    ent_coef        | 92.5      |\n",
      "|    ent_coef_loss   | 0.781     |\n",
      "|    learning_rate   | 0.00365   |\n",
      "|    n_updates       | 808136    |\n",
      "|    reward_est_loss | -2.09e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=5041.89 +/- 15.30\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 5.04e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 810000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.48e+05 |\n",
      "|    critic_loss     | 2.93e+09  |\n",
      "|    ent_coef        | 90        |\n",
      "|    ent_coef_loss   | 3         |\n",
      "|    learning_rate   | 0.00365   |\n",
      "|    n_updates       | 809998    |\n",
      "|    reward_est_loss | -2.41e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1372      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 35127     |\n",
      "|    total_timesteps | 812138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.53e+05 |\n",
      "|    critic_loss     | 4.19e+09  |\n",
      "|    ent_coef        | 104       |\n",
      "|    ent_coef_loss   | 3.98      |\n",
      "|    learning_rate   | 0.00365   |\n",
      "|    n_updates       | 812136    |\n",
      "|    reward_est_loss | -2.36e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=815000, episode_reward=4834.90 +/- 22.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.83e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 815000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.46e+05 |\n",
      "|    critic_loss     | 5.79e+09  |\n",
      "|    ent_coef        | 94.7      |\n",
      "|    ent_coef_loss   | 0.519     |\n",
      "|    learning_rate   | 0.00364   |\n",
      "|    n_updates       | 814998    |\n",
      "|    reward_est_loss | -2.16e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1376      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 35293     |\n",
      "|    total_timesteps | 816138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.45e+05 |\n",
      "|    critic_loss     | 2.92e+09  |\n",
      "|    ent_coef        | 85.8      |\n",
      "|    ent_coef_loss   | 1.08      |\n",
      "|    learning_rate   | 0.00364   |\n",
      "|    n_updates       | 816136    |\n",
      "|    reward_est_loss | -1.87e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=4715.88 +/- 25.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.72e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 820000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.46e+05 |\n",
      "|    critic_loss     | 3.42e+09  |\n",
      "|    ent_coef        | 98.5      |\n",
      "|    ent_coef_loss   | -1.08     |\n",
      "|    learning_rate   | 0.00363   |\n",
      "|    n_updates       | 819998    |\n",
      "|    reward_est_loss | -1.99e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1380      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 35458     |\n",
      "|    total_timesteps | 820138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.39e+05 |\n",
      "|    critic_loss     | 4.31e+09  |\n",
      "|    ent_coef        | 92.4      |\n",
      "|    ent_coef_loss   | -1.75     |\n",
      "|    learning_rate   | 0.00363   |\n",
      "|    n_updates       | 820136    |\n",
      "|    reward_est_loss | -1.66e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1384      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 35619     |\n",
      "|    total_timesteps | 824138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.56e+05 |\n",
      "|    critic_loss     | 2.74e+09  |\n",
      "|    ent_coef        | 88.6      |\n",
      "|    ent_coef_loss   | 1.87      |\n",
      "|    learning_rate   | 0.00363   |\n",
      "|    n_updates       | 824136    |\n",
      "|    reward_est_loss | -2.18e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=825000, episode_reward=4872.76 +/- 16.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.87e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 825000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.58e+05 |\n",
      "|    critic_loss     | 3.22e+09  |\n",
      "|    ent_coef        | 90.9      |\n",
      "|    ent_coef_loss   | -0.122    |\n",
      "|    learning_rate   | 0.00363   |\n",
      "|    n_updates       | 824998    |\n",
      "|    reward_est_loss | -2.43e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.75e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1388      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 35778     |\n",
      "|    total_timesteps | 828138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.45e+05 |\n",
      "|    critic_loss     | 5.22e+09  |\n",
      "|    ent_coef        | 97.8      |\n",
      "|    ent_coef_loss   | -1.29     |\n",
      "|    learning_rate   | 0.00362   |\n",
      "|    n_updates       | 828136    |\n",
      "|    reward_est_loss | -2.44e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=4952.64 +/- 22.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.95e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 830000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.56e+05 |\n",
      "|    critic_loss     | 5e+09     |\n",
      "|    ent_coef        | 95.5      |\n",
      "|    ent_coef_loss   | -2.64     |\n",
      "|    learning_rate   | 0.00362   |\n",
      "|    n_updates       | 829998    |\n",
      "|    reward_est_loss | -8.03e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1392      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 35936     |\n",
      "|    total_timesteps | 832138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.43e+05 |\n",
      "|    critic_loss     | 3.63e+09  |\n",
      "|    ent_coef        | 94.2      |\n",
      "|    ent_coef_loss   | -4.76     |\n",
      "|    learning_rate   | 0.00361   |\n",
      "|    n_updates       | 832136    |\n",
      "|    reward_est_loss | -2.16e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=835000, episode_reward=4380.14 +/- 15.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.38e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 835000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.52e+05 |\n",
      "|    critic_loss     | 1.99e+09  |\n",
      "|    ent_coef        | 92        |\n",
      "|    ent_coef_loss   | -0.829    |\n",
      "|    learning_rate   | 0.00361   |\n",
      "|    n_updates       | 834998    |\n",
      "|    reward_est_loss | -5.1e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1396      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 36096     |\n",
      "|    total_timesteps | 836138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.47e+05 |\n",
      "|    critic_loss     | 2.16e+09  |\n",
      "|    ent_coef        | 89.7      |\n",
      "|    ent_coef_loss   | 1.23      |\n",
      "|    learning_rate   | 0.00361   |\n",
      "|    n_updates       | 836136    |\n",
      "|    reward_est_loss | -2.25e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=4843.25 +/- 11.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.84e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 840000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.46e+05 |\n",
      "|    critic_loss     | 2.54e+09  |\n",
      "|    ent_coef        | 88.4      |\n",
      "|    ent_coef_loss   | -0.121    |\n",
      "|    learning_rate   | 0.0036    |\n",
      "|    n_updates       | 839998    |\n",
      "|    reward_est_loss | -6.36e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.77e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1400      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 36254     |\n",
      "|    total_timesteps | 840138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.41e+05 |\n",
      "|    critic_loss     | 3.31e+09  |\n",
      "|    ent_coef        | 92.2      |\n",
      "|    ent_coef_loss   | -0.967    |\n",
      "|    learning_rate   | 0.0036    |\n",
      "|    n_updates       | 840136    |\n",
      "|    reward_est_loss | -1.36e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.77e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1404      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 36412     |\n",
      "|    total_timesteps | 844138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.55e+05 |\n",
      "|    critic_loss     | 2.93e+09  |\n",
      "|    ent_coef        | 92.4      |\n",
      "|    ent_coef_loss   | -0.662    |\n",
      "|    learning_rate   | 0.00359   |\n",
      "|    n_updates       | 844136    |\n",
      "|    reward_est_loss | -2.17e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=845000, episode_reward=4706.40 +/- 35.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.71e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 845000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.62e+05 |\n",
      "|    critic_loss     | 1.99e+09  |\n",
      "|    ent_coef        | 91.7      |\n",
      "|    ent_coef_loss   | 2.28      |\n",
      "|    learning_rate   | 0.00359   |\n",
      "|    n_updates       | 844998    |\n",
      "|    reward_est_loss | -286      |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1408      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 36571     |\n",
      "|    total_timesteps | 848138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.59e+05 |\n",
      "|    critic_loss     | 2.08e+09  |\n",
      "|    ent_coef        | 91.5      |\n",
      "|    ent_coef_loss   | 1.75      |\n",
      "|    learning_rate   | 0.00359   |\n",
      "|    n_updates       | 848136    |\n",
      "|    reward_est_loss | -2.38e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=4472.06 +/- 20.29\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.47e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 850000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.6e+05  |\n",
      "|    critic_loss     | 2.64e+09  |\n",
      "|    ent_coef        | 96.8      |\n",
      "|    ent_coef_loss   | -0.725    |\n",
      "|    learning_rate   | 0.00358   |\n",
      "|    n_updates       | 849998    |\n",
      "|    reward_est_loss | -6.81e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1412      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 36735     |\n",
      "|    total_timesteps | 852138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.5e+05  |\n",
      "|    critic_loss     | 3.86e+09  |\n",
      "|    ent_coef        | 97.3      |\n",
      "|    ent_coef_loss   | 2.33      |\n",
      "|    learning_rate   | 0.00358   |\n",
      "|    n_updates       | 852136    |\n",
      "|    reward_est_loss | -6.24e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=855000, episode_reward=4847.67 +/- 19.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.85e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 855000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.6e+05  |\n",
      "|    critic_loss     | 2.87e+09  |\n",
      "|    ent_coef        | 89        |\n",
      "|    ent_coef_loss   | 0.69      |\n",
      "|    learning_rate   | 0.00358   |\n",
      "|    n_updates       | 854998    |\n",
      "|    reward_est_loss | -2.32e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1416      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 36900     |\n",
      "|    total_timesteps | 856138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.66e+05 |\n",
      "|    critic_loss     | 3.72e+09  |\n",
      "|    ent_coef        | 93.7      |\n",
      "|    ent_coef_loss   | -1.58     |\n",
      "|    learning_rate   | 0.00357   |\n",
      "|    n_updates       | 856136    |\n",
      "|    reward_est_loss | -1.35e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=4927.05 +/- 10.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.93e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 860000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.61e+05 |\n",
      "|    critic_loss     | 2.81e+09  |\n",
      "|    ent_coef        | 91.6      |\n",
      "|    ent_coef_loss   | -0.763    |\n",
      "|    learning_rate   | 0.00357   |\n",
      "|    n_updates       | 859998    |\n",
      "|    reward_est_loss | -2.42e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1420      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 37065     |\n",
      "|    total_timesteps | 860138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.54e+05 |\n",
      "|    critic_loss     | 2.29e+09  |\n",
      "|    ent_coef        | 89.6      |\n",
      "|    ent_coef_loss   | 1.25      |\n",
      "|    learning_rate   | 0.00357   |\n",
      "|    n_updates       | 860136    |\n",
      "|    reward_est_loss | -6.38e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1424      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 37229     |\n",
      "|    total_timesteps | 864138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.59e+05 |\n",
      "|    critic_loss     | 7.38e+09  |\n",
      "|    ent_coef        | 89.4      |\n",
      "|    ent_coef_loss   | 0.736     |\n",
      "|    learning_rate   | 0.00356   |\n",
      "|    n_updates       | 864136    |\n",
      "|    reward_est_loss | -1.67e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=865000, episode_reward=4478.35 +/- 20.93\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.48e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 865000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.54e+05 |\n",
      "|    critic_loss     | 2.86e+09  |\n",
      "|    ent_coef        | 94.9      |\n",
      "|    ent_coef_loss   | 0.985     |\n",
      "|    learning_rate   | 0.00356   |\n",
      "|    n_updates       | 864998    |\n",
      "|    reward_est_loss | -7.94e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.75e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1428      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 37395     |\n",
      "|    total_timesteps | 868138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.55e+05 |\n",
      "|    critic_loss     | 3.29e+09  |\n",
      "|    ent_coef        | 92.8      |\n",
      "|    ent_coef_loss   | -0.487    |\n",
      "|    learning_rate   | 0.00355   |\n",
      "|    n_updates       | 868136    |\n",
      "|    reward_est_loss | -2.41e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=4325.16 +/- 31.65\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.33e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 870000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.43e+05 |\n",
      "|    critic_loss     | 4.87e+09  |\n",
      "|    ent_coef        | 91.3      |\n",
      "|    ent_coef_loss   | -4.31     |\n",
      "|    learning_rate   | 0.00355   |\n",
      "|    n_updates       | 869998    |\n",
      "|    reward_est_loss | -2.44e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1432      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 37560     |\n",
      "|    total_timesteps | 872138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.49e+05 |\n",
      "|    critic_loss     | 3.82e+09  |\n",
      "|    ent_coef        | 91.3      |\n",
      "|    ent_coef_loss   | -0.544    |\n",
      "|    learning_rate   | 0.00355   |\n",
      "|    n_updates       | 872136    |\n",
      "|    reward_est_loss | -1.73e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=875000, episode_reward=4686.76 +/- 34.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.69e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 875000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.59e+05 |\n",
      "|    critic_loss     | 2.67e+09  |\n",
      "|    ent_coef        | 88.5      |\n",
      "|    ent_coef_loss   | -0.496    |\n",
      "|    learning_rate   | 0.00354   |\n",
      "|    n_updates       | 874998    |\n",
      "|    reward_est_loss | -2.4e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1436      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 37725     |\n",
      "|    total_timesteps | 876138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.66e+05 |\n",
      "|    critic_loss     | 3.89e+09  |\n",
      "|    ent_coef        | 94.8      |\n",
      "|    ent_coef_loss   | 0.994     |\n",
      "|    learning_rate   | 0.00354   |\n",
      "|    n_updates       | 876136    |\n",
      "|    reward_est_loss | -2.5e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=4730.69 +/- 25.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 880000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.63e+05 |\n",
      "|    critic_loss     | 6.11e+09  |\n",
      "|    ent_coef        | 89.8      |\n",
      "|    ent_coef_loss   | -0.0859   |\n",
      "|    learning_rate   | 0.00353   |\n",
      "|    n_updates       | 879998    |\n",
      "|    reward_est_loss | -2.01e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1440      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 37891     |\n",
      "|    total_timesteps | 880138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.64e+05 |\n",
      "|    critic_loss     | 2.68e+09  |\n",
      "|    ent_coef        | 90.7      |\n",
      "|    ent_coef_loss   | 0.872     |\n",
      "|    learning_rate   | 0.00353   |\n",
      "|    n_updates       | 880136    |\n",
      "|    reward_est_loss | -1.31e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1444      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 38055     |\n",
      "|    total_timesteps | 884138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.65e+05 |\n",
      "|    critic_loss     | 3.14e+09  |\n",
      "|    ent_coef        | 91        |\n",
      "|    ent_coef_loss   | -1.47     |\n",
      "|    learning_rate   | 0.00353   |\n",
      "|    n_updates       | 884136    |\n",
      "|    reward_est_loss | -2.32e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=885000, episode_reward=4745.67 +/- 24.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.75e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 885000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.61e+05 |\n",
      "|    critic_loss     | 8.1e+09   |\n",
      "|    ent_coef        | 91.2      |\n",
      "|    ent_coef_loss   | -1.31     |\n",
      "|    learning_rate   | 0.00353   |\n",
      "|    n_updates       | 884998    |\n",
      "|    reward_est_loss | -2.36e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1448      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 38219     |\n",
      "|    total_timesteps | 888138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.63e+05 |\n",
      "|    critic_loss     | 2.54e+09  |\n",
      "|    ent_coef        | 91.7      |\n",
      "|    ent_coef_loss   | 0.968     |\n",
      "|    learning_rate   | 0.00352   |\n",
      "|    n_updates       | 888136    |\n",
      "|    reward_est_loss | -2.47e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=4402.35 +/- 216.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.4e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 890000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.66e+05 |\n",
      "|    critic_loss     | 3.98e+09  |\n",
      "|    ent_coef        | 86.7      |\n",
      "|    ent_coef_loss   | -1.93     |\n",
      "|    learning_rate   | 0.00352   |\n",
      "|    n_updates       | 889998    |\n",
      "|    reward_est_loss | -2.53e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1452      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 38384     |\n",
      "|    total_timesteps | 892138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.55e+05 |\n",
      "|    critic_loss     | 2.88e+09  |\n",
      "|    ent_coef        | 90.8      |\n",
      "|    ent_coef_loss   | -0.325    |\n",
      "|    learning_rate   | 0.00351   |\n",
      "|    n_updates       | 892136    |\n",
      "|    reward_est_loss | -2.15e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=895000, episode_reward=4900.49 +/- 20.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.9e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 895000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.66e+05 |\n",
      "|    critic_loss     | 3.02e+09  |\n",
      "|    ent_coef        | 91.1      |\n",
      "|    ent_coef_loss   | -0.914    |\n",
      "|    learning_rate   | 0.00351   |\n",
      "|    n_updates       | 894998    |\n",
      "|    reward_est_loss | -2.35e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1456      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 38549     |\n",
      "|    total_timesteps | 896138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.64e+05 |\n",
      "|    critic_loss     | 2.98e+09  |\n",
      "|    ent_coef        | 91.8      |\n",
      "|    ent_coef_loss   | 0.108     |\n",
      "|    learning_rate   | 0.00351   |\n",
      "|    n_updates       | 896136    |\n",
      "|    reward_est_loss | -177      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=4791.06 +/- 26.04\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 900000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.71e+05 |\n",
      "|    critic_loss     | 2.72e+09  |\n",
      "|    ent_coef        | 92        |\n",
      "|    ent_coef_loss   | -2.18     |\n",
      "|    learning_rate   | 0.0035    |\n",
      "|    n_updates       | 899998    |\n",
      "|    reward_est_loss | -1.16e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1460      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 38714     |\n",
      "|    total_timesteps | 900138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.72e+05 |\n",
      "|    critic_loss     | 1.9e+09   |\n",
      "|    ent_coef        | 91        |\n",
      "|    ent_coef_loss   | 4.25      |\n",
      "|    learning_rate   | 0.0035    |\n",
      "|    n_updates       | 900136    |\n",
      "|    reward_est_loss | -2.22e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1464      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 38878     |\n",
      "|    total_timesteps | 904138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.65e+05 |\n",
      "|    critic_loss     | 5.42e+09  |\n",
      "|    ent_coef        | 91.3      |\n",
      "|    ent_coef_loss   | -2.11     |\n",
      "|    learning_rate   | 0.00349   |\n",
      "|    n_updates       | 904136    |\n",
      "|    reward_est_loss | -1.72e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=905000, episode_reward=4895.61 +/- 20.52\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.9e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 905000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.61e+05 |\n",
      "|    critic_loss     | 7.45e+09  |\n",
      "|    ent_coef        | 90.1      |\n",
      "|    ent_coef_loss   | -2.38     |\n",
      "|    learning_rate   | 0.00349   |\n",
      "|    n_updates       | 904998    |\n",
      "|    reward_est_loss | -2.55e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1468      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 39043     |\n",
      "|    total_timesteps | 908138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.68e+05 |\n",
      "|    critic_loss     | 6.5e+09   |\n",
      "|    ent_coef        | 88.9      |\n",
      "|    ent_coef_loss   | 0.47      |\n",
      "|    learning_rate   | 0.00349   |\n",
      "|    n_updates       | 908136    |\n",
      "|    reward_est_loss | -1.12e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=4918.08 +/- 36.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.92e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 910000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.59e+05 |\n",
      "|    critic_loss     | 3.35e+09  |\n",
      "|    ent_coef        | 91.2      |\n",
      "|    ent_coef_loss   | 1.67      |\n",
      "|    learning_rate   | 0.00348   |\n",
      "|    n_updates       | 909998    |\n",
      "|    reward_est_loss | -2.53e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1472      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 39208     |\n",
      "|    total_timesteps | 912138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.67e+05 |\n",
      "|    critic_loss     | 3.96e+09  |\n",
      "|    ent_coef        | 89        |\n",
      "|    ent_coef_loss   | -2.04     |\n",
      "|    learning_rate   | 0.00348   |\n",
      "|    n_updates       | 912136    |\n",
      "|    reward_est_loss | -2.57e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=915000, episode_reward=4986.23 +/- 28.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.99e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 915000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.8e+05  |\n",
      "|    critic_loss     | 2.45e+09  |\n",
      "|    ent_coef        | 93.6      |\n",
      "|    ent_coef_loss   | 0.976     |\n",
      "|    learning_rate   | 0.00348   |\n",
      "|    n_updates       | 914998    |\n",
      "|    reward_est_loss | -2.07e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1476      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 39374     |\n",
      "|    total_timesteps | 916138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.78e+05 |\n",
      "|    critic_loss     | 3.32e+09  |\n",
      "|    ent_coef        | 97.1      |\n",
      "|    ent_coef_loss   | 0.0978    |\n",
      "|    learning_rate   | 0.00347   |\n",
      "|    n_updates       | 916136    |\n",
      "|    reward_est_loss | -2.58e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=4836.13 +/- 76.00\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.84e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 920000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.65e+05 |\n",
      "|    critic_loss     | 2.84e+09  |\n",
      "|    ent_coef        | 94.8      |\n",
      "|    ent_coef_loss   | 1.51      |\n",
      "|    learning_rate   | 0.00347   |\n",
      "|    n_updates       | 919998    |\n",
      "|    reward_est_loss | 1.91e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1480      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 39539     |\n",
      "|    total_timesteps | 920138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.78e+05 |\n",
      "|    critic_loss     | 2.91e+09  |\n",
      "|    ent_coef        | 92.1      |\n",
      "|    ent_coef_loss   | 1.22      |\n",
      "|    learning_rate   | 0.00347   |\n",
      "|    n_updates       | 920136    |\n",
      "|    reward_est_loss | -8.6e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1484      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 39702     |\n",
      "|    total_timesteps | 924138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.66e+05 |\n",
      "|    critic_loss     | 3.38e+09  |\n",
      "|    ent_coef        | 92.2      |\n",
      "|    ent_coef_loss   | 2.43      |\n",
      "|    learning_rate   | 0.00346   |\n",
      "|    n_updates       | 924136    |\n",
      "|    reward_est_loss | -2.42e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=925000, episode_reward=4540.37 +/- 50.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.54e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 925000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.75e+05 |\n",
      "|    critic_loss     | 3.01e+09  |\n",
      "|    ent_coef        | 91.1      |\n",
      "|    ent_coef_loss   | 0.693     |\n",
      "|    learning_rate   | 0.00346   |\n",
      "|    n_updates       | 924998    |\n",
      "|    reward_est_loss | -2.53e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1488      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 39868     |\n",
      "|    total_timesteps | 928138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.72e+05 |\n",
      "|    critic_loss     | 5.69e+09  |\n",
      "|    ent_coef        | 92.1      |\n",
      "|    ent_coef_loss   | -2.6      |\n",
      "|    learning_rate   | 0.00345   |\n",
      "|    n_updates       | 928136    |\n",
      "|    reward_est_loss | -2.45e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=4545.77 +/- 25.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.55e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 930000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.72e+05 |\n",
      "|    critic_loss     | 4.4e+09   |\n",
      "|    ent_coef        | 97.2      |\n",
      "|    ent_coef_loss   | 0.207     |\n",
      "|    learning_rate   | 0.00345   |\n",
      "|    n_updates       | 929998    |\n",
      "|    reward_est_loss | -1.87e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1492      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 40033     |\n",
      "|    total_timesteps | 932138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.82e+05 |\n",
      "|    critic_loss     | 1.87e+09  |\n",
      "|    ent_coef        | 95.2      |\n",
      "|    ent_coef_loss   | 0.354     |\n",
      "|    learning_rate   | 0.00345   |\n",
      "|    n_updates       | 932136    |\n",
      "|    reward_est_loss | -2.49e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=935000, episode_reward=4856.95 +/- 53.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.86e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 935000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.73e+05 |\n",
      "|    critic_loss     | 1.37e+10  |\n",
      "|    ent_coef        | 95.2      |\n",
      "|    ent_coef_loss   | 1.81      |\n",
      "|    learning_rate   | 0.00344   |\n",
      "|    n_updates       | 934998    |\n",
      "|    reward_est_loss | -2.59e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1496      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 40198     |\n",
      "|    total_timesteps | 936138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.76e+05 |\n",
      "|    critic_loss     | 1.9e+09   |\n",
      "|    ent_coef        | 88.6      |\n",
      "|    ent_coef_loss   | -1.6      |\n",
      "|    learning_rate   | 0.00344   |\n",
      "|    n_updates       | 936136    |\n",
      "|    reward_est_loss | 9.06e+03  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=4629.69 +/- 21.66\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.63e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 940000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.95e+05 |\n",
      "|    critic_loss     | 2.1e+09   |\n",
      "|    ent_coef        | 98.2      |\n",
      "|    ent_coef_loss   | 2.23      |\n",
      "|    learning_rate   | 0.00343   |\n",
      "|    n_updates       | 939998    |\n",
      "|    reward_est_loss | -1.9e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1500      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 40363     |\n",
      "|    total_timesteps | 940138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.75e+05 |\n",
      "|    critic_loss     | 3.46e+09  |\n",
      "|    ent_coef        | 93.6      |\n",
      "|    ent_coef_loss   | 0.497     |\n",
      "|    learning_rate   | 0.00343   |\n",
      "|    n_updates       | 940136    |\n",
      "|    reward_est_loss | 1.1e+04   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1504      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 40527     |\n",
      "|    total_timesteps | 944138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.83e+05 |\n",
      "|    critic_loss     | 4.58e+09  |\n",
      "|    ent_coef        | 96.3      |\n",
      "|    ent_coef_loss   | -0.706    |\n",
      "|    learning_rate   | 0.00343   |\n",
      "|    n_updates       | 944136    |\n",
      "|    reward_est_loss | -2.19e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=945000, episode_reward=4628.80 +/- 72.03\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.63e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 945000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.75e+05 |\n",
      "|    critic_loss     | 4.29e+09  |\n",
      "|    ent_coef        | 92.9      |\n",
      "|    ent_coef_loss   | -1.89     |\n",
      "|    learning_rate   | 0.00343   |\n",
      "|    n_updates       | 944998    |\n",
      "|    reward_est_loss | -1.94e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.73e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1508      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 40692     |\n",
      "|    total_timesteps | 948138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.81e+05 |\n",
      "|    critic_loss     | 5.19e+09  |\n",
      "|    ent_coef        | 88.8      |\n",
      "|    ent_coef_loss   | 1.24      |\n",
      "|    learning_rate   | 0.00342   |\n",
      "|    n_updates       | 948136    |\n",
      "|    reward_est_loss | -2.54e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=4939.77 +/- 23.75\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.94e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 950000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.8e+05  |\n",
      "|    critic_loss     | 7.12e+09  |\n",
      "|    ent_coef        | 95.1      |\n",
      "|    ent_coef_loss   | 0.301     |\n",
      "|    learning_rate   | 0.00342   |\n",
      "|    n_updates       | 949998    |\n",
      "|    reward_est_loss | -2.03e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1512      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 40857     |\n",
      "|    total_timesteps | 952138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.84e+05 |\n",
      "|    critic_loss     | 4.33e+09  |\n",
      "|    ent_coef        | 90.9      |\n",
      "|    ent_coef_loss   | -0.0551   |\n",
      "|    learning_rate   | 0.00341   |\n",
      "|    n_updates       | 952136    |\n",
      "|    reward_est_loss | -2.09e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=955000, episode_reward=4815.35 +/- 30.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.82e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 955000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.87e+05 |\n",
      "|    critic_loss     | 1.91e+09  |\n",
      "|    ent_coef        | 98.4      |\n",
      "|    ent_coef_loss   | -0.0465   |\n",
      "|    learning_rate   | 0.00341   |\n",
      "|    n_updates       | 954998    |\n",
      "|    reward_est_loss | -8.18e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1516      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 41022     |\n",
      "|    total_timesteps | 956138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.78e+05 |\n",
      "|    critic_loss     | 2.54e+09  |\n",
      "|    ent_coef        | 94.4      |\n",
      "|    ent_coef_loss   | 0.0446    |\n",
      "|    learning_rate   | 0.00341   |\n",
      "|    n_updates       | 956136    |\n",
      "|    reward_est_loss | -2.5e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=4685.34 +/- 25.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.69e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 960000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.75e+05 |\n",
      "|    critic_loss     | 3.3e+09   |\n",
      "|    ent_coef        | 87.4      |\n",
      "|    ent_coef_loss   | 0.294     |\n",
      "|    learning_rate   | 0.0034    |\n",
      "|    n_updates       | 959998    |\n",
      "|    reward_est_loss | -2.64e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.75e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1520      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 41187     |\n",
      "|    total_timesteps | 960138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.84e+05 |\n",
      "|    critic_loss     | 2.87e+09  |\n",
      "|    ent_coef        | 92.2      |\n",
      "|    ent_coef_loss   | -3.59     |\n",
      "|    learning_rate   | 0.0034    |\n",
      "|    n_updates       | 960136    |\n",
      "|    reward_est_loss | -2.38e+04 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.75e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1524     |\n",
      "|    fps             | 23       |\n",
      "|    time_elapsed    | 41351    |\n",
      "|    total_timesteps | 964138   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.8e+05 |\n",
      "|    critic_loss     | 3.63e+09 |\n",
      "|    ent_coef        | 91.7     |\n",
      "|    ent_coef_loss   | -0.0389  |\n",
      "|    learning_rate   | 0.00339  |\n",
      "|    n_updates       | 964136   |\n",
      "|    reward_est_loss | -2.6e+04 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=965000, episode_reward=5011.44 +/- 8.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 5.01e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 965000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.02e+05 |\n",
      "|    critic_loss     | 1.4e+09   |\n",
      "|    ent_coef        | 93.4      |\n",
      "|    ent_coef_loss   | 3.26      |\n",
      "|    learning_rate   | 0.00339   |\n",
      "|    n_updates       | 964998    |\n",
      "|    reward_est_loss | -2.62e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.76e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1528      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 41516     |\n",
      "|    total_timesteps | 968138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.9e+05  |\n",
      "|    critic_loss     | 2.45e+09  |\n",
      "|    ent_coef        | 91.8      |\n",
      "|    ent_coef_loss   | -0.713    |\n",
      "|    learning_rate   | 0.00339   |\n",
      "|    n_updates       | 968136    |\n",
      "|    reward_est_loss | -1.96e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=4454.34 +/- 69.98\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.45e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 970000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.81e+05 |\n",
      "|    critic_loss     | 3.19e+09  |\n",
      "|    ent_coef        | 89.7      |\n",
      "|    ent_coef_loss   | -1.21     |\n",
      "|    learning_rate   | 0.00338   |\n",
      "|    n_updates       | 969998    |\n",
      "|    reward_est_loss | 1.66e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.77e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1532      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 41681     |\n",
      "|    total_timesteps | 972138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.89e+05 |\n",
      "|    critic_loss     | 6.12e+09  |\n",
      "|    ent_coef        | 95.9      |\n",
      "|    ent_coef_loss   | 0.214     |\n",
      "|    learning_rate   | 0.00338   |\n",
      "|    n_updates       | 972136    |\n",
      "|    reward_est_loss | -6.93e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=975000, episode_reward=4859.70 +/- 52.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.86e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 975000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.82e+05 |\n",
      "|    critic_loss     | 2e+09     |\n",
      "|    ent_coef        | 93.8      |\n",
      "|    ent_coef_loss   | 2.83      |\n",
      "|    learning_rate   | 0.00338   |\n",
      "|    n_updates       | 974998    |\n",
      "|    reward_est_loss | -2.65e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1536      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 41847     |\n",
      "|    total_timesteps | 976138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.79e+05 |\n",
      "|    critic_loss     | 3.39e+09  |\n",
      "|    ent_coef        | 91.3      |\n",
      "|    ent_coef_loss   | -1.77     |\n",
      "|    learning_rate   | 0.00337   |\n",
      "|    n_updates       | 976136    |\n",
      "|    reward_est_loss | -2.62e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=4669.06 +/- 12.67\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.67e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 980000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.83e+05 |\n",
      "|    critic_loss     | 3.26e+09  |\n",
      "|    ent_coef        | 89.8      |\n",
      "|    ent_coef_loss   | -1.32     |\n",
      "|    learning_rate   | 0.00337   |\n",
      "|    n_updates       | 979998    |\n",
      "|    reward_est_loss | -2.63e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1540      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 42012     |\n",
      "|    total_timesteps | 980138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.82e+05 |\n",
      "|    critic_loss     | 2.96e+09  |\n",
      "|    ent_coef        | 90.5      |\n",
      "|    ent_coef_loss   | -0.251    |\n",
      "|    learning_rate   | 0.00337   |\n",
      "|    n_updates       | 980136    |\n",
      "|    reward_est_loss | -2.68e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1544      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 42175     |\n",
      "|    total_timesteps | 984138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.91e+05 |\n",
      "|    critic_loss     | 2.57e+09  |\n",
      "|    ent_coef        | 91.6      |\n",
      "|    ent_coef_loss   | 0.273     |\n",
      "|    learning_rate   | 0.00336   |\n",
      "|    n_updates       | 984136    |\n",
      "|    reward_est_loss | -2.51e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=985000, episode_reward=4905.36 +/- 28.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.91e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 985000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.98e+05 |\n",
      "|    critic_loss     | 1.84e+09  |\n",
      "|    ent_coef        | 93.2      |\n",
      "|    ent_coef_loss   | -2.43     |\n",
      "|    learning_rate   | 0.00336   |\n",
      "|    n_updates       | 984998    |\n",
      "|    reward_est_loss | -2.68e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1548      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 42341     |\n",
      "|    total_timesteps | 988138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.81e+05 |\n",
      "|    critic_loss     | 2.85e+09  |\n",
      "|    ent_coef        | 92.3      |\n",
      "|    ent_coef_loss   | -1.26     |\n",
      "|    learning_rate   | 0.00335   |\n",
      "|    n_updates       | 988136    |\n",
      "|    reward_est_loss | -2.7e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=4682.77 +/- 17.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.68e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 990000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.02e+05 |\n",
      "|    critic_loss     | 3.78e+09  |\n",
      "|    ent_coef        | 92.4      |\n",
      "|    ent_coef_loss   | 0.502     |\n",
      "|    learning_rate   | 0.00335   |\n",
      "|    n_updates       | 989998    |\n",
      "|    reward_est_loss | -2.62e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.77e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1552      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 42506     |\n",
      "|    total_timesteps | 992138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.86e+05 |\n",
      "|    critic_loss     | 2.8e+09   |\n",
      "|    ent_coef        | 94.7      |\n",
      "|    ent_coef_loss   | 1.1       |\n",
      "|    learning_rate   | 0.00335   |\n",
      "|    n_updates       | 992136    |\n",
      "|    reward_est_loss | -2.55e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=995000, episode_reward=4825.92 +/- 9.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.83e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 995000    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.87e+05 |\n",
      "|    critic_loss     | 3.29e+09  |\n",
      "|    ent_coef        | 87.2      |\n",
      "|    ent_coef_loss   | -2.03     |\n",
      "|    learning_rate   | 0.00334   |\n",
      "|    n_updates       | 994998    |\n",
      "|    reward_est_loss | -2.6e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1556      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 42671     |\n",
      "|    total_timesteps | 996138    |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.92e+05 |\n",
      "|    critic_loss     | 3.33e+09  |\n",
      "|    ent_coef        | 86.9      |\n",
      "|    ent_coef_loss   | -0.34     |\n",
      "|    learning_rate   | 0.00334   |\n",
      "|    n_updates       | 996136    |\n",
      "|    reward_est_loss | -2.45e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=4790.88 +/- 19.90\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1000000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.91e+05 |\n",
      "|    critic_loss     | 6.71e+09  |\n",
      "|    ent_coef        | 93.3      |\n",
      "|    ent_coef_loss   | -0.533    |\n",
      "|    learning_rate   | 0.00333   |\n",
      "|    n_updates       | 999998    |\n",
      "|    reward_est_loss | -2.72e+04 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.78e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1560     |\n",
      "|    fps             | 23       |\n",
      "|    time_elapsed    | 42834    |\n",
      "|    total_timesteps | 1000138  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.9e+05 |\n",
      "|    critic_loss     | 2.1e+09  |\n",
      "|    ent_coef        | 94.7     |\n",
      "|    ent_coef_loss   | 2.04     |\n",
      "|    learning_rate   | 0.00333  |\n",
      "|    n_updates       | 1000136  |\n",
      "|    reward_est_loss | 1.06e+05 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1564      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 42913     |\n",
      "|    total_timesteps | 1004138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.87e+05 |\n",
      "|    critic_loss     | 2.48e+09  |\n",
      "|    ent_coef        | 87.8      |\n",
      "|    ent_coef_loss   | 0.687     |\n",
      "|    learning_rate   | 0.00333   |\n",
      "|    n_updates       | 1004136   |\n",
      "|    reward_est_loss | 4.51e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1005000, episode_reward=4796.13 +/- 21.79\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1005000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.96e+05 |\n",
      "|    critic_loss     | 2.87e+09  |\n",
      "|    ent_coef        | 87.2      |\n",
      "|    ent_coef_loss   | -2.75     |\n",
      "|    learning_rate   | 0.00333   |\n",
      "|    n_updates       | 1004998   |\n",
      "|    reward_est_loss | 7.56e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1568      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 42993     |\n",
      "|    total_timesteps | 1008138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.97e+05 |\n",
      "|    critic_loss     | 2.32e+09  |\n",
      "|    ent_coef        | 89.4      |\n",
      "|    ent_coef_loss   | -0.0322   |\n",
      "|    learning_rate   | 0.00332   |\n",
      "|    n_updates       | 1008136   |\n",
      "|    reward_est_loss | -2.16e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1010000, episode_reward=4620.05 +/- 8.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.62e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1010000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.02e+05 |\n",
      "|    critic_loss     | 7.41e+09  |\n",
      "|    ent_coef        | 84.3      |\n",
      "|    ent_coef_loss   | -1.16     |\n",
      "|    learning_rate   | 0.00332   |\n",
      "|    n_updates       | 1009998   |\n",
      "|    reward_est_loss | 9.19e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1572      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 43119     |\n",
      "|    total_timesteps | 1012138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.95e+05 |\n",
      "|    critic_loss     | 2.66e+09  |\n",
      "|    ent_coef        | 90.9      |\n",
      "|    ent_coef_loss   | -2.17     |\n",
      "|    learning_rate   | 0.00331   |\n",
      "|    n_updates       | 1012136   |\n",
      "|    reward_est_loss | -2.58e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1015000, episode_reward=4959.04 +/- 29.63\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.96e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1015000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.01e+05 |\n",
      "|    critic_loss     | 3.11e+09  |\n",
      "|    ent_coef        | 92.3      |\n",
      "|    ent_coef_loss   | 3.81      |\n",
      "|    learning_rate   | 0.00331   |\n",
      "|    n_updates       | 1014998   |\n",
      "|    reward_est_loss | -2.58e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1576      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 43286     |\n",
      "|    total_timesteps | 1016138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.86e+05 |\n",
      "|    critic_loss     | 3.47e+09  |\n",
      "|    ent_coef        | 87.9      |\n",
      "|    ent_coef_loss   | -2.4      |\n",
      "|    learning_rate   | 0.00331   |\n",
      "|    n_updates       | 1016136   |\n",
      "|    reward_est_loss | -2.31e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1020000, episode_reward=5026.34 +/- 8.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 5.03e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1020000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.04e+05 |\n",
      "|    critic_loss     | 2.1e+09   |\n",
      "|    ent_coef        | 85.5      |\n",
      "|    ent_coef_loss   | -1.33     |\n",
      "|    learning_rate   | 0.0033    |\n",
      "|    n_updates       | 1019998   |\n",
      "|    reward_est_loss | -2.03e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1580      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 43452     |\n",
      "|    total_timesteps | 1020138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -5.99e+05 |\n",
      "|    critic_loss     | 3.18e+09  |\n",
      "|    ent_coef        | 86.8      |\n",
      "|    ent_coef_loss   | 0.641     |\n",
      "|    learning_rate   | 0.0033    |\n",
      "|    n_updates       | 1020136   |\n",
      "|    reward_est_loss | -2.59e+04 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.79e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1584     |\n",
      "|    fps             | 23       |\n",
      "|    time_elapsed    | 43616    |\n",
      "|    total_timesteps | 1024138  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.1e+05 |\n",
      "|    critic_loss     | 2.32e+09 |\n",
      "|    ent_coef        | 90.8     |\n",
      "|    ent_coef_loss   | 1.79     |\n",
      "|    learning_rate   | 0.00329  |\n",
      "|    n_updates       | 1024136  |\n",
      "|    reward_est_loss | -2.2e+04 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1025000, episode_reward=4982.21 +/- 28.95\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.98e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1025000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.07e+05 |\n",
      "|    critic_loss     | 1.87e+09  |\n",
      "|    ent_coef        | 86.1      |\n",
      "|    ent_coef_loss   | 0.349     |\n",
      "|    learning_rate   | 0.00329   |\n",
      "|    n_updates       | 1024998   |\n",
      "|    reward_est_loss | -2.44e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1588      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 43783     |\n",
      "|    total_timesteps | 1028138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.11e+05 |\n",
      "|    critic_loss     | 6.3e+09   |\n",
      "|    ent_coef        | 84.9      |\n",
      "|    ent_coef_loss   | 2.03      |\n",
      "|    learning_rate   | 0.00329   |\n",
      "|    n_updates       | 1028136   |\n",
      "|    reward_est_loss | -1.4e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1030000, episode_reward=4868.51 +/- 6.31\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.87e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1030000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.1e+05  |\n",
      "|    critic_loss     | 2.63e+09  |\n",
      "|    ent_coef        | 82        |\n",
      "|    ent_coef_loss   | 0.136     |\n",
      "|    learning_rate   | 0.00328   |\n",
      "|    n_updates       | 1029998   |\n",
      "|    reward_est_loss | -1.87e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1592      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 43948     |\n",
      "|    total_timesteps | 1032138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.1e+05  |\n",
      "|    critic_loss     | 1.5e+09   |\n",
      "|    ent_coef        | 80.5      |\n",
      "|    ent_coef_loss   | 1.13      |\n",
      "|    learning_rate   | 0.00328   |\n",
      "|    n_updates       | 1032136   |\n",
      "|    reward_est_loss | -1.38e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1035000, episode_reward=4554.11 +/- 23.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.55e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1035000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.17e+05 |\n",
      "|    critic_loss     | 1.56e+09  |\n",
      "|    ent_coef        | 85        |\n",
      "|    ent_coef_loss   | 0.515     |\n",
      "|    learning_rate   | 0.00328   |\n",
      "|    n_updates       | 1034998   |\n",
      "|    reward_est_loss | -2.59e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1596      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 44115     |\n",
      "|    total_timesteps | 1036138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.07e+05 |\n",
      "|    critic_loss     | 2.92e+09  |\n",
      "|    ent_coef        | 85.8      |\n",
      "|    ent_coef_loss   | -2.42     |\n",
      "|    learning_rate   | 0.00327   |\n",
      "|    n_updates       | 1036136   |\n",
      "|    reward_est_loss | -2.57e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1040000, episode_reward=4775.75 +/- 19.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1040000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.09e+05 |\n",
      "|    critic_loss     | 2.01e+09  |\n",
      "|    ent_coef        | 87.2      |\n",
      "|    ent_coef_loss   | -0.418    |\n",
      "|    learning_rate   | 0.00327   |\n",
      "|    n_updates       | 1039998   |\n",
      "|    reward_est_loss | -2.43e+04 |\n",
      "----------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 4.8e+03  |\n",
      "| time/              |          |\n",
      "|    episodes        | 1600     |\n",
      "|    fps             | 23       |\n",
      "|    time_elapsed    | 44281    |\n",
      "|    total_timesteps | 1040138  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.1e+05 |\n",
      "|    critic_loss     | 2.1e+09  |\n",
      "|    ent_coef        | 85.3     |\n",
      "|    ent_coef_loss   | 2.01     |\n",
      "|    learning_rate   | 0.00327  |\n",
      "|    n_updates       | 1040136  |\n",
      "|    reward_est_loss | -2.4e+04 |\n",
      "---------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1604      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 44446     |\n",
      "|    total_timesteps | 1044138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.21e+05 |\n",
      "|    critic_loss     | 1.76e+09  |\n",
      "|    ent_coef        | 83.6      |\n",
      "|    ent_coef_loss   | -0.534    |\n",
      "|    learning_rate   | 0.00326   |\n",
      "|    n_updates       | 1044136   |\n",
      "|    reward_est_loss | -2.21e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1045000, episode_reward=4954.36 +/- 29.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.95e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1045000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.22e+05 |\n",
      "|    critic_loss     | 1.44e+09  |\n",
      "|    ent_coef        | 88.6      |\n",
      "|    ent_coef_loss   | 3.92      |\n",
      "|    learning_rate   | 0.00326   |\n",
      "|    n_updates       | 1044998   |\n",
      "|    reward_est_loss | -1.15e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1608      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 44612     |\n",
      "|    total_timesteps | 1048138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.21e+05 |\n",
      "|    critic_loss     | 1.85e+09  |\n",
      "|    ent_coef        | 80.1      |\n",
      "|    ent_coef_loss   | 2.43      |\n",
      "|    learning_rate   | 0.00325   |\n",
      "|    n_updates       | 1048136   |\n",
      "|    reward_est_loss | -1.76e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1050000, episode_reward=4797.84 +/- 14.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1050000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.23e+05 |\n",
      "|    critic_loss     | 2.21e+09  |\n",
      "|    ent_coef        | 85.1      |\n",
      "|    ent_coef_loss   | 1.12      |\n",
      "|    learning_rate   | 0.00325   |\n",
      "|    n_updates       | 1049998   |\n",
      "|    reward_est_loss | -2.18e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1612      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 44778     |\n",
      "|    total_timesteps | 1052138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.2e+05  |\n",
      "|    critic_loss     | 1.24e+09  |\n",
      "|    ent_coef        | 81.7      |\n",
      "|    ent_coef_loss   | -1.63     |\n",
      "|    learning_rate   | 0.00325   |\n",
      "|    n_updates       | 1052136   |\n",
      "|    reward_est_loss | -1.99e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1055000, episode_reward=4580.29 +/- 17.53\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.58e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1055000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.14e+05 |\n",
      "|    critic_loss     | 1.55e+09  |\n",
      "|    ent_coef        | 83        |\n",
      "|    ent_coef_loss   | 2.22      |\n",
      "|    learning_rate   | 0.00324   |\n",
      "|    n_updates       | 1054998   |\n",
      "|    reward_est_loss | -1.88e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1616      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 44945     |\n",
      "|    total_timesteps | 1056138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.21e+05 |\n",
      "|    critic_loss     | 1.76e+09  |\n",
      "|    ent_coef        | 80        |\n",
      "|    ent_coef_loss   | -0.322    |\n",
      "|    learning_rate   | 0.00324   |\n",
      "|    n_updates       | 1056136   |\n",
      "|    reward_est_loss | -2.63e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1060000, episode_reward=4694.69 +/- 17.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.69e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1060000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.16e+05 |\n",
      "|    critic_loss     | 2.89e+09  |\n",
      "|    ent_coef        | 83.3      |\n",
      "|    ent_coef_loss   | -0.359    |\n",
      "|    learning_rate   | 0.00323   |\n",
      "|    n_updates       | 1059998   |\n",
      "|    reward_est_loss | -6.09e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1620      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 45111     |\n",
      "|    total_timesteps | 1060138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.26e+05 |\n",
      "|    critic_loss     | 2.15e+09  |\n",
      "|    ent_coef        | 81.9      |\n",
      "|    ent_coef_loss   | 0.0841    |\n",
      "|    learning_rate   | 0.00323   |\n",
      "|    n_updates       | 1060136   |\n",
      "|    reward_est_loss | -2.49e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1624      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 45276     |\n",
      "|    total_timesteps | 1064138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.23e+05 |\n",
      "|    critic_loss     | 1.66e+09  |\n",
      "|    ent_coef        | 79.7      |\n",
      "|    ent_coef_loss   | -0.218    |\n",
      "|    learning_rate   | 0.00323   |\n",
      "|    n_updates       | 1064136   |\n",
      "|    reward_est_loss | -1.47e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1065000, episode_reward=4801.83 +/- 11.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1065000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.3e+05  |\n",
      "|    critic_loss     | 1.94e+09  |\n",
      "|    ent_coef        | 79.4      |\n",
      "|    ent_coef_loss   | -0.115    |\n",
      "|    learning_rate   | 0.00323   |\n",
      "|    n_updates       | 1064998   |\n",
      "|    reward_est_loss | -2.64e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1628      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 45442     |\n",
      "|    total_timesteps | 1068138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.31e+05 |\n",
      "|    critic_loss     | 1.21e+09  |\n",
      "|    ent_coef        | 74.9      |\n",
      "|    ent_coef_loss   | -0.845    |\n",
      "|    learning_rate   | 0.00322   |\n",
      "|    n_updates       | 1068136   |\n",
      "|    reward_est_loss | -2.64e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1070000, episode_reward=4894.28 +/- 28.91\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.89e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1070000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.36e+05 |\n",
      "|    critic_loss     | 2.1e+09   |\n",
      "|    ent_coef        | 82.4      |\n",
      "|    ent_coef_loss   | 1.64      |\n",
      "|    learning_rate   | 0.00322   |\n",
      "|    n_updates       | 1069998   |\n",
      "|    reward_est_loss | -2.65e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1632      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 45609     |\n",
      "|    total_timesteps | 1072138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.32e+05 |\n",
      "|    critic_loss     | 1.84e+09  |\n",
      "|    ent_coef        | 76.3      |\n",
      "|    ent_coef_loss   | -1.37     |\n",
      "|    learning_rate   | 0.00321   |\n",
      "|    n_updates       | 1072136   |\n",
      "|    reward_est_loss | -2.56e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1075000, episode_reward=4804.28 +/- 81.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1075000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.28e+05 |\n",
      "|    critic_loss     | 1.5e+09   |\n",
      "|    ent_coef        | 79.2      |\n",
      "|    ent_coef_loss   | 1.67      |\n",
      "|    learning_rate   | 0.00321   |\n",
      "|    n_updates       | 1074998   |\n",
      "|    reward_est_loss | -2.44e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1636      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 45775     |\n",
      "|    total_timesteps | 1076138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.32e+05 |\n",
      "|    critic_loss     | 2.37e+09  |\n",
      "|    ent_coef        | 78.8      |\n",
      "|    ent_coef_loss   | 1.7       |\n",
      "|    learning_rate   | 0.00321   |\n",
      "|    n_updates       | 1076136   |\n",
      "|    reward_est_loss | -2.64e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1080000, episode_reward=4790.64 +/- 9.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1080000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.29e+05 |\n",
      "|    critic_loss     | 1.81e+09  |\n",
      "|    ent_coef        | 77.1      |\n",
      "|    ent_coef_loss   | -1.88     |\n",
      "|    learning_rate   | 0.0032    |\n",
      "|    n_updates       | 1079998   |\n",
      "|    reward_est_loss | -2.66e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1640      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 45941     |\n",
      "|    total_timesteps | 1080138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.32e+05 |\n",
      "|    critic_loss     | 1.29e+09  |\n",
      "|    ent_coef        | 72.9      |\n",
      "|    ent_coef_loss   | -0.116    |\n",
      "|    learning_rate   | 0.0032    |\n",
      "|    n_updates       | 1080136   |\n",
      "|    reward_est_loss | -7.91e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1644      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 46106     |\n",
      "|    total_timesteps | 1084138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.43e+05 |\n",
      "|    critic_loss     | 1.04e+09  |\n",
      "|    ent_coef        | 78.7      |\n",
      "|    ent_coef_loss   | 0.203     |\n",
      "|    learning_rate   | 0.00319   |\n",
      "|    n_updates       | 1084136   |\n",
      "|    reward_est_loss | -2.4e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1085000, episode_reward=4808.18 +/- 12.23\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.81e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1085000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.37e+05 |\n",
      "|    critic_loss     | 1.41e+09  |\n",
      "|    ent_coef        | 77.5      |\n",
      "|    ent_coef_loss   | 0.55      |\n",
      "|    learning_rate   | 0.00319   |\n",
      "|    n_updates       | 1084998   |\n",
      "|    reward_est_loss | -2.48e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1648      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 46273     |\n",
      "|    total_timesteps | 1088138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.32e+05 |\n",
      "|    critic_loss     | 1.17e+09  |\n",
      "|    ent_coef        | 77.4      |\n",
      "|    ent_coef_loss   | -2.58     |\n",
      "|    learning_rate   | 0.00319   |\n",
      "|    n_updates       | 1088136   |\n",
      "|    reward_est_loss | -2.53e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1090000, episode_reward=4700.85 +/- 14.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.7e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1090000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.41e+05 |\n",
      "|    critic_loss     | 1.65e+09  |\n",
      "|    ent_coef        | 77.2      |\n",
      "|    ent_coef_loss   | -2.63     |\n",
      "|    learning_rate   | 0.00318   |\n",
      "|    n_updates       | 1089998   |\n",
      "|    reward_est_loss | -2.44e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.81e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1652      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 46439     |\n",
      "|    total_timesteps | 1092138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.42e+05 |\n",
      "|    critic_loss     | 1.33e+09  |\n",
      "|    ent_coef        | 79.8      |\n",
      "|    ent_coef_loss   | 1.46      |\n",
      "|    learning_rate   | 0.00318   |\n",
      "|    n_updates       | 1092136   |\n",
      "|    reward_est_loss | -2.6e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1095000, episode_reward=4539.98 +/- 100.41\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.54e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1095000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.37e+05 |\n",
      "|    critic_loss     | 1.97e+09  |\n",
      "|    ent_coef        | 78.7      |\n",
      "|    ent_coef_loss   | -0.406    |\n",
      "|    learning_rate   | 0.00318   |\n",
      "|    n_updates       | 1094998   |\n",
      "|    reward_est_loss | -1.15e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.8e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1656      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 46605     |\n",
      "|    total_timesteps | 1096138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.51e+05 |\n",
      "|    critic_loss     | 2.57e+09  |\n",
      "|    ent_coef        | 81.9      |\n",
      "|    ent_coef_loss   | -0.818    |\n",
      "|    learning_rate   | 0.00317   |\n",
      "|    n_updates       | 1096136   |\n",
      "|    reward_est_loss | -2.47e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1100000, episode_reward=4822.40 +/- 72.80\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.82e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1100000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.56e+05 |\n",
      "|    critic_loss     | 1.12e+09  |\n",
      "|    ent_coef        | 77.7      |\n",
      "|    ent_coef_loss   | 1.66      |\n",
      "|    learning_rate   | 0.00317   |\n",
      "|    n_updates       | 1099998   |\n",
      "|    reward_est_loss | -2.55e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1660      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 46771     |\n",
      "|    total_timesteps | 1100138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.47e+05 |\n",
      "|    critic_loss     | 1.08e+09  |\n",
      "|    ent_coef        | 78.5      |\n",
      "|    ent_coef_loss   | -0.333    |\n",
      "|    learning_rate   | 0.00317   |\n",
      "|    n_updates       | 1100136   |\n",
      "|    reward_est_loss | -2.6e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1664      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 46936     |\n",
      "|    total_timesteps | 1104138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.58e+05 |\n",
      "|    critic_loss     | 1.41e+09  |\n",
      "|    ent_coef        | 75.9      |\n",
      "|    ent_coef_loss   | 2.1       |\n",
      "|    learning_rate   | 0.00316   |\n",
      "|    n_updates       | 1104136   |\n",
      "|    reward_est_loss | -2.67e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1105000, episode_reward=4751.00 +/- 26.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.75e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1105000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.45e+05 |\n",
      "|    critic_loss     | 2.51e+09  |\n",
      "|    ent_coef        | 75.6      |\n",
      "|    ent_coef_loss   | 1.51      |\n",
      "|    learning_rate   | 0.00316   |\n",
      "|    n_updates       | 1104998   |\n",
      "|    reward_est_loss | -2.08e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1668      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 47103     |\n",
      "|    total_timesteps | 1108138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.58e+05 |\n",
      "|    critic_loss     | 9.04e+08  |\n",
      "|    ent_coef        | 73.1      |\n",
      "|    ent_coef_loss   | -1        |\n",
      "|    learning_rate   | 0.00315   |\n",
      "|    n_updates       | 1108136   |\n",
      "|    reward_est_loss | -2.65e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1110000, episode_reward=4611.96 +/- 52.07\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.61e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1110000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.47e+05 |\n",
      "|    critic_loss     | 1.45e+09  |\n",
      "|    ent_coef        | 72.7      |\n",
      "|    ent_coef_loss   | -1.4      |\n",
      "|    learning_rate   | 0.00315   |\n",
      "|    n_updates       | 1109998   |\n",
      "|    reward_est_loss | -2.28e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1672      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 47269     |\n",
      "|    total_timesteps | 1112138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.56e+05 |\n",
      "|    critic_loss     | 1.35e+09  |\n",
      "|    ent_coef        | 75.4      |\n",
      "|    ent_coef_loss   | -1.57     |\n",
      "|    learning_rate   | 0.00315   |\n",
      "|    n_updates       | 1112136   |\n",
      "|    reward_est_loss | -2.56e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1115000, episode_reward=4491.75 +/- 85.77\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.49e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1115000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.61e+05 |\n",
      "|    critic_loss     | 1.15e+09  |\n",
      "|    ent_coef        | 76.9      |\n",
      "|    ent_coef_loss   | -1.04     |\n",
      "|    learning_rate   | 0.00314   |\n",
      "|    n_updates       | 1114998   |\n",
      "|    reward_est_loss | -2.65e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.79e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1676      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 47435     |\n",
      "|    total_timesteps | 1116138   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.6e+05  |\n",
      "|    critic_loss     | 1.96e+09  |\n",
      "|    ent_coef        | 72.5      |\n",
      "|    ent_coef_loss   | -1.99     |\n",
      "|    learning_rate   | 0.00314   |\n",
      "|    n_updates       | 1116136   |\n",
      "|    reward_est_loss | -2.15e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 974       |\n",
      "|    ep_rew_mean     | 4.65e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1680      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 47494     |\n",
      "|    total_timesteps | 1117563   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.64e+05 |\n",
      "|    critic_loss     | 1.37e+09  |\n",
      "|    ent_coef        | 81.4      |\n",
      "|    ent_coef_loss   | 0.511     |\n",
      "|    learning_rate   | 0.00314   |\n",
      "|    n_updates       | 1117561   |\n",
      "|    reward_est_loss | -2.69e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 941       |\n",
      "|    ep_rew_mean     | 4.46e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1684      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 47521     |\n",
      "|    total_timesteps | 1118196   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.65e+05 |\n",
      "|    critic_loss     | 1.73e+09  |\n",
      "|    ent_coef        | 77.5      |\n",
      "|    ent_coef_loss   | 0.0297    |\n",
      "|    learning_rate   | 0.00314   |\n",
      "|    n_updates       | 1118194   |\n",
      "|    reward_est_loss | -2.71e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 908       |\n",
      "|    ep_rew_mean     | 4.28e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1688      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 47550     |\n",
      "|    total_timesteps | 1118919   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.69e+05 |\n",
      "|    critic_loss     | 1.56e+09  |\n",
      "|    ent_coef        | 85.1      |\n",
      "|    ent_coef_loss   | -1.15     |\n",
      "|    learning_rate   | 0.00314   |\n",
      "|    n_updates       | 1118917   |\n",
      "|    reward_est_loss | -2.29e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1120000, episode_reward=4419.91 +/- 53.43\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.42e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1120000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.7e+05  |\n",
      "|    critic_loss     | 1.8e+09   |\n",
      "|    ent_coef        | 79.2      |\n",
      "|    ent_coef_loss   | -0.78     |\n",
      "|    learning_rate   | 0.00313   |\n",
      "|    n_updates       | 1119998   |\n",
      "|    reward_est_loss | -2.31e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 908       |\n",
      "|    ep_rew_mean     | 4.27e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1692      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 47717     |\n",
      "|    total_timesteps | 1122919   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.69e+05 |\n",
      "|    critic_loss     | 9.97e+08  |\n",
      "|    ent_coef        | 79.3      |\n",
      "|    ent_coef_loss   | -0.886    |\n",
      "|    learning_rate   | 0.00313   |\n",
      "|    n_updates       | 1122917   |\n",
      "|    reward_est_loss | -2.67e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1125000, episode_reward=4640.46 +/- 51.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.64e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1125000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.71e+05 |\n",
      "|    critic_loss     | 1.42e+09  |\n",
      "|    ent_coef        | 77.5      |\n",
      "|    ent_coef_loss   | -0.145    |\n",
      "|    learning_rate   | 0.00313   |\n",
      "|    n_updates       | 1124998   |\n",
      "|    reward_est_loss | -1.43e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 908       |\n",
      "|    ep_rew_mean     | 4.25e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1696      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 47883     |\n",
      "|    total_timesteps | 1126919   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.71e+05 |\n",
      "|    critic_loss     | 2.02e+09  |\n",
      "|    ent_coef        | 80.5      |\n",
      "|    ent_coef_loss   | -1.07     |\n",
      "|    learning_rate   | 0.00312   |\n",
      "|    n_updates       | 1126917   |\n",
      "|    reward_est_loss | -2.48e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1130000, episode_reward=4196.52 +/- 47.85\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.2e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1130000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.77e+05 |\n",
      "|    critic_loss     | 2.11e+09  |\n",
      "|    ent_coef        | 78.1      |\n",
      "|    ent_coef_loss   | 0.662     |\n",
      "|    learning_rate   | 0.00312   |\n",
      "|    n_updates       | 1129998   |\n",
      "|    reward_est_loss | -2.68e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 908       |\n",
      "|    ep_rew_mean     | 4.24e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1700      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 48050     |\n",
      "|    total_timesteps | 1130919   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.83e+05 |\n",
      "|    critic_loss     | 1.29e+09  |\n",
      "|    ent_coef        | 77.1      |\n",
      "|    ent_coef_loss   | -0.999    |\n",
      "|    learning_rate   | 0.00312   |\n",
      "|    n_updates       | 1130917   |\n",
      "|    reward_est_loss | -2.47e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 908       |\n",
      "|    ep_rew_mean     | 4.22e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1704      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 48215     |\n",
      "|    total_timesteps | 1134919   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.79e+05 |\n",
      "|    critic_loss     | 3.22e+09  |\n",
      "|    ent_coef        | 83        |\n",
      "|    ent_coef_loss   | -0.633    |\n",
      "|    learning_rate   | 0.00311   |\n",
      "|    n_updates       | 1134917   |\n",
      "|    reward_est_loss | -2.73e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1135000, episode_reward=4571.92 +/- 31.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.57e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1135000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.76e+05 |\n",
      "|    critic_loss     | 2.26e+09  |\n",
      "|    ent_coef        | 75.3      |\n",
      "|    ent_coef_loss   | -3.77     |\n",
      "|    learning_rate   | 0.00311   |\n",
      "|    n_updates       | 1134998   |\n",
      "|    reward_est_loss | -2.71e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 908       |\n",
      "|    ep_rew_mean     | 4.22e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1708      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 48381     |\n",
      "|    total_timesteps | 1138919   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.85e+05 |\n",
      "|    critic_loss     | 8.66e+08  |\n",
      "|    ent_coef        | 76.7      |\n",
      "|    ent_coef_loss   | 0.266     |\n",
      "|    learning_rate   | 0.0031    |\n",
      "|    n_updates       | 1138917   |\n",
      "|    reward_est_loss | -2.74e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1140000, episode_reward=4471.56 +/- 153.09\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.47e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1140000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.91e+05 |\n",
      "|    critic_loss     | 1.77e+09  |\n",
      "|    ent_coef        | 77.6      |\n",
      "|    ent_coef_loss   | -2.23     |\n",
      "|    learning_rate   | 0.0031    |\n",
      "|    n_updates       | 1139998   |\n",
      "|    reward_est_loss | -2.29e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 908       |\n",
      "|    ep_rew_mean     | 4.2e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1712      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 48548     |\n",
      "|    total_timesteps | 1142919   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.82e+05 |\n",
      "|    critic_loss     | 2.56e+09  |\n",
      "|    ent_coef        | 78.5      |\n",
      "|    ent_coef_loss   | -1.33     |\n",
      "|    learning_rate   | 0.0031    |\n",
      "|    n_updates       | 1142917   |\n",
      "|    reward_est_loss | -2.65e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1145000, episode_reward=1039.82 +/- 0.37\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 1.04e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1145000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.89e+05 |\n",
      "|    critic_loss     | 1.94e+09  |\n",
      "|    ent_coef        | 77.7      |\n",
      "|    ent_coef_loss   | -0.779    |\n",
      "|    learning_rate   | 0.00309   |\n",
      "|    n_updates       | 1144998   |\n",
      "|    reward_est_loss | -1.25e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 908       |\n",
      "|    ep_rew_mean     | 4.18e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1716      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 48714     |\n",
      "|    total_timesteps | 1146919   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.93e+05 |\n",
      "|    critic_loss     | 1.38e+09  |\n",
      "|    ent_coef        | 72.8      |\n",
      "|    ent_coef_loss   | 0.896     |\n",
      "|    learning_rate   | 0.00309   |\n",
      "|    n_updates       | 1146917   |\n",
      "|    reward_est_loss | -2.7e+04  |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1150000, episode_reward=1003.28 +/- 0.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 1e+03     |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1150000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.96e+05 |\n",
      "|    critic_loss     | 1.53e+09  |\n",
      "|    ent_coef        | 77.4      |\n",
      "|    ent_coef_loss   | -0.38     |\n",
      "|    learning_rate   | 0.00308   |\n",
      "|    n_updates       | 1149998   |\n",
      "|    reward_est_loss | -2.06e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 901       |\n",
      "|    ep_rew_mean     | 4.14e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1720      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 48853     |\n",
      "|    total_timesteps | 1150246   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.96e+05 |\n",
      "|    critic_loss     | 1.05e+09  |\n",
      "|    ent_coef        | 77.1      |\n",
      "|    ent_coef_loss   | 0.796     |\n",
      "|    learning_rate   | 0.00308   |\n",
      "|    n_updates       | 1150244   |\n",
      "|    reward_est_loss | -2.76e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 897       |\n",
      "|    ep_rew_mean     | 4.05e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1724      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 49000     |\n",
      "|    total_timesteps | 1153798   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.06e+05 |\n",
      "|    critic_loss     | 1.01e+09  |\n",
      "|    ent_coef        | 75.7      |\n",
      "|    ent_coef_loss   | 2.86      |\n",
      "|    learning_rate   | 0.00308   |\n",
      "|    n_updates       | 1153796   |\n",
      "|    reward_est_loss | -2.41e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1155000, episode_reward=997.21 +/- 0.62\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 997       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1155000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.05e+05 |\n",
      "|    critic_loss     | 1.47e+09  |\n",
      "|    ent_coef        | 75.5      |\n",
      "|    ent_coef_loss   | 0.608     |\n",
      "|    learning_rate   | 0.00308   |\n",
      "|    n_updates       | 1154998   |\n",
      "|    reward_est_loss | -2.62e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 886       |\n",
      "|    ep_rew_mean     | 3.89e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1728      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 49123     |\n",
      "|    total_timesteps | 1156746   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.96e+05 |\n",
      "|    critic_loss     | 2.75e+09  |\n",
      "|    ent_coef        | 72.1      |\n",
      "|    ent_coef_loss   | -1.57     |\n",
      "|    learning_rate   | 0.00307   |\n",
      "|    n_updates       | 1156744   |\n",
      "|    reward_est_loss | -2.73e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 869       |\n",
      "|    ep_rew_mean     | 3.74e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1732      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 49217     |\n",
      "|    total_timesteps | 1159014   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.01e+05 |\n",
      "|    critic_loss     | 1.1e+09   |\n",
      "|    ent_coef        | 78.8      |\n",
      "|    ent_coef_loss   | 0.538     |\n",
      "|    learning_rate   | 0.00307   |\n",
      "|    n_updates       | 1159012   |\n",
      "|    reward_est_loss | -2.76e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1160000, episode_reward=500.23 +/- 290.65\n",
      "Episode length: 551.00 +/- 245.08\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 551       |\n",
      "|    mean_reward     | 500       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1160000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -6.99e+05 |\n",
      "|    critic_loss     | 1.1e+09   |\n",
      "|    ent_coef        | 74.8      |\n",
      "|    ent_coef_loss   | 0.107     |\n",
      "|    learning_rate   | 0.00307   |\n",
      "|    n_updates       | 1159998   |\n",
      "|    reward_est_loss | -2.65e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 855       |\n",
      "|    ep_rew_mean     | 3.57e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1736      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 49324     |\n",
      "|    total_timesteps | 1161607   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.09e+05 |\n",
      "|    critic_loss     | 1.18e+09  |\n",
      "|    ent_coef        | 74.8      |\n",
      "|    ent_coef_loss   | 2.39      |\n",
      "|    learning_rate   | 0.00306   |\n",
      "|    n_updates       | 1161605   |\n",
      "|    reward_est_loss | -2.59e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 846       |\n",
      "|    ep_rew_mean     | 3.43e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1740      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 49453     |\n",
      "|    total_timesteps | 1164726   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.06e+05 |\n",
      "|    critic_loss     | 1.08e+09  |\n",
      "|    ent_coef        | 78.1      |\n",
      "|    ent_coef_loss   | 1.75      |\n",
      "|    learning_rate   | 0.00306   |\n",
      "|    n_updates       | 1164724   |\n",
      "|    reward_est_loss | -2.73e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1165000, episode_reward=4357.51 +/- 110.74\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.36e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1165000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.01e+05 |\n",
      "|    critic_loss     | 1.51e+09  |\n",
      "|    ent_coef        | 72.3      |\n",
      "|    ent_coef_loss   | 0.115     |\n",
      "|    learning_rate   | 0.00306   |\n",
      "|    n_updates       | 1164998   |\n",
      "|    reward_est_loss | -2.56e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 834       |\n",
      "|    ep_rew_mean     | 3.3e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1744      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 49571     |\n",
      "|    total_timesteps | 1167560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.04e+05 |\n",
      "|    critic_loss     | 1.26e+09  |\n",
      "|    ent_coef        | 74.3      |\n",
      "|    ent_coef_loss   | 0.263     |\n",
      "|    learning_rate   | 0.00305   |\n",
      "|    n_updates       | 1167558   |\n",
      "|    reward_est_loss | -2.23e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1170000, episode_reward=4596.19 +/- 53.54\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.6e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1170000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.15e+05 |\n",
      "|    critic_loss     | 1.19e+09  |\n",
      "|    ent_coef        | 74.5      |\n",
      "|    ent_coef_loss   | 0.489     |\n",
      "|    learning_rate   | 0.00305   |\n",
      "|    n_updates       | 1169998   |\n",
      "|    reward_est_loss | -2.39e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 834       |\n",
      "|    ep_rew_mean     | 3.29e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1748      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 49738     |\n",
      "|    total_timesteps | 1171560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.17e+05 |\n",
      "|    critic_loss     | 1.22e+09  |\n",
      "|    ent_coef        | 75.4      |\n",
      "|    ent_coef_loss   | -0.935    |\n",
      "|    learning_rate   | 0.00305   |\n",
      "|    n_updates       | 1171558   |\n",
      "|    reward_est_loss | -1.82e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1175000, episode_reward=4391.07 +/- 59.36\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.39e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1175000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.21e+05 |\n",
      "|    critic_loss     | 1.16e+09  |\n",
      "|    ent_coef        | 78.4      |\n",
      "|    ent_coef_loss   | -2.44     |\n",
      "|    learning_rate   | 0.00304   |\n",
      "|    n_updates       | 1174998   |\n",
      "|    reward_est_loss | -2.64e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 834       |\n",
      "|    ep_rew_mean     | 3.27e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1752      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 49904     |\n",
      "|    total_timesteps | 1175560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.15e+05 |\n",
      "|    critic_loss     | 2.19e+09  |\n",
      "|    ent_coef        | 77.5      |\n",
      "|    ent_coef_loss   | 1.79      |\n",
      "|    learning_rate   | 0.00304   |\n",
      "|    n_updates       | 1175558   |\n",
      "|    reward_est_loss | -2.67e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 834       |\n",
      "|    ep_rew_mean     | 3.26e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1756      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 50069     |\n",
      "|    total_timesteps | 1179560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.24e+05 |\n",
      "|    critic_loss     | 1.66e+09  |\n",
      "|    ent_coef        | 75.9      |\n",
      "|    ent_coef_loss   | 1.97      |\n",
      "|    learning_rate   | 0.00303   |\n",
      "|    n_updates       | 1179558   |\n",
      "|    reward_est_loss | -2.84e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1180000, episode_reward=4410.82 +/- 54.08\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.41e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1180000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.25e+05 |\n",
      "|    critic_loss     | 1.03e+09  |\n",
      "|    ent_coef        | 75.6      |\n",
      "|    ent_coef_loss   | -1.2      |\n",
      "|    learning_rate   | 0.00303   |\n",
      "|    n_updates       | 1179998   |\n",
      "|    reward_est_loss | -2.72e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 834       |\n",
      "|    ep_rew_mean     | 3.26e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1760      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 50236     |\n",
      "|    total_timesteps | 1183560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.36e+05 |\n",
      "|    critic_loss     | 8.88e+08  |\n",
      "|    ent_coef        | 79.4      |\n",
      "|    ent_coef_loss   | -2.38     |\n",
      "|    learning_rate   | 0.00303   |\n",
      "|    n_updates       | 1183558   |\n",
      "|    reward_est_loss | -4.83e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1185000, episode_reward=4314.72 +/- 29.40\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.31e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1185000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.37e+05 |\n",
      "|    critic_loss     | 9.54e+08  |\n",
      "|    ent_coef        | 78.5      |\n",
      "|    ent_coef_loss   | -1.77     |\n",
      "|    learning_rate   | 0.00303   |\n",
      "|    n_updates       | 1184998   |\n",
      "|    reward_est_loss | -1.51e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 834       |\n",
      "|    ep_rew_mean     | 3.25e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1764      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 50402     |\n",
      "|    total_timesteps | 1187560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.34e+05 |\n",
      "|    critic_loss     | 1.88e+09  |\n",
      "|    ent_coef        | 75.7      |\n",
      "|    ent_coef_loss   | 1.48      |\n",
      "|    learning_rate   | 0.00302   |\n",
      "|    n_updates       | 1187558   |\n",
      "|    reward_est_loss | -5.32e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1190000, episode_reward=4696.56 +/- 37.51\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.7e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1190000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.37e+05 |\n",
      "|    critic_loss     | 2.48e+09  |\n",
      "|    ent_coef        | 73.9      |\n",
      "|    ent_coef_loss   | -1.2      |\n",
      "|    learning_rate   | 0.00302   |\n",
      "|    n_updates       | 1189998   |\n",
      "|    reward_est_loss | -1.88e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 834       |\n",
      "|    ep_rew_mean     | 3.23e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1768      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 50569     |\n",
      "|    total_timesteps | 1191560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.38e+05 |\n",
      "|    critic_loss     | 1.11e+09  |\n",
      "|    ent_coef        | 71.8      |\n",
      "|    ent_coef_loss   | -0.0435   |\n",
      "|    learning_rate   | 0.00301   |\n",
      "|    n_updates       | 1191558   |\n",
      "|    reward_est_loss | -2.85e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1195000, episode_reward=4680.82 +/- 44.60\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.68e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1195000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.47e+05 |\n",
      "|    critic_loss     | 1.79e+09  |\n",
      "|    ent_coef        | 78.4      |\n",
      "|    ent_coef_loss   | 1.15      |\n",
      "|    learning_rate   | 0.00301   |\n",
      "|    n_updates       | 1194998   |\n",
      "|    reward_est_loss | 1.36e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 834       |\n",
      "|    ep_rew_mean     | 3.22e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1772      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 50735     |\n",
      "|    total_timesteps | 1195560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.48e+05 |\n",
      "|    critic_loss     | 1.5e+09   |\n",
      "|    ent_coef        | 76        |\n",
      "|    ent_coef_loss   | 0.714     |\n",
      "|    learning_rate   | 0.00301   |\n",
      "|    n_updates       | 1195558   |\n",
      "|    reward_est_loss | -2.83e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 834       |\n",
      "|    ep_rew_mean     | 3.2e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1776      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 50899     |\n",
      "|    total_timesteps | 1199560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.54e+05 |\n",
      "|    critic_loss     | 1.62e+09  |\n",
      "|    ent_coef        | 73.1      |\n",
      "|    ent_coef_loss   | 1.99      |\n",
      "|    learning_rate   | 0.003     |\n",
      "|    n_updates       | 1199558   |\n",
      "|    reward_est_loss | -2.85e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1200000, episode_reward=4339.64 +/- 30.28\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 1e+03    |\n",
      "|    mean_reward     | 4.34e+03 |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1200000  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.6e+05 |\n",
      "|    critic_loss     | 1.87e+09 |\n",
      "|    ent_coef        | 75.8     |\n",
      "|    ent_coef_loss   | 0.156    |\n",
      "|    learning_rate   | 0.003    |\n",
      "|    n_updates       | 1199998  |\n",
      "|    reward_est_loss | -2.8e+04 |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 860      |\n",
      "|    ep_rew_mean     | 3.33e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 1780     |\n",
      "|    fps             | 23       |\n",
      "|    time_elapsed    | 51066    |\n",
      "|    total_timesteps | 1203560  |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -7.6e+05 |\n",
      "|    critic_loss     | 1.82e+09 |\n",
      "|    ent_coef        | 73.9     |\n",
      "|    ent_coef_loss   | -1.16    |\n",
      "|    learning_rate   | 0.00299  |\n",
      "|    n_updates       | 1203558  |\n",
      "|    reward_est_loss | 6.21e+03 |\n",
      "---------------------------------\n",
      "Eval num_timesteps=1205000, episode_reward=4033.05 +/- 46.88\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.03e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1205000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.65e+05 |\n",
      "|    critic_loss     | 7.77e+08  |\n",
      "|    ent_coef        | 72.2      |\n",
      "|    ent_coef_loss   | -0.289    |\n",
      "|    learning_rate   | 0.00299   |\n",
      "|    n_updates       | 1204998   |\n",
      "|    reward_est_loss | -2.81e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 894       |\n",
      "|    ep_rew_mean     | 3.49e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1784      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 51232     |\n",
      "|    total_timesteps | 1207560   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.64e+05 |\n",
      "|    critic_loss     | 1.43e+09  |\n",
      "|    ent_coef        | 76.7      |\n",
      "|    ent_coef_loss   | 1.01      |\n",
      "|    learning_rate   | 0.00299   |\n",
      "|    n_updates       | 1207558   |\n",
      "|    reward_est_loss | -2.57e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1210000, episode_reward=3875.89 +/- 1542.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 3.88e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1210000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.67e+05 |\n",
      "|    critic_loss     | 1.32e+09  |\n",
      "|    ent_coef        | 80.3      |\n",
      "|    ent_coef_loss   | 0.425     |\n",
      "|    learning_rate   | 0.00298   |\n",
      "|    n_updates       | 1209998   |\n",
      "|    reward_est_loss | -2.87e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 921       |\n",
      "|    ep_rew_mean     | 3.62e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1788      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 51374     |\n",
      "|    total_timesteps | 1210971   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.73e+05 |\n",
      "|    critic_loss     | 9.46e+08  |\n",
      "|    ent_coef        | 75.1      |\n",
      "|    ent_coef_loss   | 0.465     |\n",
      "|    learning_rate   | 0.00298   |\n",
      "|    n_updates       | 1210969   |\n",
      "|    reward_est_loss | -3.38e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 906       |\n",
      "|    ep_rew_mean     | 3.48e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1792      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 51481     |\n",
      "|    total_timesteps | 1213559   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.77e+05 |\n",
      "|    critic_loss     | 2.05e+09  |\n",
      "|    ent_coef        | 77.2      |\n",
      "|    ent_coef_loss   | 1.28      |\n",
      "|    learning_rate   | 0.00298   |\n",
      "|    n_updates       | 1213557   |\n",
      "|    reward_est_loss | -2.76e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1215000, episode_reward=1382.84 +/- 1335.08\n",
      "Episode length: 685.00 +/- 385.81\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 685       |\n",
      "|    mean_reward     | 1.38e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1215000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.75e+05 |\n",
      "|    critic_loss     | 1.04e+09  |\n",
      "|    ent_coef        | 76.5      |\n",
      "|    ent_coef_loss   | 0.749     |\n",
      "|    learning_rate   | 0.00298   |\n",
      "|    n_updates       | 1214998   |\n",
      "|    reward_est_loss | -2.88e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 906       |\n",
      "|    ep_rew_mean     | 3.34e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1796      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 51647     |\n",
      "|    total_timesteps | 1217559   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.83e+05 |\n",
      "|    critic_loss     | 9.35e+08  |\n",
      "|    ent_coef        | 82.7      |\n",
      "|    ent_coef_loss   | 0.0666    |\n",
      "|    learning_rate   | 0.00297   |\n",
      "|    n_updates       | 1217557   |\n",
      "|    reward_est_loss | -7.94e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1220000, episode_reward=996.20 +/- 1.11\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 996       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1220000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.84e+05 |\n",
      "|    critic_loss     | 1.71e+09  |\n",
      "|    ent_coef        | 83.7      |\n",
      "|    ent_coef_loss   | -2.04     |\n",
      "|    learning_rate   | 0.00297   |\n",
      "|    n_updates       | 1219998   |\n",
      "|    reward_est_loss | -2.02e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 906       |\n",
      "|    ep_rew_mean     | 3.2e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1800      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 51813     |\n",
      "|    total_timesteps | 1221559   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.86e+05 |\n",
      "|    critic_loss     | 1.55e+09  |\n",
      "|    ent_coef        | 77.1      |\n",
      "|    ent_coef_loss   | -0.0181   |\n",
      "|    learning_rate   | 0.00296   |\n",
      "|    n_updates       | 1221557   |\n",
      "|    reward_est_loss | -2.91e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1225000, episode_reward=1000.96 +/- 1.06\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 1e+03     |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1225000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.95e+05 |\n",
      "|    critic_loss     | 1.64e+09  |\n",
      "|    ent_coef        | 80.1      |\n",
      "|    ent_coef_loss   | 1.19      |\n",
      "|    learning_rate   | 0.00296   |\n",
      "|    n_updates       | 1224998   |\n",
      "|    reward_est_loss | -2e+04    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 906       |\n",
      "|    ep_rew_mean     | 3.06e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1804      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 51979     |\n",
      "|    total_timesteps | 1225559   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.86e+05 |\n",
      "|    critic_loss     | 2.11e+09  |\n",
      "|    ent_coef        | 71.4      |\n",
      "|    ent_coef_loss   | -0.693    |\n",
      "|    learning_rate   | 0.00296   |\n",
      "|    n_updates       | 1225557   |\n",
      "|    reward_est_loss | -2.74e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 906       |\n",
      "|    ep_rew_mean     | 2.95e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1808      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 52144     |\n",
      "|    total_timesteps | 1229559   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8e+05    |\n",
      "|    critic_loss     | 1.34e+09  |\n",
      "|    ent_coef        | 73        |\n",
      "|    ent_coef_loss   | -2.57     |\n",
      "|    learning_rate   | 0.00295   |\n",
      "|    n_updates       | 1229557   |\n",
      "|    reward_est_loss | -2.72e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1230000, episode_reward=4913.12 +/- 28.56\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.91e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1230000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -7.94e+05 |\n",
      "|    critic_loss     | 3.57e+09  |\n",
      "|    ent_coef        | 74        |\n",
      "|    ent_coef_loss   | 0.816     |\n",
      "|    learning_rate   | 0.00295   |\n",
      "|    n_updates       | 1229998   |\n",
      "|    reward_est_loss | -1.98e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 906       |\n",
      "|    ep_rew_mean     | 2.93e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1812      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 52310     |\n",
      "|    total_timesteps | 1233559   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8e+05    |\n",
      "|    critic_loss     | 2.1e+09   |\n",
      "|    ent_coef        | 73.5      |\n",
      "|    ent_coef_loss   | 1.2       |\n",
      "|    learning_rate   | 0.00294   |\n",
      "|    n_updates       | 1233557   |\n",
      "|    reward_est_loss | -2.78e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1235000, episode_reward=4776.49 +/- 39.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.78e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1235000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.08e+05 |\n",
      "|    critic_loss     | 1.75e+09  |\n",
      "|    ent_coef        | 81.6      |\n",
      "|    ent_coef_loss   | 0.762     |\n",
      "|    learning_rate   | 0.00294   |\n",
      "|    n_updates       | 1234998   |\n",
      "|    reward_est_loss | -2.7e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 906       |\n",
      "|    ep_rew_mean     | 2.94e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1816      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 52476     |\n",
      "|    total_timesteps | 1237559   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.09e+05 |\n",
      "|    critic_loss     | 1.28e+09  |\n",
      "|    ent_coef        | 83.9      |\n",
      "|    ent_coef_loss   | -0.0168   |\n",
      "|    learning_rate   | 0.00294   |\n",
      "|    n_updates       | 1237557   |\n",
      "|    reward_est_loss | -2.95e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1240000, episode_reward=4562.51 +/- 32.27\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.56e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1240000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.15e+05 |\n",
      "|    critic_loss     | 1.49e+09  |\n",
      "|    ent_coef        | 81.3      |\n",
      "|    ent_coef_loss   | 1.02      |\n",
      "|    learning_rate   | 0.00293   |\n",
      "|    n_updates       | 1239998   |\n",
      "|    reward_est_loss | -2.8e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 905       |\n",
      "|    ep_rew_mean     | 2.94e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1820      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 52608     |\n",
      "|    total_timesteps | 1240784   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.22e+05 |\n",
      "|    critic_loss     | 9.27e+08  |\n",
      "|    ent_coef        | 81.1      |\n",
      "|    ent_coef_loss   | -1.05     |\n",
      "|    learning_rate   | 0.00293   |\n",
      "|    n_updates       | 1240782   |\n",
      "|    reward_est_loss | -2.92e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 894       |\n",
      "|    ep_rew_mean     | 2.92e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1824      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 52704     |\n",
      "|    total_timesteps | 1243155   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.24e+05 |\n",
      "|    critic_loss     | 1.65e+09  |\n",
      "|    ent_coef        | 83.2      |\n",
      "|    ent_coef_loss   | 3.48      |\n",
      "|    learning_rate   | 0.00293   |\n",
      "|    n_updates       | 1243153   |\n",
      "|    reward_est_loss | -1.88e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1245000, episode_reward=4429.85 +/- 17.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1245000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.31e+05 |\n",
      "|    critic_loss     | 1.25e+09  |\n",
      "|    ent_coef        | 80.5      |\n",
      "|    ent_coef_loss   | -0.867    |\n",
      "|    learning_rate   | 0.00293   |\n",
      "|    n_updates       | 1244998   |\n",
      "|    reward_est_loss | -2.59e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 904       |\n",
      "|    ep_rew_mean     | 3.07e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1828      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 52867     |\n",
      "|    total_timesteps | 1247155   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.34e+05 |\n",
      "|    critic_loss     | 1.51e+09  |\n",
      "|    ent_coef        | 78.6      |\n",
      "|    ent_coef_loss   | 2.4       |\n",
      "|    learning_rate   | 0.00292   |\n",
      "|    n_updates       | 1247153   |\n",
      "|    reward_est_loss | -2.51e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=4522.56 +/- 56.19\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.52e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1250000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.37e+05 |\n",
      "|    critic_loss     | 4.67e+09  |\n",
      "|    ent_coef        | 75.5      |\n",
      "|    ent_coef_loss   | 0.661     |\n",
      "|    learning_rate   | 0.00292   |\n",
      "|    n_updates       | 1249998   |\n",
      "|    reward_est_loss | -2.41e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 918       |\n",
      "|    ep_rew_mean     | 3.19e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1832      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 53016     |\n",
      "|    total_timesteps | 1250793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.33e+05 |\n",
      "|    critic_loss     | 9.41e+08  |\n",
      "|    ent_coef        | 79.6      |\n",
      "|    ent_coef_loss   | -1.16     |\n",
      "|    learning_rate   | 0.00292   |\n",
      "|    n_updates       | 1250791   |\n",
      "|    reward_est_loss | -2.98e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 932       |\n",
      "|    ep_rew_mean     | 3.35e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1836      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 53178     |\n",
      "|    total_timesteps | 1254793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.38e+05 |\n",
      "|    critic_loss     | 1.27e+09  |\n",
      "|    ent_coef        | 74.1      |\n",
      "|    ent_coef_loss   | 1.53      |\n",
      "|    learning_rate   | 0.00291   |\n",
      "|    n_updates       | 1254791   |\n",
      "|    reward_est_loss | -2.92e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1255000, episode_reward=4401.14 +/- 31.81\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.4e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1255000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.38e+05 |\n",
      "|    critic_loss     | 1.24e+09  |\n",
      "|    ent_coef        | 75.2      |\n",
      "|    ent_coef_loss   | 1.48      |\n",
      "|    learning_rate   | 0.00291   |\n",
      "|    n_updates       | 1254998   |\n",
      "|    reward_est_loss | -2.64e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 941       |\n",
      "|    ep_rew_mean     | 3.47e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1840      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 53341     |\n",
      "|    total_timesteps | 1258793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.43e+05 |\n",
      "|    critic_loss     | 1.49e+09  |\n",
      "|    ent_coef        | 75.5      |\n",
      "|    ent_coef_loss   | -1.86     |\n",
      "|    learning_rate   | 0.0029    |\n",
      "|    n_updates       | 1258791   |\n",
      "|    reward_est_loss | -2.13e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1260000, episode_reward=4205.87 +/- 19.49\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.21e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1260000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.43e+05 |\n",
      "|    critic_loss     | 1.6e+09   |\n",
      "|    ent_coef        | 74.7      |\n",
      "|    ent_coef_loss   | 0.0907    |\n",
      "|    learning_rate   | 0.0029    |\n",
      "|    n_updates       | 1259998   |\n",
      "|    reward_est_loss | -2.44e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.58e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1844      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 53504     |\n",
      "|    total_timesteps | 1262793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.4e+05  |\n",
      "|    critic_loss     | 1.78e+09  |\n",
      "|    ent_coef        | 76.4      |\n",
      "|    ent_coef_loss   | 0.578     |\n",
      "|    learning_rate   | 0.0029    |\n",
      "|    n_updates       | 1262791   |\n",
      "|    reward_est_loss | -2.97e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1265000, episode_reward=4559.28 +/- 8.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.56e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1265000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.45e+05 |\n",
      "|    critic_loss     | 1.02e+09  |\n",
      "|    ent_coef        | 70.6      |\n",
      "|    ent_coef_loss   | 0.192     |\n",
      "|    learning_rate   | 0.00289   |\n",
      "|    n_updates       | 1264998   |\n",
      "|    reward_est_loss | -3.01e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.59e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1848      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 53667     |\n",
      "|    total_timesteps | 1266793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.34e+05 |\n",
      "|    critic_loss     | 2.24e+09  |\n",
      "|    ent_coef        | 78.8      |\n",
      "|    ent_coef_loss   | 0.247     |\n",
      "|    learning_rate   | 0.00289   |\n",
      "|    n_updates       | 1266791   |\n",
      "|    reward_est_loss | -2.75e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1270000, episode_reward=4634.38 +/- 32.20\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.63e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1270000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.55e+05 |\n",
      "|    critic_loss     | 1.51e+09  |\n",
      "|    ent_coef        | 84.2      |\n",
      "|    ent_coef_loss   | -1.68     |\n",
      "|    learning_rate   | 0.00288   |\n",
      "|    n_updates       | 1269998   |\n",
      "|    reward_est_loss | -2.93e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.6e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1852      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 53830     |\n",
      "|    total_timesteps | 1270793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.5e+05  |\n",
      "|    critic_loss     | 7.36e+08  |\n",
      "|    ent_coef        | 76        |\n",
      "|    ent_coef_loss   | -0.132    |\n",
      "|    learning_rate   | 0.00288   |\n",
      "|    n_updates       | 1270791   |\n",
      "|    reward_est_loss | -1.74e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.61e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1856      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 53992     |\n",
      "|    total_timesteps | 1274793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.63e+05 |\n",
      "|    critic_loss     | 5.31e+09  |\n",
      "|    ent_coef        | 76.4      |\n",
      "|    ent_coef_loss   | 0.186     |\n",
      "|    learning_rate   | 0.00288   |\n",
      "|    n_updates       | 1274791   |\n",
      "|    reward_est_loss | -2.65e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1275000, episode_reward=4607.66 +/- 7.73\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.61e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1275000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.68e+05 |\n",
      "|    critic_loss     | 1.14e+09  |\n",
      "|    ent_coef        | 78.4      |\n",
      "|    ent_coef_loss   | 2.47      |\n",
      "|    learning_rate   | 0.00288   |\n",
      "|    n_updates       | 1274998   |\n",
      "|    reward_est_loss | -1.93e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.61e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1860      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 54154     |\n",
      "|    total_timesteps | 1278793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.74e+05 |\n",
      "|    critic_loss     | 1.02e+09  |\n",
      "|    ent_coef        | 74.4      |\n",
      "|    ent_coef_loss   | -1.52     |\n",
      "|    learning_rate   | 0.00287   |\n",
      "|    n_updates       | 1278791   |\n",
      "|    reward_est_loss | -1.49e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1280000, episode_reward=4512.18 +/- 17.10\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.51e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1280000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.74e+05 |\n",
      "|    critic_loss     | 9.22e+08  |\n",
      "|    ent_coef        | 75.1      |\n",
      "|    ent_coef_loss   | -2.62     |\n",
      "|    learning_rate   | 0.00287   |\n",
      "|    n_updates       | 1279998   |\n",
      "|    reward_est_loss | -2.01e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.62e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1864      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 54317     |\n",
      "|    total_timesteps | 1282793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.72e+05 |\n",
      "|    critic_loss     | 2.98e+09  |\n",
      "|    ent_coef        | 76.1      |\n",
      "|    ent_coef_loss   | -1.12     |\n",
      "|    learning_rate   | 0.00286   |\n",
      "|    n_updates       | 1282791   |\n",
      "|    reward_est_loss | -2.86e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1285000, episode_reward=4567.13 +/- 33.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.57e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1285000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.77e+05 |\n",
      "|    critic_loss     | 1.11e+09  |\n",
      "|    ent_coef        | 75.4      |\n",
      "|    ent_coef_loss   | 0.791     |\n",
      "|    learning_rate   | 0.00286   |\n",
      "|    n_updates       | 1284998   |\n",
      "|    reward_est_loss | -3.04e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.63e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1868      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 54480     |\n",
      "|    total_timesteps | 1286793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.76e+05 |\n",
      "|    critic_loss     | 1.79e+09  |\n",
      "|    ent_coef        | 73        |\n",
      "|    ent_coef_loss   | 1.54      |\n",
      "|    learning_rate   | 0.00286   |\n",
      "|    n_updates       | 1286791   |\n",
      "|    reward_est_loss | -3.02e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1290000, episode_reward=4498.66 +/- 33.02\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.5e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1290000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.8e+05  |\n",
      "|    critic_loss     | 1.41e+09  |\n",
      "|    ent_coef        | 74.1      |\n",
      "|    ent_coef_loss   | 0.331     |\n",
      "|    learning_rate   | 0.00285   |\n",
      "|    n_updates       | 1289998   |\n",
      "|    reward_est_loss | -2.91e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.63e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1872      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 54644     |\n",
      "|    total_timesteps | 1290793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.89e+05 |\n",
      "|    critic_loss     | 1.24e+09  |\n",
      "|    ent_coef        | 75.7      |\n",
      "|    ent_coef_loss   | 0.223     |\n",
      "|    learning_rate   | 0.00285   |\n",
      "|    n_updates       | 1290791   |\n",
      "|    reward_est_loss | -3.04e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.64e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1876      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 54807     |\n",
      "|    total_timesteps | 1294793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.85e+05 |\n",
      "|    critic_loss     | 1.09e+09  |\n",
      "|    ent_coef        | 82        |\n",
      "|    ent_coef_loss   | 0.0241    |\n",
      "|    learning_rate   | 0.00284   |\n",
      "|    n_updates       | 1294791   |\n",
      "|    reward_est_loss | -2.38e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1295000, episode_reward=4645.93 +/- 31.26\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.65e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1295000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.89e+05 |\n",
      "|    critic_loss     | 1.91e+09  |\n",
      "|    ent_coef        | 77.2      |\n",
      "|    ent_coef_loss   | 0.0958    |\n",
      "|    learning_rate   | 0.00284   |\n",
      "|    n_updates       | 1294998   |\n",
      "|    reward_est_loss | -3.06e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.64e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1880      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 54970     |\n",
      "|    total_timesteps | 1298793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.93e+05 |\n",
      "|    critic_loss     | 8.24e+08  |\n",
      "|    ent_coef        | 75        |\n",
      "|    ent_coef_loss   | -0.104    |\n",
      "|    learning_rate   | 0.00284   |\n",
      "|    n_updates       | 1298791   |\n",
      "|    reward_est_loss | -2.73e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1300000, episode_reward=4630.31 +/- 25.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.63e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1300000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -8.92e+05 |\n",
      "|    critic_loss     | 2.02e+09  |\n",
      "|    ent_coef        | 81.2      |\n",
      "|    ent_coef_loss   | 0.369     |\n",
      "|    learning_rate   | 0.00283   |\n",
      "|    n_updates       | 1299998   |\n",
      "|    reward_est_loss | -2.62e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 952       |\n",
      "|    ep_rew_mean     | 3.66e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1884      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 55133     |\n",
      "|    total_timesteps | 1302793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.04e+05 |\n",
      "|    critic_loss     | 1.42e+09  |\n",
      "|    ent_coef        | 83.4      |\n",
      "|    ent_coef_loss   | 1.5       |\n",
      "|    learning_rate   | 0.00283   |\n",
      "|    n_updates       | 1302791   |\n",
      "|    reward_est_loss | -2.83e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1305000, episode_reward=4689.49 +/- 24.42\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.69e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1305000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.01e+05 |\n",
      "|    critic_loss     | 1.15e+09  |\n",
      "|    ent_coef        | 76.3      |\n",
      "|    ent_coef_loss   | 1.01      |\n",
      "|    learning_rate   | 0.00283   |\n",
      "|    n_updates       | 1304998   |\n",
      "|    reward_est_loss | -2.51e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 958       |\n",
      "|    ep_rew_mean     | 3.71e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1888      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 55298     |\n",
      "|    total_timesteps | 1306793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.09e+05 |\n",
      "|    critic_loss     | 9.31e+08  |\n",
      "|    ent_coef        | 74.2      |\n",
      "|    ent_coef_loss   | 0.649     |\n",
      "|    learning_rate   | 0.00282   |\n",
      "|    n_updates       | 1306791   |\n",
      "|    reward_est_loss | -2.82e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1310000, episode_reward=4586.90 +/- 26.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.59e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1310000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.07e+05 |\n",
      "|    critic_loss     | 1.12e+09  |\n",
      "|    ent_coef        | 72.3      |\n",
      "|    ent_coef_loss   | -1.41     |\n",
      "|    learning_rate   | 0.00282   |\n",
      "|    n_updates       | 1309998   |\n",
      "|    reward_est_loss | -3.03e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 972       |\n",
      "|    ep_rew_mean     | 3.85e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1892      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 55466     |\n",
      "|    total_timesteps | 1310793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.22e+05 |\n",
      "|    critic_loss     | 7.68e+08  |\n",
      "|    ent_coef        | 73.4      |\n",
      "|    ent_coef_loss   | -0.988    |\n",
      "|    learning_rate   | 0.00282   |\n",
      "|    n_updates       | 1310791   |\n",
      "|    reward_est_loss | -2.96e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 972       |\n",
      "|    ep_rew_mean     | 4e+03     |\n",
      "| time/              |           |\n",
      "|    episodes        | 1896      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 55632     |\n",
      "|    total_timesteps | 1314793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.17e+05 |\n",
      "|    critic_loss     | 1.3e+09   |\n",
      "|    ent_coef        | 70        |\n",
      "|    ent_coef_loss   | -2.14     |\n",
      "|    learning_rate   | 0.00281   |\n",
      "|    n_updates       | 1314791   |\n",
      "|    reward_est_loss | -3.07e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1315000, episode_reward=4851.63 +/- 33.92\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.85e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1315000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.21e+05 |\n",
      "|    critic_loss     | 1.21e+09  |\n",
      "|    ent_coef        | 71.9      |\n",
      "|    ent_coef_loss   | 0.487     |\n",
      "|    learning_rate   | 0.00281   |\n",
      "|    n_updates       | 1314998   |\n",
      "|    reward_est_loss | -3.09e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 972       |\n",
      "|    ep_rew_mean     | 4.14e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1900      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 55798     |\n",
      "|    total_timesteps | 1318793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.18e+05 |\n",
      "|    critic_loss     | 1.61e+09  |\n",
      "|    ent_coef        | 78.5      |\n",
      "|    ent_coef_loss   | 2.56      |\n",
      "|    learning_rate   | 0.0028    |\n",
      "|    n_updates       | 1318791   |\n",
      "|    reward_est_loss | -2.46e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1320000, episode_reward=4721.26 +/- 25.17\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.72e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1320000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.25e+05 |\n",
      "|    critic_loss     | 1.05e+09  |\n",
      "|    ent_coef        | 76.8      |\n",
      "|    ent_coef_loss   | -1.16     |\n",
      "|    learning_rate   | 0.0028    |\n",
      "|    n_updates       | 1319998   |\n",
      "|    reward_est_loss | -8.66e+03 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 972       |\n",
      "|    ep_rew_mean     | 4.29e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1904      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 55964     |\n",
      "|    total_timesteps | 1322793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.34e+05 |\n",
      "|    critic_loss     | 1.06e+09  |\n",
      "|    ent_coef        | 76.8      |\n",
      "|    ent_coef_loss   | 2.52      |\n",
      "|    learning_rate   | 0.0028    |\n",
      "|    n_updates       | 1322791   |\n",
      "|    reward_est_loss | -2.29e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1325000, episode_reward=4715.47 +/- 36.44\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.72e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1325000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.35e+05 |\n",
      "|    critic_loss     | 4.13e+09  |\n",
      "|    ent_coef        | 80        |\n",
      "|    ent_coef_loss   | 1.5       |\n",
      "|    learning_rate   | 0.00279   |\n",
      "|    n_updates       | 1324998   |\n",
      "|    reward_est_loss | -1.96e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 972       |\n",
      "|    ep_rew_mean     | 4.4e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1908      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 56130     |\n",
      "|    total_timesteps | 1326793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.33e+05 |\n",
      "|    critic_loss     | 7.42e+08  |\n",
      "|    ent_coef        | 70.9      |\n",
      "|    ent_coef_loss   | 0.932     |\n",
      "|    learning_rate   | 0.00279   |\n",
      "|    n_updates       | 1326791   |\n",
      "|    reward_est_loss | -1.98e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1330000, episode_reward=4651.37 +/- 15.34\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.65e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1330000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.45e+05 |\n",
      "|    critic_loss     | 8.4e+08   |\n",
      "|    ent_coef        | 68        |\n",
      "|    ent_coef_loss   | 0.0473    |\n",
      "|    learning_rate   | 0.00278   |\n",
      "|    n_updates       | 1329998   |\n",
      "|    reward_est_loss | -2.76e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 972       |\n",
      "|    ep_rew_mean     | 4.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1912      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 56296     |\n",
      "|    total_timesteps | 1330793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.39e+05 |\n",
      "|    critic_loss     | 1.41e+09  |\n",
      "|    ent_coef        | 73.3      |\n",
      "|    ent_coef_loss   | -0.841    |\n",
      "|    learning_rate   | 0.00278   |\n",
      "|    n_updates       | 1330791   |\n",
      "|    reward_est_loss | -2.62e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 972       |\n",
      "|    ep_rew_mean     | 4.44e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1916      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 56461     |\n",
      "|    total_timesteps | 1334793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.53e+05 |\n",
      "|    critic_loss     | 1.88e+09  |\n",
      "|    ent_coef        | 69.8      |\n",
      "|    ent_coef_loss   | 0.0124    |\n",
      "|    learning_rate   | 0.00278   |\n",
      "|    n_updates       | 1334791   |\n",
      "|    reward_est_loss | -4.07e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1335000, episode_reward=3513.11 +/- 1681.51\n",
      "Episode length: 764.60 +/- 288.31\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 765       |\n",
      "|    mean_reward     | 3.51e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1335000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.47e+05 |\n",
      "|    critic_loss     | 2.14e+09  |\n",
      "|    ent_coef        | 75.2      |\n",
      "|    ent_coef_loss   | -0.0347   |\n",
      "|    learning_rate   | 0.00278   |\n",
      "|    n_updates       | 1334998   |\n",
      "|    reward_est_loss | -1.76e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 980       |\n",
      "|    ep_rew_mean     | 4.49e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1920      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 56627     |\n",
      "|    total_timesteps | 1338793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.56e+05 |\n",
      "|    critic_loss     | 1.03e+09  |\n",
      "|    ent_coef        | 72.9      |\n",
      "|    ent_coef_loss   | 0.113     |\n",
      "|    learning_rate   | 0.00277   |\n",
      "|    n_updates       | 1338791   |\n",
      "|    reward_est_loss | -3.11e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1340000, episode_reward=4657.72 +/- 55.83\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.66e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1340000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.62e+05 |\n",
      "|    critic_loss     | 1.08e+09  |\n",
      "|    ent_coef        | 75.9      |\n",
      "|    ent_coef_loss   | 0.732     |\n",
      "|    learning_rate   | 0.00277   |\n",
      "|    n_updates       | 1339998   |\n",
      "|    reward_est_loss | -3.08e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 996       |\n",
      "|    ep_rew_mean     | 4.59e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1924      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 56794     |\n",
      "|    total_timesteps | 1342793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.7e+05  |\n",
      "|    critic_loss     | 1.78e+09  |\n",
      "|    ent_coef        | 80.4      |\n",
      "|    ent_coef_loss   | 0.76      |\n",
      "|    learning_rate   | 0.00276   |\n",
      "|    n_updates       | 1342791   |\n",
      "|    reward_est_loss | -2.15e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1345000, episode_reward=4392.84 +/- 27.48\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.39e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1345000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.69e+05 |\n",
      "|    critic_loss     | 8.94e+08  |\n",
      "|    ent_coef        | 76.9      |\n",
      "|    ent_coef_loss   | -0.652    |\n",
      "|    learning_rate   | 0.00276   |\n",
      "|    n_updates       | 1344998   |\n",
      "|    reward_est_loss | -2.71e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 996       |\n",
      "|    ep_rew_mean     | 4.59e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1928      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 56960     |\n",
      "|    total_timesteps | 1346793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.69e+05 |\n",
      "|    critic_loss     | 1.05e+09  |\n",
      "|    ent_coef        | 75.3      |\n",
      "|    ent_coef_loss   | 0.691     |\n",
      "|    learning_rate   | 0.00276   |\n",
      "|    n_updates       | 1346791   |\n",
      "|    reward_est_loss | -3.15e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1350000, episode_reward=4222.33 +/- 38.78\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.22e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1350000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.74e+05 |\n",
      "|    critic_loss     | 1.75e+09  |\n",
      "|    ent_coef        | 77.7      |\n",
      "|    ent_coef_loss   | -0.579    |\n",
      "|    learning_rate   | 0.00275   |\n",
      "|    n_updates       | 1349998   |\n",
      "|    reward_est_loss | -3.16e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 1e+03     |\n",
      "|    ep_rew_mean     | 4.6e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1932      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 57126     |\n",
      "|    total_timesteps | 1350793   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.78e+05 |\n",
      "|    critic_loss     | 1.04e+09  |\n",
      "|    ent_coef        | 76.8      |\n",
      "|    ent_coef_loss   | 1.36      |\n",
      "|    learning_rate   | 0.00275   |\n",
      "|    n_updates       | 1350791   |\n",
      "|    reward_est_loss | 9.1e+03   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | 4.57e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1936      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 57266     |\n",
      "|    total_timesteps | 1354191   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.81e+05 |\n",
      "|    critic_loss     | 1.33e+09  |\n",
      "|    ent_coef        | 80.7      |\n",
      "|    ent_coef_loss   | 0.838     |\n",
      "|    learning_rate   | 0.00274   |\n",
      "|    n_updates       | 1354189   |\n",
      "|    reward_est_loss | 604       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1355000, episode_reward=4567.45 +/- 10.82\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.57e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1355000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.78e+05 |\n",
      "|    critic_loss     | 1.42e+09  |\n",
      "|    ent_coef        | 81.5      |\n",
      "|    ent_coef_loss   | -1.1      |\n",
      "|    learning_rate   | 0.00274   |\n",
      "|    n_updates       | 1354998   |\n",
      "|    reward_est_loss | -2.74e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | 4.58e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1940      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 57433     |\n",
      "|    total_timesteps | 1358191   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.89e+05 |\n",
      "|    critic_loss     | 1.39e+09  |\n",
      "|    ent_coef        | 75        |\n",
      "|    ent_coef_loss   | -1.52     |\n",
      "|    learning_rate   | 0.00274   |\n",
      "|    n_updates       | 1358189   |\n",
      "|    reward_est_loss | -3.17e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1360000, episode_reward=4631.11 +/- 20.25\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.63e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1360000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.87e+05 |\n",
      "|    critic_loss     | 1.81e+09  |\n",
      "|    ent_coef        | 75.7      |\n",
      "|    ent_coef_loss   | 0.153     |\n",
      "|    learning_rate   | 0.00273   |\n",
      "|    n_updates       | 1359998   |\n",
      "|    reward_est_loss | -2.89e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | 4.59e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1944      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 57603     |\n",
      "|    total_timesteps | 1362191   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -9.96e+05 |\n",
      "|    critic_loss     | 1.21e+09  |\n",
      "|    ent_coef        | 79.4      |\n",
      "|    ent_coef_loss   | 0.183     |\n",
      "|    learning_rate   | 0.00273   |\n",
      "|    n_updates       | 1362189   |\n",
      "|    reward_est_loss | -2.74e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1365000, episode_reward=4901.87 +/- 4.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.9e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1365000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+06 |\n",
      "|    critic_loss     | 9.43e+08  |\n",
      "|    ent_coef        | 80.4      |\n",
      "|    ent_coef_loss   | -0.97     |\n",
      "|    learning_rate   | 0.00273   |\n",
      "|    n_updates       | 1364998   |\n",
      "|    reward_est_loss | -3.15e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | 4.59e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1948      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 57791     |\n",
      "|    total_timesteps | 1366191   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+06 |\n",
      "|    critic_loss     | 9.9e+08   |\n",
      "|    ent_coef        | 77.6      |\n",
      "|    ent_coef_loss   | -0.151    |\n",
      "|    learning_rate   | 0.00272   |\n",
      "|    n_updates       | 1366189   |\n",
      "|    reward_est_loss | -3.11e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1370000, episode_reward=4772.52 +/- 76.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.77e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1370000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+06 |\n",
      "|    critic_loss     | 1.06e+09  |\n",
      "|    ent_coef        | 78        |\n",
      "|    ent_coef_loss   | 0.852     |\n",
      "|    learning_rate   | 0.00272   |\n",
      "|    n_updates       | 1369998   |\n",
      "|    reward_est_loss | -3.19e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | 4.6e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 1952      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 57962     |\n",
      "|    total_timesteps | 1370191   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.01e+06 |\n",
      "|    critic_loss     | 2.37e+09  |\n",
      "|    ent_coef        | 78.1      |\n",
      "|    ent_coef_loss   | -3.18     |\n",
      "|    learning_rate   | 0.00272   |\n",
      "|    n_updates       | 1370189   |\n",
      "|    reward_est_loss | -3.18e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 994       |\n",
      "|    ep_rew_mean     | 4.62e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1956      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 58133     |\n",
      "|    total_timesteps | 1374191   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+06 |\n",
      "|    critic_loss     | 1.32e+09  |\n",
      "|    ent_coef        | 72.9      |\n",
      "|    ent_coef_loss   | 1.14      |\n",
      "|    learning_rate   | 0.00271   |\n",
      "|    n_updates       | 1374189   |\n",
      "|    reward_est_loss | -7.85e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1375000, episode_reward=790.50 +/- 1622.47\n",
      "Episode length: 261.40 +/- 370.57\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 261       |\n",
      "|    mean_reward     | 791       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1375000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+06 |\n",
      "|    critic_loss     | 1.52e+09  |\n",
      "|    ent_coef        | 75.9      |\n",
      "|    ent_coef_loss   | -1.25     |\n",
      "|    learning_rate   | 0.00271   |\n",
      "|    n_updates       | 1374998   |\n",
      "|    reward_est_loss | -3.1e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 985       |\n",
      "|    ep_rew_mean     | 4.57e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1960      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 58266     |\n",
      "|    total_timesteps | 1377304   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+06 |\n",
      "|    critic_loss     | 1.48e+09  |\n",
      "|    ent_coef        | 73.9      |\n",
      "|    ent_coef_loss   | 2.79      |\n",
      "|    learning_rate   | 0.0027    |\n",
      "|    n_updates       | 1377302   |\n",
      "|    reward_est_loss | -3.01e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 959       |\n",
      "|    ep_rew_mean     | 4.43e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1964      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 58325     |\n",
      "|    total_timesteps | 1378688   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+06 |\n",
      "|    critic_loss     | 8.95e+08  |\n",
      "|    ent_coef        | 76.1      |\n",
      "|    ent_coef_loss   | 0.143     |\n",
      "|    learning_rate   | 0.0027    |\n",
      "|    n_updates       | 1378686   |\n",
      "|    reward_est_loss | -2.95e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1380000, episode_reward=515.71 +/- 438.06\n",
      "Episode length: 616.20 +/- 470.06\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 616       |\n",
      "|    mean_reward     | 516       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1380000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.03e+06 |\n",
      "|    critic_loss     | 1.43e+09  |\n",
      "|    ent_coef        | 77.5      |\n",
      "|    ent_coef_loss   | -4.3      |\n",
      "|    learning_rate   | 0.0027    |\n",
      "|    n_updates       | 1379998   |\n",
      "|    reward_est_loss | -2.65e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 948       |\n",
      "|    ep_rew_mean     | 4.31e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1968      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 58454     |\n",
      "|    total_timesteps | 1381586   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.02e+06 |\n",
      "|    critic_loss     | 1.26e+09  |\n",
      "|    ent_coef        | 74.5      |\n",
      "|    ent_coef_loss   | -0.63     |\n",
      "|    learning_rate   | 0.0027    |\n",
      "|    n_updates       | 1381584   |\n",
      "|    reward_est_loss | -3.09e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1385000, episode_reward=1462.59 +/- 1177.70\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 1.46e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1385000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+06 |\n",
      "|    critic_loss     | 1.02e+09  |\n",
      "|    ent_coef        | 73.7      |\n",
      "|    ent_coef_loss   | 0.11      |\n",
      "|    learning_rate   | 0.00269   |\n",
      "|    n_updates       | 1384998   |\n",
      "|    reward_est_loss | -3.22e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 946       |\n",
      "|    ep_rew_mean     | 4.23e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1972      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 58615     |\n",
      "|    total_timesteps | 1385364   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+06 |\n",
      "|    critic_loss     | 1.06e+09  |\n",
      "|    ent_coef        | 78        |\n",
      "|    ent_coef_loss   | 1.6       |\n",
      "|    learning_rate   | 0.00269   |\n",
      "|    n_updates       | 1385362   |\n",
      "|    reward_est_loss | -3.2e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 928       |\n",
      "|    ep_rew_mean     | 4.06e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1976      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 58712     |\n",
      "|    total_timesteps | 1387638   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+06 |\n",
      "|    critic_loss     | 1.32e+09  |\n",
      "|    ent_coef        | 73.8      |\n",
      "|    ent_coef_loss   | 1.97      |\n",
      "|    learning_rate   | 0.00269   |\n",
      "|    n_updates       | 1387636   |\n",
      "|    reward_est_loss | -3.17e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1390000, episode_reward=3066.86 +/- 1510.17\n",
      "Episode length: 862.80 +/- 274.40\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 863       |\n",
      "|    mean_reward     | 3.07e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1390000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+06 |\n",
      "|    critic_loss     | 1.32e+09  |\n",
      "|    ent_coef        | 73.2      |\n",
      "|    ent_coef_loss   | -2.47     |\n",
      "|    learning_rate   | 0.00268   |\n",
      "|    n_updates       | 1389998   |\n",
      "|    reward_est_loss | -3.04e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 913       |\n",
      "|    ep_rew_mean     | 3.93e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1980      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 58815     |\n",
      "|    total_timesteps | 1390065   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+06 |\n",
      "|    critic_loss     | 1.15e+09  |\n",
      "|    ent_coef        | 82        |\n",
      "|    ent_coef_loss   | 1.03      |\n",
      "|    learning_rate   | 0.00268   |\n",
      "|    n_updates       | 1390063   |\n",
      "|    reward_est_loss | -2.81e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 890       |\n",
      "|    ep_rew_mean     | 3.75e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1984      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 58889     |\n",
      "|    total_timesteps | 1391802   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.04e+06 |\n",
      "|    critic_loss     | 2.55e+09  |\n",
      "|    ent_coef        | 78.8      |\n",
      "|    ent_coef_loss   | -0.439    |\n",
      "|    learning_rate   | 0.00268   |\n",
      "|    n_updates       | 1391800   |\n",
      "|    reward_est_loss | -3.24e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 876       |\n",
      "|    ep_rew_mean     | 3.61e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1988      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 58997     |\n",
      "|    total_timesteps | 1394355   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05e+06 |\n",
      "|    critic_loss     | 2.43e+09  |\n",
      "|    ent_coef        | 79.7      |\n",
      "|    ent_coef_loss   | 0.0277    |\n",
      "|    learning_rate   | 0.00268   |\n",
      "|    n_updates       | 1394353   |\n",
      "|    reward_est_loss | -3.07e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1395000, episode_reward=826.16 +/- 82.38\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 826       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1395000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05e+06 |\n",
      "|    critic_loss     | 5.56e+09  |\n",
      "|    ent_coef        | 81.2      |\n",
      "|    ent_coef_loss   | -0.786    |\n",
      "|    learning_rate   | 0.00268   |\n",
      "|    n_updates       | 1394998   |\n",
      "|    reward_est_loss | -2.71e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 868       |\n",
      "|    ep_rew_mean     | 3.45e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1992      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 59134     |\n",
      "|    total_timesteps | 1397549   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05e+06 |\n",
      "|    critic_loss     | 1.79e+09  |\n",
      "|    ent_coef        | 77.5      |\n",
      "|    ent_coef_loss   | -1.74     |\n",
      "|    learning_rate   | 0.00267   |\n",
      "|    n_updates       | 1397547   |\n",
      "|    reward_est_loss | -2.96e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1400000, episode_reward=925.09 +/- 51.76\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 925       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1400000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.05e+06 |\n",
      "|    critic_loss     | 3.42e+09  |\n",
      "|    ent_coef        | 84.2      |\n",
      "|    ent_coef_loss   | 1.31      |\n",
      "|    learning_rate   | 0.00267   |\n",
      "|    n_updates       | 1399998   |\n",
      "|    reward_est_loss | 2.24e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 868       |\n",
      "|    ep_rew_mean     | 3.33e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 1996      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 59307     |\n",
      "|    total_timesteps | 1401549   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.07e+06 |\n",
      "|    critic_loss     | 1.19e+09  |\n",
      "|    ent_coef        | 85.1      |\n",
      "|    ent_coef_loss   | 0.13      |\n",
      "|    learning_rate   | 0.00266   |\n",
      "|    n_updates       | 1401547   |\n",
      "|    reward_est_loss | -2.53e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1405000, episode_reward=781.32 +/- 308.12\n",
      "Episode length: 848.20 +/- 303.60\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 848       |\n",
      "|    mean_reward     | 781       |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1405000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.06e+06 |\n",
      "|    critic_loss     | 2.83e+09  |\n",
      "|    ent_coef        | 82.1      |\n",
      "|    ent_coef_loss   | 0.0947    |\n",
      "|    learning_rate   | 0.00266   |\n",
      "|    n_updates       | 1404998   |\n",
      "|    reward_est_loss | -2.5e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 868       |\n",
      "|    ep_rew_mean     | 3.19e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2000      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 59482     |\n",
      "|    total_timesteps | 1405549   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.07e+06 |\n",
      "|    critic_loss     | 1.42e+09  |\n",
      "|    ent_coef        | 77.1      |\n",
      "|    ent_coef_loss   | -0.651    |\n",
      "|    learning_rate   | 0.00266   |\n",
      "|    n_updates       | 1405547   |\n",
      "|    reward_est_loss | -3.24e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 860       |\n",
      "|    ep_rew_mean     | 3.08e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2004      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 59624     |\n",
      "|    total_timesteps | 1408832   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.06e+06 |\n",
      "|    critic_loss     | 2.69e+09  |\n",
      "|    ent_coef        | 83.5      |\n",
      "|    ent_coef_loss   | -2.41     |\n",
      "|    learning_rate   | 0.00265   |\n",
      "|    n_updates       | 1408830   |\n",
      "|    reward_est_loss | -3.11e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1410000, episode_reward=3285.01 +/- 1395.55\n",
      "Episode length: 700.40 +/- 247.38\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 700       |\n",
      "|    mean_reward     | 3.29e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1410000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.07e+06 |\n",
      "|    critic_loss     | 1.83e+09  |\n",
      "|    ent_coef        | 86.9      |\n",
      "|    ent_coef_loss   | -0.675    |\n",
      "|    learning_rate   | 0.00265   |\n",
      "|    n_updates       | 1409998   |\n",
      "|    reward_est_loss | -3.3e+04  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 860       |\n",
      "|    ep_rew_mean     | 3.1e+03   |\n",
      "| time/              |           |\n",
      "|    episodes        | 2008      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 59796     |\n",
      "|    total_timesteps | 1412832   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+06 |\n",
      "|    critic_loss     | 1.53e+09  |\n",
      "|    ent_coef        | 89        |\n",
      "|    ent_coef_loss   | 0.726     |\n",
      "|    learning_rate   | 0.00265   |\n",
      "|    n_updates       | 1412830   |\n",
      "|    reward_est_loss | -2.88e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1415000, episode_reward=5041.39 +/- 26.01\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 5.04e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1415000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+06 |\n",
      "|    critic_loss     | 1.41e+09  |\n",
      "|    ent_coef        | 84.2      |\n",
      "|    ent_coef_loss   | 0.637     |\n",
      "|    learning_rate   | 0.00264   |\n",
      "|    n_updates       | 1414998   |\n",
      "|    reward_est_loss | -3.26e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 860       |\n",
      "|    ep_rew_mean     | 3.11e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2012      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 59971     |\n",
      "|    total_timesteps | 1416832   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.09e+06 |\n",
      "|    critic_loss     | 1.1e+09   |\n",
      "|    ent_coef        | 88.4      |\n",
      "|    ent_coef_loss   | 1.01      |\n",
      "|    learning_rate   | 0.00264   |\n",
      "|    n_updates       | 1416830   |\n",
      "|    reward_est_loss | -2.68e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1420000, episode_reward=4096.84 +/- 1540.89\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.1e+03   |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1420000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.09e+06 |\n",
      "|    critic_loss     | 1.52e+09  |\n",
      "|    ent_coef        | 88.2      |\n",
      "|    ent_coef_loss   | -0.635    |\n",
      "|    learning_rate   | 0.00263   |\n",
      "|    n_updates       | 1419998   |\n",
      "|    reward_est_loss | -2.86e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 860       |\n",
      "|    ep_rew_mean     | 3.11e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2016      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 60142     |\n",
      "|    total_timesteps | 1420832   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.08e+06 |\n",
      "|    critic_loss     | 4.82e+09  |\n",
      "|    ent_coef        | 86        |\n",
      "|    ent_coef_loss   | 0.698     |\n",
      "|    learning_rate   | 0.00263   |\n",
      "|    n_updates       | 1420830   |\n",
      "|    reward_est_loss | -1.65e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 860       |\n",
      "|    ep_rew_mean     | 3.12e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2020      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 60318     |\n",
      "|    total_timesteps | 1424832   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.09e+06 |\n",
      "|    critic_loss     | 4.48e+09  |\n",
      "|    ent_coef        | 85.8      |\n",
      "|    ent_coef_loss   | -1.25     |\n",
      "|    learning_rate   | 0.00263   |\n",
      "|    n_updates       | 1424830   |\n",
      "|    reward_est_loss | -3.01e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1425000, episode_reward=4912.04 +/- 21.69\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.91e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1425000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.1e+06  |\n",
      "|    critic_loss     | 1.19e+09  |\n",
      "|    ent_coef        | 84.4      |\n",
      "|    ent_coef_loss   | -2.81     |\n",
      "|    learning_rate   | 0.00263   |\n",
      "|    n_updates       | 1424998   |\n",
      "|    reward_est_loss | -3.26e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 860       |\n",
      "|    ep_rew_mean     | 3.13e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2024      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 60490     |\n",
      "|    total_timesteps | 1428804   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+06 |\n",
      "|    critic_loss     | 2.22e+09  |\n",
      "|    ent_coef        | 82.4      |\n",
      "|    ent_coef_loss   | -1.11     |\n",
      "|    learning_rate   | 0.00262   |\n",
      "|    n_updates       | 1428802   |\n",
      "|    reward_est_loss | -2.74e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1430000, episode_reward=3488.68 +/- 770.75\n",
      "Episode length: 734.40 +/- 134.57\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 734       |\n",
      "|    mean_reward     | 3.49e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1430000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+06 |\n",
      "|    critic_loss     | 1.98e+09  |\n",
      "|    ent_coef        | 89.8      |\n",
      "|    ent_coef_loss   | 1.81      |\n",
      "|    learning_rate   | 0.00262   |\n",
      "|    n_updates       | 1429998   |\n",
      "|    reward_est_loss | -3.11e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 855       |\n",
      "|    ep_rew_mean     | 3.12e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2028      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 60640     |\n",
      "|    total_timesteps | 1432323   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+06 |\n",
      "|    critic_loss     | 1.35e+09  |\n",
      "|    ent_coef        | 84.1      |\n",
      "|    ent_coef_loss   | -0.0831   |\n",
      "|    learning_rate   | 0.00261   |\n",
      "|    n_updates       | 1432321   |\n",
      "|    reward_est_loss | -2.34e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1435000, episode_reward=4969.26 +/- 24.96\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.97e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1435000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.1e+06  |\n",
      "|    critic_loss     | 2.31e+09  |\n",
      "|    ent_coef        | 85.4      |\n",
      "|    ent_coef_loss   | -0.516    |\n",
      "|    learning_rate   | 0.00261   |\n",
      "|    n_updates       | 1434998   |\n",
      "|    reward_est_loss | -2.59e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 852       |\n",
      "|    ep_rew_mean     | 3.12e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2032      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 60801     |\n",
      "|    total_timesteps | 1435945   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.11e+06 |\n",
      "|    critic_loss     | 2.85e+09  |\n",
      "|    ent_coef        | 92        |\n",
      "|    ent_coef_loss   | 1.4       |\n",
      "|    learning_rate   | 0.00261   |\n",
      "|    n_updates       | 1435943   |\n",
      "|    reward_est_loss | -3.34e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 858       |\n",
      "|    ep_rew_mean     | 3.17e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2036      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 60998     |\n",
      "|    total_timesteps | 1439945   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+06 |\n",
      "|    critic_loss     | 2.59e+09  |\n",
      "|    ent_coef        | 81.5      |\n",
      "|    ent_coef_loss   | -0.927    |\n",
      "|    learning_rate   | 0.0026    |\n",
      "|    n_updates       | 1439943   |\n",
      "|    reward_est_loss | -3.29e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1440000, episode_reward=4988.94 +/- 18.99\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.99e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1440000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.12e+06 |\n",
      "|    critic_loss     | 1.6e+09   |\n",
      "|    ent_coef        | 82.5      |\n",
      "|    ent_coef_loss   | 1.46      |\n",
      "|    learning_rate   | 0.0026    |\n",
      "|    n_updates       | 1439998   |\n",
      "|    reward_est_loss | 1.68e+03  |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 858       |\n",
      "|    ep_rew_mean     | 3.18e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2040      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 61172     |\n",
      "|    total_timesteps | 1443945   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+06 |\n",
      "|    critic_loss     | 1.41e+09  |\n",
      "|    ent_coef        | 80.5      |\n",
      "|    ent_coef_loss   | 1.33      |\n",
      "|    learning_rate   | 0.00259   |\n",
      "|    n_updates       | 1443943   |\n",
      "|    reward_est_loss | -8.88e+03 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1445000, episode_reward=4993.33 +/- 27.87\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 4.99e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1445000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+06 |\n",
      "|    critic_loss     | 1.3e+09   |\n",
      "|    ent_coef        | 88        |\n",
      "|    ent_coef_loss   | -0.232    |\n",
      "|    learning_rate   | 0.00259   |\n",
      "|    n_updates       | 1444998   |\n",
      "|    reward_est_loss | -3.32e+04 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 849       |\n",
      "|    ep_rew_mean     | 3.14e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2044      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 61306     |\n",
      "|    total_timesteps | 1447083   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.13e+06 |\n",
      "|    critic_loss     | 2.8e+09   |\n",
      "|    ent_coef        | 87.4      |\n",
      "|    ent_coef_loss   | -2.21     |\n",
      "|    learning_rate   | 0.00259   |\n",
      "|    n_updates       | 1447081   |\n",
      "|    reward_est_loss | -3.34e+04 |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1450000, episode_reward=5054.12 +/- 77.12\n",
      "Episode length: 1000.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 1e+03     |\n",
      "|    mean_reward     | 5.05e+03  |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1450000   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.14e+06 |\n",
      "|    critic_loss     | 3.65e+09  |\n",
      "|    ent_coef        | 91        |\n",
      "|    ent_coef_loss   | 0.313     |\n",
      "|    learning_rate   | 0.00258   |\n",
      "|    n_updates       | 1449998   |\n",
      "|    reward_est_loss | -2.22e+04 |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 849       |\n",
      "|    ep_rew_mean     | 3.15e+03  |\n",
      "| time/              |           |\n",
      "|    episodes        | 2048      |\n",
      "|    fps             | 23        |\n",
      "|    time_elapsed    | 61477     |\n",
      "|    total_timesteps | 1451083   |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.15e+06 |\n",
      "|    critic_loss     | 2.49e+09  |\n",
      "|    ent_coef        | 79.6      |\n",
      "|    ent_coef_loss   | -0.115    |\n",
      "|    learning_rate   | 0.00258   |\n",
      "|    n_updates       | 1451081   |\n",
      "|    reward_est_loss | -2.83e+04 |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m make_vec_env(env_id, n_envs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(eval_env, best_model_save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-iapmd-oct3/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env_id),\n\u001b[1;32m     12\u001b[0m                              log_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-iapmd-oct3/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(env_id), eval_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,\n\u001b[1;32m     13\u001b[0m                              deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, render\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mipmd_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Developer/Inverse-Policy-Mirror-Descent/apmd_off/iapmd.py:372\u001b[0m, in \u001b[0;36mIPMD.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    360\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    361\u001b[0m         total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m         reset_num_timesteps: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    370\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OffPolicyAlgorithm:\n\u001b[0;32m--> 372\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_log_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_log_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/irl/lib/python3.10/site-packages/stable_baselines3/common/off_policy_algorithm.py:366\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 366\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    368\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Developer/Inverse-Policy-Mirror-Descent/apmd_off/iapmd.py:330\u001b[0m, in \u001b[0;36mIPMD.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    328\u001b[0m     reward_est_losses\u001b[38;5;241m.\u001b[39mappend(reward_est_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_est\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 330\u001b[0m     \u001b[43mreward_est_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_est\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# Update target networks\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/irl/lib/python3.10/site-packages/torch/_tensor.py:400\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    393\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    394\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    398\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    399\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 400\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/irl/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from apmd_off.iapmd import IPMD\n",
    "env_id = 'Walker2d-v4'\n",
    "expert_samples_replay_buffer_loc = f\"utils/logs/expert/{env_id}-sac/buffer.pkl\"\n",
    "\n",
    "env = make_vec_env(env_id, n_envs=1)\n",
    "ipmd_model = IPMD(\"MlpPolicy\", env, gamma=1.0, verbose=1,\n",
    "                  batch_size=256, train_freq=1, learning_rate=linear_schedule(5e-3),\n",
    "                  gradient_steps=1, expert_replay_buffer_loc=expert_samples_replay_buffer_loc)\n",
    "\n",
    "eval_env = make_vec_env(env_id, n_envs=1)\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='logs/{}-iapmd-oct3/'.format(env_id),\n",
    "                             log_path='logs/{}-iapmd-oct3/'.format(env_id), eval_freq=5000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "ipmd_model.learn(total_timesteps=3e6, log_interval=4, callback=eval_callback)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d2e891c9d48cbc5657a17ab4ab08b2c1d2ec0060cc3e51694592beb2a6aa825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
